[
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification",
    "start_abstract":"Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "LoRA: Low-Rank Adaptation of Large Language Models"
      ],
      "abstract":[
        "An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Small Models Struggle to Learn from Strong Reasoners",
        "Modeling Arbitrarily Applicable Relational Responding with the\n  Non-Axiomatic Reasoning System: A Machine Psychology Approach",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Empirical Evaluation of the Implicit Hitting Set Approach for Weighted\n  CSPs",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "On Sequential Fault-Intolerant Process Planning",
        "Unveiling the Potential of Text in High-Dimensional Time Series\n  Forecasting",
        "Prognostics and Health Management of Wafer Chemical-Mechanical Polishing\n  System using Autoencoder",
        "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
        "Opus: A Workflow Intention Framework for Complex Workflow Generation",
        "PEA: Enhancing LLM Performance on Computational-Reasoning Tasks",
        "Probing electric field tunable multiband superconductivity in\n  alternating twisted quadralayer graphene",
        "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content",
        "Online Meta-learning for AutoML in Real-time (OnMAR)",
        "Correlated vibration-solvent and Duschinsky effects on optical\n  spectroscopy",
        "Rise of the Community Champions: From Reviewer Crunch to Community Power",
        "On the diagonals of rational functions: the minimal number of variables\n  (unabridged version)",
        "Magnetic moments in the Poynting theorem, Maxwell equations, Dirac\n  equation, and QED",
        "Analysis and Extension of Noisy-target Training for Unsupervised Target\n  Signal Enhancement",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "AI-Driven Solutions for Falcon Disease Classification: Concatenated\n  ConvNeXt cum EfficientNet AI Model Approach",
        "Memory-dependent abstractions of stochastic systems through the lens of\n  transfer operators",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Invariance properties of the solution operator for measure-valued\n  semilinear transport equations",
        "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
        "Extraction of HI gas with bulk motions in the disk of galaxies"
      ],
      "abstract":[
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Arbitrarily Applicable Relational Responding (AARR) is a cornerstone of human\nlanguage and reasoning, referring to the learned ability to relate symbols in\nflexible, context-dependent ways. In this paper, we present a novel theoretical\napproach for modeling AARR within an artificial intelligence framework using\nthe Non-Axiomatic Reasoning System (NARS). NARS is an adaptive reasoning system\ndesigned for learning under uncertainty. By integrating principles from\nRelational Frame Theory - the behavioral psychology account of AARR - with the\nreasoning mechanisms of NARS, we conceptually demonstrate how key properties of\nAARR (mutual entailment, combinatorial entailment, and transformation of\nstimulus functions) can emerge from the inference rules and memory structures\nof NARS. Two theoretical experiments illustrate this approach: one modeling\nstimulus equivalence and transfer of function, and another modeling complex\nrelational networks involving opposition frames. In both cases, the system\nlogically demonstrates the derivation of untrained relations and\ncontext-sensitive transformations of stimulus significance, mirroring\nestablished human cognitive phenomena. These results suggest that AARR - long\nconsidered uniquely human - can be conceptually captured by suitably designed\nAI systems, highlighting the value of integrating behavioral science insights\ninto artificial general intelligence (AGI) research.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "SAT technology has proven to be surprisingly effective in a large variety of\ndomains. However, for the Weighted CSP problem dedicated algorithms have always\nbeen superior. One approach not well-studied so far is the use of SAT in\nconjunction with the Implicit Hitting Set approach. In this work, we explore\nsome alternatives to the existing algorithm of reference. The alternatives,\nmostly borrowed from related boolean frameworks, consider trade-offs for the\ntwo main components of the IHS approach: the computation of low-cost hitting\nvectors, and their transformation into high-cost cores. For each one, we\npropose 4 levels of intensity. Since we also test the usefulness of cost\nfunction merging, our experiments consider 32 different implementations. Our\nempirical study shows that for WCSP it is not easy to identify the best\nalternative. Nevertheless, the cost-function merging encoding and extracting\nmaximal cores seems to be a robust approach.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "We propose and study a planning problem we call Sequential Fault-Intolerant\nProcess Planning (SFIPP). SFIPP captures a reward structure common in many\nsequential multi-stage decision problems where the planning is deemed\nsuccessful only if all stages succeed. Such reward structures are different\nfrom classic additive reward structures and arise in important applications\nsuch as drug\/material discovery, security, and quality-critical product design.\nWe design provably tight online algorithms for settings in which we need to\npick between different actions with unknown success chances at each stage. We\ndo so both for the foundational case in which the behavior of actions is\ndeterministic, and the case of probabilistic action outcomes, where we\neffectively balance exploration for learning and exploitation for planning\nthrough the usage of multi-armed bandit algorithms. In our empirical\nevaluations, we demonstrate that the specialized algorithms we develop, which\nleverage additional information about the structure of the SFIPP instance,\noutperform our more general algorithm.",
        "Time series forecasting has traditionally focused on univariate and\nmultivariate numerical data, often overlooking the benefits of incorporating\nmultimodal information, particularly textual data. In this paper, we propose a\nnovel framework that integrates time series models with Large Language Models\nto improve high-dimensional time series forecasting. Inspired by multimodal\nmodels, our method combines time series and textual data in the dual-tower\nstructure. This fusion of information creates a comprehensive representation,\nwhich is then processed through a linear layer to generate the final forecast.\nExtensive experiments demonstrate that incorporating text enhances\nhigh-dimensional time series forecasting performance. This work paves the way\nfor further research in multimodal time series forecasting.",
        "The Prognostics and Health Management Data Challenge (PHM) 2016 tracks the\nhealth state of components of a semiconductor wafer polishing process. The\nultimate goal is to develop an ability to predict the measurement on the wafer\nsurface wear through monitoring the components health state. This translates to\ncost saving in large scale production. The PHM dataset contains many time\nseries measurements not utilized by traditional physics based approach. On the\nother hand task, applying a data driven approach such as deep learning to the\nPHM dataset is non-trivial. The main issue with supervised deep learning is\nthat class label is not available to the PHM dataset. Second, the feature space\ntrained by an unsupervised deep learner is not specifically targeted at the\npredictive ability or regression. In this work, we propose using the\nautoencoder based clustering whereby the feature space trained is found to be\nmore suitable for performing regression. This is due to having a more compact\ndistribution of samples respective to their nearest cluster means. We justify\nour claims by comparing the performance of our proposed method on the PHM\ndataset with several baselines such as the autoencoder as well as\nstate-of-the-art approaches.",
        "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
        "This paper introduces Workflow Intention, a novel framework for identifying\nand encoding process objectives within complex business environments. Workflow\nIntention is the alignment of Input, Process and Output elements defining a\nWorkflow's transformation objective interpreted from Workflow Signal inside\nBusiness Artefacts. It specifies how Input is processed to achieve desired\nOutput, incorporating quality standards, business rules, compliance\nrequirements and constraints. We adopt an end-to-end Business Artefact Encoder\nand Workflow Signal interpretation methodology involving four steps:\nModality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion\nAttention then Intention Decoding. We provide training procedures and critical\nloss function definitions. In this paper we introduce the concepts of Workflow\nSignal and Workflow Intention, where Workflow Signal decomposed into Input,\nProcess and Output elements is interpreted from Business Artefacts, and\nWorkflow Intention is a complete triple of these elements. We introduce a\nmathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\nFinally, we propose a modular, scalable, trainable, attention-based multimodal\ngenerative system to resolve Workflow Intention from Business Artefacts.",
        "Large Language Models (LLMs) have exhibited remarkable capabilities across\ndiverse domains, prompting investigations into their potential as generic\nreasoning engines. While recent studies have explored inference-time\ncomputation to enhance model performance on complex problems, current research\nlacks a formal framework to characterize the complexity of reasoning tasks.\nThis study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a\nformal approach to describe and solve a class of important reasoning tasks\ntermed computational reasoning problems. The PEA framework decomposes these\nproblems into predicate and enumeration components, using LLMs to synthesize\nprograms based on specified predicates, enumeration, and aggregation rules.\nThese synthesized programs are then executed to obtain solutions to the\ncomputational tasks. We demonstrate the framework's efficacy on benchmark tasks\nincluding Boolean satisfiability problems, game of $24$, and planning problems.\nEmpirical evaluation reveals that PEA substantially enhances the performance of\nunderlying models on benchmark computational problems, yielding an average\naccuracy improvement of approximately $50\\%$, coupled with increased\nefficiency.",
        "Alternating twisted multilayer graphene presents a compelling multiband\nsystem for exploring superconductivity. Here we investigate robust\nsuperconductivity in alternating twisted quadralayer graphene, elucidating\ncarrier contributions from both flat and dispersive bands. The\nsuperconductivity is robust, with a strong electrical field tunability, a\nmaximum BKT transition temperature of 1.6 K, and high critical magnetic fields\nbeyond the Pauli limit. We disentangle the carrier density of Dirac bands and\nflat bands from the Landau fan diagram. Moreover, we could estimate the\nflatband Fermi velocity from the obtained high critical current near half\nfilling when superconductivity is killed at finite magnetic fields, and further\nquantify the superfluid stiffness from the low critical current in the\nsuperconducting regime. Our results exhibit the electric field tunable coupling\nstrength within the superconducting phase, revealing unconventional properties\nwith vanishing Fermi velocity and large superfluid stiffness. These phenomena,\nattributed to substantial quantum metric contributions, offer new insights into\nthe mechanisms underlying unconventional superconductivity in moire systems.",
        "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.",
        "Automated machine learning (AutoML) is a research area focusing on using\noptimisation techniques to design machine learning (ML) algorithms, alleviating\nthe need for a human to perform manual algorithm design. Real-time AutoML\nenables the design process to happen while the ML algorithm is being applied to\na task. Real-time AutoML is an emerging research area, as such existing\nreal-time AutoML techniques need improvement with respect to the quality of\ndesigns and time taken to create designs. To address these issues, this study\nproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.\nMeta-learning gathers information about the optimisation process undertaken by\nthe ML algorithm in the form of meta-features. Meta-features are used in\nconjunction with a meta-learner to optimise the optimisation process. The OnMAR\napproach uses a meta-learner to predict the accuracy of an ML design. If the\naccuracy predicted by the meta-learner is sufficient, the design is used, and\nif the predicted accuracy is low, an optimisation technique creates a new\ndesign. A genetic algorithm (GA) is the optimisation technique used as part of\nthe OnMAR approach. Different meta-learners (k-nearest neighbours, random\nforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. not\nspecific to a single real-time AutoML application) and therefore evaluated on\nthree different real-time AutoML applications, namely: composing an image\nclustering algorithm, configuring the hyper-parameters of a convolutional\nneural network, and configuring a video classification pipeline. The OnMAR\napproach is effective, matching or outperforming existing real-time AutoML\napproaches, with the added benefit of a faster runtime.",
        "Understanding the role of vibrations in optical spectroscopies is essential\nfor the precise interpretation of spectroscopic behavior, especially in systems\nwith complex solvation effects. This workstudies the correlated Duschinsky and\nsolvent effects on the optical spectra using the extended\ndissipaton-equation-of-motion (ext-DEOM) approach, which is an exact and\nnon-Markovian, nonperturbative approach for nonlinear environmental couplings.\nIn the paper, the environment (bath) is composed of the solvent and\nintramolecular vibrational modes whose Duschinsky rotations constitute the\nquardratic couplings to the electronic states. To apply the ext-DEOM, one key\nstep is to obtain the bath coupling descriptors, which is elaborated. As an\naccurate description of solvated molecular systems, the simulating results\ndemonstrate how the above factors affect the position and shape of spectral\nbands.",
        "Academic publishing is facing a crisis driven by exponential growth in\nsubmissions and an overwhelmed peer review system, leading to inconsistent\ndecisions and a severe reviewer shortage. This paper introduces Panvas, a\nplatform that reimagines academic publishing as a continuous, community-driven\nprocess. Panvas addresses these systemic failures with a novel combination of\neconomic incentives (paid reviews) and rich interaction mechanisms\n(multi-dimensional ratings, threaded discussions, and expert-led reviews). By\nmoving beyond the traditional accept\/reject paradigm and integrating paper\nhosting with code\/data repositories and social networking, Panvas fosters a\nmeritocratic environment for scholarly communication and presents a radical\nrethinking of how we evaluate and disseminate scientific knowledge. We present\nthe system design, development roadmap, and a user study plan to evaluate its\neffectiveness.",
        "From some observations on the linear differential operators occurring in the\nLattice Green function of the d-dimensional face centred and simple cubic\nlattices, and on the linear differential operators occurring in the n-particle\ncontributions\n  to the magnetic susceptibility of the square Ising model, we forward some\nconjectures on the diagonals of rational functions. These conjectures are also\nin agreement with exact results we obtain for many Calabi-Yau operators, and\nmany other examples related, or not related to physics.\n  Consider a globally bounded power series which is the diagonal of rational\nfunctions of a certain number of variables, annihilated by an irreducible\nminimal order linear differential operator homomorphic to its adjoint. Among\nthe logarithmic formal series solutions, at the origin, of this operator, call\nn the highest power of the logarithm. We conjecture that this diagonal series\ncan be represented as a diagonal of a rational function with a minimal number\nof variables N_v related to this highest power n by the relation N_v = n +2.\n  Since the operator is homomorphic to its adjoint, its differential Galois\ngroup is symplectic or orthogonal. We also conjecture that the symplectic or\northogonal character of the differential Galois group is related to the parity\nof the highest power n, namely symplectic for n odd and orthogonal for n even.\n  We also sketch the case where the denominator of the rational function is not\nirreducible and is the product of, for instance, two polynomials. The analysis\nof the linear differential operators annihilating the diagonal of rational\nfunction where the denominator is the product of two polynomials, sheds some\nlight on the emergence of such mixture of direct sums and products of factors.\n  The conjecture N_v = n +2 still holds for such reducible linear differential\noperators.",
        "The role of magnetic moments in electrodynamics is examined in this work. The\neffects are described in the context of conventional quantum electrodynamics\nexpressed in terms of the electromagnetic fields or in the context of an\nextended Poynting theorem and extended Maxwell equations. These extensions take\ninto account the energetics of interaction of magnetic moments with\ninhomogeneous magnetic fields. We show how magnetic moment effects are included\nin either version of electrodynamics and that these apparently different\nformulations can give consistent results. In either case, we express the\ninteractions in terms of electromagnetic fields only, avoiding use of a vector\npotential.",
        "Deep neural network-based target signal enhancement (TSE) is usually trained\nin a supervised manner using clean target signals. However, collecting clean\ntarget signals is costly and such signals are not always available. Thus, it is\ndesirable to develop an unsupervised method that does not rely on clean target\nsignals. Among various studies on unsupervised TSE methods, Noisy-target\nTraining (NyTT) has been established as a fundamental method. NyTT simply\nreplaces clean target signals with noisy ones in the typical supervised\ntraining, and it has been experimentally shown to achieve TSE. Despite its\neffectiveness and simplicity, its mechanism and detailed behavior are still\nunclear. In this paper, to advance NyTT and, thus, unsupervised methods as a\nwhole, we analyze NyTT from various perspectives. We experimentally demonstrate\nthe mechanism of NyTT, the desirable conditions, and the effectiveness of\nutilizing noisy signals in situations where a small number of clean target\nsignals are available. Furthermore, we propose an improved version of NyTT\nbased on its properties and explore its capabilities in the dereverberation and\ndeclipping tasks, beyond the denoising task.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Falconry, an ancient practice of training and hunting with falcons,\nemphasizes the need for vigilant health monitoring to ensure the well-being of\nthese highly valued birds, especially during hunting activities. This research\npaper introduces a cutting-edge approach, which leverages the power of\nConcatenated ConvNeXt and EfficientNet AI models for falcon disease\nclassification. Focused on distinguishing 'Normal,' 'Liver,' and\n'Aspergillosis' cases, the study employs a comprehensive dataset for model\ntraining and evaluation, utilizing metrics such as accuracy, precision, recall,\nand f1-score. Through rigorous experimentation and evaluation, we demonstrate\nthe superior performance of the concatenated AI model compared to traditional\nmethods and standalone architectures. This novel approach contributes to\naccurate falcon disease classification, laying the groundwork for further\nadvancements in avian veterinary AI applications.",
        "With the increasing ubiquity of safety-critical autonomous systems operating\nin uncertain environments, there is a need for mathematical methods for formal\nverification of stochastic models. Towards formally verifying properties of\nstochastic systems, methods based on discrete, finite Markov approximations --\nabstractions -- thereof have surged in recent years. These are found in\ncontexts where: either a) one only has partial, discrete observations of the\nunderlying continuous stochastic process, or b) the original system is too\ncomplex to analyze, so one partitions the continuous state-space of the\noriginal system to construct a handleable, finite-state model thereof. In both\ncases, the abstraction is an approximation of the discrete stochastic process\nthat arises precisely from the discretization of the underlying continuous\nprocess. The fact that the abstraction is Markov and the discrete process is\nnot (even though the original one is) leads to approximation errors. Towards\naccounting for non-Markovianity, we introduce memory-dependent abstractions for\nstochastic systems, capturing dynamics with memory effects. Our contribution is\ntwofold. First, we provide a formalism for memory-dependent abstractions based\non transfer operators. Second, we quantify the approximation error by upper\nbounding the total variation distance between the true continuous state\ndistribution and its discrete approximation.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "We provide conditions under which we prove for measure-valued transport\nequations with non-linear reaction term in the space of finite signed Radon\nmeasures, that positivity is preserved, as well as absolute continuity with\nrespect to Lebesgue measure, if the initial condition has that property.\nMoreover, if the initial condition has $L^p$ regular density, then the solution\nhas the same property.",
        "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
        "We propose a new method for extracting bulk motion gases in the disk of a\ngalaxy from HI data cubes, offering improvements over classical techniques like\nmoment analysis and line profile fitting. Our approach decomposes the\nline-of-sight velocity profiles into multiple Gaussian components, which are\nthen classified into (underlying and dominant) bulk and non-bulk motion gases\nbased on criteria such as HI surface density, velocity dispersion, kinetic\nenergy, and rotation velocity. A 2D tilted-ring analysis is employed to refine\nthe kinematical parametres of the galaxy disk, ensuring robust extraction of\nthe bulk motion gases. We demonstrate the effectiveness of this method using\nthe HI data cubes of NGC 4559 from the WSRT-HALOGAS survey, distinguishing\nbetween bulk and non-bulk gas components. From this, we find that approximately\n50% of the HI gas in NGC 4559 is classified as non-bulk, possibly linked to\nprocesses such as stellar feedback. This work provides a robust framework for\nanalysing HI kinematics of galaxies from high sensitivity HI observations of\ngalaxies like MeerKAT-MHONGOOSE and FAST-FEASTS and allows us to best exploit\nthe kinematic information of the complex gas dynamics within galaxy disks."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"LoRA: Low-Rank Adaptation of Large Language Models",
    "start_abstract":"An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification"
      ],
      "abstract":[
        "Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "The effect of environmental factors in biofilm and phage interactions in\n  Agent Based Model",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Multicellular self-organization in Escherichia coli",
        "Structural and Practical Identifiability of Phenomenological Growth\n  Models for Epidemic Forecasting",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "Stochastic Time to Extinction of an SIQS Epidemic Model with Quiescence",
        "Nanostructured thin films of indium oxide nanocrystals confined in\n  alumina matrixes",
        "Steady-state coherence in multipartite quantum systems: its connection\n  with thermodynamic quantities and impact on quantum thermal machines",
        "Solving Superconformal Ward Identities in Mellin Space",
        "Footprint in fitting $B\\to D$ vector form factor and determination for\n  $D$-meson leading-twist LCDA",
        "SU(4) gate design via unitary process tomography: its application to\n  cross-resonance based superconducting quantum devices",
        "Constrained mean-field control with singular control: Existence,\n  stochastic maximum principle and constrained FBSDE",
        "Jet rates in Higgs boson decay at third order in QCD",
        "On (in)consistency of M-estimators under contamination",
        "Score Matching Riemannian Diffusion Means",
        "Lower bound on the radii of circular orbits in the extremal Kerr\n  black-hole spacetime",
        "Cartan Quantum Metrology",
        "GPU Accelerated Image Quality Assessment-Based Software for Transient\n  Detection",
        "Multiaccuracy and Multicalibration via Proxy Groups",
        "Federated Variational Inference for Bayesian Mixture Models",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$"
      ],
      "abstract":[
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "As antibiotic resistance continues to pose a significant threat to public\nhealth, alternative treatments are urgently needed. Phage therapy, which\nutilizes bacteriophages to specifically target bacterial pathogens, has emerged\nas a promising solution. Given that bacteria often exist in biofilms -complex\nmicro-communities that complicate treatment strategies, there is a clear need\nfor models that account for spatial dynamics. This study aims to employ\nmathematical and statistical methodologies to identify optimal treatment\nstrategies involving phage-antibiotic combinations. We developed an agent-based\nmodel to analyze how environmental factors (e.g. temperature, pH, and resource\navailability) influence bacteria-phage interactions during therapy, focusing on\nboth healthy and immunocompromised patients. Utilizing \\textit{Escherichia\ncoli} as a case study, we observed that bacterial cells exhibit mutations that\nenhance their adaptability to varying environmental conditions and treatment\napproaches. Our findings suggest that the effectiveness of therapies targeting\npathogenic and mutated bacterial cells can be significantly improved through\nstrategic control of application timing and dosing. Additionally, we\ninvestigated the impact of biofilm structure on the efficacy of phage therapy,\nunderscoring its importance in developing targeted treatment strategies.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Phenomenological models are highly effective tools for forecasting disease\ndynamics using real world data, particularly in scenarios where detailed\nknowledge of disease mechanisms is limited. However, their reliability depends\non the model parameters' structural and practical identifiability. In this\nstudy, we systematically analyze the identifiability of six commonly used\ngrowth models in epidemiology: the generalized growth model (GGM), the\ngeneralized logistic model (GLM), the Richards model, the generalized Richards\nmodel (GRM), the Gompertz model, and a modified SEIR model with inhomogeneous\nmixing. To address challenges posed by non integer power exponents in these\nmodels, we reformulate them by introducing additional state variables. This\nenables rigorous structural identifiability analysis using the\nStructuralIdentifiability.jl package in JULIA. We validate the structural\nidentifiability results by performing parameter estimation and forecasting\nusing the GrowthPredict MATLAB toolbox. This toolbox is designed to fit and\nforecast time series trajectories based on phenomenological growth models. We\napplied it to three epidemiological datasets: weekly incidence data for\nmonkeypox, COVID 19, and Ebola. Additionally, we assess practical\nidentifiability through Monte Carlo simulations to evaluate parameter\nestimation robustness under varying levels of observational noise. Our results\ndemonstrate the structural and practical identifiability of the models,\nemphasizing how noise affects parameter estimation accuracy. These findings\nprovide valuable insights into the utility and limitations of phenomenological\nmodels for epidemic data analysis, highlighting their adaptability to real\nworld challenges and their role in guiding public health decision making.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
        "Nanocrystals of indium oxide (In$_2$O$_3$) with sizes below 10 nm were\nprepared in alumina matrixes by using a co-pulverization method. The used\nsubstrates such as borosilicate glasses or (100) silicon as well as the\nsubstrate temperatures during the deposition process were modified and their\neffects characterized on the structural and physical properties of\nalumina-In$_2$O$_3$ films. Complementary investigation methods including X-ray\ndiffraction, optical transmittance in the range 250-1100 nm and transmission\nelectron microscopy were used to analyze the nanostructured films. The\ncrystalline order, morphology and optical responses were monitored as function\nof the deposition parameters and the post-synthesis annealing. The optimal\nconditions were found and allow realizing suitable nanostructured films with a\nmajor crystalline order of cubic phase for the In$_2$O$_3$ nanocrystals. The\noptical properties of the films were analyzed and the key parameters such as\ndirect and indirect band gaps were evaluated as function of the synthesis\nconditions and the crystalline quality of the films.",
        "Understanding how coherence of quantum systems affects thermodynamic\nquantities, such as work and heat, is essential for harnessing quantumness\neffectively in thermal quantum technologies. Here, we study the unique\ncontributions of quantum coherence among different subsystems of a multipartite\nsystem, specifically in non-equilibrium steady states, to work and heat\ncurrents. Our system comprises two coupled ensembles, each consisting of $N$\nparticles, interacting with two baths of different temperatures, respectively.\nThe particles in an ensemble interact with their bath either simultaneously or\nsequentially, leading to non-local dissipation and enabling the decomposition\nof work and heat currents into local and non-local components.We find that the\nnon-local heat current, as well as both the local and non-local work\ncurrents,are linked to the system quantum coherence. We provide explicit\nexpressions of coherence-related quantities that determine the work currents\nunder various intrasystem interactions.Our scheme is versatile, capable of\nfunctioning as a refrigerator, an engine, and an accelerator, with its\nperformance being highly sensitive to the configuration settings. These\nfindings establish a connection between thermodynamic quantities and quantum\ncoherence, supplying valuable insights for the design of quantum thermal\nmachines.",
        "We study four-point correlators in superconformal theories in various\ndimensions. We develop an efficient method to solve the superconformal Ward\nidentities in Mellin space. For 4d $\\mathcal{N}=4$ SYM and the 6d\n$\\mathcal{N}=(2,0)$ theory, our method reproduces the known solutions. As novel\napplications of this method, we also derive solutions in 3d $\\mathcal{N} = 8$\nABJM, and in 4d $\\mathcal{N} = 4$ SYM with line defects.",
        "In this paper, we fit the $B\\to D$ vector transition form factor (TFF) by\nusing the data measured by BABAR and Belle Collaborations within Monte Carlo\n(MC) method. Meanwhile, the $B\\to D$ TFF is also calculated by using the QCD\nlight-cone sum rules approach (LCSRs) within right-handed chiral current\ncorrelation function. In which, the $D$-meson leading-twist light-cone\ndistribution amplitude (LCDA) serves as crucial input parameter is\nreconstructed with light-cone harmonic oscillator model where its longitudinal\nbehavior primarily determined by the model-free parameter $B_{2;D}$. After\nmatching the TFF with two scenarios from MC and LCSRs, we have $B_{2;D}=0.17$.\nThen, we present the curve of $D$-meson leading-twist LCDA in comparison with\nother theoretical approaches. Subsequently, the $B\\to D$ TFF $f_{+}^{BD}(q^2)$\nat the large recoil region is $f_{+}^{BD}(0)=0.625^{+0.087}_{-0.113}$, which is\ncompared in detail with theoretical estimates and experimental measurements.\nFurthermore, we calculate the decay width and branching ratio of the\nCabibbo-favored semileptonic decays $B\\to D\\ell \\bar{\\nu}_{\\ell}$, which lead\nto the results $\\mathcal{B}(B^0\\to D^-\\ell ^+\\nu _{\\ell})\n=(1.96_{-0.55}^{+0.51})\\times 10^{-2}$ and $\\mathcal{B}(B^+\\to \\bar{D}^0\\ell\n^+\\nu _{\\ell}) =(2.12_{-0.59}^{+0.55})\\times 10^{-2}$. Finally, we predict the\nCKM matrix element with two scenarios $|V_{cb}|_{\\rm\nSR}=42.97_{-2.57}^{+2.42}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=42.82_{-1.29}^{+1.07}\\times 10^{-3}$ from $B^0\\to D^-\\ell^+\\nu_{\\ell}$,\n$|V_{cb}|_{\\rm SR}=41.93_{-1.05}^{+1.03}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=41.82_{-0.25}^{+0.23}\\times 10^{-3}$ from $B^+\\to\n\\bar{D}^0\\ell^+\\nu_{\\ell}$ which are in good agreement with theoretical and\nexperimental predictions.",
        "We present a novel approach for implementing pulse-efficient SU(4) gates on\ncross resonance (CR)-based superconducting quantum devices. Our method\nintroduces a parameterized unitary derived from the CR-Hamiltonian propagator,\nwhich accounts for static-$ZZ$ interactions. Leveraging the Weyl chamber's\ngeometric structure, we successfully realize a continuous 2-qubit basis gate,\n$R_{ZZ}(\\theta)$, as an echo-free pulse schedule on the IBM Quantum device\nibm_kawasaki. We evaluate the average fidelity and gate time of various SU(4)\ngates generated using the $R_{ZZ}(\\theta)$ to confirm the advantages of our\nimplementation.",
        "This paper studies some mean-field control (MFC) problems with singular\ncontrol under general dynamic state-control-law constraints. We first propose a\ncustomized relaxed control formulation to cope with the dynamic mixed\nconstraints and establish the existence of an optimal control using some\ncompactification arguments in the proper canonical spaces to accommodate the\nsingular control. To characterize the optimal pair of regular and singular\ncontrols, we treat the controlled McKean-Vlasov process as an\ninfinite-dimensional equality constraint and recast the MFC problem as an\noptimization problem on canonical spaces with constraints on Banach space,\nallowing us to derive the stochastic maximum principle (SMP) and a constrained\nBSDE using a novel Lagrange multipliers method. In addition, we further\ninvestigate the uniqueness and the stability result of the solution to the\nconstrained FBSDE associated to the constrained MFC problem with singular\ncontrol.",
        "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
        "We consider robust location-scale estimators under contamination. We show\nthat commonly used robust estimators such as the median and the Huber estimator\nare inconsistent under asymmetric contamination, while the Tukey estimator is\nconsistent. In order to make nuisance parameter free inference based on the\nTukey estimator a consistent scale estimator is required. However, standard\nrobust scale estimators such as the interquartile range and the median absolute\ndeviation are inconsistent under contamination.",
        "Estimating means on Riemannian manifolds is generally computationally\nexpensive because the Riemannian distance function is not known in closed-form\nfor most manifolds. To overcome this, we show that Riemannian diffusion means\ncan be efficiently estimated using score matching with the gradient of Brownian\nmotion transition densities using the same principle as in Riemannian diffusion\nmodels. Empirically, we show that this is more efficient than Monte Carlo\nsimulation while retaining accuracy and is also applicable to learned\nmanifolds. Our method, furthermore, extends to computing the Fr\\'echet mean and\nthe logarithmic map for general Riemannian manifolds. We illustrate the\napplicability of the estimation of diffusion mean by efficiently extending\nEuclidean algorithms to general Riemannian manifolds with a Riemannian\n$k$-means algorithm and maximum likelihood Riemannian regression.",
        "It is often stated in the physics literature that maximally-spinning Kerr\nblack-hole spacetimes are characterized by near-horizon co-rotating circular\ngeodesics of radius $r_{\\text{circular}}$ with the property\n$r_{\\text{circular}}\\to r^+_{\\text{H}}$, where $r_{\\text{H}}$ is the horizon\nradius of the extremal black hole. Based on the famous Thorne hoop conjecture,\nin the present compact paper we provide evidence for the existence of a\nnon-trivial lower bound\n${{r_{\\text{circular}}-r_{\\text{H}}}\\over{r_{\\text{H}}}}\\gtrsim (\\mu\/M)^{1\/2}$\non the radii of circular orbits in the extremal Kerr black-hole spacetime,\nwhere $\\mu\/M$ is the dimensionless mass ratio which characterizes the composed\nblack-hole-orbiting-particle system.",
        "We address the characterization of two-qubit gates, focusing on bounds to\nprecision in the joint estimation of the three parameters that define their\nCartan decomposition. We derive the optimal probe states that jointly maximize\nprecision, minimize sloppiness, and eliminate quantum incompatibility.\nAdditionally, we analyze the properties of the set of optimal probes and\nevaluate their robustness against noise.",
        "Fast imaging localises celestial transients using source finders in the image\ndomain. The need for high computational throughput in this process is driven by\nnext-generation telescopes such as Square Kilometre Array (SKA), which, upon\ncompletion, will be the world's largest aperture synthesis radio telescope. It\nwill collect data at unprecedented velocity and volume. Due to the vast amounts\nof data the SKA will produce, current source finders based on source extraction\nmay be inefficient in a wide-field search. In this paper, we focus on the\nsoftware development of GPU-accelerated transient finders based on Image\nQuality Assessment (IQA) methods -- Low-Information Similarity Index (LISI) and\naugmented LISI (augLISI). We accelerate the algorithms using GPUs, achieving\nkernel time of approximately 0.1 milliseconds for transient finding in\n2048X2048 images.",
        "As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. Unfortunately, measuring and enforcing fairness in real-world\napplications can be challenging due to missing or incomplete sensitive group\ndata. Proxy-sensitive attributes have been proposed as a practical and\neffective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer and more flexible frameworks, such as\nmultiaccuracy and multicalibration, remains unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably be used to derive actionable upper\nbounds on the true multiaccuracy and multicalibration, providing insights into\na model's potential worst-case fairness violations. Additionally, we show that\nadjusting models to satisfy multiaccuracy and multicalibration across\nproxy-sensitive attributes can significantly mitigate these violations for the\ntrue, but unknown, sensitive groups. Through several experiments on real-world\ndatasets, we illustrate that approximate multiaccuracy and multicalibration can\nbe achieved even when sensitive group information is incomplete or unavailable.",
        "We present a federated learning approach for Bayesian model-based clustering\nof large-scale binary and categorical datasets. We introduce a principled\n'divide and conquer' inference procedure using variational inference with local\nmerge and delete moves within batches of the data in parallel, followed by\n'global' merge moves across batches to find global clustering structures. We\nshow that these merge moves require only summaries of the data in each batch,\nenabling federated learning across local nodes without requiring the full\ndataset to be shared. Empirical results on simulated and benchmark datasets\ndemonstrate that our method performs well in comparison to existing clustering\nalgorithms. We validate the practical utility of the method by applying it to\nlarge scale electronic health record (EHR) data.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project",
    "start_abstract":"The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints"
      ],
      "abstract":[
        "An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization",
        "Genetic AI: Evolutionary Simulation for Data Analysis",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Abnormal Mutations: Evolution Strategies Don't Require Gaussianity",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Optimization Landscapes Learned: Proxy Networks Boost Convergence in\n  Physics-based Inverse Problems",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "How do recollimation-induced instabilities shape the propagation of\n  hydrodynamic relativistic jets?",
        "Structural Perturbation in Large Language Model Representations through\n  Recursive Symbolic Regeneration",
        "Understanding and Evaluating Hallucinations in 3D Visual Language Models",
        "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
        "Comparative Analysis of Control Strategies for Position Regulation in DC\n  Servo Motors",
        "ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting",
        "Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal\n  Medical Images",
        "On Choquard-Kirchhoff Type Critical Multiphase Problem",
        "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
        "Primordial Origin of Methane on Eris and Makemake Supported by D\/H\n  Ratios",
        "Quarkonia and Deconfined Quark-Gluon Matter in Heavy-Ion Collisions",
        "Quantum-Inspired Fidelity-based Divergence"
      ],
      "abstract":[
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA\/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications.",
        "We introduce Genetic AI, a novel method for data analysis by evolutionary\nsimulations. The method can be applied to data of any domain and allows for a\ndata-less training of AI models. Without employing predefined rules or training\ndata, Genetic AI first converts the input data into genes and organisms. In a\nsimulation from first principles, these genes and organisms compete for\nfitness, where their behavior is governed by universal evolutionary strategies.\nInvestigating evolutionary stable equilibriums, Genetic AI helps understanding\ncorrelations and symmetries in general input data. Several numerical\nexperiments demonstrate the dynamics of exemplary systems.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "The mutation process in evolution strategies has been interlinked with the\nnormal distribution since its inception. Many lines of reasoning have been\ngiven for this strong dependency, ranging from maximum entropy arguments to the\nneed for isotropy. However, some theoretical results suggest that other\ndistributions might lead to similar local convergence properties. This paper\nempirically shows that a wide range of evolutionary strategies, from the\n(1+1)-ES to CMA-ES, show comparable optimization performance when using a\nmutation distribution other than the standard Gaussian. Replacing it with,\ne.g., uniformly distributed mutations, does not deteriorate the performance of\nES, when using the default adaptation mechanism for the strategy parameters. We\nobserve that these results hold not only for the sphere model but also for a\nwider range of benchmark problems.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Solving inverse problems in physics is central to understanding complex\nsystems and advancing technologies in various fields. Iterative optimization\nalgorithms, commonly used to solve these problems, often encounter local\nminima, chaos, or regions with zero gradients. This is due to their\noverreliance on local information and highly chaotic inverse loss landscapes\ngoverned by underlying partial differential equations (PDEs). In this work, we\nshow that deep neural networks successfully replicate such complex loss\nlandscapes through spatio-temporal trajectory inputs. They also offer the\npotential to control the underlying complexity of these chaotic loss landscapes\nduring training through various regularization methods. We show that optimizing\non network-smoothened loss landscapes leads to improved convergence in\npredicting optimum inverse parameters over conventional momentum-based\noptimizers such as BFGS on multiple challenging problems.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "Recollimation is a phenomenon of particular importance in the dynamic\nevolution of jets and in the emission of high-energy radiation. Additionally,\nthe full comprehension of this phenomenon provides insights into fundamental\nproperties of jets in the vicinity of the Active Galactic Nucleus (AGN).\nThree-dimensional (magneto-)hydrodynamic simulations revealed that the jet\nconditions at recollimation favor the growth of strong instabilities,\nchallenging the traditional view-supported from two-dimensional simulations-of\nconfined jets undergoing a series of recollimation and reflection shocks. To\ninvestigate the stability of relativistic jets in AGNs at recollimation sites,\nwe perform a set of long duration three-dimensional relativistic hydrodynamic\nsimulations with the state-of-the-art PLUTO code, to focus on the development\nof hydrodynamical instabilities. We explore the non-linear growth of the\ninstabilities and their effects on the physical jet properties as a function of\nthe initial jet parameters: jet Lorentz factor, temperature, opening angle and\njet-environment density-contrast. The parameter space is designed to describe\nlow-power, weakly magnetized jets at small distances from the core (around the\nparsec scale). All collimating jets we simulated develop instabilities.\nRecollimation instabilities decelerate the jet, heat it, entrain external\nmaterial, and move the recollimation point to shorter distances from the core.\nThis is true for both conical and cylindrical jets. The instabilities, that are\nfirst triggered by the centrifugal instability, appear to be less disruptive in\nthe case of narrower, denser, more relativistic, and warmer jets. These results\nprovide valuable insights into the complex processes governing AGN jets and\ncould be used to model the properties of low-power, weakly magnetized jetted\nAGNs.",
        "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.",
        "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have\nbeen proposed to tackle complex tasks in embodied intelligence and scene\nunderstanding. In addition to showing promising results on 3D tasks, we found\nthat they are significantly affected by hallucinations. For instance, they may\ngenerate objects that do not exist in the scene or produce incorrect\nrelationships between objects. To investigate this issue, this work presents\nthe first systematic study of hallucinations in 3D-LLMs. We begin by quickly\nevaluating hallucinations in several representative 3D-LLMs and reveal that\nthey are all significantly affected by hallucinations. We then define\nhallucinations in 3D scenes and, through a detailed analysis of datasets,\nuncover the underlying causes of these hallucinations. We find three main\ncauses: (1) Uneven frequency distribution of objects in the dataset. (2) Strong\ncorrelations between objects. (3) Limited diversity in object attributes.\nAdditionally, we propose new evaluation metrics for hallucinations, including\nRandom Point Cloud Pair and Opposite Question Evaluations, to assess whether\nthe model generates responses based on visual information and aligns it with\nthe text's meaning.",
        "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
        "A servomotor is a closed-loop system designed for precise movement control,\nutilizing position feedback to achieve accurate final positions. Due to the\nability to deliver higher power output and operate at enhanced speeds, DC servo\nmotors are considered ideal for applications requiring precision and\nperformance. This research aims to design, simulate, and compare various\ncontrol strategies for precise position control in DC servo motors (DSM). The\ncontrollers evaluated in this study include proportional (P),\nproportional-integral (PI), proportional-integral-derivative (PID),\nstate-feedback controllers (SFC), and state-feedback controllers augmented with\nintegral action (SFCIA). The performance of these controllers was evaluated\nusing MATLAB simulations, characterized by overshoot, settling time,\nsteady-state error, rise time, and peak time. The results indicate that the\nstate-feedback controller with integral action (SFCIA) surpasses other control\nstrategies by achieving zero steady-state error, minimal overshoot, the\nshortest settling time, and optimized rise and peak times. These findings\nhighlight the effectiveness of SFCIA for tasks requiring high levels of\nstability, precision, and dynamic performance.",
        "We address the challenge of task-oriented navigation in unstructured and\nunknown environments, where robots must incrementally build and reason on rich,\nmetric-semantic maps in real time. Since tasks may require clarification or\nre-specification, it is necessary for the information in the map to be rich\nenough to enable generalization across a wide range of tasks. To effectively\nexecute tasks specified in natural language, we propose a hierarchical\nrepresentation built on language-embedded Gaussian splatting that enables both\nsparse semantic planning that lends itself to online operation and dense\ngeometric representation for collision-free navigation. We validate the\neffectiveness of our method through real-world robot experiments conducted in\nboth cluttered indoor and kilometer-scale outdoor environments, with a\ncompetitive ratio of about 60% against privileged baselines. Experiment videos\nand more details can be found on our project page: https:\/\/atlasnav.github.io",
        "Medical image denoising is considered among the most challenging vision\ntasks. Despite the real-world implications, existing denoising methods have\nnotable drawbacks as they often generate visual artifacts when applied to\nheterogeneous medical images. This study addresses the limitation of the\ncontemporary denoising methods with an artificial intelligence (AI)-driven\ntwo-stage learning strategy. The proposed method learns to estimate the\nresidual noise from the noisy images. Later, it incorporates a novel noise\nattention mechanism to correlate estimated residual noise with noisy inputs to\nperform denoising in a course-to-refine manner. This study also proposes to\nleverage a multi-modal learning strategy to generalize the denoising among\nmedical image modalities and multiple noise patterns for widespread\napplications. The practicability of the proposed method has been evaluated with\ndense experiments. The experimental results demonstrated that the proposed\nmethod achieved state-of-the-art performance by significantly outperforming the\nexisting medical image denoising methods in quantitative and qualitative\ncomparisons. Overall, it illustrates a performance gain of 7.64 in Peak\nSignal-to-Noise Ratio (PSNR), 0.1021 in Structural Similarity Index (SSIM),\n0.80 in DeltaE ($\\Delta E$), 0.1855 in Visual Information Fidelity Pixel-wise\n(VIFP), and 18.54 in Mean Squared Error (MSE) metrics.",
        "In this paper, we obtain the existence of weak solutions to the\nChoquard-Kirchhoff type critical multiphase problem: \\begin{equation*}\n\\left\\{\\begin{array}{cc}\n  &-M(\\varphi_{\\h}(\\lvert{\\nabla u}\\rvert))div(\\lvert{\\nabla\nu}\\rvert^{p(x)-2}\\nabla u+a_1(x)\\lvert{\\nabla u}\\rvert^{q(x)-2}\\nabla\nu+a_2(x)\\lvert{\\nabla u}\\rvert^{r(x)-2}\\nabla u)\n  & =\\lambda g(x)\\lvert{u}\\rvert^{\\gamma(x)-2}u+\\theta B(x,u)+\\kappa\n\\left(\\int_{\\q}\\frac{F(y,u(y))}{\\lvert{x-y}\\rvert^{d(x,y)}}\\, dy\\right) f(x,u)\n\\ \\text{in} \\ \\Omega,\n  & u=0 \\ \\text{on} \\ {\\partial \\Omega}. \\end{array}\\right. \\end{equation*}\n  The term $B(x,u)$ on the right-hand side generalizes the critical growth. We\nobtain existence and multiplicity results by establishing certain embedding\nresults and concentration compactness principle along with the\nHardy-Littlewood-Sobolev type inequality for the Musielak Orlicz Sobolev space\n$ W^{1,\\mathcal{T}}(\\q)$.",
        "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards.",
        "Deuterium, a heavy isotope of hydrogen, is a key tracer of the formation of\nthe Solar System. Recent JWST observations have expanded the dataset of D\/H\nratios in methane on the KBOs Eris and Makemake, providing new insights into\ntheir origins. This study examines the elevated D\/H ratios in methane on these\nKBOs in the context of protosolar nebula dynamics and chemistry, and proposes a\nprimordial origin for the methane, in contrast to previous hypotheses\nsuggesting abiotic production by internal heating. A time-dependent disk model\ncoupled with a deuterium chemistry module was used to simulate the isotopic\nexchange between methane and hydrogen. Observational constraints, including the\nD\/H ratio measured in methane in comet 67P\/Churyumov-Gerasimenko, were used to\nrefine the primordial D\/H abundance. The simulations show that the observed D\/H\nratios in methane on Eris and Makemake are consistent with a primordial origin.\nThe results suggest that methane on these KBOs likely originates from the\nprotosolar nebula, similar to cometary methane, and was sequestered in solid\nform -- either as pure condensates or clathrates -- within their building\nblocks prior to accretion. These results provide a { simple} explanation for\nthe high D\/H ratios in methane on Eris and Makemake, without the need to invoke\ninternal production mechanisms.",
        "In this report, we present an experimental overview of quarkonium results\nobtained in nucleus-nucleus heavy-ion collisions, with a focus on the data\ncollected at the LHC. We discuss the current understanding of charmonium and\nbottomonium behavior in the deconfined medium produced in such collisions,\ncomparing the various observables now accessible to state-of-the-art\ntheoretical models. We also discuss the open points and how future heavy-ion\nexperiments aim to clarify these aspects.",
        "Kullback--Leibler (KL) divergence is a fundamental measure of the\ndissimilarity between two probability distributions, but it can become unstable\nin high-dimensional settings due to its sensitivity to mismatches in\ndistributional support. To address robustness limitations, we propose a novel\nQuantum-Inspired Fidelity-based Divergence (QIF), leveraging quantum\ninformation principles yet efficiently computable on classical hardware.\nCompared to KL divergence, QIF demonstrates improved numerical stability under\npartial or near-disjoint support conditions, thereby reducing the need for\nextensive regularization in specific scenarios. Moreover, QIF admits\nwell-defined theoretical bounds and continuous similarity measures. Building on\nthis, we introduce a novel regularization method, QR-Drop, which utilizes QIF\nto improve generalization in machine learning models. Empirical results show\nthat QR-Drop effectively mitigates overfitting and outperforms state-of-the-art\nmethods."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints",
    "start_abstract":"An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project"
      ],
      "abstract":[
        "The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate"
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "CoverM: Read alignment statistics for metagenomics",
        "The Relativity of Causal Knowledge",
        "Meta-Learning-Based People Counting and Localization Models Employing\n  CSI from Commodity WiFi NICs",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator",
        "Semadeni derivative of Banach spaces and functions on nonmetrizable\n  rectangles",
        "Minimum numbers of Dehn colors of knots and $\\mathcal{R}$-palette graphs",
        "The Third Generation of Nanogenerators: The Irreplaceable Potential\n  Source Enabled by the Flexoelectric Nanogenerator",
        "Simplicial effects and weakly associative partial groups",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "Uniform-in-time error estimate of random batch method with replacement\n  for the Cucker-Smale model",
        "Electrical Control of the Exchange Bias Effect at\n  Ferromagnet-Altermagnet Junctions",
        "Hybrid Quantum-Classical Optimisation of Traveling Salesperson Problem",
        "Limited attention and models of choice: A behavioral equivalence",
        "A Pathwise Coordinate Descent Algorithm for LASSO Penalized Quantile\n  Regression",
        "Precompactness in bivariate metric semigroup-valued bounded variation\n  spaces",
        "Thomas-Wigner rotation via Clifford algebras"
      ],
      "abstract":[
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Recent advances in artificial intelligence reveal the limits of purely\npredictive systems and call for a shift toward causal and collaborative\nreasoning. Drawing inspiration from the revolution of Grothendieck in\nmathematics, we introduce the relativity of causal knowledge, which posits\nstructural causal models (SCMs) are inherently imperfect, subjective\nrepresentations embedded within networks of relationships. By leveraging\ncategory theory, we arrange SCMs into a functor category and show that their\nobservational and interventional probability measures naturally form convex\nstructures. This result allows us to encode non-intervened SCMs with convex\nspaces of probability measures. Next, using sheaf theory, we construct the\nnetwork sheaf and cosheaf of causal knowledge. These structures enable the\ntransfer of causal knowledge across the network while incorporating\ninterventional consistency and the perspective of the subjects, ultimately\nleading to the formal, mathematical definition of relative causal knowledge.",
        "In this paper, we consider people counting and localization systems\nexploiting channel state information (CSI) measured from commodity WiFi network\ninterface cards (NICs). While CSI has useful information of amplitude and phase\nto describe signal propagation in a measurement environment of interest, CSI\nmeasurement suffers from offsets due to various uncertainties. Moreover, an\nuncontrollable external environment where other WiFi devices communicate each\nother induces interfering signals, resulting in erroneous CSI captured at a\nreceiver. In this paper, preprocessing of CSI is first proposed for offset\nremoval, and it guarantees low-latency operation without any filtering process.\nAfterwards, we design people counting and localization models based on\npre-training. To be adaptive to different measurement environments,\nmeta-learning-based people counting and localization models are also proposed.\nNumerical results show that the proposed meta-learning-based people counting\nand localization models can achieve high sensing accuracy, compared to other\nlearning schemes that follow simple training and test procedures.",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator is\ninvestigated both theoretically and experimentally. When the Kerr nonlinear\nresonator is driven strongly such that the induced Rabi frequency is comparable\nto or larger than the Kerr nonlinearity, the system cannot be approximated as a\ntwo-level system. We theoretically derive characteristic features in the\nfluorescence spectra such as the decrease of the center-peak intensity and the\nasymmetric sideband peaks in the presence of finite dephasing. Those features\nare consistently explained by the population of the initial dressed state and\nits transition matrix element to the final dressed state of the transition\ncorresponding to each peak. Finally, we experimentally measure the resonance\nfluorescence spectra of a driven superconducting Kerr nonlinear resonator and\nfind a quantitative agreement with our theory.",
        "We study Banach spaces $C(K)$ of real-valued continuous functions from the\nfinite product of compact lines. It turns out that the topological character of\nthese compact lines can be used to distinguish whether two spaces of continuous\nfunctions on products are isomorphic or embeddable to each other. In\nparticular, for compact lines $K_1, \\dots, K_n, L_1, \\dots, L_k$ of uncountable\ncharacter and $k \\neq n$, we claim that Banach spaces $C(\\prod_{i=1}^n K_i)$\nand $C(\\prod_{j=1}^k L_j)$ are not isomorphic.",
        "In this paper, we consider minimum numbers of colors of knots for Dehn\ncolorings. In particular, we will show that for any odd prime number $p$ and\nany Dehn $p$-colorable knot $K$, the minimum number of colors for $K$ is at\nleast $\\lfloor \\log_2 p \\rfloor +2$. Moreover, we will define the $\\R$-palette\ngraph for a set of colors. The $\\R$-palette graphs are quite useful to give\ncandidates of sets of colors which might realize a nontrivially Dehn\n$p$-colored diagram. In Appendix, we also prove that for Dehn $5$-colorable\nknot, the minimum number of colors is $4$.",
        "The electroneutrality assumption has long been adopted by scholars; however,\nthis assumption may lead to an oversight of certain physical effects. Using\nderivations from a discontinuous medium, we have obtained an expression for the\npotential and energy of a many-body unipolar charge system, which corresponds\nwell to its counterpart in a continuous medium. The compressed form of this\nexpression suggests that compressing a macroscale charged body to the nanoscale\ncan yield an enormous electric potential and energy, thereby establishing a\nconcrete research framework for third-generation nanogenerators. This effect\nmay serve as a crucial reference for understanding anomalous spatial\nelectromagnetic distributions and divergent energy fields.",
        "In this paper, we introduce a new category of simplicial effects that extends\nthe categories of effect algebras and their multi-object counterpart, effect\nalgebroids. Our approach is based on relaxing the associativity condition\nsatisfied by effect algebras and, more generally, partial monoids. Within this\nframework, simplicial effects and weakly associative partial groups arise as\ntwo extreme cases in the category of weak partial monoids. Our motivation is to\ncapture simplicial structures from the theory of simplicial distributions and\nmeasurements that behave like effects.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "The Random Batch Method (RBM), proposed by Jin et al. in 2020, is an\nefficient algorithm for simulating interacting particle systems. The\nuniform-in-time error estimates of the RBM without replacement have been\nobtained for various interacting particle systems, while the analysis of the\nRBM with replacement is just considered in (Cai et al., 2024) recently for the\nfirst-order systems governed by Langevin dynamics. In this work, we present the\nerror estimate for the RBM with replacement applied to a second-order system\nknown as the Cucker-Smale model. By introducing a crucial auxiliary system and\nleveraging the intrinsic characteristics of the Cucker-Smale model, we derive\nan estimate that is uniform in both time and particle numbers. Additionally, we\nprovide numerical simulations to validate the analytical results.",
        "This work analyzes the behavior of the interface between a ferromagnetic\nmaterial and an alter-magnet. We use a well-established line of arguments based\non electronic mean-field calculations to show that new surface phenomena that\nlead to altermagnetic materials induce an exchange bias effect on the nearby\nferromagnet. We reveal the physical mechanisms behind this phenomenon that lead\nto quantitative control over its strength. Interestingly, we predict exotic\nelectric-field-induced phenomena. This is an analogy to the relationship\nbetween exchange bias and the injection of spin currents in\nspin-transfer-dominated scenarios, which has been reported earlier in the\ntraditional antiferromagnetic\/ferromagnetic junction.",
        "The Traveling Salesperson Problem (TSP) is a fundamental NP-hard optimisation\nchallenge with widespread applications in logistics, operations research, and\nnetwork design. While classical algorithms effectively solve small to\nmedium-sized instances, they struggle with scalability due to exponential\ncomplexity. In this work, we present a hybrid quantum-classical approach that\nleverages IBM's Qiskit Runtime to integrate quantum optimisation techniques\nwith classical machine learning methods, specifically K-Means clustering and\nRandom Forest classifiers. These machine learning components aid in problem\ndecomposition and noise mitigation, improving the quality of quantum solutions.\nExperimental results for TSP instances ranging from 4 to 8 cities reveal that\nthe quantum-only approach produces solutions up to 21.7% worse than the\nclassical baseline, while the hybrid method reduces this cost increase to 11.3%\nfor 8 cities. This demonstrates that hybrid approaches improve solution quality\ncompared to purely quantum methods but remain suboptimal compared to classical\nsolvers. Despite current hardware limitations, these results highlight the\npotential of quantum-enhanced methods for combinatorial optimisation, paving\nthe way for future advancements in scalable quantum computing frameworks.",
        "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), and the properties of the unobserved attention filters that\nexplain the observed choices are singled out. Moreover, for each specification,\nwe infer information about the DM's attention and preference from irrational\nfeatures of choice data.",
        "$\\ell_1$ penalized quantile regression is used in many fields as an\nalternative to penalized least squares regressions for high-dimensional data\nanalysis. Existing algorithms for penalized quantile regression either use\nlinear programming, which does not scale well in high dimension, or an\napproximate coordinate descent (CD) which does not solve for exact\ncoordinatewise minimum of the nonsmooth loss function. Further, neither\napproaches build fast, pathwise algorithms commonly used in high-dimensional\nstatistics to leverage sparsity structure of the problem in large-scale data\nsets. To avoid the computational challenges associated with the nonsmooth\nquantile loss, some recent works have even advocated using smooth\napproximations to the exact problem. In this work, we develop a fast, pathwise\ncoordinate descent algorithm to compute exact $\\ell_1$ penalized quantile\nregression estimates for high-dimensional data. We derive an easy-to-compute\nexact solution for the coordinatewise nonsmooth loss minimization, which, to\nthe best of our knowledge, has not been reported in the literature. We also\nemploy a random perturbation strategy to help the algorithm avoid getting stuck\nalong the regularization path. In simulated data sets, we show that our\nalgorithm runs substantially faster than existing alternatives based on\napproximate CD and linear program, while retaining the same level of estimation\naccuracy.",
        "In this paper, we show that if a set in bivariate metric semigroups-valued\nbounded variation spaces is pointwise totally bounded and joint equivariated\nthen it is precompact. These spaces include bounded Jordan variation spaces,\nbounded Wiener variation spaces, bounded Waterman variation spaces, bounded\nRiesz variation spaces and bounded Korenblum variation spaces. To do so, we\nintroduce the concept of equimetric set.",
        "We derive Macfarlane's formula for the Thomas-Wigner angle of rotation using\nClifford-algebra methods. The presentation is pedagogical and elementary,\nsuitable for students with some basic knowledge of special relativity; no prior\nknowledge of Clifford algebras is required."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry",
    "start_abstract":"Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "A ConvNet for the 2020s"
      ],
      "abstract":[
        "The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Hierarchical Alignment-enhanced Adaptive Grounding Network for\n  Generalized Referring Expression Comprehension",
        "Can Generative Geospatial Diffusion Models Excel as Discriminative\n  Geospatial Foundation Models?",
        "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
        "Geometric Mean Improves Loss For Few-Shot Learning",
        "SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary\n  Surgery",
        "The Devil is in the Details: Simple Remedies for Image-to-LiDAR\n  Representation Learning",
        "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative\n  Perception",
        "Pointmap Association and Piecewise-Plane Constraint for Consistent and\n  Compact 3D Gaussian Segmentation Field",
        "Sequence models for continuous cell cycle stage prediction from\n  brightfield images",
        "Benchmarking Multimodal Models for Fine-Grained Image Analysis: A\n  Comparative Study Across Diverse Visual Features",
        "End-to-End HOI Reconstruction Transformer with Graph-based Encoding",
        "Towards a Unified Copernicus Foundation Model for Earth Vision",
        "Intrinsic Donaldson-Thomas theory. I. Component lattices of stacks",
        "Ferromagnetic Resonance in a Magnetically Dilute Percolating\n  Ferromagnet: An Experimental and Theoretical Study",
        "Towards Enterprise-Ready Computer Using Generalist Agent",
        "Maps from Grassmannians of 2-planes to projective spaces",
        "Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners",
        "Sensitivity of Double Deeply Virtual Compton Scattering observables to\n  Generalized Parton Distributions",
        "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices",
        "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from\n  Sparse Matrix Decomposition",
        "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
        "Post-Quantum Stealth Address Protocols",
        "MinD the gap: Membrane proteins form 3D patterns in a suspension of\n  liposomes",
        "Causes and Strategies in Multiagent Systems",
        "Hybrid MIMO in the Upper Mid-Band: Architectures, Processing, and Energy\n  Efficiency",
        "Unit Edge-Length Rectilinear Drawings with Crossings and Rectangular\n  Faces",
        "DBRouting: Routing End User Queries to Databases for Answerability"
      ],
      "abstract":[
        "In this work, we address the challenging task of Generalized Referring\nExpression Comprehension (GREC). Compared to the classic Referring Expression\nComprehension (REC) that focuses on single-target expressions, GREC extends the\nscope to a more practical setting by further encompassing no-target and\nmulti-target expressions. Existing REC methods face challenges in handling the\ncomplex cases encountered in GREC, primarily due to their fixed output and\nlimitations in multi-modal representations. To address these issues, we propose\na Hierarchical Alignment-enhanced Adaptive Grounding Network (HieA2G) for GREC,\nwhich can flexibly deal with various types of referring expressions. First, a\nHierarchical Multi-modal Semantic Alignment (HMSA) module is proposed to\nincorporate three levels of alignments, including word-object, phrase-object,\nand text-image alignment. It enables hierarchical cross-modal interactions\nacross multiple levels to achieve comprehensive and robust multi-modal\nunderstanding, greatly enhancing grounding ability for complex cases. Then, to\naddress the varying number of target objects in GREC, we introduce an Adaptive\nGrounding Counter (AGC) to dynamically determine the number of output targets.\nAdditionally, an auxiliary contrastive loss is employed in AGC to enhance\nobject-counting ability by pulling in multi-modal features with the same\ncounting and pushing away those with different counting. Extensive experimental\nresults show that HieA2G achieves new state-of-the-art performance on the\nchallenging GREC task and also the other 4 tasks, including REC, Phrase\nGrounding, Referring Expression Segmentation (RES), and Generalized Referring\nExpression Segmentation (GRES), demonstrating the remarkable superiority and\ngeneralizability of the proposed HieA2G.",
        "Self-supervised learning (SSL) has revolutionized representation learning in\nRemote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage\nvast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs\nprimarily focus on discriminative objectives, such as contrastive learning or\nmasked image modeling, owing to their proven success in learning transferable\nrepresentations. However, generative diffusion models--which demonstrate the\npotential to capture multi-grained semantics essential for RS tasks during\nimage generation--remain underexplored for discriminative applications. This\nprompts the question: can generative diffusion models also excel and serve as\nGFMs with sufficient discriminative power? In this work, we answer this\nquestion with SatDiFuser, a framework that transforms a diffusion-based\ngenerative geospatial foundation model into a powerful pretraining tool for\ndiscriminative RS. By systematically analyzing multi-stage, noise-dependent\ndiffusion features, we develop three fusion strategies to effectively leverage\nthese diverse representations. Extensive experiments on remote sensing\nbenchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving\ngains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in\nclassification, demonstrating the capacity of diffusion-based generative\nfoundation models to rival or exceed discriminative GFMs. Code will be\nreleased.",
        "Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion technique, which\nfuses images captured by different exposure levels, to increase dynamic range.\nHowever, this approach can only handle images with limited exposure difference,\nnormally 3-4 stops. When applying to very high dynamic scenes where a large\nexposure difference is required, this approach often fails due to incorrect\nalignment or inconsistent lighting between inputs, or tone mapping artifacts.\nIn this work, we propose UltraFusion, the first exposure fusion technique that\ncan merge input with 9 stops differences. The key idea is that we model the\nexposure fusion as a guided inpainting problem, where the under-exposed image\nis used as a guidance to fill the missing information of over-exposed highlight\nin the over-exposed region. Using under-exposed image as a soft guidance,\ninstead of a hard constrain, our model is robust to potential alignment issue\nor lighting variations. Moreover, utilizing the image prior of the generative\nmodel, our model also generates natural tone mapping, even for very\nhigh-dynamic range scene. Our approach outperforms HDR-Transformer on latest\nHDR benchmarks. Moreover, to test its performance in ultra high dynamic range\nscene, we capture a new real-world exposure fusion benchmark, UltraFusion\nDataset, with exposure difference up to 9 stops, and experiments show that\n\\model~can generate beautiful and high-quality fusion results under various\nscenarios. An online demo is provided at\nhttps:\/\/openimaginglab.github.io\/UltraFusion\/.",
        "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-$\\alpha$, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-$\\alpha$ into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https:\/\/tdm-t2x.github.io\/",
        "Few-shot learning (FSL) is a challenging task in machine learning, demanding\na model to render discriminative classification by using only a few labeled\nsamples. In the literature of FSL, deep models are trained in a manner of\nmetric learning to provide metric in a feature space which is well\ngeneralizable to classify samples of novel classes; in the space, even a few\namount of labeled training examples can construct an effective classifier. In\nthis paper, we propose a novel FSL loss based on \\emph{geometric mean} to embed\ndiscriminative metric into deep features. In contrast to the other losses such\nas utilizing arithmetic mean in softmax-based formulation, the proposed method\nleverages geometric mean to aggregate pair-wise relationships among samples for\nenhancing discriminative metric across class categories. The proposed loss is\nnot only formulated in a simple form but also is thoroughly analyzed in\ntheoretical ways to reveal its favorable characteristics which are favorable\nfor learning feature metric in FSL. In the experiments on few-shot image\nclassification tasks, the method produces competitive performance in comparison\nto the other losses.",
        "Image-guided surgery demands adaptive, real-time decision support, yet static\nAI models struggle with structured task planning and providing interactive\nguidance. Large vision-language models (VLMs) offer a promising solution by\nenabling dynamic task planning and predictive decision support. We introduce\nSurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable\nof conversation, planning, and task execution. The agent dynamically processes\nsurgeon queries and plans the tasks such as MRI tumor segmentation, endoscope\nanatomy segmentation, overlaying preoperative imaging with intraoperative\nviews, instrument tracking, and surgical visual question answering (VQA). To\nenable structured task planning, we develop the PitAgent dataset, a surgical\ncontext-aware dataset covering segmentation, overlaying, instrument\nlocalization, tool tracking, tool-tissue interactions, phase identification,\nand surgical activity recognition. Additionally, we propose FFT-GaLore, a fast\nFourier transform (FFT)-based gradient projection technique for efficient\nlow-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical\nenvironments. We validate SurgicalVLM-Agent by assessing task planning and\nprompt generation on our PitAgent dataset and evaluating zero-shot VQA using a\npublic pituitary dataset. Results demonstrate state-of-the-art performance in\ntask planning and query interpretation, with highly semantically meaningful VQA\nresponses, advancing AI-driven surgical assistance.",
        "LiDAR is a crucial sensor in autonomous driving, commonly used alongside\ncameras. By exploiting this camera-LiDAR setup and recent advances in image\nrepresentation learning, prior studies have shown the promising potential of\nimage-to-LiDAR distillation. These prior arts focus on the designs of their own\nlosses to effectively distill the pre-trained 2D image representations into a\n3D model. However, the other parts of the designs have been surprisingly\nunexplored. We find that fundamental design elements, e.g., the LiDAR\ncoordinate system, quantization according to the existing input interface, and\ndata utilization, are more critical than developing loss functions, which have\nbeen overlooked in prior works. In this work, we show that simple fixes to\nthese designs notably outperform existing methods by 16% in 3D semantic\nsegmentation on the nuScenes dataset and 13% in 3D object detection on the\nKITTI dataset in downstream task performance. We focus on overlooked design\nchoices along the spatial and temporal axes. Spatially, prior work has used\ncylindrical coordinate and voxel sizes without considering their side effects\nyielded with a commonly deployed sparse convolution layer input interface,\nleading to spatial quantization errors in 3D models. Temporally, existing work\nhas avoided cumbersome data curation by discarding unsynced data, limiting the\nuse to only the small portion of data that is temporally synced across sensors.\nWe analyze these effects and propose simple solutions for each overlooked\naspect.",
        "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.",
        "Achieving a consistent and compact 3D segmentation field is crucial for\nmaintaining semantic coherence across views and accurately representing scene\nstructures. Previous 3D scene segmentation methods rely on video segmentation\nmodels to address inconsistencies across views, but the absence of spatial\ninformation often leads to object misassociation when object temporarily\ndisappear and reappear. Furthermore, in the process of 3D scene reconstruction,\nsegmentation and optimization are often treated as separate tasks. As a result,\noptimization typically lacks awareness of semantic category information, which\ncan result in floaters with ambiguous segmentation. To address these\nchallenges, we introduce CCGS, a method designed to achieve both view\nconsistent 2D segmentation and a compact 3D Gaussian segmentation field. CCGS\nincorporates pointmap association and a piecewise-plane constraint. First, we\nestablish pixel correspondence between adjacent images by minimizing the\nEuclidean distance between their pointmaps. We then redefine object mask\noverlap accordingly. The Hungarian algorithm is employed to optimize mask\nassociation by minimizing the total matching cost, while allowing for partial\nmatches. To further enhance compactness, the piecewise-plane constraint\nrestricts point displacement within local planes during optimization, thereby\npreserving structural integrity. Experimental results on ScanNet and Replica\ndatasets demonstrate that CCGS outperforms existing methods in both 2D panoptic\nsegmentation and 3D Gaussian segmentation.",
        "Understanding cell cycle dynamics is crucial for studying biological\nprocesses such as growth, development and disease progression. While\nfluorescent protein reporters like the Fucci system allow live monitoring of\ncell cycle phases, they require genetic engineering and occupy additional\nfluorescence channels, limiting broader applicability in complex experiments.\nIn this study, we conduct a comprehensive evaluation of deep learning methods\nfor predicting continuous Fucci signals using non-fluorescence brightfield\nimaging, a widely available label-free modality. To that end, we generated a\nlarge dataset of 1.3 M images of dividing RPE1 cells with full cell cycle\ntrajectories to quantitatively compare the predictive performance of distinct\nmodel categories including single time-frame models, causal state space models\nand bidirectional transformer models. We show that both causal and\ntransformer-based models significantly outperform single- and fixed frame\napproaches, enabling the prediction of visually imperceptible transitions like\nG1\/S within 1h resolution. Our findings underscore the importance of sequence\nmodels for accurate predictions of cell cycle dynamics and highlight their\npotential for label-free imaging.",
        "This article introduces a benchmark designed to evaluate the capabilities of\nmultimodal models in analyzing and interpreting images. The benchmark focuses\non seven key visual aspects: main object, additional objects, background,\ndetail, dominant colors, style, and viewpoint. A dataset of 14,580 images,\ngenerated from diverse text prompts, was used to assess the performance of\nseven leading multimodal models. These models were evaluated on their ability\nto accurately identify and describe each visual aspect, providing insights into\ntheir strengths and weaknesses for comprehensive image understanding. The\nfindings of this benchmark have significant implications for the development\nand selection of multimodal models for various image analysis tasks.",
        "With the diversification of human-object interaction (HOI) applications and\nthe success of capturing human meshes, HOI reconstruction has gained widespread\nattention. Existing mainstream HOI reconstruction methods often rely on\nexplicitly modeling interactions between humans and objects. However, such a\nway leads to a natural conflict between 3D mesh reconstruction, which\nemphasizes global structure, and fine-grained contact reconstruction, which\nfocuses on local details. To address the limitations of explicit modeling, we\npropose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding\n(HOI-TG). It implicitly learns the interaction between humans and objects by\nleveraging self-attention mechanisms. Within the transformer architecture, we\ndevise graph residual blocks to aggregate the topology among vertices of\ndifferent spatial structures. This dual focus effectively balances global and\nlocal representations. Without bells and whistles, HOI-TG achieves\nstate-of-the-art performance on BEHAVE and InterCap datasets. Particularly on\nthe challenging InterCap dataset, our method improves the reconstruction\nresults for human and object meshes by 8.9% and 8.6%, respectively.",
        "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps:\/\/github.com\/zhu-xlab\/Copernicus-FM.",
        "This is the first paper in a series on intrinsic Donaldson-Thomas theory, a\ngeneralization of Donaldson-Thomas theory from the linear case, or the case of\nmoduli stacks of objects in $3$-Calabi-Yau abelian categories, to the\nnon-linear case of general $(-1)$-shifted symplectic stacks. This is done by\ndeveloping a new framework for studying the enumerative geometry of general\nalgebraic stacks, and we expect that this framework can also be applied to\nextending other types of enumerative theories for linear stacks to the\nnon-linear case.\n  In this paper, we establish the foundations of our framework. We introduce\nthe component lattice of an algebraic stack, which is the key combinatorial\nobject in our theory. It generalizes and globalizes the cocharacter lattice and\nthe Weyl group of an algebraic group, and is defined as the set of connected\ncomponents of the stack of graded points of the original stack.\n  We prove several results on the structure of graded and filtered points of a\nstack using the component lattice. The first is the constancy theorem, which\nstates that there is a wall-and-chamber structure on the component lattice,\nsuch that the isomorphism types of connected components of the stacks of graded\nand filtered points stay constant within each chamber. The second is the\nfiniteness theorem, providing a criterion for the finiteness of the number of\npossible isomorphism types of these components. The third is the associativity\ntheorem, generalizing the structure of Hall algebras from linear stacks to\ngeneral stacks, involving a notion of Hall categories.\n  Finally, we discuss some applications of these results outside\nDonaldson-Thomas theory, including a construction of stacks of real-weighted\nfiltrations, and a generalization of the semistable reduction theorem to\nreal-weighted filtrations.",
        "Ferromagnetic resonance (FMR) serves as a powerful probe of magnetization\ndynamics and anisotropy in percolating ferromagnets, where short-range\ninteractions govern long-range magnetic order. We apply this approach to\nGa$_{1-x}$Mn$_x$N ($x \\simeq 8$), a dilute ferromagnetic semiconductor,\ncombining FMR and superconducting quantum interference device magnetometry. Our\nresults confirm the percolative nature of ferromagnetism in (Ga,Mn)N, with a\nCurie temperature $T_{\\mathrm{C}} = 12$ K, and reveal that despite magnetic\ndilution, key features of conventional ferromagnets are retained. FMR\nmeasurements establish a robust uniaxial anisotropy, dictated by Mn$^{3+}$\nsingle-ion anisotropy, with an easy-plane character at low Mn content. While\nexcessive line broadening suppresses FMR signals below 9 K, they persist up to\n70 K, indicating the presence of non-percolating ferromagnetic clusters well\nabove $T_{\\mathrm{C}}$. The temperature dependence of the FMR intensity follows\nthat of the magnetization, underscoring the stability of these clusters.\nAnalysis of the FMR linewidth provides insights into relaxation processes,\nrevealing large Gilbert damping due to the low magnetization of the system.\nStrikingly, atomistic spin model simulations reproduce the experimentally\nobserved resonance fields, anisotropy trends, and linewidth evolution with\nremarkable accuracy. This agreement underscores the predictive power of our\nmodeling approach in describing percolating ferromagnets. This study advances\nthe understanding of percolating ferromagnetic systems, demonstrating that FMR\nis a key technique for probing their unique dynamic and anisotropic properties.\nOur findings contribute to the broader exploration of dilute ferromagnets and\nprovide new insights into percolating ferromagnetic systems, which will be\nrelevant for spintronic opportunities.",
        "This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.",
        "Using quaternions and octonions, we construct some maps from the Grassmannian\nof 2-dimensional planes of $\\mathbb{R}^n$, $\\mathrm{Gr}_2(\\mathbb{R}^n)$, to\nthe projective space $\\mathbb{R}\\mathrm{P}^k$, for certain values of $n$ and\n$k$. All of our maps induce an isomorphism at the level of fundamental groups,\nand two of them are shown to be submersions. As an application, we obtain new\nestimates of the Lusternik-Schnirelmann category of\n$\\mathrm{Gr}_2(\\mathbb{R}^n)$ for specific values of $n$.",
        "Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.",
        "Double Deeply Virtual Compton Scattering (DDVCS) is a promising channel for\nGeneralized Parton Distribution (GPD) studies as it is a generalization of the\nDeeply Virtual Compton Scattering (DVCS) and Timelike Compton Scattering (TCS)\nprocesses. Contrary to DVCS and TCS, the GPD phase space accessed through DDVCS\nis not constrained by on-shell conditions on the incoming and outgoing photons\nthus allowing unrestricted GPD extraction from experimental observables.\nConsidering polarized electron and positron beams directed to a polarized\nproton target, we study the sensitivity of the DDVCS cross-section asymmetries\nto the chiral-even proton GPDs from different model predictions. The\nfeasibility of such measurements is further investigated in the context of the\nCLAS and SoLID spectrometers at the Thomas Jefferson National Accelerator\nFacility and the future Electron-Ion Collider at the Brookhaven National\nLaboratory.",
        "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals.",
        "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial\/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training\/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions.",
        "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
        "The Stealth Address Protocol (SAP) allows users to receive assets through\nstealth addresses that are unlinkable to their stealth meta-addresses. The most\nwidely used SAP, Dual-Key SAP (DKSAP), and the most performant SAP, Elliptic\nCurve Pairing Dual-Key SAP (ECPDKSAP), are based on elliptic curve\ncryptography, which is vulnerable to quantum attacks. These protocols depend on\nthe elliptic curve discrete logarithm problem, which could be efficiently\nsolved on a sufficiently powerful quantum computer using the Shor algorithm. In\nthis paper three novel post-quantum SAPs based on lattice-based cryptography\nare presented: LWE SAP, Ring-LWE SAP and Module-LWE SAP. These protocols\nleverage Learning With Errors (LWE) problem to ensure quantum-resistant\nprivacy. Among them, Module-LWE SAP, which is based on the Kyber key\nencapsulation mechanism, achieves the best performance and outperforms ECPDKSAP\nby approximately 66.8% in the scan time of the ephemeral public key registry.",
        "The self-organization of pattern-forming systems depends not only on the\nchemical but also physical properties of their components. In this work, we\nfragmented and dispersed the MinDE protein system's lipid substrate into\ndiffusive sub-micrometer-sized liposomes, and report that the ATP-fueled\nprotein-protein interactions continue to drive spatially extended patterns at\nscales well separated from those of the requisite liposomes, despite the\ncomplete loss of membrane continuity. The patterns form in three-dimensions\nbecause the membrane is dispersed in a volume. By varying protein\nconcentration, liposome size distribution, and density, we observed and\ncharacterized rich 3D dynamical patterns at steady state, including traveling\nwaves, dynamical spirals and a mixed phase where both patterns coexist.\nSimulations and linear stability analysis of a coarse-grained model reveal that\nthe dispersed membranes's physical properties effectively rescale two key\nfactors that govern pattern formation and wavelength selection:\nprotein-membrane binding rates and diffusion. This work highlights the\nrobustness of pattern formation in membrane-bulk systems despite membrane\nfragmentation. It suggests that biological protein systems have the potential\nto serve as adaptable templates for out-of-equilibrium self-organization in 3D,\nbeyond in vivo biological contexts.",
        "Causality plays an important role in daily processes, human reasoning, and\nartificial intelligence. There has however not been much research on causality\nin multi-agent strategic settings. In this work, we introduce a systematic way\nto build a multi-agent system model, represented as a concurrent game\nstructure, for a given structural causal model. In the obtained so-called\ncausal concurrent game structure, transitions correspond to interventions on\nagent variables of the given causal model. The Halpern and Pearl framework of\ncausality is used to determine the effects of a certain value for an agent\nvariable on other variables. The causal concurrent game structure allows us to\nanalyse and reason about causal effects of agents' strategic decisions. We\nformally investigate the relation between causal concurrent game structures and\nthe original structural causal models.",
        "As 6G networks evolve, the upper mid-band spectrum (7 GHz to 24 GHz), or\nfrequency range 3 (FR3), is emerging as a promising balance between the\ncoverage offered by sub-6 GHz bands and the high-capacity of millimeter wave\n(mmWave) frequencies. This paper explores the structure of FR3 hybrid MIMO\nsystems and proposes two architectural classes: Frequency Integrated (FI) and\nFrequency Partitioned (FP). FI architectures enhance spectral efficiency by\nexploiting multiple sub-bands parallelism, while FP architectures dynamically\nallocate sub-band access according to specific application requirements.\nAdditionally, two approaches, fully digital (FD) and hybrid analog-digital\n(HAD), are considered, comparing shared (SRF) versus dedicated RF (DRF) chain\nconfigurations. Herein signal processing solutions are investigated,\nparticularly for an uplink multi-user scenario with power control optimization.\n  Results demonstrate that SRF and DRF architectures achieve comparable\nspectral efficiency; however, SRF structures consume nearly half the power of\nDRF in the considered setup. While FD architectures provide higher spectral\nefficiency, they do so at the cost of increased power consumption compared to\nHAD. Additionally, FI architectures show slightly greater power consumption\ncompared to FP; however, they provide a significant benefit in spectral\nefficiency (over 4 x), emphasizing an important trade-off in FR3 engineering.",
        "Unit edge-length drawings, rectilinear drawings (where each edge is either a\nhorizontal or a vertical segment), and rectangular face drawings are among the\nmost studied subjects in Graph Drawing. However, most of the literature on\nthese topics refers to planar graphs and planar drawings. In this paper we\nstudy drawings with all the above nice properties but that can have edge\ncrossings; we call them Unit Edge length Rectilinear drawings with Rectangular\nFaces (UER-RF drawings). We consider crossings as dummy vertices and apply the\nunit edge-length convention to the edge segments connecting any two (real or\ndummy) vertices. Note that UER-RF drawings are grid drawings (vertices are\nplaced at distinct integer coordinates), which is another classical requirement\nof graph visualizations. We present several efficient and easily implementable\nalgorithms for recognizing graphs that admit UER-RF drawings and for\nconstructing such drawings if they exist. We consider restrictions on the\ndegree of the vertices or on the size of the faces. For each type of\nrestriction, we consider both the general unconstrained setting and a setting\nin which either the external boundary of the drawing is fixed or the rotation\nsystem of the graph is fixed as part of the input.",
        "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"A ConvNet for the 2020s",
    "start_abstract":"The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry"
      ],
      "abstract":[
        "Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Fixed-budget simulation method for growing cell populations",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Optimal compound downselection to promote diversity and parallel\n  chemistry",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "VenusMutHub: A systematic evaluation of protein mutation effect\n  predictors on small-scale experimental data",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "From Site Response to Site-city Interaction: a Case Study in the Tokyo\n  Area",
        "Well-to-Tank Carbon Intensity Variability of Fossil Marine Fuels: A\n  Country-Level Assessment",
        "The IDEA detector concept for FCC-ee",
        "A simple magnetic field stabilization technique for atomic Bose-Einstein\n  condensate experiments",
        "Terahertz Magnon Excitations and Switching in Non-Collinear\n  Antiferromagnets",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Diffusion Approximation for Slow-Fast SDEs with State-Dependent\n  Switching",
        "Direct Nucleation of Hierarchical Nanostructures on Plasmonic Fiber\n  Optics Enables Enhanced SERS Performance",
        "Periodic Variability of the Central Stars of Planetary Nebulae Surveyed\n  through the Zwicky Transient Facility",
        "Further results on relative, divergence measures based on extropy and\n  their applications",
        "On recurrence and entropy in hyperspace of continua in dimension one",
        "Emerging Excess Consistent with a Narrow Resonance at 152 GeV in\n  High-Energy Proton-Proton Collisions",
        "Fibonacci-Modulation-Induced Multiple Topological Anderson Insulators",
        "The route of shear to Ising superconductivity in bilayer graphene",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning"
      ],
      "abstract":[
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "Investigating the dynamics of growing cell populations is crucial for\nunraveling key biological mechanisms in living organisms, with many important\napplications in therapeutics and biochemical engineering. Classical agent-based\nsimulation algorithms are often inefficient for these systems because they\ntrack each individual cell, making them impractical for fast (or even\nexponentially) growing cell populations. To address this challenge, we\nintroduce a novel stochastic simulation approach based on a Feynman-Kac-like\nrepresentation of the population dynamics. This method, named the\nFeynman-Kac-inspired Gillespie's Stochastic Simulation Algorithm (FKG-SSA),\nalways employs a fixed number of independently simulated cells for Monte Carlo\ncomputation of the system, resulting in a constant computational complexity\nregardless of the population size. Furthermore, we theoretically show the\nstatistical consistency of the proposed method, indicating its accuracy and\nreliability. Finally, a couple of biologically relevant numerical examples are\npresented to illustrate the approach. Overall, the proposed FKG-SSA effectively\naddresses the challenge of simulating growing cell populations, providing a\nsolid foundation for better analysis of these systems.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Early stage drug discovery and molecular design projects often follow\niterative design-make-test cycles. The selection of which compounds to\nsynthesize from all possible candidate compounds is a complex decision inherent\nto these design cycles that must weigh multiple factors. We build upon the\nalgorithmic downselection framework SPARROW that considers synthetic cost,\nsynthetic feasibility, and compound utility, extending it to address additional\ncritical factors related to the risk of synthesis failure, molecular diversity,\nand parallel chemistry capabilities. These design considerations further align\nalgorithmic compound selection with the true complexity of this decision-making\nprocess, allowing SPARROW to capture a broader set of principles typically\nreliant on expert chemist intuition. The application of these formulations to\nan exemplary case study highlights SPARROW's ability to promote the selection\nof diverse batches of compounds whose syntheses are amenable to parallel\nchemistry.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Considering the purpose of the session relating early engineering\ndevelopments in site response and soil-structure interaction, this paper\nfocuses on the development of studies regarding site-city interaction following\nthe striking site response observations obtained in Mexico City during the 1985\nGuerrero-Michoacan event, The first part presents an overview of the\ninvestigations on multiple structure-soil-structure interaction, starting with\nMexico-city like environments with dense urbanization on soft soils, which\nlater evolved with the concept of metamaterials. Up to now, such investigations\nhave been largely relying on numerical simulations in 2D and 3D media, coupling\nsoft surface soil layers and simplified building models, including also some\ntheoretical developments using various mechanical concepts. They also relied on\na number of laboratory experiments on reduced-scale mock-ups with diverse\nvibratory sources (shaking table, acoustic devices). The latest studies coupled\nfull-scale experiments on mechanical analogs such as forests or wind turbine\nfarms involving sets of resonators with similar frequencies, and numerical\nsimulation to investigate their impact on the propagation of surface (Rayleigh)\nwaves. Almost all such studies converge in predicting lower ground motion\namplitude for sites located within the ''urbanized'' area, but none of them can\nbe considered a ''groundtruth'' proof for a real earthquake in a real city. The\nsecond part thus takes advantage of the long duration of strong motion\nobservations in the Kanto area thanks to the KiK-net, K-NET and JMA\n(Shin-dokei) networks, to investigate the possible changes in site response\nwith time. The first results obtained with the event-specific site terms\nderived from Generalized Inversion Techniques (Nakano et al., 2015) indicate a\nsystematic reduction of the low frequency (0.2 -1 Hz) site amplification, in\nthe central-south Tokyo area. As this frequency band corresponds both to the\nsite frequency (very thick deposits) and to the high-rise buildings, the\ndiscussion focuses on the possible relation with the extensive construction in\nsome areas of downtown Tokyo over the last 2 decades.",
        "The transition toward a low-carbon maritime transportation requires\nunderstanding lifecycle carbon intensity (CI) of marine fuels. While\nwell-to-tank emissions significantly contribute to total greenhouse gas\nemissions, many studies lack global perspective in accounting for upstream\noperations, transportation, refining, and distribution. This study evaluates\nwell-to-tank CI of High Sulphur Fuel Oil (HSFO) and well-to-refinery exit CI of\nLiquefied Petroleum Gas (LPG) worldwide at asset level. HSFO represents\ntraditional marine fuel, while LPG serves as potential transition fuel due to\nlower tank-to-wake emissions and compatibility with low-carbon fuels. Using\nOPGEE and PRELIM tools with R-based geospatial methods, we derive country-level\nCI values for 72 countries (HSFO) and 74 countries (LPG), covering 98% of\nglobal production. Results show significant variation in climate impacts\nglobally. HSFO upstream CI ranges 1-22.7 gCO2e\/MJ, refining CI 1.2-12.6\ngCO2e\/MJ, with global volume-weighted-average well-to-tank CI of 12.4 gCO2e\/MJ.\nUpstream and refining account for 55% and 32% of HSFO well-to-tank CI, with\nlarge exporters and intensive refining practices showing higher emissions. For\nLPG, upstream CI ranges 0.9-22.7 gCO2e\/MJ, refining CI 2.8-13.9 gCO2e\/MJ, with\nvolume-weighted-average well-to-refinery CI of 15.6 gCO2e\/MJ. Refining\ncomprises 49% of LPG well-to-refinery CI, while upstream and transport\nrepresent 44% and 6%. Major players include China, United States and Russia.\nThese findings reveal significant CI variability across countries and supply\nchains, offering opportunities for targeted emission reduction policies.",
        "A detector concept, named IDEA, optimized for the physics and running\nconditions at the FCC-ee is presented. After discussing the expected running\nconditions and the main physics drivers, a detailed description of the\nindividual sub-detectors is given. These include: a very light tracking system\nwith a powerful vertex detector inside a large drift chamber surrounded by a\nsilicon wrapper, a high resolution dual readout crystal electromagnetic\ncalorimeter, an HTS based superconducting solenoid, a dual readout fiber\ncalorimeter and three layers of muon chambers embedded in the magnet flux\nreturn yoke. Some examples of the expected detector performance, based on fast\nand full simulation, are also given.",
        "We demonstrate a simple magnetic field stabilization technique in a\nBose-Einstein condensate experiment. Our technique is based on the precise\nmeasurement of the current fluctuations in the main magnetic field coils and\namounts to their compensation using an auxiliary coil. It has the advantage of\nsimplicity as compensation is done using a low inductance coil that can be\nstraightforwardly driven at the relevant frequencies (1 kHz). The performances\nof the different components (power supply, current transducer, electronics...)\nare precisely characterized. In addition, for optimal stability the ambient\nmagnetic field is also measured and compensated. The magnetic field stability\naround 57 G is measured by Ramsey spectroscopy of magnetic field sensitive\nradiofrequency transition between two spin states of potassium 39 and the\nshot-to-shot fluctuations are reduced to 64(7) $\\mu$G rms, i.e. at the 1 x 10\n-6 level. In the context of our experiment, this result opens interesting\nprospects for the study of three-body interactions in Bose-Einstein condensate\npotassium spin mixtures.",
        "We investigate how spatiotemporal spin polarized current can lead to\nterahertz frequency excitations in non-collinear antiferromagnets. By solving\nthe Landau-Lifshitz-Gilbert equation numerically for non-collinear\nantiferromagnet, we show that the magnon frequency spectrum exhibits standing\nspin wave modes and depends on the thickness of Mn$_3$Ge in heterostructure\nFe|Au|Mn$_3$Ge. Also, we analyze the switching process of ground state as a\nfunction of a spin current. We show a switching phase diagram, which contains\nswitching and non-switching regions. Our work suggests non-collinear\nantiferromagnets as an efficient platform for terahertz magnonics and ultrafast\nmemory devices.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "In this paper, we study the diffusion approximation for slow-fast stochastic\ndifferential equations with state-dependent switching, where the slow component\n$X^{\\varepsilon}$ is the solution of a stochastic differential equation with\nadditional homogenization term, while the fast component $\\alpha^{\\varepsilon}$\nis a switching process. We first prove the weak convergence of\n$\\{X^\\varepsilon\\}_{0<\\varepsilon\\leq 1}$ to $\\bar{X}$ in the space of\ncontinuous functions, as $\\varepsilon\\rightarrow 0$. Using the martingale\nproblem approach and Poisson equation associated with a Markov chain, we\nidentify this weak limiting process as the unique solution $\\bar{X}$ of a new\nstochastic differential equation, which has new drift and diffusion terms that\ndiffer from those in the original equation. Next, we prove the order $1\/2$ of\nweak convergence of $X^{\\varepsilon}_t$ to $\\bar{X}_t$ by applying suitable\ntest functions $\\phi$, for any $t\\in [0, T]$. Additionally, we provide an\nexample to illustrate that the order we achieve is optimal.",
        "We present an innovative fabrication method to achieve bottom-up in situ\nsurface-overstructured Au nanoislands (NIs) with tunable grades of surface\ncoverage, elongation, and branching, directly on micro-optical fibers for\nsensing applications. These all-in-gold hierarchical nanostructures consist of\nNIs coated with surface protrusions of various morphologies. They are created\nin solution using a selective seeded growth approach, whereby additional gold\ngrowth is achieved over Au NIs formerly developed on the fiber facet by a\nsolid-state dewetting approach. The morphology of nanosized surface-NI\noverstructuring can be adjusted from multi-dot-decorated Au NIs to\nmulti-arm-decorated Au NIs. This engineering of optical fibers allows for\nimproved remote surface-enhanced Raman spectroscopy (SERS) molecular detection.\nBy combining solid-state dewetting and wet-chemical approaches, we achieve\nstable in-contact deposition of surface-overstructured NIs with the optical\nfiber solid substrate, alongside precise control over branching morphology and\nanisotropy extent. The fiber optic probes engineered by surface-overstructured\nNIs exhibit outstanding sensing performance in an instant and through-fiber\ndetection scheme, achieving a remarkable detection limit at 10-7 M for the R6G\naqueous solution. These engineered probes demonstrate an improved detection\nlimit by one order of magnitude and enhanced peak prominence compared to\ndevices solely decorated with pristine NIs.",
        "A consensus has been reached in recent years that binarity plays an important\nrole in the formation and evolution of a significant fraction of planetary\nnebulae (PNe). Utilizing the archived photometric data from the Zwicky\nTransient Facility survey, we conducted a comprehensive data mining in search\nfor brightness variations in a large sample of Galactic PNe. This effort leads\nto identification of 39 PNe, whose central stars exhibit periodic variation in\nlight curves. Among these objects, 20 are known binary central stars of PNe,\nwhile the remaining 19 are new discoveries. Additionally, we identified 14 PNe\nwith central stars displaying anomalous variation in light curves, as well as\neight variables based on the high-cadence photometric data. Among the new\ndiscoveries of periodicity, we found compelling evidence of binary systems at\nthe centres of two archetypal quadrupolar PNe. We also report on very peculiar\nbrightness variation observed in the central core of the compact PN NGC6833.\nSeveral PNe in our sample deserve follow-up observations, both high-dispersion\nspectroscopy and high-precision photometry, to reveal the true nature of their\ncentral binarity or even multiplicity.",
        "This study explores information measures based on extropy, introducing\ndynamic relative extropy measures for residual and past lifetimes, and\ninvestigating their various properties. Furthermore, the study analyzes the\nrelationships between extropy-based divergence with dynamic relative extropy\nand other extropy measures. A nonparametric estimator for relative extropy is\ndeveloped, and its performance is assessed through numerical simulation\nstudies. The practical applicability of the relative extropy is demonstrated\nthrough some real-life data sets.",
        "We show that if $G$ is a topological graph, and $f$ is continuous map, then\nthe induced map $\\tilde{f}$ acting on the hyperspace $C(G)$ of all connected\nsubsets of $G$ by natural formula $\\tilde{f}(C)=f(C)$ carries the same entropy\nas $f$.\n  This is well known that it does not hold on the larger hyperspace of all\ncompact subsets. Also negative examples were given for the hyperspace $C(X)$ on\nsome continua $X$, including dendrites.\n  Our work extends previous positive results obtained first for much simpler\ncase of compact interval by completely different tools.",
        "The Higgs boson discovery at the Large Hadron Collider (LHC) at CERN\nconfirmed the existence of the last missing particle of the Standard Model\n(SM). The existence of new fundamental constituents of matter beyond the SM is\nof great importance for our understanding of Nature. In this context, indirect\n(non-resonant) indications for new scalar bosons were found in the data from\nthe first run of the LHC, taken between 2010 and 2012 at CERN: an excess in the\ninvariant mass of muon-electron pairs, consistent with a new Higgs boson ($S$)\nwith a mass of $150\\pm5$ GeV. Other processes with multiple leptons in the\nfinal state, moderate missing energy, and possibly (bottom quark) jets exhibit\ndeviations from the SM predictions. These anomalies can be explained within a\nsimplified model in which a new heavy Higgs boson $H$ decays into two lighter\nHiggses $S$. This lighter Higgs $S$ subsequently decays to $W$ bosons, bottom\nquarks and has also an invisible decay mode.\n  Here, we demonstrate that using this model we can identify narrow excesses in\ndi-photon and $Z$-photon spectra around 152 GeV. By incorporating the latest\nmeasurements of di-photons in association with leptons, we obtain a combined\nglobal significance of $5.4\\sigma$. This represents the highest significance\never reported for an excess consistent with a narrow resonance beyond the SM\n(BSM) in high-energy proton-proton collision data at the LHC. Such findings\nhave the potential to usher in a new era in particle physics - the BSM epoch -\noffering crucial insights into unresolved puzzles of nature.",
        "We uncover the emergence of multiple topological Anderson insulators (TAIs)\nin a 1D spin-orbit coupled (SOC) chain driven by Fibonacci modulation,\ntransforming a trivial band structure into a cascade of topologically\nnontrivial phases. This intriguing phenomenon is marked by the appearance of\nzero-energy modes and transitions in the $\\mathcal{Z}_2$ topological quantum\nnumber. Strikingly, as the SOC amplitude decreases, the number of TAI phases\ngrows, a behavior intricately linked to the fractal structure of the energy\nspectrum induced by Fibonacci modulation. Unlike conventional TAI phases, which\nexhibit fully localized eigenstates, the wave functions in the\nFibonacci-modulated TAI phases exhibit multifractal behavior. Furthermore, this\nmodel can be experimentally realized in a Bose-Einstein condensate along the\nmomentum lattice, where its topological transitions and multifractal properties\ncan be probed through quench dynamics. Our findings open new avenues for\nexploring exotic disorder-induced topological phases and their intricate\nmultifractal nature.",
        "We show that the sheared graphene bilayers can be tuned to have flat\nlow-energy bands for sufficiently large size of their moir\\'e supercell. In\nthis regime, the interacting system becomes prone to develop broken-symmetry\nphases, with valley symmetry breaking as the dominant pattern. The strong\nsignal of symmetry breaking favors the onset of a pairing instability in which\nthe electrons with opposite spin projection in the Cooper pairs live in\ndifferent valleys. The Fermi lines become distorted due to the repulsive\nCoulomb interaction, which makes the screening highly anisotropic, leading\neasily to attraction in some of the interaction channels. We also show that the\nsheared graphene bilayers offer the possibility to realize the combined\nbreakdown of parity and valley symmetry, making them very suitable to study the\ninterplay between correlations and topology in a two-dimensional electron\nsystem.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association",
    "start_abstract":"We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.",
    "start_categories":[
      "cs.DC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "A Global Geometric Framework for Nonlinear Dimensionality Reduction"
      ],
      "abstract":[
        "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure."
      ],
      "categories":[
        "math.ST"
      ]
    },
    "list":{
      "title":[
        "Partial domination of middle graphs",
        "Thermodynamical limits for models of car-sharing systems: the Autolib\n  example",
        "Regular rigid Korovin orbits",
        "Hazard Rate for Associated Data in Deconvolution Problems: Asymptotic\n  Normality",
        "Multi-View Clustering Meets High-Dimensional Mixed Data: A Fusion\n  Regularized Method",
        "Asymptotics of solutions to the porous medium equation near conical\n  singularities",
        "Second Quantization and Evolution Operators in infinite dimension",
        "Constructions of Covering Sequences and Arrays",
        "Simultaneous analysis of approximate leave-one-out cross-validation and\n  mean-field inference",
        "On a non-local area-preserving curve flow",
        "Quasisymmetric mappings in b-metric spaces",
        "CoHAs of Torsion Sheaves on Weighted Projective Curves",
        "The feasibility of multi-graph alignment: a Bayesian approach",
        "Efficient inference of rankings from multi-body comparisons",
        "GWSkyNet-Multi II: an updated deep learning model for rapid\n  classification of gravitational-wave events",
        "On the components of random geometric graphs in the dense limit",
        "Infinitely many saturated travelling waves for epidemic models with\n  distributed-contacts",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Chance-Constrained Covariance Steering for Discrete-Time Markov Jump\n  Linear Systems",
        "Noncommutative Novikov bialgebras and differential antisymmetric\n  infinitesimal bialgebras with weight",
        "The second-order intrinsic Wiedemann-Franz law",
        "A model calculation of the CKM matrix",
        "A Unified Blockwise Measurement Design for Learning Quantum Channels and\n  Lindbladians via Low-Rank Matrix Sensing",
        "Generalized Recurrence Criteria for Classes of Open Quantum Walks",
        "Survival Concept-Based Learning Models",
        "Is a phonon excitation of a superfluid Bose gas a Goldstone boson?",
        "Multiple Horn problems for planar networks and invertible matrices",
        "Torsion in Magnitude homology theories"
      ],
      "abstract":[
        "For any graph $G=(V,E)$, a subset $S\\subseteq V$ is called {\\it an isolating\nset} of $G$ if $V\\setminus N_G[S]$ is an independent set of $G$, where\n$N_G[S]=S\\cup N_G(S)$, and {\\it the isolation number} of $G$, denoted by\n$\\iota(G)$, is the size of a smallest isolating set of $G$. In this article, we\nshow that the isolation number of the middle graph of $G$ is equal to the size\nof a smallest maximal matching of $G$.",
        "We analyze mean-field equations obtained for models motivated by a large\nstation-based car-sharing system in France called Autolib. The main focus is on\na version where users reserve a parking space when they take a car. In a first\nmodel, the reservation of parking spaces is effective for all users (see [4])\nand capacity constraints are ignored. The model is carried out in\nthermodynamical limit, that is when the number $N$ of stations and the number\nof cars $M_N$ tend to infinity, with $U = \\lim_{N\\to\\infty} M_N\/N$. This limit\nis described by Kolmogorov equations of a two-dimensional time-inhomogeneous\nMarkov process depicting the numbers of reservations and cars at a station. It\nsatisfies a non-linear differential system. We prove analytically that this\nsystem has a unique solution, which converges, as $t\\to\\infty$, to an\nequilibrium point exponentially fast. Moreover, this equilibrium point\ncorresponds to the stationary distribution of a two queue tandem (reservations,\ncars), which is here always ergodic. The intensity factor of each queue has an\nexplicit form obtained from an intrinsic mass conservation relationship. Two\nrelated models with capacity constraints are briefly presented in the last\nsection: the simplest one with no reservation leads to a one-dimensional\nproblem; the second one corresponds to our first model with finite total\ncapacity $K$.",
        "An example of an infinite regular feebly compact quasitopological group is\npresented such that all continuous real-valued functions on the group are\nconstant. The example is based on the use of Korovin orbits in $X^G$, where $X$\nis a special regular countably compact space constructed by S.Bardyla and\nL.Zdomskyy and $G$ is an abstract Abelian group of an appropriate cardinality.\nAlso, we study the interplay between the separation properties of the space $X$\nand Korovin orbits in $X^G$. We show in particular that if $X$ contains two\nnonempty disjoint open subsets, then every Korovin orbit in $X^G$ is Hausdorff.",
        "In reliability theory and survival analysis, observed data are often weakly\ndependent and subject to additive measurement errors. Such contamination arises\nwhen the underlying data are neither independent nor strongly mixed but instead\nexhibit association. This paper focuses on estimating the hazard rate by\ndeconvolving the density function and constructing an estimator of the\ndistribution function. We assume that the data originate from a strictly\nstationary sequence satisfying association conditions. Under appropriate\nsmoothness assumptions on the error distribution, we establish the\nquadratic-mean convergence and asymptotic normality of the proposed estimators.\nThe finite-sample performance of both the hazard rate and distribution function\nestimators is evaluated through a simulation study. We conclude with a\ndiscussion of open problems and potential future research directions.",
        "Multi-view clustering leverages consistent and complementary information\nacross multiple views to provide more comprehensive insights than analysis of\nsingle-view data. However, the heterogeneity and redundancy of high-dimensional\nmixed multi-view data pose significant challenges to the existing clustering\ntechniques. In this paper, we propose a novel multi-view fusion regularized\nclustering method with adaptive group sparsity, enabling reliable clustering\nwhile effectively capturing local features. Technically, for multi-view data\nwith mixed features exhibiting different distributions, different losses or\ndivergence metrics are considered with a collective fusion penalty to obtain\ncommon groups. Moreover, the non-convex group sparsity consisting of\ninter-group sparsity and intra-group sparsity is utilized to screen informative\nfeatures, thereby enhancing the robustness. Furthermore, we develop an\neffective proximal alternating direction method of multipliers (ADMM) and each\nsubproblem admits a closed-form solution. It is rigorously proven that this\nalgorithm globally converges to a Karush-Kuhn-Tucker (KKT) point, while\nestablishing the equivalence between local minimum points and KKT points within\na certain region. Extensive numerical experiments on both simulated and real\ndata validate the superior performance of the presented method in clustering\naccuracy and feature selection.",
        "We show that, on a manifold with conical singularities, the geometry of the\ncross-section is reflected in the solutions to the porous medium equation near\nthe conic points: We prove that the asymptotics of the solutions near the\nconical points are determined by the spectrum of the Laplacian on the\ncross-section. The key to this result is a precise description of the maximal\ndomain of the cone Laplacian.",
        "In an infinite dimensional separable Hilbert space $X$, we study compactness\nproperties and the hypercobtractivity of the realizations of Ornstein-Uhlenbeck\nevolution operators $P_{s,t}$ in the spaces $L^p(X,\\gamma_t)$,\n$\\{\\gamma_t\\}_{t\\in\\R}$ being a suitable evolution system of measures for\n$P_{s,t}$. Moreover we study the asymptotic behavior of $P_{s,t}$. All these\nresults are produced thank to a representation of $P_{s,t}$ through the second\nquantization operator. Among the examples we consider the transition evolution\noperator associated to a non-autonomous stochastic parabolic PDE.",
        "An $(n,R)$-covering sequence is a cyclic sequence whose consecutive\n$n$-tuples form a code of length $n$ and covering radius $R$. Using several\nconstruction methods improvements of the upper bounds on the length of such\nsequences for $n \\leq 20$ and $1 \\leq R \\leq 3$, are obtained. The definition\nis generalized in two directions. An $(n,m,R)$-covering sequence code is a set\nof cyclic sequences of length $m$ whose consecutive $n$-tuples form a code of\nlength~$n$ and covering radius $R$. The definition is also generalized to\narrays in which the $m \\times n$ sub-matrices form a covering code with\ncovering radius $R$. We prove that asymptotically there are covering sequences\nthat attain the sphere-covering bound up to a constant factor.",
        "Approximate Leave-One-Out Cross-Validation (ALO-CV) is a method that has been\nproposed to estimate the generalization error of a regularized estimator in the\nhigh-dimensional regime where dimension and sample size are of the same order,\nthe so called ``proportional regime''. A new analysis is developed to derive\nthe consistency of ALO-CV for non-differentiable regularizer under Gaussian\ncovariates and strong-convexity of the regularizer. Using a conditioning\nargument, the difference between the ALO-CV weights and their counterparts in\nmean-field inference is shown to be small. Combined with upper bounds between\nthe mean-field inference estimate and the leave-one-out quantity, this provides\na proof that ALO-CV approximates the leave-one-out quantity as well up to\nnegligible error terms. Linear models with square loss, robust linear\nregression and single-index models are explicitly treated.",
        "In this paper, we study a new area-preserving curvature flow for closed\nconvex planar curves. This flow will decrease the length of the evolving curve\nand make the curve more and more circular during the evolution process. And\nfinally, the curve converges to a finite circle in $C^{\\infty}$ sense as time\ngoes to infinity.",
        "Considering quasisymmetric mappings between b-metric spaces we have found a\nnew estimation for the ratio of diameters of two subsets which are images of\ntwo bounded subsets. This result generalizes the well-known\nTukia-V\\\"{a}is\\\"{a}l\\\"{a} inequality. The condition under which the image of a\nb-metric space under quasisymmetry is also a b-metric space is established.\nMoreover, the latter question is investigated for additive metric spaces.",
        "We describe the cohomological Hall algebra of torsion sheaves on a weighted\nprojective line with weights $(2, \\dots, 2)$ in terms of generators and\nrelations.",
        "We establish thresholds for the feasibility of random multi-graph alignment\nin two models. In the Gaussian model, we demonstrate an \"all-or-nothing\"\nphenomenon: above a critical threshold, exact alignment is achievable with high\nprobability, while below it, even partial alignment is statistically\nimpossible. In the sparse Erd\\H{o}s-R\\'enyi model, we rigorously identify a\nthreshold below which no meaningful partial alignment is possible and\nconjecture that above this threshold, partial alignment can be achieved. To\nprove these results, we develop a general Bayesian estimation framework over\nmetric spaces, which provides insight into a broader class of high-dimensional\nstatistical problems.",
        "Many of the existing approaches to assess and predict the performance of\nplayers, teams or products in competitive contests rely on the assumption that\ncomparisons occur between pairs of such entities. There are, however, several\nreal contests where more than two entities are part of each comparison, e.g.,\nsports tournaments,multiplayer board and card games, and preference surveys.\nThe Plackett-Luce (PL) model provides a principled approach to infer the\nranking of entities involved in such contests characterized by multi-body\ncomparisons. Unfortunately, traditional algorithms used to compute PL rankings\nsuffer from slow convergence limiting the application of the PL model to\nrelatively small-scale systems. We present here an alternative implementation\nthat allows for significant speed-ups and validate its efficiency in both\nsynthetic and real-world sets of data. Further, we perform systematic\ncross-validation tests concerning the ability of the PL model to predict\nunobserved comparisons. We find that a PL model trained on a set composed of\nmulti-body comparisons is more predictive than a PL model trained on a set of\nprojected pairwise comparisons derived from the very same training set,\nemphasizing the need of properly accounting for the true multi-body nature of\nreal-world systems whenever such an information is available.",
        "Multi-messenger observations of gravitational waves and electromagnetic\nemission from compact object mergers offer unique insights into the structure\nof neutron stars, the formation of heavy elements, and the expansion rate of\nthe Universe. With the LIGO-Virgo-KAGRA (LVK) gravitational-wave detectors\ncurrently in their fourth observing run (O4), it is an exciting time for\ndetecting these mergers. However, assessing whether to follow up a candidate\ngravitational-wave event given limited telescope time and resources is\nchallenging; the candidate can be a false alert due to detector glitches, or\nmay not have any detectable electromagnetic counterpart even if it is real.\nGWSkyNet-Multi is a deep learning model developed to facilitate follow-up\ndecisions by providing real-time classification of candidate events, using\nlocalization information released in LVK rapid public alerts. Here we introduce\nGWSkyNet-Multi II, an updated model targeted towards providing more robust and\ninformative predictions during O4 and beyond. Specifically, the model now\nprovides normalized probability scores and associated uncertainties for each of\nthe four corresponding source categories released by the LVK: glitch, binary\nblack hole, neutron star-black hole, and binary neutron star. Informed by\nexplainability studies of the original model, the updated model architecture is\nalso significantly simplified, including replacing input images with intuitive\nsummary values, making it more interpretable. For significant O4 event alerts\nissued between May 2023 and December 2024, GWSkyNet-Multi II produces a\nprediction that is consistent with the updated LVK classification for 93% of\nevents. The updated model can be used by the community to help make\ntime-critical follow-up decisions.",
        "Consider the geometric graph on $n$ independent uniform random points in a\nconnected compact region $A$ of ${\\bf R}^d, d \\geq 2$ with $C^2$ boundary, or\nin the unit square, with distance parameter $r_n$. Let $K_n$ be the number of\ncomponents of this graph, and $R_n$ the number of vertices not in the giant\ncomponent. Let $S_n$ be the number of isolated vertices. We show that if $r_n$\nis chosen so that $nr_n^d$ tends to infinity but slowly enough that ${\\bf\nE}[S_n]$ also tends to infinity, then $K_n$, $R_n$ and $S_n$ are all asymptotic\nto $\\mu_n$ in probability as $n \\to \\infty$ where (with $|A|$, $\\theta_d$ and\n$|\\partial A|$ denoting the volume of $A$, of the unit $d$-ball, and the\nperimeter of $A$ respectively) $\\mu_n := ne^{-\\pi n r_n^d\/|A|}$ if $d=2$ and\n$\\mu_n := ne^{-\\theta_d n r_n^d\/|A|} + \\theta_{d-1}^{-1} |\\partial A| r_n^{1-d}\ne^{- \\theta_d n r_n^d\/(2|A|)}$ if $d\\geq 3$. We also give variance asymptotics\nand central limit theorems for $K_n$ and $R_n$ in this limiting regime when $d\n\\geq 3$, and for Poisson input with $d \\geq 2$. We extend these results\n(substituting ${\\bf E}[S_n]$ for $\\mu_n$) to a class of non-uniform\ndistributions on $A$.",
        "We consider an epidemic model with distributed-contacts. When the contact\nkernel concentrates, one formally reaches a very degenerate Fisher-KPP equation\nwith a diffusion term that is not in divergence form. We make an exhaustive\nstudy of its travelling waves. For every admissible speed, there exists not\nonly a non-saturated (smooth) wave but also infinitely many saturated (sharp)\nones. Furthermore their tails may differ from what is usually expected. These\nresults are thus in sharp contrast with their counterparts on related models.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "In this paper, we propose a novel convex optimization framework to solve the\noptimal covariance steering problem for discrete-time Markov Jump Linear\nSystems (MJLS) with chance constraints. We derive the analytical expressions\nfor the mean and covariance trajectories of time-varying discrete-time MJLS and\nshow that they cannot be separated even without chance constraints, unlike the\nsingle-mode dynamics case. To solve the covariance steering problem, we propose\na two-step convex optimization framework, which optimizes the mean and\ncovariance subproblems sequentially. Further, we use Gaussian approximations to\nincorporate chance constraints and propose an iterative optimization framework\nto solve the chance-constrained covariance steering problem. Both problems are\noriginally nonconvex, and we derive convex relaxations which are proved to be\nlossless at optimality using the Karush-Kuhn-Tucker (KKT) conditions. Numerical\nsimulations demonstrate the proposed method by achieving target covariances\nwhile respecting chance constraints under Gaussian noise and Markovian jump\ndynamics.",
        "This paper first develops a bialgebra theory for a noncommutative Novikov\nalgebra, called a noncommutative Novikov bialgebra, which is further\ncharacterized by matched pairs and Manin triples of noncommutative Novikov\nalgebras. The classical Yang-Baxter type equation, $\\mathcal{O}$-operators, and\nnoncommutative pre-Novikov algebras are introduced to study noncommutative\nNovikov bialgebra. As an application, noncommutative pre-Novikov algebras are\nobtained from differential dendriform algebras. Next, to generalize Gelfand's\nclassical construction of a Novikov algebra from a commutative differential\nalgebra to the bialgebra context in the noncommutative case, we establish\nantisymmetric infinitesimal (ASI) bialgebras for (noncommutative) differential\nalgebras, and obtain the condition under which a differential ASI bialgebra\ninduces a noncommutative Novikov bialgebra.",
        "In recent years, the nonlinear anomalous thermal Hall effect has attracted\nsubstantial attention. In this paper, we carry out a theoretical exploration of\nthe intrinsic anomalous thermal Hall and Nernst effect that is induced by the\nthermal Berry connection polarizability. This effect is independent of the\nrelaxation time and can be present in antiferromagnets possessing PT symmetry.\nAdditionally, we put forward a second-order intrinsic Wiedemann-Franz law,\nwhich represents the ratio of the second-order intrinsic thermal conductivity\ncoefficient to the second-order intrinsic electrical conductivity coefficient .\nWhen analyzed within a four-band PT symmetric Dirac model, we observe that the\nsecond-order intrinsic thermal conductivity coefficient is linearly\nproportional to the second-order intrinsic electrical conductivity coefficient\n, and the second-order intrinsic Wiedemann-Franz law is characterized by the\nchemical potential $\\mu$ in the low-temperature regime. These findings provide\nsignificant implications for experimental verification.",
        "We propose a strategy to compute the CKM matrix based on the conjecture,\nrecently put forward in the literature, according to which elementary particle\nmasses are not generated like in the standard Higgs scenario, but emerge from a\nnon-perturbative mechanism triggered by the presence in the fundamental\nLagrangian of ``irrelevant'' chiral breaking operators of the Wilson type of\ndimension $d\\geq 6$ scaled by $d-4$ powers of the UV cutoff. Non-perturbatively\ngenerated quark masses have the form $m_q\\sim C_q(\\alpha) \\Lambda_{RGI}$ where\n$\\Lambda_{RGI}$ is the RGI scale of the theory and $C_q(\\alpha)$ is a function\nof the gauge couplings. For the (elementary) fermion $q$ the $C_q(\\alpha)$\nleading behaviour is $C_q(\\alpha)={\\mbox{O}}(\\alpha^{1+(d_q-4)\/2})$. The\ndependence of the gauge coupling power behaviour from the dimension $d_q$ of\nthe Wilson-like operators associated with the fermion $q$ can be exploited to\nconstruct hierarchically organized up and down ''proto-mass matrices'' for\n''proto-flavours'', the diagonalization of which yields flavoured quarks with\ndefinite masses and a first principle construction of the CKM matrix.",
        "Quantum superoperator learning is a pivotal task in quantum information\nscience, enabling accurate reconstruction of unknown quantum operations from\nmeasurement data. We propose a robust approach based on the matrix sensing\ntechniques for quantum superoperator learning that extends beyond the positive\nsemidefinite case, encompassing both quantum channels and Lindbladians. We\nfirst introduce a randomized measurement design using a near-optimal number of\nmeasurements. By leveraging the restricted isometry property (RIP), we provide\ntheoretical guarantees for the identifiability and recovery of low-rank\nsuperoperators in the presence of noise. Additionally, we propose a blockwise\nmeasurement design that restricts the tomography to the sub-blocks,\nsignificantly enhancing performance while maintaining a comparable scale of\nmeasurements. We also provide a performance guarantee for this setup. Our\napproach employs alternating least squares (ALS) with acceleration for\noptimization in matrix sensing. Numerical experiments validate the efficiency\nand scalability of the proposed methods.",
        "In this paper, we study the recurrence of open quantum walks (OQWs) induced\nby finite-dimensional coins $(L,B,R)$. The focus is on homogeneous OQWs with a\nset of vertices $\\mathbb{Z}$, the set of integers. We present three distinct\nrecurrence criteria, each adapted to different types of coins. The first\ncriterion was developed for a class of Lazy OQWs in any finite dimension, where\nthe presented criterion is associated with an auxiliary map and its only\ninvariant state, resulting in the first recurrence criterion for Lazy OQWs. The\nsecond one is restricted to Lazy OQWs of dimension 2, where we provide a\ncomplete characterization of the recurrence for this lower dimension. Finally,\nwe present a general criterion for finite-dimensional coins in the non-lazy\ncase $(B=0)$, which generalizes many of the previously known results. This new\ncriterion holds for irreducible and reducible OQWs through a decomposition of\nthe Hilbert space where our quantum states act.",
        "Concept-based learning enhances prediction accuracy and interpretability by\nleveraging high-level, human-understandable concepts. However, existing CBL\nframeworks do not address survival analysis tasks, which involve predicting\nevent times in the presence of censored data -- a common scenario in fields\nlike medicine and reliability analysis. To bridge this gap, we propose two\nnovel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM\n(Survival Regularized Concept-based Model), which integrate concept-based\nlearning with survival analysis to handle censored event time data. The models\nemploy the Cox proportional hazards model and the Beran estimator. SurvCBM is\nbased on the architecture of the well-known concept bottleneck model, offering\ninterpretable predictions through concept-based explanations. SurvRCM uses\nconcepts as regularization to enhance accuracy. Both models are trained\nend-to-end and provide interpretable predictions in terms of concepts. Two\ninterpretability approaches are proposed: one leveraging the linear\nrelationship in the Cox model and another using an instance-based explanation\nframework with the Beran estimator. Numerical experiments demonstrate that\nSurvCBM outperforms SurvRCM and traditional survival models, underscoring the\nimportance and advantages of incorporating concept information. The code for\nthe proposed algorithms is publicly available.",
        "It is generally accepted that phonons in a superfluid Bose gas are Goldstone\nbosons. This is justified by spontaneous symmetry breaking (SSB), which is\nusually defined as follows: the Hamiltonian of the system is invariant under\nthe $U(1)$ transformation $\\hat{\\Psi}(\\mathbf{r},t)\\rightarrow e^{i\\alpha}%\n\\hat{\\Psi}(\\mathbf{r},t)$, whereas the order parameter $\\Psi(\\mathbf{r},t)$ is\nnot. However, the strict definition of SSB is different: the Hamiltonian and\nthe boundary conditions are invariant under a symmetry transformation, while\nthe \\emph{ground state} is not. Based on the latter criterion, we study a\nfinite system of spinless, weakly interacting bosons using three approaches:\nthe standard Bogoliubov method, the particle-number-conserving Bogoliubov\nmethod, and the approach based on the exact ground-state wave function. Our\nresults show that the answer to the question in the title is \\textquotedblleft\nno\\textquotedblright. Thus, phonons in a real-world (finite) superfluid Bose\ngas are similar to sound in a classical gas: they are not Goldstone bosons, but\nquantised collective vibrational modes arising from the interaction between\natoms. In the case of an infinite Bose gas, however, the picture becomes\nparadoxical: the ground state can be regarded as either infinitely degenerate\nor non-degenerate, making the phonon both similar to a Goldstone boson and\ndifferent from it.",
        "The multiplicative multiple Horn problem is asking to determine possible\nsingular values of the combinations $AB, BC$ and $ABC$ for a triple of\ninvertible matrices $A,B,C$ with given singular values. There are similar\nproblems for eigenvalues of sums of Hermitian matrices (the additive problem),\nand for maximal weights of multi-paths in concatenations of planar networks\n(the tropical problem).\n  For the planar network multiple Horn problem, we establish necessary\nconditions, and we conjecture that for large enough networks they are also\nsufficient. These conditions are given by the trace equalities and rhombus\ninequalities (familiar from the hive description of the classical Horn\nproblem), and by the new set of tetrahedron equalities. Furthermore, if one\nimposes Gelfand-Zeitlin conditions on weights of planar networks, tetrahedron\nequalities turn into the octahedron recurrence from the theory of crystals. We\ngive a geometric interpretation of our results in terms of positive varieties\nwith potential. In this approach, rhombus inequalities follow from the\ninequality $\\Phi^t \\leqslant 0$ for the tropicalized potential, and tetrahedron\nequalities are obtained as tropicalization of certain Pl\\\"ucker relations.\n  For the multiplicative problem, we introduce a scaling parameter $s$, and we\nshow that for $s$ large enough (corresponding to exponentially large\/small\nsingular values) the Duistermaat-Heckman measure associated to the\nmultiplicative problem concentrates in a small neighborhood of the octahedron\nrecurrence locus.",
        "In this article, we analyze the structure and relationships between magnitude\nhomology and Eulerian magnitude homology of finite graphs. Building on the work\nof Kaneta and Yoshinaga, Sazdanovic and Summers, and Asao and Izumihara, we\nprovide two proofs of the existence of torsion in Eulerian magnitude homology,\noffer insights into the types and orders of torsion, and present explicit\ncomputations for various classes of graphs."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"A Global Geometric Framework for Nonlinear Dimensionality Reduction",
    "start_abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure.",
    "start_categories":[
      "math.ST"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association"
      ],
      "abstract":[
        "We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure."
      ],
      "categories":[
        "cs.DC"
      ]
    },
    "list":{
      "title":[
        "ED-DAO: Energy Donation Algorithms based on Decentralized Autonomous\n  Organization",
        "Access Specification-Aware Software Transactional Memory Techniques for\n  Efficient Execution of Smart Contract Transactions",
        "GMB-ECC: Guided Measuring and Benchmarking of the Edge Cloud Continuum",
        "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
        "Byzantine-Tolerant Consensus in GPU-Inspired Shared Memory",
        "Performance Models for a Two-tiered Storage System",
        "Light Virtualization: a proof-of-concept for hardware-based\n  virtualization",
        "NM-SpMM: Accelerating Matrix Multiplication Using N:M Sparsity with\n  GPGPU",
        "Atomic Smart Contract Interoperability with High Efficiency via\n  Cross-Chain Integrated Execution",
        "MonadBFT: Fast, Responsive, Fork-Resistant Streamlined Consensus",
        "Understanding the Communication Needs of Asynchronous Many-Task Systems\n  -- A Case Study of HPX+LCI",
        "Closing a Source Complexity Gap between Chapel and HPX",
        "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
        "The effect of accretion on scalar superradiant instability",
        "Scaling of the elastic proton-proton cross-section",
        "Feedback-enhanced squeezing or cooling of fluctuations in a parametric\n  resonator",
        "Ising superconductivity in noncentrosymmetric bulk NbSe2",
        "Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "Selection Function of Clusters in Dark Energy Survey Year 3 Data from\n  Cross-Matching with South Pole Telescope Detections",
        "The Complexity of Local Stoquastic Hamiltonians on 2D Lattices",
        "Neural cyberattacks applied to the vision under realistic visual stimuli",
        "Proof-Producing Translation of Functional Programs into a Time \\& Space\n  Reasonable Model",
        "Neural Radiance Fields for the Real World: A Survey",
        "Israel-Hamas war through Telegram, Reddit and Twitter",
        "Roles of the $N(1535)$ and $a_0(980)$ in the process $\\Lambda_c^+ \\to\n  \\pi^+\\eta n$",
        "Resurgence of the Tilted Cusp Anomalous Dimension",
        "New Frontiers in Fighting Misinformation"
      ],
      "abstract":[
        "Energy is a fundamental component of modern life, driving nearly all aspects\nof daily activities. As such, the inability to access energy when needed is a\nsignificant issue that requires innovative solutions. In this paper, we propose\nED-DAO, a novel fully transparent and community-driven decentralized autonomous\norganization (DAO) designed to facilitate energy donations. We analyze the\nenergy donation process by exploring various approaches and categorizing them\nbased on both the source of donated energy and funding origins. We propose a\nnovel Hybrid Energy Donation (HED) algorithm, which enables contributions from\nboth external and internal donors. External donations are payments sourced from\nentities such as charities and organizations, where energy is sourced from the\nutility grid and prosumers. Internal donations, on the other hand, come from\npeer contributors with surplus energy. HED prioritizes donations in the\nfollowing sequence: peer-sourced energy (P2D), utilitygrid-sourced energy\n(UG2D), and direct energy donations by peers (P2PD). By merging these donation\napproaches, the HED algorithm increases the volume of donated energy, providing\na more effective means to address energy poverty. Experiments were conducted on\na dataset to evaluate the effectiveness of the proposed method. The results\nshowed that HED increased the total donated energy by at least 0.43% (64\nmegawatts) compared to the other algorithms (UG2D, P2D, and P2PD).",
        "For a high-performance blockchain like Supra's Layer 1, minimizing latencies\nacross key components is crucial-such as data dissemination, consensus (or\nordering), and transaction execution. While through significant innovations we\nhave improved the first two, transaction execution remains an area for further\noptimization. Software Transactional Memory (STM) is a widely used technique\nfor parallel execution, with Aptos' BlockSTM pioneering its application of\nefficient blockchain transaction processing on multi-core validator nodes.\nSubsequently, PEVM [13] adapted BlockSTM for EVM transaction execution.\nHowever, we identified a gap in existing STM techniques-while access\nspecifications have been used in industry (e.g., Solana's user-provided\nread-write sets), they have not been leveraged to enhance STM efficiency. Our\nexperimental analysis demonstrates that specification-aware STMs outperform\ntheir plain counterparts on both EVM and MoveVM. To maximize these benefits, we\nhave designed specification-aware SupraSTM (saSupraSTM), a novel algorithm that\nfully utilizes access specifications. Through extensive testing, saSupraSTM\noutperforms both our specification-aware adaptation of Aptos' BlockSTM and\nspecification-aware PEVM, setting a new benchmark for transaction execution\nefficiency in the context of blockchain networks.",
        "In the evolving landscape of cloud computing, optimizing energy efficiency\nacross the edge-cloud continuum is crucial for sustainability and\ncost-effectiveness. We introduce GMB-ECC, a framework for measuring and\nbenchmarking energy consumption across the software and hardware layers of the\nedge-cloud continuum. GMB-ECC enables energy assessments in diverse\nenvironments and introduces a precision parameter to adjust measurement\ncomplexity, accommodating system heterogeneity. We demonstrate GMB-ECC's\napplicability in an autonomous intra-logistic use case, highlighting its\nadaptability and capability in optimizing energy efficiency without\ncompromising performance. Thus, this framework not only assists in accurate\nenergy assessments but also guides strategic optimizations, cultivating\nsustainable and cost-effective operations.",
        "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
        "In this work, we formalize a novel shared memory model inspired by the\npopular GPU architecture. Within this model, we develop algorithmic solutions\nto the Byzantine Consensus problem.",
        "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read\/write workloads. The paper provides examples to\nillustrate the use of these models.",
        "Virtualization has become widespread across all computing environments, from\nedge devices to cloud systems. Its main advantages are resource management\nthrough abstraction and improved isolation of platform resources and processes.\nHowever, there are still some important tradeoffs as it requires significant\nsupport from the existing hardware infrastructure and negatively impacts\nperformance. Additionally, the current approaches to resource virtualization\nare inflexible, using a model that doesn't allow for dynamic adjustments during\noperation. This research introduces Light Virtualization (LightV), a new\nvirtualization method for commercial platforms. LightV uses programmable\nhardware to direct cache coherence traffic, enabling precise and seamless\ncontrol over which resources are virtualized. The paper explains the core\nprinciples of LightV, explores its capabilities, and shares initial findings\nfrom a basic proof-of-concept module tested on commercial hardware.",
        "Deep learning demonstrates effectiveness across a wide range of tasks.\nHowever, the dense and over-parameterized nature of these models results in\nsignificant resource consumption during deployment. In response to this issue,\nweight pruning, particularly through N:M sparsity matrix multiplication, offers\nan efficient solution by transforming dense operations into semi-sparse ones.\nN:M sparsity provides an option for balancing performance and model accuracy,\nbut introduces more complex programming and optimization challenges. To address\nthese issues, we design a systematic top-down performance analysis model for\nN:M sparsity. Meanwhile, NM-SpMM is proposed as an efficient general N:M\nsparsity implementation. Based on our performance analysis, NM-SpMM employs a\nhierarchical blocking mechanism as a general optimization to enhance data\nlocality, while memory access optimization and pipeline design are introduced\nas sparsity-aware optimization, allowing it to achieve close-to-theoretical\npeak performance across different sparsity levels. Experimental results show\nthat NM-SpMM is 2.1x faster than nmSPARSE (the state-of-the-art for general N:M\nsparsity) and 1.4x to 6.3x faster than cuBLAS's dense GEMM operations, closely\napproaching the theoretical maximum speedup resulting from the reduction in\ncomputation due to sparsity. NM-SpMM is open source and publicly available at\nhttps:\/\/github.com\/M-H482\/NM-SpMM.",
        "With the development of Ethereum, numerous blockchains compatible with\nEthereum's execution environment (i.e., Ethereum Virtual Machine, EVM) have\nemerged. Developers can leverage smart contracts to run various complex\ndecentralized applications on top of blockchains. However, the increasing\nnumber of EVM-compatible blockchains has introduced significant challenges in\ncross-chain interoperability, particularly in ensuring efficiency and atomicity\nfor the whole cross-chain application. Existing solutions are either limited in\nguaranteeing overall atomicity for the cross-chain application, or inefficient\ndue to the need for multiple rounds of cross-chain smart contract execution. To\naddress this gap, we propose IntegrateX, an efficient cross-chain\ninteroperability system that ensures the overall atomicity of cross-chain smart\ncontract invocations. The core idea is to deploy the logic required for\ncross-chain execution onto a single blockchain, where it can be executed in an\nintegrated manner. This allows cross-chain applications to perform all\ncross-chain logic efficiently within the same blockchain. IntegrateX consists\nof a cross-chain smart contract deployment protocol and a cross-chain smart\ncontract integrated execution protocol. The former achieves efficient and\nsecure cross-chain deployment by decoupling smart contract logic from state,\nand employing an off-chain cross-chain deployment mechanism combined with\non-chain cross-chain verification. The latter ensures atomicity of cross-chain\ninvocations through a 2PC-based mechanism, and enhances performance through\ntransaction aggregation and fine-grained state lock. We implement a prototype\nof IntegrateX. Extensive experiments demonstrate that it reduces up to 61.2%\nlatency compared to the state-of-the-art baseline while maintaining low gas\nconsumption.",
        "This paper introduces MonadBFT, a novel Byzantine Fault Tolerant (BFT)\nconsensus protocol designed to significantly enhance both performance and\nscalability. MonadBFT achieves linear message and authenticator complexity on\nthe happy path, enabling it to improve decentralization. It achieves\nspeculative finality within a single round and is optimistically responsive.\nThe speculative mechanism is refined such that only block equivocation can\nrevert speculative execution, enabling the protocol to ensure accountability\nfor malicious behavior. A notable innovation of MonadBFT is its built-in\nresistance to a specific form of Maximal Extractable Value (MEV) vulnerability\nknown as tail-forking. Tail-forking occurs when a malicious leader forks away\nfrom its predecessor's block, causing that block to be abandoned and depriving\nthe predecessor of rewards. This allows the malicious leader to reorder, steal,\nor exploit transactions, thereby exacerbating MEV exploitation. MonadBFT\neffectively mitigates such vulnerabilities, ensuring fairness and integrity in\ntransaction processing. To our knowledge, no other pipelined leader-based BFT\nconsensus protocol combines all these features.",
        "Asynchronous Many-Task (AMT) systems offer a potential solution for\nefficiently programming complicated scientific applications on extreme-scale\nheterogeneous architectures. However, they exhibit different communication\nneeds from traditional bulk-synchronous parallel (BSP) applications, posing new\nchallenges for underlying communication libraries. This work systematically\nstudies the communication needs of AMTs and explores how communication\nlibraries can be structured to better satisfy them through a case study of a\nreal-world AMT system, HPX. We first examine its communication stack layout and\nformalize the communication abstraction that underlying communication libraries\nneed to support. We then analyze its current MPI backend (parcelport) and\nidentify four categories of needs that are not typical in the BSP model and are\nnot well covered by the MPI standard. To bridge these gaps, we design from the\nnative network layer and incorporate various techniques, including one-sided\ncommunication, queue-based completion notification, explicit progressing, and\ndifferent ways of resource contention mitigation, in a new parcelport with an\nexperimental communication library, LCI. Overall, the resulting LCI parcelport\noutperforms the existing MPI parcelport with up to 50x in microbenchmarks and\n2x in a real-world application. Using it as a testbed, we design LCI parcelport\nvariants to quantify the performance contributions of each technique. This work\ncombines conceptual analysis and experiment results to offer a practical\nguideline for the future development of communication libraries and AMT\ncommunication layers.",
        "A previous case study measured performance vs source-code complexity across\nmultiple languages. The case study identified Chapel and HPX provide similar\nperformance and code complexity. This paper is the result of initial steps\ntoward closing the source-code complexity gap between Chapel and HPX by using a\nsource-to-source compiler. The investigation assesses the single-machine\nperformance of both Chapel and Chplx applications across Arm and x86.",
        "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
        "Superradiance can lead to the formation of a black hole (BH) condensate\nsystem. We thoroughly investigate the accretion effect on the evolution of this\nsystem, and the gravitational wave signals it emits in the presence of multiple\nsuperradiance modes. Assuming the multiplication of the BH mass and scalar mass\nas a small number, we obtain the analytical approximations of all important\nquantities, which can be directly applied to phenomenological studies. In\naddition, we confirm that accretion could significantly enhance the\ngravitational wave (GW) emission and reduce its duration, and show that the GW\nbeat signature is similarly modified.",
        "We discuss scaling properties of the elastic $pp$ cross-section both at the\nISR and the LHC. We observe that the ratio of bump to dip positions of the\ndifferential cross-section $d\\sigma_{\\rm el}\/dt$ is constant over a wide energy\nrange. We next study the consequences of this property, including geometric\nscaling at the ISR and new scaling laws at the LHC.",
        "Here we analyse ways to achieve deep subthreshold parametric squeezing of\nfluctuations beyond the $-6$~dB limit of single degree-of-freedom parametric\nresonators. One way of accomplishing this is via a lock-in amplifier feedback\nloop. Initially, we calculate the phase-dependent parametric amplification with\nfeedback of an added ac signal. In one approach, we use the averaging method to\nobtain the amplification gain, while in the second approach, we obtain the ac\nresponse of the parametric amplifier with feedback using the harmonic balance\nmethod. In this latter approach, the feedback is proportional to an integral\nterm that emulates the cosine quadrature output of a lock-in amplifier\nmultiplied by a sine at the same tone of the lock-in. We find that the gain\nobtained via these two methods are the same whenever the integration time span\nof the integral is a multiple of the tone period. When this is not the case, we\ncan obtain considerable deamplification. Finally, we analyse the response of\nthe parametric resonator with feedback, described by this integro-differential\nmodel, to an added white noise in the frequency domain. Using this model we\nwere able to calculate, in addition to squeezing, the noise spectral density in\nthis resonator with feedback. Very strong squeezing or cooling can be obtained.",
        "Ising superconductivity allows in-plane upper critical magnetic fields to\nvastly surpass Pauli limit by locking the antiparallel electron spins of Cooper\npairs in the out-of-plane direction. It was first explicitly demonstrated in\nfully two-dimensional monolayers of transition metal dichalcogenides with large\nspin-orbit coupling and broken inversion symmetry. Since then, several studies\nhave shown that it can be present in layered bulk materials, too. In our\nprevious study, we have clarified the underlying microscopic mechanism of Ising\nsuperconductivity in bulk, based on a reduced electronic coupling between\nsuperconducting layers due to intercalation by insulating layers and restricted\ninversion symmetry. But earlier studies suggest that in some transition metal\ndichalcogenide polytypes Pauli paramagnetic limit is violated even without\nintercalation. Here, using heat capacity measurements we unambiguously\ndemonstrate, that the pristine noncentrosymmetric bulk 4Ha-NbSe2 polytype\nsignificantly violates the Pauli limit. The band structure parameters obtained\nfrom ab initio calculations using the experimentally determined crystal\nstructure are used in the theoretical model which provides the microscopic\nmechanism of the Ising protection based solely on broken inversion symmetry.",
        "Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Galaxy clusters selected based on overdensities of galaxies in photometric\nsurveys provide the largest cluster samples. Yet modeling the selection\nfunction of such samples is complicated by non-cluster members projected along\nthe line of sight (projection effects) and the potential detection of\nunvirialized objects (contamination). We empirically constrain the magnitude of\nthese effects by cross-matching galaxy clusters selected in the Dark Energy\nsurvey data with the \\rdmpr$\\,$ algorithm with significant detections in three\nSouth Pole Telescope surveys (SZ, pol-ECS, pol-500d). For matched clusters, we\naugment the \\rdmpr$\\,$catalog by the SPT detection significance. For unmatched\nobjects we use the SPT detection threshold as an upper limit on the SZe\nsignature. Using a Bayesian population model applied to the collected\nmulti-wavelength data, we explore various physically motivated models to\ndescribe the relationship between observed richness and halo mass. Our analysis\nreveals the limitations of a simple lognormal scatter model in describing the\ndata. We rule out significant contamination by unvirialized objects at the\nhigh-richness end of the sample. While dedicated simulations offer a\nwell-fitting calibration of projection effects, our findings suggest the\npresence of redshift-dependent trends that these simulations may not have\ncaptured. Our findings highlight that modeling the selection function of\noptically detected clusters remains a complicated challenge, requiring a\ncombination of simulation and data-driven approaches.",
        "We show the 2-Local Stoquastic Hamiltonian problem on a 2D square lattice is\nStoqMA-complete. We achieve this by extending the spatially sparse circuit\nconstruction of Oliveira and Terhal, as well as the perturbative gadgets of\nBravyi, DiVincenzo, Oliveira, and Terhal. Our main contributions demonstrate\nStoqMA circuits can be made spatially sparse and that geometrical,\nstoquastic-preserving, perturbative gadgets can be constructed.",
        "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine\nand designed to interact with the brain to record or stimulate neurons. Despite\ntheir benefits, the literature has demonstrated that invasive BCIs focused on\nneurostimulation present vulnerabilities allowing attackers to gain control. In\nthis context, neural cyberattacks emerged as threats able to disrupt\nspontaneous neural activity by performing neural overstimulation or inhibition.\nPrevious work validated these attacks in small-scale simulations with a reduced\nnumber of neurons, lacking real-world complexity. Thus, this work tackles this\nlimitation by analyzing the impact of two existing neural attacks, Neuronal\nFlooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of\nthe primary visual cortex of mice consisting of approximately 230,000 neurons,\ntested on three realistic visual stimuli: flash effect, movie, and drifting\ngratings. Each attack was evaluated over three relevant events per stimulus,\nalso testing the impact of attacking 25% and 50% of the neurons. The results,\nbased on the number of spikes and shift percentages metrics, showed that the\nattacks caused the greatest impact on the movie, while dark and fixed events\nare the most robust. Although both attacks can significantly affect neural\nactivity, JAM was generally more damaging, producing longer temporal delays,\nand had a larger prevalence. Finally, JAM did not require to alter many neurons\nto significantly affect neural activity, while the impact in FLO increased with\nthe number of neurons attacked.",
        "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle\/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.",
        "Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.",
        "The Israeli-Palestinian conflict started on 7 October 2023, have resulted\nthus far to over 48,000 people killed including more than 17,000 children with\na majority from Gaza, more than 30,000 people injured, over 10,000 missing, and\nover 1 million people displaced, fleeing conflict zones. The infrastructure\ndamage includes the 87\\% of housing units, 80\\% of public buildings and 60\\% of\ncropland 17 out of 36 hospitals, 68\\% of road networks and 87\\% of school\nbuildings damaged. This conflict has as well launched an online discussion\nacross various social media platforms. Telegram was no exception due to its\nencrypted communication and highly involved audience. The current study will\ncover an analysis of the related discussion in relation to different\nparticipants of the conflict and sentiment represented in those discussion. To\nthis end, we prepared a dataset of 125K messages shared on channels in Telegram\nspanning from 23 October 2025 until today. Additionally, we apply the same\nanalysis in two publicly available datasets from Twitter containing 2001 tweets\nand from Reddit containing 2M opinions. We apply a volume analysis across the\nthree datasets, entity extraction and then proceed to BERT topic analysis in\norder to extract common themes or topics. Next, we apply sentiment analysis to\nanalyze the emotional tone of the discussions. Our findings hint at polarized\nnarratives as the hallmark of how political factions and outsiders mold public\nopinion. We also analyze the sentiment-topic prevalence relationship, detailing\nthe trends that may show manipulation and attempts of propaganda by the\ninvolved parties. This will give a better understanding of the online discourse\non the Israel-Palestine conflict and contribute to the knowledge on the\ndynamics of social media communication during geopolitical crises.",
        "We have investigated the process $\\Lambda_c^+ \\to \\pi^+\\eta n$ by taking into\naccount the contributions from the nucleon resonance $N(1535)$ and the scalar\nmeson $a_0(980)$, which could be dynamically generated by the interaction of\nthe $S$-wave pseudosalar meson-octet baryon and the $S$-wave pseudosalar\nmeson-pseudosalar meson, respectively. Our results show that, in $\\eta n$\ninvariant mass distribution, there is a significant near-threshold enhancement\nstructure, which could be associated with $N(1535)$. On the other hand, one can\nfind a clear cusp structure of $a_0(980)$ in $\\pi^+\\eta$ invariant mass\ndistribution. We further estimate the ratio $R$ = $\\mathcal{B}(\\Lambda_c^+ \\to\na_0(980)^+ n)\/\\mathcal{B}(\\Lambda_c^+ \\to \\pi^+\\eta n)\\approx 0.313$. Our\nresults can be tested by BESIII, Belle~II, and the proposed Super Tau-Charm\nFacility experiments in the future.",
        "We use resurgent extrapolation and continuation methods to extract detailed\nanalytic information about the tilted cusp anomalous dimension solely from its\nweak coupling and strong coupling expansions. This enables accurate and smooth\ninterpolation between the weak and strong coupling limits, and identifies the\nrelevant singularities governing the finite radius of convergence of the weak\ncoupling expansion and the asymptotic nature of the strong coupling expansion.\nThe input data is purely perturbative, generated from the BES equations, and\nthese resurgent methods extract accurate non-perturbative information which\nmatches the underlying physical structure.",
        "Despite extensive research and development of tools and technologies for\nmisinformation tracking and detection, we often find ourselves largely on the\nlosing side of the battle against misinformation. In an era where\nmisinformation poses a substantial threat to public discourse, trust in\ninformation sources, and societal and political stability, it is imperative\nthat we regularly revisit and reorient our work strategies. While we have made\nsignificant strides in understanding how and why misinformation spreads, we\nmust now broaden our focus and explore how technology can help realise new\napproaches to address this complex challenge more efficiently."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "start_abstract":"When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Symbolic Transfer Entropy"
      ],
      "abstract":[
        "We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Dynamic Metadata Schemes in the Neutron and Photon Science Communities:\n  A Case Study of X-Ray Photon Correlation Spectroscopy",
        "Disentangling sources of multifractality in time series",
        "Ordinal language of antipersistent binary walks",
        "Sparse identification of evolution equations via Bayesian model\n  selection",
        "Halo spin and orientation in Interacting Dark Matter Dark Energy\n  Cosmology",
        "Signatures of extreme events in the cumulative entropic spectrum",
        "High-Performance Data Format for Scientific Data Storage and Analysis",
        "Time-resolved Hubble Space Telescope UV observations of an X-ray\n  quasi-periodic eruption source",
        "Gravitational Wave Scattering via the Born Series: Scalar Tidal Matching\n  to $\\mathcal{O}(G^7)$ and Beyond",
        "Geometrically Templated Dynamic Wrinkling from Suspended Poly(vinyl\n  alcohol) Soap Films",
        "An Accessible Formulation for Defining the SI Second Based on Multiple\n  Atomic Transitions",
        "A skeletonization based image segmentation algorithm to isolate slender\n  regions in 3D microstructures",
        "Maximum likelihood estimation of burst-merging kernels for bursty time\n  series",
        "A domain decomposition strategy for natural imposition of mixed boundary\n  conditions in port-Hamiltonian systems",
        "Non-reciprocal interactions drive emergent chiral crystallites",
        "Reduced Basis Model for Compressible Flow",
        "Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy",
        "Application of resolved low-J multi-CO line modeling with RADEX to\n  constrain the molecular gas properties in the starburst M82",
        "A regional implementation of a mixed finite-element, semi-implicit\n  dynamical core",
        "1\/f and Random Telegraph Noise of Single-Layer Graphene Devices with\n  Interdigitated Electrodes",
        "Multivariate Distribution-Free Nonparametric Testing: Generalizing\n  Wilcoxon's Tests via Optimal Transport",
        "Spectrum management and the EVN",
        "Multipole generalization of the Witten effect in Mie-resonant photonics",
        "Congruence properties of prime sums and Bernoulli polynomials",
        "Determination of unscaled blood input for human dynamic FDG brain PET",
        "Harnessing Hybrid Frequency-Entangled Qudits through Quantum\n  Interference",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Quantum Maslov classes"
      ],
      "abstract":[
        "Metadata is one of the most important aspects for advancing data management\npractices within all research communities. Definitions and schemes of metadata\nare inter alia of particular significance in the domain of neutron and photon\nscattering experiments covering a broad area of different scientific\ndisciplines. The demand of describing continuously evolving highly\nnonstandardized experiments, including the resulting processed and published\ndata, constitutes a considerable challenge for a static definition of metadata.\nHere, we present the concept of dynamic metadata for the neutron and photon\nscientific community, which enriches a static set of defined basic metadata. We\nexplore the idea of dynamic metadata with the help of the use case of X-ray\nPhoton Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering\ntechnique that allows the investigation of nanoscale dynamic processes. It\nserves here as a demonstrator of how dynamic metadata can improve data\nacquisition, sharing, and analysis workflows. Our approach enables researchers\nto tailor metadata definitions dynamically and adapt them to the evolving\ndemands of describing data and results from a diverse set of experiments. We\ndemonstrate that dynamic metadata standards yield advantages that enhance data\nreproducibility, interoperability, and the dissemination of knowledge.",
        "This contribution addresses the question commonly asked in scientific\nliterature about the sources of multifractality in time series. Two primary\nsources are typically considered. These are temporal correlations and heavy\ntails in the distribution of fluctuations. Most often, they are treated as two\nindependent components, while true multifractality cannot occur without\ntemporal correlations. The distributions of fluctuations affect the span of the\nmultifractal spectrum only when correlations are present. These issues are\nillustrated here using series generated by several model mathematical cascades,\nwhich by design build correlations into these series. The thickness of the\ntails of fluctuations in such series is then governed by an appropriate\nprocedure of adjusting them to $q$-Gaussian distributions, and $q$ is treated\nas a variable parameter that, while preserving correlations, allows to tune\nthese distributions to the desired functional form. Multifractal detrended\nfluctuation analysis (MFDFA), as the most commonly used practical method for\nquantifying multifractality, is then used to identify the influence of the\nthickness of the fluctuation tails in the presence of temporal correlations on\nthe width of multifractal spectra. The obtained results point to the Gaussian\ndistribution, so $q=1$, as the appropriate reference distribution to evaluate\nthe contribution of fatter tails to the width of multifractal spectra. An\nappropriate procedure is presented to make such estimates.",
        "This paper explores the effectiveness of using ordinal pattern probabilities\nto evaluate antipersistency in the sign decomposition of long-range\nanti-correlated Gaussian fluctuations. It is numerically shown that ordinal\npatterns are able to effectively measure both persistent and antipersistent\ndynamics by analyzing the sign decomposition derived from fractional Gaussian\nnoise. These findings are crucial given that traditional methods such as\nDetrended Fluctuation Analysis are unsuccessful in detecting anti-correlations\nin such sequences. The numerical results are supported by physiological and\nenvironmental data, illustrating its applicability in real-world situations.",
        "The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships.",
        "In recent years, the interaction between dark matter (DM) and dark energy has\nbecome a topic of interest in cosmology. Interacting dark matter-dark energy\n(IDE) models have a substantial impact on the formation of cosmological\nlarge-scale structures, which serve as the background for DM halo evolution.\nThis impact can be examined through the shape and spin orientation of halos in\nnumerical simulations incorporating IDE effects. In our work, we use the N-body\nsimulation pipeline ME-GADGET to simulate and study the halo spin and\norientation in IDE models. We found that in models where DM transfers into DE\n(IDE I), the alignment of halo shapes with the surrounding tidal field is\nenhanced, while the alignment of halo spins with the tidal field is decreased\ncompared to {\\Lambda}CDM. Conversely, in models where DE transfers into DM (IDE\nII), the opposite occurs. We have provided fitted functions to describe these\nalignment signals. Our study provides the foundation for more accurate modeling\nof observations in the future such as China Space Station Telescope.",
        "In this study, the cumulative effect of the empirical probability\ndistribution of a random variable is identified as a factor that amplifies the\noccurrence of extreme events in datasets. To quantify this observation, a\ncorresponding information measure is introduced, drawing upon Shannon entropy\nfor joint probabilities. The proposed approach is validated using selected\nmarket data as case studies, encompassing various instances of extreme events.\nIn particular, the results indicate that the introduced cumulative measure\nexhibits distinctive signatures of such events, even when the data is\nrelatively noisy. These findings highlight the potential of the discussed\nconcept for developing a new class of related indicators or classifiers.",
        "In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete).",
        "X-ray quasi-periodic eruptions (QPEs) are a novel mode of variability in\nnearby galactic nuclei whose origin remains unknown. Their multi-wavelength\nproperties are poorly constrained, as studies have focused almost entirely on\nthe X-ray band. Here we report on time-resolved, coordinated Hubble Space\nTelescope far ultraviolet and XMM-Newton X-ray observations of the shortest\nperiod X-ray QPE source currently known, eRO-QPE2. We detect a bright UV point\nsource ($L_{\\rm FUV} \\approx {\\rm few} \\times 10^{41}$ erg s$^{-1}$) that does\nnot show statistically significant variability between the X-ray eruption and\nquiescent phases. This emission is unlikely to be powered by a young stellar\npopulation in a nuclear stellar cluster. The X-ray-to-UV spectral energy\ndistribution can be described by a compact accretion disk ($R_{\\rm out} =\n343^{+202}_{-138} \\ R_{\\rm g}$). Such compact disks are incompatible with\ntypical disks in active galactic nuclei, but form naturally following the tidal\ndisruption of a star. Our results rule out models (for eRO-QPE2) invoking i) a\nclassic AGN accretion disk and ii) no accretion disk at all. For orbiter\nmodels, the expected radius derived from the timing properties would naturally\nlead to disk-orbiter interactions for both quasi-spherical and eccentric\ntrajectories. We infer a black hole mass of log$_{10}(M_{\\rm BH}) = 5.9 \\pm\n0.3$ M$_{\\odot}$ and Eddington ratio of 0.13$^{+0.18}_{-0.07}$; in combination\nwith the compact outer radius this is inconsistent with existing disk\ninstability models. After accounting for the quiescent disk emission, we\nconstrain the ratio of X-ray to FUV luminosity of the eruption component to be\n$L_{\\rm X} \/ L_{\\rm FUV} > 16-85$ (depending on the intrinsic extinction).",
        "We introduce a novel method to compute gravitational wave amplitudes within\nthe framework of effective field theory. By reinterpreting the Feynman diagram\nexpansion as a Born series, our method offers several key advantages. It\ndirectly yields partial wave amplitudes, streamlining the matching with black\nhole perturbation theory. Long-distance gravitational interactions are\nunambiguously factorized from short-distance tidal effects, including\ndissipation, which are systematically incorporated via an in-in worldline\neffective action. Crucially, at every order in perturbation theory, integrals\nare expressed in terms of harmonic polylogarithms, enabling an end-to-end\ncomputation scalable to arbitrary orders. We illustrate the method with new\npredictions for scalar black hole Love numbers and their Renormalization Group\nequations to $\\mathcal{O}(G^7)$.",
        "Wrinkling is commonly observed as mechanical instability when a stiff thin\nfilm bound on a compliant thick substrate undergoes in-plane compression\nexceeding a threshold. Despite significant efforts to create a broad range of\nsurface patterns via wrinkling, little has been studied about a dynamic and\ntransient wrinkling process, where a suspended polymer thin film undergoes\nliquid-to-solid phase transitions. Here, a spontaneous wrinkling process is\nreported, when drying poly(vinyl alcohol) (PVA) soap films suspended on 3D\nprinted wireframes with near zero or negative Gaussian curvatures. As water\nevaporates, a thickness gradient across the sample is developed, leading to\nnon-uniform drying rates, and a concentration gradient between the inner and\nouter sides (exposed to air) of the suspended PVA soap film induces a\ndifferential osmotic pressure. Together, these effects contribute to an\nin-plane compressive stress, leading to the formation of surface wrinkles,\nwhose growth is guided by the geometry of the frame. Importantly, the wrinkles\nevolve dynamically: the wavelength and number of the wrinkles can be tuned by\naltering the concentration of the PVA aqueous solutions, the initial mass, the\nrelative humidity of the drying environment; the patterns of the resulting\nwrinkles can be programmed by the geometry of the wireframe.",
        "This work presents a novel formulation for a redefinition of the second based\non the weighted arithmetic mean of multiple normalized frequencies. We\ndemonstrate that it is mathematically equivalent to the previously discussed\nimplementation employing a geometric mean. In our reformulation, the\nnormalization of frequencies provides the defining constants with immediate\nphysical meaning, while maintaining the decoupling of assigned weights from the\nfrequencies of the reference transitions. We believe that a definition based on\nthis formulation would be significantly more accessible to both experts and\nnon-specialists, enhancing understanding and facilitating broader acceptance.\nWe hope that this approach will help overcome barriers to the adoption of a\nredefinition that effectively values all state-of-the-art atomic clocks.",
        "The work proposes an image segmentation algorithm that isolates slender\nregions in three-dimensional microstructures. Characterizing slender regions in\nmaterial microstructures is an extremely important aspect in material science\nbecause these regions govern the macroscopic behavior of materials for many\napplications like energy absorption, activation of metamaterials, stability of\nhigh temperature filters, etc. This work utilizes skeletonization method to\ncalculate centerline of the microstructure geometry followed by a novel pruning\nstrategy based on cross-sectional area to identify slender regions in the\nmicrostructure. 3D images of such microstructures obtained from micro-CT often\nsuffer from low image resolution resulting in high surface noise. The skeleton\nof such an image has many spurious skeletal branches that do not represent the\nactual microstructure geometry. The proposed pruning method of cross-sectional\narea is insensitive to surface noise and hence is a reliable method of\nidentifying skeletal branches that represent the slender regions in the\nmicrostructure. The proposed algorithm is implemented on a test case to\nshowcase its effectiveness. Further it is implemented on a 3D microstructure of\nceramic foam to identify the slender regions present in it. It is shown that\nthe method can be used to segment slender regions of varying dimensions and to\nstudy their geometric properties.",
        "Various time series in natural and social processes have been found to be\nbursty. Events in the time series rapidly occur within short time periods,\nforming bursts, which are alternated with long inactive periods. As the\ntimescale defining bursts increases, individual events are sequentially merged\nto become small bursts and then bigger ones, eventually leading to the single\nburst containing all events. Such a merging pattern has been depicted by a tree\nthat fully reveals the hierarchical structure of bursts, thus called a burst\ntree. The burst-tree structure can be simply characterized by a burst-merging\nkernel that dictates which bursts are merged together as the timescale\nincreases. In this work, we develop the maximum likelihood estimation method of\nthe burst-merging kernel from time series, which is successfully tested against\nthe time series generated using several model kernels. We also apply our method\nto some empirical time series from various backgrounds. Our method provides a\nuseful tool to precisely characterize the time series data, hence enabling to\nstudy their underlying mechanisms more accurately.",
        "In this contribution, a finite element scheme to impose mixed boundary\nconditions without introducing Lagrange multipliers is presented for wave\npropagation phenomena described as port-Hamiltonian systems. The strategy\nrelies on finite element exterior calculus and a domain decomposition to\ninterconnect two systems with different causalities. The spatial domain is\nsplit into two parts by introducing an arbitrary interface. Each subdomain is\ndiscretized with a mixed finite element formulation that introduces a uniform\nboundary condition in a natural way as the input. In each subdomain the spaces\nare selected from a finite element subcomplex to obtain a stable\ndiscretization. The two systems are then interconnected together by making use\nof a feedback interconnection. This is achieved by discretizing the boundary\ninputs using appropriate spaces that couple the two formulations. The final\nsystems includes all boundary conditions explicitly and does not contain any\nLagrange multiplier. Each subdomain is integrated using an implicit midpoint\nscheme in an uncoupled way from the other by means of a leapfrog scheme. The\nproposed strategy is tested on three different examples: the Euler-Bernoulli\nbeam, the wave equation and the Maxwell equations. Numerical tests assess the\nconservation properties of the scheme and the effectiveness of the methodology.",
        "We study a new type of 2D active material that exhibits macroscopic phases\nwith two emergent broken symmetries: self-propelled achiral particles that form\ndense hexatic clusters, which spontaneously rotate. We experimentally realise\nactive colloids that self-organise into both polar and hexatic crystallites,\nexhibiting exotic emergent phenomena. This is accompanied by a field theory of\ncoupled order parameters formulated on symmetry principles, including\nnon-reciprocity, to capture the non-equilibrium dynamics. We find that the\npresence of two interacting broken symmetry fields leads to the emergence of\nnovel chiral phases built from (2D) achiral active colloids (here Quincke\nrollers). These phases are characterised by the presence of both clockwise and\ncounterclockwise rotating clusters. We thus show that spontaneous rotation can\nemerge in non-equilibrium systems, even when the building blocks are achiral,\ndue to non-reciprocally coupled broken symmetries. This interplay leads to\nself-organized stirring through counter-rotating vortices in confined colloidal\nsystems, with cluster size controlled by external electric fields.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients\/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.",
        "The distribution and physical conditions of molecular gas are closely linked\nto star formation and the subsequent evolution of galaxies. Emission from\ncarbon monoxide (CO) and its isotopologues traces the bulk of molecular gas and\nprovides constraints on the physical conditions through their line ratios.\nHowever, comprehensive understanding on how the particular choice of line\nmodeling approach impacts derived molecular properties remain incomplete. Here,\nwe study the nearby starburst galaxy M82, known for its intense star formation\nand molecular emission, using the large set of available multi-CO line\nobservations. We present high-resolution (${\\sim}85$ pc) emission of seven CO\nisotopologue lines, including $^{12}$CO, $^{13}$CO, and C$^{18}$O from the $J =\n1-0$, $2-1$ and $3-2$ transitions. Using \\texttt{RADEX} for radiative transfer\nmodeling, we analyze M82\\textsc{\\char39}s molecular properties with (i) a\none-zone model and (ii) a variable density model, comparing observed and\nsimulated emissions via a minimum $\\chi^2$ analysis. We find that inferred gas\nconditions -- kinetic temperature and density -- are consistent across models,\nwith minimal statistical differences. However, due to their low critical\ndensities (${<}10^{4}$ cm$^{-3}$), low-$J$ CO isotopologue lines do not\neffectively probe higher density gas prevalent in starburst environments like\nthat of M82. Our results further imply that this limitation extends to\nhigh-redshift ($z{\\gtrapprox}1$) galaxies with similar conditions, where\nlow-$J$ CO lines are inadequate for density constraints. Future studies of\nextreme star-forming regions like M82 will require higher-$J$ CO lines or\nalternative molecular tracers with higher critical densities.",
        "This paper explores how to adapt a new dynamical core to enable its use in\none-way nested regional weather and climate models, where lateral boundary\nconditions (LBCs) are provided by a lower-resolution driving model. The\ndynamical core has recently been developed by the Met Office and uses an\niterated-semi-implicit time discretisation and mixed finite-element spatial\ndiscretisation.\n  The essential part of the adaptation is the addition of the LBCs to the\nright-hand-side of the linear system which solves for pressure and momentum\nsimultaneously. The impacts on the associated Helmholtz preconditioner and\nmultigrid techniques are also described.\n  The regional version of the dynamical core is validated through big-brother\nexperiments based on idealised dynamical core tests. These experiments\ndemonstrate that the subdomain results are consistent with those from the full\ndomain, confirming the correct application of LBCs. Inconsistencies arise in\ncases where the LBCs are not perfect, but it is shown that the application of\nblending can be used to overcome these problems.",
        "Single-layer Graphene (SLG) is a promising material for sensing applications.\nHigh performance graphene sensors can be achieved when Interdigitated\nElectrodes (IDE) are used. In this research work, we fabricated SLG\nmicro-ribbon (GMR) devices with IDE having different geometric parameters. 1\/f\nnoise behavior was observed in all of the examined devices, and in some cases\nrandom telegraph noise (RTN) signals suggesting that carrier\ntrapping\/de-trapping is taking place. Our experimental results indicate that\nthe geometrical characteristics can have a crucial impact on device\nperformance, due to the direct area dependence of the noise level.",
        "This paper reviews recent advancements in the application of optimal\ntransport (OT) to multivariate distribution-free nonparametric testing.\nInspired by classical rank-based methods, such as Wilcoxon's rank-sum and\nsigned-rank tests, we explore how OT-based ranks and signs generalize these\nconcepts to multivariate settings, while preserving key properties, including\ndistribution-freeness, robustness, and efficiency. Using the framework of\nasymptotic relative efficiency (ARE), we compare the power of the proposed\n(generalized Wilcoxon) tests against the Hotelling's $T^2$ test. The ARE lower\nbounds reveal the Hodges-Lehmann and Chernoff-Savage phenomena in the context\nof multivariate location testing, underscoring the high power and efficiency of\nthe proposed methods. We also demonstrate how OT-based ranks and signs can be\nseamlessly integrated with more modern techniques, such as kernel methods, to\ndevelop universally consistent, distribution-free tests. Additionally, we\npresent novel results on the construction of consistent and distribution-free\nkernel-based tests for multivariate symmetry, leveraging OT-based ranks and\nsigns.",
        "In recent years, the utilisation of the radio spectrum has dramatically\nincreased. Digital telecommunication applications, be it terrestrial cell-phone\nnetworks or new-space low-earth orbit satellite constellations, have not only\nacquired unprecedented amounts of spectrum but also use their frequencies\neverywhere on Earth. The consequences for radio astronomy and other scientific\nradio services are severe. A single cell-phone tower within hundreds of\nkilometers around a radio telescope can blind us and there is no place on Earth\nto escape the ubiquitous transmissions of satellite megaconstellations.\n  Since 1988, the Committee on Radio Astronomy Frequencies (CRAF) is advocating\nfor astronomers' rights to use the spectrum. CRAF does this by participation in\nthe national and international regulatory frameworks. Hundreds if not thousands\nof documents need to be processed every year. CRAF not only contributes to\nregulatory texts, but even more importantly, performs spectrum compatibility\ncalculations. In this contribution, CRAF's latest activities are summarized\nwith a focus on matters relevant to EVN operations.",
        "We present a generalization of the Witten effect on the case of oscillating\nmultipole sources exciting nonreciprocal sphere with effective axion response.\nWe find that the fields outside of the sphere are presented as a superposition\nof electric and magnetic multipoles. In addition to appearance of\ncross-polarized component in the radiation, Mie resonances of the system\nhybridize with each other, exhibiting characteristic double peaks in Mie\nspectra observed especially clearly for higher-order multipole resonances. This\ncharacteristic feature may provide a sensitive probe of axion-type\nnonreciprocal responses in Mie-resonant photonics.",
        "In this article, we derive a congruence property of particular sum rules\ninvolving prime numbers. The resulting expression involves Bernoulli numbers\nand polynomials, for which we obtain, as a consequence, a general congruence\nrelation as well.",
        "Objectives: Many existing techniques for the non-invasive quantification of\nthe blood input function in dynamic FDG-PET imaging require strong historical\ninformation or user input. The technique proposed in this work utilizes the\nassumption that a dynamic PET scan can be modeled by the Patlak plot to\ndetermine an unscaled blood input function. Materials and Methods: The time\nactivity curve (TAC) for each voxel in a dynamic image can be considered as an\nn-dimensional vector. In this context, a TAC follows the Patlak plot if and\nonly if the TAC is a linear combination of the blood input function and the\nintegral of the blood input function. Given a set of TACs which follow the\nPatlak plot, we can thus use PCA to determine a basis which spans the same\nvector space as the blood input function and the integral of the blood input\nfunction. We then seek to find two TACs in this vector space which best satisfy\nthat the estimated anti-derivative of one of the TACs is close to the other\nTAC; such TACs are candidates for the blood input function and the integral of\nthe blood input function. We were able to construct a low (2) dimensional\noptimization problem to find such TACs. Results: We applied our results to\nobtain predicted blood input functions and Ki maps for twelve normal subjects.\nScaling the predicted blood input function to best match the ground truth, we\nachieved an average SSE of $0.042 \\pm 0.032$ and an average DTW distance of\n$0.141 \\pm 0.053$. Matching the means of the predicted and ground truth Ki\nmaps, we achieved an average MAPE of $2.539 \\pm 0.928$ and an average SSIM of\n$0.991 \\pm 0.005$. Conclusion: While not often viewed as such, the assumption\nthat some dynamic data follows a kinetic model gives strong prior information.\nIn the case of the Patlak plot, we can use this assumption to estimate an\nunscaled blood input function and unscaled Ki map.",
        "High-dimensional (HD) quantum entanglement expands the Hilbert space,\noffering a robust framework for quantum information processing with enhanced\ncapacity and error resilience. In this work, we present a novel HD\nfrequency-domain entangled state, the hybrid frequency-entangled qudit (HFEQ),\ngenerated via Hong-Ou-Mandel (HOM) interference, exhibiting both\ndiscrete-variable (DV) and continuous-variable (CV) characteristics. By tuning\nHOM interference, we generate and control HFEQs with dimensions $D=5,7,9,11$,\nconfirming their DV nature. Franson interferometry confirms the global\nfrequency correlations with visibility exceeding 98% and verifies the CV\nentanglement within individual frequency modes with visibility greater than\n95%. Our findings provide deeper insight into the physical nature of\nfrequency-entangled qudits generated by quantum interference and introduce a\nnovel resource for HD time-frequency quantum information processing.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "We give a construction of ``quantum Maslov characteristic classes'',\ngeneralizing to higher dimensional cycles the Hu-Lalonde-Seidel morphism. We\nalso state a conjecture extending this to an $A _{\\infty}$ functor from the\nexact path category of the space of monotone Lagrangian branes to the Fukaya\ncategory. Quantum Maslov classes are used here for the study of Hofer geometry\nof Lagrangian equators in $S ^{2}$, giving a rigidity phenomenon for the Hofer\nmetric 2-systole, which stands in contrast to the flexibility phenomenon of the\nclosely related Hofer metric girth studied by Rauch ~\\cite{cite_Itamar}, in the\nsame context of Lagrangian equators of $S ^{2}$. More applications appear in\n~\\cite{cite_SavelyevGlobalFukayacategoryII}."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Symbolic Transfer Entropy",
    "start_abstract":"We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
      ],
      "abstract":[
        "When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity\n  Analysis",
        "Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization",
        "When do neural networks learn world models?",
        "mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep\n  Neural Networks",
        "A Survey of Direct Preference Optimization",
        "Probabilistic neural operators for functional uncertainty quantification",
        "Techniques for Enhancing Memory Capacity of Reservoir Computing",
        "Temporal Distribution Shift in Real-World Pharmaceutical Data:\n  Implications for Uncertainty Quantification in QSAR Models",
        "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
        "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations",
        "Modeling Attention during Dimensional Shifts with Counterfactual and\n  Delayed Feedback",
        "Multi-Source Urban Traffic Flow Forecasting with Drone and Loop Detector\n  Data",
        "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs",
        "Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth",
        "Predictive Target-to-User Association in Complex Scenarios via\n  Hybrid-Field ISAC Signaling",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Structure and Context of Retweet Coordination in the 2022 U.S. Midterm\n  Elections",
        "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN\n  Model",
        "Ro-To-Go! Robust Reactive Control with Signal Temporal Logic",
        "Inverse Intersections for Boolean Satisfiability Problems",
        "Electromagnetic Side-Channel Analysis of PRESENT Lightweight Cipher",
        "Local well-posedness for nonlinear Schr\\\"odinger equations on compact\n  product manifolds",
        "Spectrum of L\\'evy-Ornstein-Uhlenbeck semigroups on $\\mathbb{R}^d$",
        "Spike-and-Slab Posterior Sampling in High Dimensions",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review",
        "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
        "Complexity of approximate conflict-free, linearly-ordered, and\n  nonmonochromatic hypergraph colourings"
      ],
      "abstract":[
        "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance.",
        "In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.",
        "Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.",
        "Deep neural networks are increasingly employed in fields such as climate\nmodeling, robotics, and industrial control, where strict output constraints\nmust be upheld. Although prior methods like the POLICE algorithm can enforce\naffine constraints in a single convex region by adjusting network parameters,\nthey struggle with multiple disjoint regions, often leading to conflicts or\nunintended affine extensions. We present mPOLICE, a new method that extends\nPOLICE to handle constraints imposed on multiple regions. mPOLICE assigns a\ndistinct activation pattern to each constrained region, preserving exact affine\nbehavior locally while avoiding overreach into other parts of the input domain.\nWe formulate a layer-wise optimization problem that adjusts both the weights\nand biases to assign unique activation patterns to each convex region, ensuring\nthat constraints are met without conflicts, while maintaining the continuity\nand smoothness of the learned function. Our experiments show the enforcement of\nmulti-region constraints for multiple scenarios, including regression and\nclassification, function approximation, and non-convex regions through\napproximation. Notably, mPOLICE adds zero inference overhead and minimal\ntraining overhead.",
        "Large Language Models (LLMs) have demonstrated unprecedented generative\ncapabilities, yet their alignment with human values remains critical for\nensuring helpful and harmless deployments. While Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with\nhuman preferences, its reliance on complex reward modeling introduces inherent\ntrade-offs in computational efficiency and training stability. In this context,\nDirect Preference Optimization (DPO) has recently gained prominence as a\nstreamlined alternative that directly optimizes LLMs using human preferences,\nthereby circumventing the need for explicit reward modeling. Owing to its\ntheoretical elegance and computational efficiency, DPO has rapidly attracted\nsubstantial research efforts exploring its various implementations and\napplications. However, this field currently lacks systematic organization and\ncomparative analysis. In this survey, we conduct a comprehensive overview of\nDPO and introduce a novel taxonomy, categorizing previous works into four key\ndimensions: data strategy, learning framework, constraint mechanism, and model\nproperty. We further present a rigorous empirical analysis of DPO variants\nacross standardized benchmarks. Additionally, we discuss real-world\napplications, open challenges, and future directions for DPO. This work\ndelivers both a conceptual framework for understanding DPO and practical\nguidance for practitioners, aiming to advance robust and generalizable\nalignment paradigms. All collected resources are available and will be\ncontinuously updated at\nhttps:\/\/github.com\/liushunyu\/awesome-direct-preference-optimization.",
        "Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.",
        "Reservoir Computing (RC) is a bio-inspired machine learning framework, and\nvarious models have been proposed. RC is a well-suited model for time series\ndata processing, but there is a trade-off between memory capacity and\nnonlinearity. In this study, we propose methods to improve the memory capacity\nof reservoir models by modifying their network configuration except for the\ninside of reservoirs. The Delay method retains past inputs by adding delay node\nchains to the input layer with the specified number of delay steps. To suppress\nthe effect of input value increase due to the Delay method, we divide the input\nweights by the number of added delay steps. The Pass through method feeds input\nvalues directly to the output layer. The Clustering method divides the input\nand reservoir nodes into multiple parts and integrates them at the output\nlayer. We applied these methods to an echo state network (ESN), a typical RC\nmodel, and the chaotic Boltzmann machine (CBM)-RC, which can be efficiently\nimplemented in integrated circuits. We evaluated their performance on the NARMA\ntask, and measured information processing capacity (IPC) to evaluate the\ntrade-off between memory capacity and nonlinearity.",
        "The estimation of uncertainties associated with predictions from quantitative\nstructure-activity relationship (QSAR) models can accelerate the drug discovery\nprocess by identifying promising experiments and allowing an efficient\nallocation of resources. Several computational tools exist that estimate the\npredictive uncertainty in machine learning models. However, deviations from the\ni.i.d. setting have been shown to impair the performance of these uncertainty\nquantification methods. We use a real-world pharmaceutical dataset to address\nthe pressing need for a comprehensive, large-scale evaluation of uncertainty\nestimation methods in the context of realistic distribution shifts over time.\nWe investigate the performance of several uncertainty estimation methods,\nincluding ensemble-based and Bayesian approaches. Furthermore, we use this\nreal-world setting to systematically assess the distribution shifts in label\nand descriptor space and their impact on the capability of the uncertainty\nestimation methods. Our study reveals significant shifts over time in both\nlabel and descriptor space and a clear connection between the magnitude of the\nshift and the nature of the assay. Moreover, we show that pronounced\ndistribution shifts impair the performance of popular uncertainty estimation\nmethods used in QSAR models. This work highlights the challenges of identifying\nuncertainty quantification methods that remain reliable under distribution\nshifts introduced by real-world data.",
        "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
        "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime\/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.",
        "Attention can be used to inform choice selection in contextual bandit tasks\neven when context features have not been previously experienced. One example of\nthis is in dimensional shifts, where additional feature values are introduced\nand the relationship between features and outcomes can either be static or\nvariable. Attentional mechanisms have been extensively studied in contextual\nbandit tasks where the feedback of choices is provided immediately, but less\nresearch has been done on tasks where feedback is delayed or in counterfactual\nfeedback cases. Some methods have successfully modeled human attention with\nimmediate feedback based on reward prediction errors (RPEs), though recent\nresearch raises questions of the applicability of RPEs onto more general\nattentional mechanisms. Alternative models suggest that information theoretic\nmetrics can be used to model human attention, with broader applications to\nnovel stimuli. In this paper, we compare two different methods for modeling how\nhumans attend to specific features of decision making tasks, one that is based\non calculating an information theoretic metric using a memory of past\nexperiences, and another that is based on iteratively updating attention from\nreward prediction errors. We compare these models using simulations in a\ncontextual bandit task with both intradimensional and extradimensional domain\nshifts, as well as immediate, delayed, and counterfactual feedback. We find\nthat calculating an information theoretic metric over a history of experiences\nis best able to account for human-like behavior in tasks that shift dimensions\nand alter feedback presentation. These results indicate that information\ntheoretic metrics of attentional mechanisms may be better suited than RPEs to\npredict human attention in decision making, though further studies of human\nbehavior are necessary to support these results.",
        "Traffic forecasting is a fundamental task in transportation research, however\nthe scope of current research has mainly focused on a single data modality of\nloop detectors. Recently, the advances in Artificial Intelligence and drone\ntechnologies have made possible novel solutions for efficient, accurate and\nflexible aerial observations of urban traffic. As a promising traffic\nmonitoring approach, drone-captured data can create an accurate multi-sensor\nmobility observatory for large-scale urban networks, when combined with\nexisting infrastructure. Therefore, this paper investigates the problem of\nmulti-source traffic speed prediction, simultaneously using drone and loop\ndetector data. A simple yet effective graph-based model HiMSNet is proposed to\nintegrate multiple data modalities and learn spatio-temporal correlations.\nDetailed analysis shows that predicting accurate segment-level speed is more\nchallenging than the regional speed, especially under high-demand scenarios\nwith heavier congestions and varying traffic dynamics. Utilizing both drone and\nloop detector data, the prediction accuracy can be improved compared to\nsingle-modality cases, when the sensors have lower coverages and are subject to\nnoise. Our simulation study based on vehicle trajectories in a real urban road\nnetwork has highlighted the added value of integrating drones in traffic\nforecasting and monitoring.",
        "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory.",
        "We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.",
        "This paper presents a novel and robust target-to-user (T2U) association\nframework to support reliable vehicle-to-infrastructure (V2I) networks that\npotentially operate within the hybrid field (near-field and far-field). To\naddress the challenges posed by complex vehicle maneuvers and user association\nambiguity, an interacting multiple-model filtering scheme is developed, which\ncombines coordinated turn and constant velocity models for predictive\nbeamforming. Building upon this foundation, a lightweight association scheme\nleverages user-specific integrated sensing and communication (ISAC) signaling\nwhile employing probabilistic data association to manage clutter measurements\nin dense traffic. Numerical results validate that the proposed framework\nsignificantly outperforms conventional methods in terms of both tracking\naccuracy and association reliability.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "The ability to detect coordinated activity in communication networks is an\nongoing challenge. Prior approaches emphasize considering any activity\nexceeding a specific threshold of similarity to be coordinated. However,\nidentifying such a threshold is often arbitrary and can be difficult to\ndistinguish from grassroots organized behavior. In this paper, we investigate a\nset of Twitter retweeting data collected around the 2022 US midterm elections,\nusing a latent sharing-space model, in which we identify the main components of\nan association network, thresholded with a k-nearest neighbor criterion. This\napproach identifies a distribution of association values with different roles\nin the network at different ranges, where the shape of the distribution\nsuggests a natural place to threshold for coordinated user candidates. We find\ncoordination candidates belonging to two broad categories, one involving music\nawards and promotion of Korean pop or Taylor Swift, the other being users\nengaged in political mobilization. In addition, the latent space suggests\ncommon motivations for different coordinated groups otherwise fragmented by\nusing an appropriately high threshold criterion for coordination.",
        "The Graphical User Interface (GUI) plays a critical role in the interaction\nbetween users and mobile applications (apps), aiming at facilitating the\noperation process. However, due to the variety of functions and\nnon-standardized design, GUIs might have many accessibility issues, like the\nsize of components being too small or their intervals being narrow. These\nissues would hinder the operation of low vision users, preventing them from\nobtaining information accurately and conveniently. Although several\ntechnologies and methods have been proposed to address these issues, they are\ntypically confined to issue identification, leaving the resolution in the hands\nof developers. Moreover, it can be challenging to ensure that the color, size,\nand interval of the fixed GUIs are appropriately compared to the original ones.\nIn this work, we propose a novel approach named AccessFixer, which utilizes the\nRelational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix\nthree kinds of accessibility issues, including small sizes, narrow intervals,\nand low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a\nconsistent color palette, uniform intervals, and adequate size changes achieved\nthrough coordinated adjustments to the attributes of related components. Our\nexperiments demonstrate the effectiveness and usefulness of AccessFixer in\nfixing GUI accessibility issues. After fixing 30 real-world apps, our approach\nsolves an average of 81.2% of their accessibility issues. Also, we apply\nAccessFixer to 10 open-source apps by submitting the fixed results with pull\nrequests (PRs) on GitHub. The results demonstrate that developers approve of\nour submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study\nexamines that low vision users host a positive attitude toward the GUIs fixed\nby our method.",
        "Signal Temporal Logic (STL) robustness is a common objective for optimal\nrobot control, but its dependence on history limits the robot's decision-making\ncapabilities when used in Model Predictive Control (MPC) approaches. In this\nwork, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new\nquantitative semantics for the logic that isolates the contributions of suffix\ntrajectories. We prove its relationship to formula progression for Metric\nTemporal Logic, and show that the robustness-to-go depends only on the suffix\ntrajectory and progressed formula. We implement robustness-to-go as the\nobjective in an MPC algorithm and use formula progression to efficiently\nevaluate it online. We test the algorithm in simulation and compare it to MPC\nusing traditional STL robustness. Our experiments show that using\nrobustness-to-go results in a higher success rate.",
        "Boolean Satisfiability (SAT) problems are expressed as mathematical formulas.\nThis paper presents an alternative matrix representation for any type of these\nSAT problems. It shows how to use this matrix representation to get the full\nset of valid assignments. It proves that this is the full set of answers for\nthe given problem, and it shows that this is exponential in size, relative to\nthe matrix. It then presents an algorithm that utilizes the inverses of the\nclauses in this matrix for faster searching through this set of answers. It\nshows that this algorithm is both correct and polynomial.",
        "Side-channel vulnerabilities pose an increasing threat to cryptographically\nprotected devices. Consequently, it is crucial to observe information leakages\nthrough physical parameters such as power consumption and electromagnetic (EM)\nradiation to reduce susceptibility during interactions with cryptographic\nfunctions. EM side-channel attacks are becoming more prevalent. PRESENT is a\npromising lightweight cryptographic algorithm expected to be incorporated into\nInternet-of-Things (IoT) devices in the future. This research investigates the\nEM side-channel robustness of PRESENT using a correlation attack model. This\nwork extends our previous Correlation EM Analysis (CEMA) of PRESENT with\nimproved results. The attack targets the Substitution box (S-box) and can\nretrieve 8 bytes of the 10-byte encryption key with a minimum of 256 EM\nwaveforms. This paper presents the process of EM attack modelling, encompassing\nboth simple and correlation attacks, followed by a critical analysis.",
        "We prove new local well-posedness results for nonlinear Schr\\\"odinger\nequations posed on a general product of spheres and tori, by the standard\napproach of multi-linear Strichartz estimates. To prove these estimates, we\nestablish and utilize multi-linear bounds for the joint spectral projector\nassociated to the Laplace--Beltrami operators on the individual sphere factors\nof the product manifold. To treat the particular case of the cubic NLS on a\nproduct of two spheres at critical regularity, we prove a sharp\n$L^\\infty_xL^p_t$ estimate of the solution to the linear Schr\\\"odinger equation\non the two-torus.",
        "We investigate the spectral properties of Markov semigroups associated with\nOrnstein-Uhlenbeck (OU) processes driven by L\\'evy processes. These semigroups\nare generated by non-local, non-self-adjoint operators. In the special case\nwhere the driving L\\'evy process is Brownian motion, one recovers the classical\ndiffusion OU semigroup, whose spectral properties have been extensively studied\nover past few decades. Our main results establish that, under suitable\nconditions on the L\\'evy process, the spectrum of the L\\'evy-OU semigroup in\nthe $L^p$-space weighted with the invariant distribution coincides with that of\nthe diffusion OU semigroup. Furthermore, when the drift matrix $B$ is\ndiagonalizable with real eigenvalues, we derive explicit formulas for\neigenfunctions and co-eigenfunctions--an observation that, to the best of our\nknowledge, has not appeared in the literature. We also show that the\nmultiplicities of the eigenvalues remain independent of the choice of the\nL\\'evy process. A key ingredient in our approach is intertwining relationship:\nwe prove that every L\\'evy-OU semigroup is intertwined with a diffusion OU\nsemigroup. Additionally, we examine the compactness properties of these\nsemigroups and provide examples of non-compact L\\'evy-OU semigroups.",
        "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.",
        "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.",
        "Using the algebraic approach to promise constraint satisfaction problems, we\nestablish complexity classifications of three natural variants of hypergraph\ncolourings: standard nonmonochromatic colourings, conflict-free colourings, and\nlinearly-ordered colourings.\n  Firstly, we show that finding an $\\ell$-colouring of a $k$-colourable\n$r$-uniform hypergraph is NP-hard for all constant $2\\leq k\\leq \\ell$ and\n$r\\geq 3$. This provides a shorter proof of a celebrated result by Dinur et al.\n[FOCS'02\/Combinatorica'05].\n  Secondly, we show that finding an $\\ell$-conflict-free colouring of an\n$r$-uniform hypergraph that admits a $k$-conflict-free colouring is NP-hard for\nall constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, except for $r=4$ and $k=2$ (and\nany $\\ell$); this case is solvable in polynomial time. The case of $r=3$ is the\nstandard nonmonochromatic colouring, and the case of $r=2$ is the notoriously\ndifficult open problem of approximate graph colouring.\n  Thirdly, we show that finding an $\\ell$-linearly-ordered colouring of an\n$r$-uniform hypergraph that admits a $k$-linearly-ordered colouring is NP-hard\nfor all constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, thus improving on the results\nof Nakajima and \\v{Z}ivn\\'y~[ICALP'22\/ACM TocT'23]."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging",
    "start_abstract":"Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Magnetic resonance fingerprinting"
      ],
      "abstract":[
        "Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "The 3$\\alpha$ correlations of ground and excited $0^+$ states of\n  $^{12}\\mathrm{C}$ within the microscopic cluster model",
        "Low-energy spectra of nobelium isotopes: Skyrme\n  random-phase-approximation analysis",
        "Nuclear cross sections from low-energy interactions",
        "Shear Viscosity of Collider-Produced QCD Matter II: Comparing a\n  Multi-Component Chapman-Enskog Framework with AMPT in Full Equilibrium",
        "Estimating nuclear equation of state parameters away from saturation\n  density",
        "Fourier shape parametrization in covariant density functional theory for\n  nuclear fission",
        "$\\Lambda$ and $\\Sigma$ potentials in neutron stars, hypernuclei, and\n  heavy-ion collisions",
        "Particle number projection on a spatial domain",
        "Time Evolution of Prompt Gamma-Ray Emission in $^{252}$Cf(sf) and\n  $^{233,235}$U($n$,f) Reactions",
        "Pauli Blocking effects in Nilsson states of weakly bound exotic nuclei",
        "In-medium nucleon-nucleon cross sections from relativistic ab initio\n  calculations",
        "Towards shell model interactions with credible uncertainties",
        "Emulators for scarce and noisy data II: Application to auxiliary-field\n  diffusion Monte Carlo for neutron matter",
        "Integrating UX Design in Astronomical Software Development: A Case Study",
        "Bibliometric Analysis of Scientific Production on the COVID-19 Effect in\n  Information Sciences",
        "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed\n  Sensing Imaging",
        "Hybrid Channel- and Coding-Based Challenge-Response Physical-Layer\n  Authentication",
        "Thermoelectric properties of magic angle twisted bilayer\n  graphene-superconductor hetero-junction: effect of valley polarization and\n  trigonal warping",
        "Low-Eddington ratio, changing-look active galactic nuclei: the case of\n  NGC 4614",
        "Impacto del Enfoque Matematicas en Tres Actos en la Educacion Matematica",
        "Molecular Weight-Dependent Evaporation Dynamics and Morphology of PEG\n  Sessile Drops on Hydrophobic Substrates",
        "Quantized crystalline-electromagnetic responses in insulators",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Evolution of SMBHs in light of PTA measurements: implications for growth\n  by mergers and accretion",
        "Optimizing Portfolios with Pakistan-Exposed ETFs: Risk and Performance\n  Insight",
        "Quantum locally recoverable code with intersecting recovery sets",
        "How many unseen species are in multiple areas?"
      ],
      "abstract":[
        "The cluster structures of the $0^+$ states in $^{12}\\mathrm{C}$, including\nthe ground state, the Hoyle state, and the recently identified $0_3^+$ and\n$0_4^+$ states, are analyzed to explore the cluster configurations and\n$3\\alpha$ correlations without assuming the existence of $^8\\mathrm{Be}$. In\nparticular, the key quantity -- two-cluster overlap amplitudes -- is calculated\nfor the $3\\alpha$ clustering channels to reveal the essential features of these\n$0^+$ states. The results clearly show the distinction between the compact\nstructure of the ground state and the gas-like structures of the excited $0^+$\nstates. The Hoyle state exhibits the expected gas-like dominant ($0S$)\nconfiguration, while the $0_3^+$ state shows a more extended $3\\alpha$\nclustering structure, which can be viewed as a breathing-like excitation of the\nHoyle state, with an additional nodal structure. The $0_4^+$ state is found to\nhave a mixed configuration, featuring a bent-arm-like structure in the\n$S\\otimes S$ channel and an enhanced $2\\alpha$ correlation in the $D\\otimes D$\nchannel.",
        "Low-energy spectra in the isotopic chain $^{250-262}$No are systematically\ninvestigated within the fully self-consistent Quasiparticle\nRandom-Phase-Approximation (QRPA) using Skyrme forces SLy6, SkM* and SVbas.\nQRPA states of multipolarity $\\lambda\\mu$=20, 22, 30, 31, 32, 33, 43, 44 and 98\nare considered. The main attention is paid to isotopes $^{252}$No and\n$^{254}$No, for which the most extensive experimental spectroscopic information\nis available. In these two nuclei, a reasonable description of $K^{\\pi}=8^-,\n2^-$and $3^+$ isomers is obtained with the force SLy6. The disputed $8^-$\nisomer in $^{254}$No is assigned as neutron two-quasiparticle configuration\n$nn[734\\uparrow,613\\uparrow]$. At the energies 1.2 - 1.4 MeV, the 2qp K-isomers\n$4^-, 7^-$ in $^{252}$No and $4^-, 6^-, 7^-$ in $^{254}$No are predicted. In\n$^{254}$No, the $K^{\\pi}=3^+$ isomer should be accompanied by the nearby\n$K^{\\pi}=4^+$ counterpart. It is shown that, in the chain $^{250-262}$No, some\nfeatures of $^{252}$No and $^{254}$No exhibit essential irregularities caused\nby a shell gap in the neutron single-particle spectrum and corresponding\nbreakdown of the neutron pairing. In particular, low-energy pairing-vibrational\n$K^{\\pi}=0^+$ states in $^{252,254}$No are predicted.",
        "We present a method to calculate neutron scattering cross sections for\ndeformed nuclei using many--body wavefunctions described with multiple\nreference states. Nuclear states are calculated with the generator coordinate\nmethod using a low energy effective Hamiltonian. Using these states, a\nnon--local and energy dependent optical potential is consistently constructed,\nallowing to directly investigate the role of nuclear structure properties in\nnuclear scattering. The case of neutron scattering on $^{24}$Mg is presented.\nThe results are compared to experiment and to phenomenological optical\npotentials at energies below 13 MeV, demonstrating the importance of\nlow--energy collectivity in elastic and non--elastic scattering.",
        "Transport properties of the quark-gluon plasma are instrumental to testing\nperturbative quantum chromodynamics and understanding the extreme conditions of\nrelativistic heavy-ion collisions. This study presents an analytical\ninvestigation of the shear viscosity $\\eta$ and the shear viscosity-to-entropy\ndensity ratio $\\eta\/s$ of the QGP using a novel multi-component Chapman-Enskog\nframework assuming full thermalization. The approach incorporates\nspecies-specific contributions from gluons and (anti-)quarks into the plasma\nshear viscosity, temperature-dependent running parameters for the Debye mass\nand strong coupling, and a time-dependent cooling model. Our findings show that\nboth $\\eta$ and $\\eta\/s$ are enhanced by the inclusion of (anti-)quarks with\ngluons, and the parameters decrease over time due to the cooling and expansion\nof the QGP. These results align with perturbative QCD predictions, offering a\nmore optimistic representation of QGP transport properties under dynamic\nconditions. This multi-component framework is compared with a multi-phase\ntransport model that treats the QGP as a gluon gas with (anti-)quark\naugmentation.",
        "We explore the density variation of the correlation coefficient of the key\nparameters of the nuclear equation of state (EoS) with the bulk and crustal\nproperties of neutron stars. The analysis was performed using two diverse sets\nof nuclear effective interaction theories based on nonrelativistic\nSkyrme-Hartree Fock model and relativistic mean field model. We find that the\ncommonly studied EoS parameters, namely the isoscalar incompressibility of\nsymmetric nuclear matter $K(\\rho)$ and the isovector slope of symmetry energy\n$L(\\rho)$, reveal consistently maximum correlation with the radius, tidal\ndeformability, and moment of inertia all around twice the saturation density.\nWe find even more tighter and robust correlations beyond the saturation density\nfor constructed parameter $\\eta = [KL^2]^{1\/3}$ allowing the possibility to\nimpose stringent constraint on high-density $K(\\rho)$ and $L(\\rho)$. Extensive\ncorrelation analysis of the EoS parameters with the radius and tidal\ndeformability bounds from gravitational wave GW190814 event allows us to\nprovide reliable constraints on the central values of $K(\\rho_0) \\approx 248$\nMeV and $L(\\rho_0) \\approx 65$ MeV at the saturation density and $K(1.6\\rho_0)\n\\approx 391-517$ MeV and $L(1.6\\rho_0) \\approx 153-169$ MeV at 1.6 times the\nsaturation density. The crust-core transition density and the crustal fraction\nof moment of inertia is shown to correlate more strongly with $L(\\rho)$ and\n$\\eta(\\rho)$ near the subsaturation density.",
        "We implement the Fourier shape parametrization within the point-coupling\ncovariant density functional theory to construct the collective space,\npotential energy surface (PES), and mass tensor, which serve as inputs for the\ntime-dependent generator coordinate method to simulate the fission dynamics.\nTaking \\(^{226}\\)Th as a benchmark, we demonstrate the superiority of Fourier\nshape parametrization over conventional spherical harmonic parametrization: it\nsignificantly enhances the convergence of higher-order collective shape\nparameters by efficiently characterizing extreme nuclear deformations.\nConsequently, the new framework generates more reasonable elongated\nconfigurations, particularly for the scission configurations, and significantly\nimproves the description of charge distribution near the symmetric fission\npeak. Moreover, the Fourier shape parametrization provides a smooth and\nwell-defined three-dimensional (3D) PES with minimal correlations between\ndegrees of freedom, enabling high-precision 3D dynamical simulations of\nfission.",
        "With an appropriate $YNN$ force, the $\\Lambda$ single-particle potential\n($\\Lambda$ potential) can be made strongly repulsive at high density, and one\ncan solve the hyperon puzzle of neutron stars. We investigate the consistency\nof such a $\\Lambda$ potential, evaluated recently from $YN$ and $YNN$ forces\nbased on chiral effective field theory, with hypernuclear data and heavy-ion\ncollision data. It is found that model calculations with such a $\\Lambda$\npotential can reproduce the data of the $\\Lambda$ hypernuclear spectroscopy and\nthe $\\Lambda$ directed flow in heavy-ion collisions. Also, we evaluate the\n$\\Sigma$ potential, which can be calculated by using the same hyperon forces as\nfor the $\\Lambda$ potential. Specifically, we show that the low-energy\nconstants characterizing the strength of the $YNN$ force can be chosen to\nsuppress the appearance of the $\\Lambda$'s in neutron stars while at the same\ntime the empirical value of the $\\Sigma$ potential is reproduced.",
        "The formalism of particle number on a spatial domain for mean field wave\nfunctions with pairing is revisited to account for the case where finite\ndimensional basis are used. The formulas differ from the ones previously used\nin the literature. It is shown that the present formalism has the right limit\nin the well known case of zero pairing whereas the other formalism do not\nsatisfy this basic requirement. By using a simple one-dimensional model we\nillustrate the differences in the results for particle number distribution\nprobability obtained with the two methods.",
        "We investigate the time evolution of prompt fission $\\gamma$ emission due to\nthe presence of ns to ms isomers in the fragments produced in the\nneutron-induced fission of $^{233,235}$U and in the spontaneous fission of\n$^{252}$Cf. Calculations performed with the CGMF fission event generator are\ncompared with recent experimental data on $^{252}$Cf(sf) and $^{235}$U($n$,f)\nobtained with the DANCE+NEUANCE setup at the Los Alamos Neutron Science Center.\nOf particular interest are the average $\\gamma$-ray energy spectrum as a\nfunction of time since scission, $\\phi(\\epsilon_\\gamma,t)$, the increase of the\naverage $\\gamma$-ray multiplicity over time, $N_\\gamma(t)$, and its evolution\nin time as a function of the fission fragment mass, $N_\\gamma(A,t)$. The time\nevolution of isomeric ratios in post-neutron emission fission fragments can be\ndefined and used to test and reveal some deficiencies in our knowledge of the\nlow-lying levels of neutron-rich nuclei produced in fission reactions.",
        "The description of weakly bound nuclei using deformed few-body models has\nproven to be crucial in the study of reactions involving certain exotic nuclei.\nHowever, these core+valence models face the challenge of applying the Pauli\nexclusion principle, since the factorisation of the system does not allow\ncomplete antisymmetrization. Therefore, states occupied by core nucleons should\nbe blocked for the valence nucleons. We aim to study $^{17}$C and $^{19}$C,\nwhich are good examples of weakly bound exotic nuclei with significant\ndeformation where the valence shell is partially filled. The structure of\n$^{17}$C and $^{19}$C is described with deformed two-body models where a\nNilsson Hamiltonian is constructed using Antisymmetrized Molecular Dynamic\ncalculations of the cores. Different methods of blocking occupied Nilsson\nstates are considered using the Bardeen$-$Cooper$-$Schrieffer formalism:\nwithout blocking, total blocking and partial blocking. The latter also takes\ninto account pair correlations to some extent. These models are later used to\nstudy $^{16}$C$(d,p)^{17}$C, $^{17}$C$(p,d)^{16}$C and $^{18}$C$(d,p)^{19}$C\ntransfer reactions within the Adiabatic Distorted Wave Approximation. In the\nfirst case, the results are compared with experimental data. A good\nreproduction of the structure of $^{17}$C is found, significantly improving the\nagreement in the $^{16}$C$(d,p)^{17}$C reaction including blocking effects. The\n$^{19}$C spectrum is better reproduced considering blocking, in particular, the\npartial blocking method that considers the pairing interaction provides the\nbest description. Promising results are shown for the study of transfer\nreactions involving weakly bound exotic nuclei, by highlighting the effect of\nblocking occupied Nilsson states. We envision to extend the models to the study\nof breakup reactions and to newly discovered halo nuclei.",
        "The in-medium nucleon-nucleon scattering cross section is a pivotal quantity\nfor studying the medium effects of strong interaction, and its precise\nknowledge is critical for understanding the equation of state for dense matter,\nintermediate-energy heavy-ion collision dynamics, and related phenomena. In\nthis work, we perform a microscopic investigation of in-medium nucleon-nucleon\nscattering cross sections, by utilizing the relativistic Brueckner-Hartree-Fock\n(RBHF) theory with the Bonn potential. The fully incorporation of both\npositive- and negative-energy states in the RBHF solutions allows us to\ndetermine the single-particle potentials, the effective G matrix, and the\nscattering cross section uniquely. The momentum, density, and isospin\ndependence of the cross section for pp, nn, and np scattering are studied in\ndetail. Our results provide a solid foundation for future parameterization\nstudies of multi-parameter dependency of total scattering cross sections.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction.",
        "Understanding the equation of state (EOS) of pure neutron matter is necessary\nfor interpreting multi-messenger observations of neutron stars. Reliable data\nanalyses of these observations require well-quantified uncertainties for the\nEOS input, ideally propagating uncertainties from nuclear interactions directly\nto the EOS. This, however, requires calculations of the EOS for a prohibitively\nlarger number of nuclear Hamiltonians, solving the nuclear many-body problem\nfor each one. Quantum Monte Carlo methods, such as Auxiliary field diffusion\nMonte Carlo (AFDMC), provide precise and accurate results for the neutron\nmatter EOS but they are very computationally expensive, making them unsuitable\nfor the fast evaluations necessary for uncertainty propagation. Here, we employ\nparametric matrix models to develop fast emulators for AFDMC calculations of\nneutron matter, and use them to directly propagate uncertainties of coupling\nconstants in the Hamiltonian to the EOS. As these uncertainties include\nestimates of the EFT truncation uncertainty, this approach provides robust\nuncertainty estimates for use in astrophysical data analyses. This work will\nenable novel applications such as using astrophysical observations to put\nconstraints on coupling constants for nuclear interactions.",
        "In 2023, ASTRON took the step of incorporating a dedicated User Experience\n(UX) designer into its software development process. This decision aimed to\nenhance the accessibility and usability of services providing access to the\ndata holdings from the telescopes we are developing.\n  The field of astronomical software development has historically under\nemphasized UX design. ASTRON's initiative not only improves our own tools, but\ncan also be used to demonstrate to the broader community the value of\nintegrating UX expertise into development teams.\n  We discuss how we integrate the UX designer at the start of our software\ndevelopment lifecycle. We end with providing some considerations on how other\nprojects could make use of UX knowledge in their development process.",
        "This paper analyzes the scientific production on the COVID-19 effect in the\narea of Information Sciences from a bibliometric perspective. The objectives\nfocused on: 1) determining the most productive authors, countries, institutions\nand journals; 2) identifying the sources that constitute the core of scientific\nproduction; 3) examining the manuscripts with the greatest impact; and 4)\nvisualizing the thematic and conceptual structure of the scientific domain\nanalyzed. Bibliometric indicators and factor analysis techniques were used for\ndata analysis. A total of 1,175 publications indexed in the Web of Science\n(WoS) core collection from 2020 to 2022 were retrieved. The results showed that\nthe most relevant countries were the United States, United Kingdom, China and\nSpain. The core of the scientific production was formed by the publications:\nJournal of the American Medical Informatics Association, Information\nProfessional, Scientometrics and Journal of Health Communication. The papers\nwith the greatest impact were concentrated in those dedicated to the analysis\nof the role of telemedicine in medical care. The conceptual structure showed\nthe main research fronts, such as the role of telehealth, academic libraries\nand digital literacy in the fight against the pandemic, the role of social\nnetworks in the health crisis, as well as the problem of misinformation and\nfake news",
        "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
        "This letter proposes a new physical layer authentication mechanism operating\nat the physical layer of a communication system where the receiver has partial\ncontrol of the channel conditions (e.g., using an intelligent reflecting\nsurface). We aim to exploit both instantaneous channel state information (CSI)\nand a secret shared key for authentication. This is achieved by both\ntransmitting an identifying key by wiretap coding (to conceal the key from the\nattacker) and checking that the instantaneous CSI corresponds to the channel\nconfiguration randomly selected by the receiver. We investigate the trade-off\nbetween the pilot signals used for CSI estimation and the coding rate (or key\nlength) to improve the overall security of the authentication procedure.",
        "We theoretically investigate the thermoelectric properties (electronic\ncontribution) of a normal-superconductor (NS) hybrid junction, where the normal\nregion consists of magic-angle twisted bilayer graphene (MATBG). The\nsuperconducting region is characterized by a common $s$-wave superconductor\nclosely proximitized to the MATBG. We compute various thermoelectric\ncoefficients, including thermal conductance, thermopower, and the figure of\nmerit ($zT$), using the scattering matrix formalism. These results are further\nsupported by calculations based on a lattice-regularized version of the\neffective Hamiltonian. Additionally, we explore the impact of trigonal warping\nand valley polarization on the thermoelectric coefficients. Notably, we find a\nsignificant variation in $zT$ as a function of these parameters, reaching\nvalues as high as 2.5. Interestingly, we observe a violation of the\nWiedemann-Franz law near the charge neutrality point with the superconducting\ncorrelation, indicating that MATBG electrons behave as slow Dirac fermions in\nthis regime. This observation is further confirmed by the damped oscillatory\nbehavior of the thermal conductance as a function of the barrier strength when\nan insulating barrier is modelled at the interface of the NS junction. Beyond\ntheoretical insights, our findings suggest new possibilities for thermoelectric\napplications using MATBG based NS junctions.",
        "Active galactic nuclei (AGN) are known to be variable sources across the\nentire electromagnetic spectrum, in particular at optical\/ultraviolet and X-ray\nenergies. Over the past decades, a growing number of AGN have displayed type\ntransitions: from type 1 to type 2 or viceversa within a few years or even\nseveral months. These galaxies have been commonly referred to as changing-look\nAGN (CLAGN). Here we report on a new CLAGN, NGC 4614, which transitioned from a\ntype 1.9 to a type 2 state. NGC 4614 is a nearly face-on barred galaxy at\nredshift $z = 0.016$, classified as a low-luminosity AGN. Its central black\nhole has a mass of about $1.6\\times 10^7 M_\\odot$ and an Eddington ratio around\n1 percent. We recently acquired optical spectra of NGC 4614 at the Telescopio\nNazionale Galileo and the data clearly suggest that the broad H$\\alpha$\ncomponent has strongly dimmed, if not disappeared. A very recent Swift\nobservation confirmed our current optical data, with the AGN weakened by almost\na factor of 10 with respect to previous X-ray observations. Indeed, NGC 4614\nhad been also observed by Swift\/XRT 6 times in 2011, when the source was\nclearly detected in all observations. By fitting the stack of the 2011 Swift\nobservations we obtain a photon index of $\\Gamma=1.3\\pm0.3$ and an equivalent\nhydrogen column density of $N_{\\rm H}$=$1.2\\pm0.3$ $\\times$10$^{22}$ cm$^{-2}$,\nindicating that NGC 4614 can be moderately absorbed in the X-rays. Although a\nsignificant change in the foreground gas absorption that may have obscured the\nbroad line region cannot be entirely ruled out, the most likely explanation for\nour optical and X-ray data is that NGC 4614 is experiencing a change in the\naccretion state that reduces the radiative efficiency of the X-ray corona.",
        "The \"Mathematics in Three Acts\" approach, proposed by Dan Meyer, aims to\ntransform the teaching of mathematics through a model that encourages active\nstudent participation, fostering creativity, problem-solving, and\nmetacognition. This study explores the implementation of this approach in a\nmathematics contest for secondary school students, evaluating its impact on\nvarious key competencies. Aspects such as mathematical creativity,\nproblem-solving skills, metacognitive abilities, and students' perceptions of\nmathematics are examined. The results show that the approach contributes to the\ndevelopment of creative skills, improves understanding and problem-solving\nabilities, and increases student motivation and confidence. However, areas for\nimprovement are also identified, particularly in the justification of\nprocedures and cognitive flexibility. This study highlights the effectiveness\nof the \"Mathematics in Three Acts\" approach as an innovative methodology that\nfosters more meaningful, reflective, and autonomous learning, suggesting its\npotential to transform mathematics teaching in diverse educational contexts.",
        "The evaporation dynamics of sessile drops are crucial for material deposition\nin applications like inkjet printing and pharmaceutical development. However,\nthe evaporation behavior of high molecular weight polymer solutions and their\nimpact on deposit morphology and flow fields are not well understood. This\nstudy investigates the evaporation dynamics and deposit morphology of\npolyethylene glycol (PEG) solution drops on hydrophobic substrates, with\nmolecular weights ranging from 200 to 1000k g\/mol, covering five orders of\nmagnitude. The results show that vapor diffusion dominates the evaporation\nprocess across all PEG molecular weights. Using image analysis and\nmicro-particle image velocimetry ($\\mu$-PIV), we reveal that molecular weight\naffects contact line dynamics and internal flow, leading to diverse deposit\nmorphologies, including spherical caps, pillars, pool-shaped disks, and flat\ndisks. Transient divergence and P\\'eclet number calculations further confirm\nthe role of hydrodynamics in deposit formation. These findings provide insights\ninto the hydrodynamic and thermodynamic factors governing evaporation in\npolymeric sessile drops, with implications for material fabrication and the\ndevelopment of inkjet printing and coating techniques.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "We study the growth of supermassive black holes accounting for both accretion\nand mergers. The former is informed by observations of the quasar luminosity\nfunction (QLF) and the latter by the gravitational wave-background (GWB)\nrecently detected by PTAs, while estimates of the present-day black hole mass\nfunction provide a boundary condition. The GWB is dominated by the most massive\nblack holes ($\\gtrsim10^{9}M_{\\odot}$). We show that their evolution can be\nsimplified into a two-step process: mergers dominate at $z\\leq1$, while\naccretion peaks at $1.4\\leq z\\leq2$. The large amplitude of the observed GWB\nsuggests a significant number of mergers. We show that this generically implies\na higher average Eddington ratio for quasars relative to a scenario in which\nmergers are negligible. In the absence of mergers, matching local estimates of\nBH abundance to the QLF implies a radiative efficiency $\\epsilon_r=0.12$ and\nEddington ratio $\\lambda=0.2$. With mergers, a progenitor of mass $M_i$ is\nboosted to a final total mass $M_f$ and there is a direct relation between the\nmass gained in mergers and the average Eddington ratio of the quasar\npopulation, given by $M_f\/M_i\\sim\\lambda\/0.2$. There is thus a tension between\nthe observed GWB, quasar properties, and the BH mass function: estimates of the\nmass function consistent with Eddington ratios inferred in quasars and\n$\\epsilon_r\\sim0.1$ underpredict the GWB; multiple\/equal mass mergers can boost\nthe GWB, but lead to a high Eddington ratio. If the local mass function is on\nthe high end of current estimates, the GWB is more readily explained, but\nrequires low efficiencies $\\epsilon_r\\sim10^{-2}$ not expected in standard\nluminous accretion models. The significant merger rate implied by the GWB also\nstrongly suggests that the most massive BHs in the local universe have\nsignificant spin due to the orbital angular momentum from mergers, perhaps\n$a\\sim0.5$.",
        "This study examines the investment landscape of Pakistan as an emerging and\nfrontier market, focusing on implications for international investors,\nparticularly those in the United States, through exchange-traded funds (ETFs)\nwith exposure to Pakistan. The analysis encompasses 30 ETFs with varying\ndegrees of exposure to Pakistan, covering the period from January 1, 2016, to\nFebruary 2024. This research highlights the potential benefits and risks\nassociated with investing in these ETFs, emphasizing the importance of thorough\nrisk assessments and portfolio performance comparisons. By providing\ndescriptive statistics and performance metrics based on historical\noptimization, this paper aims to equip investors with the necessary insights to\nmake informed decisions when optimizing their portfolios with Pakistan-exposed\nETFs. The second part of the paper introduces and assesses dynamic optimization\nmethodologies. This section is designed to explore the adaptability and\nperformance metrics of dynamic optimization techniques in comparison with\nconventional historical optimization methods. By integrating dynamic\noptimization into the investigation, this research aims to offer insights into\nthe efficacy of these contrasting methodologies in the context of\nPakistan-exposed ETFs. The findings underscore the significance of Pakistan's\nmarket dynamics within the broader context of emerging markets, offering a\npathway for diversification and potential growth in investment strategies.",
        "We introduce the concept of quantum locally recoverable codes (qLRCs) with\nintersecting recovery sets. We derive a singleton-like bound for these codes by\nleveraging the additional information provided by the intersecting recovery\nsets. Furthermore, we provide a construction for qLRCs with intersecting\nrecovery sets by introducing a variation of the hypergraph product. Finally, we\napply our qLRC methods to obtain improved results for classical LRCs. These\nresults may provide new insights into the locality of quantum error correction\ncode.",
        "In ecology, the description of species composition and biodiversity calls for\nstatistical methods that involve estimating features of interest in unobserved\nsamples based on an observed one. In the last decade, the Bayesian\nnonparametrics literature has thoroughly investigated the case where data arise\nfrom a homogeneous population. In this work, we propose a novel framework to\naddress heterogeneous populations, specifically dealing with scenarios where\ndata arise from two areas. This setting significantly increases the\nmathematical complexity of the problem and, as a consequence, it received\nlimited attention in the literature. While early approaches leverage on\ncomputational methods, we provide a distributional theory for the in-sample\nanalysis of any observed sample and we enable out-of-sample prediction for the\nnumber of unseen distinct and shared species in additional samples of arbitrary\nsizes. The latter also extends the frequentist estimators which solely deal\nwith the one-step ahead prediction. Furthermore, our results can be applied to\naddress the sample size determination in sampling problems aimed at detecting\nshared species. Our results are illustrated in a real-world dataset concerning\na population of ants in the city of Trieste."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Magnetic resonance fingerprinting",
    "start_abstract":"Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging"
      ],
      "abstract":[
        "Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Imitation Learning from a Single Temporally Misaligned Video",
        "HyperArm Bandit Optimization: A Novel approach to Hyperparameter\n  Optimization and an Analysis of Bandit Algorithms in Stochastic and\n  Adversarial Settings",
        "A method for classification of data with uncertainty using hypothesis\n  testing",
        "A2Perf: Real-World Autonomous Agents Benchmark",
        "Model Monitoring in the Absence of Labeled Data via Feature Attributions\n  Distributions",
        "Covering Multiple Objectives with a Small Set of Solutions Using\n  Bayesian Optimization",
        "A Partial Initialization Strategy to Mitigate the Overfitting Problem in\n  CATE Estimation with Hidden Confounding",
        "Evaluating Time Series Foundation Models on Noisy Periodic Time Series",
        "Online-BLS: An Accurate and Efficient Online Broad Learning System for\n  Data Stream Classification",
        "Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis\n  Prediction with Uncertainty Quantification using Conformal Prediction",
        "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration\n  Strategies",
        "How Your Location Relates to Health: Variable Importance and\n  Interpretable Machine Learning for Environmental and Sociodemographic Data",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "Eigenvalue conditions implying edge-disjoint spanning trees and a forest\n  with constraints",
        "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws",
        "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "Personalized Interpolation: An Efficient Method to Tame Flexible\n  Optimization Window Estimation",
        "Logarithmic Width Suffices for Robust Memorization",
        "On the stress transit function",
        "Time Series Language Model for Descriptive Caption Generation",
        "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent\n  Collaborative Field Coverage",
        "TULIP: Towards Unified Language-Image Pretraining",
        "Towards Generalizable Trajectory Prediction Using Dual-Level\n  Representation Learning And Adaptive Prompting",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "The Ball-Proximal (=\"Broximal\") Point Method: a New Algorithm,\n  Convergence Theory, and Applications",
        "Further Results for the Capacity Statistic Distribution on Compositions\n  of 1's and 2's",
        "3D Point Cloud Generation via Autoregressive Up-sampling",
        "Rerailing Automata"
      ],
      "abstract":[
        "We examine the problem of learning sequential tasks from a single visual\ndemonstration. A key challenge arises when demonstrations are temporally\nmisaligned due to variations in timing, differences in embodiment, or\ninconsistencies in execution. Existing approaches treat imitation as a\ndistribution-matching problem, aligning individual frames between the agent and\nthe demonstration. However, we show that such frame-level matching fails to\nenforce temporal ordering or ensure consistent progress. Our key insight is\nthat matching should instead be defined at the level of sequences. We propose\nthat perfect matching occurs when one sequence successfully covers all the\nsubgoals in the same order as the other sequence. We present ORCA (ORdered\nCoverage Alignment), a dense per-timestep reward function that measures the\nprobability of the agent covering demonstration frames in the correct order. On\ntemporally misaligned demonstrations, we show that agents trained with the ORCA\nreward achieve $4.5$x improvement ($0.11 \\rightarrow 0.50$ average normalized\nreturns) for Meta-world tasks and $6.6$x improvement ($6.55 \\rightarrow 43.3$\naverage returns) for Humanoid-v4 tasks compared to the best frame-level\nmatching algorithms. We also provide empirical analysis showing that ORCA is\nrobust to varying levels of temporal misalignment. Our code is available at\nhttps:\/\/github.com\/portal-cornell\/orca\/",
        "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization.",
        "Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.",
        "Autonomous agents and systems cover a number of application areas, from\nrobotics and digital assistants to combinatorial optimization, all sharing\ncommon, unresolved research challenges. It is not sufficient for agents to\nmerely solve a given task; they must generalize to out-of-distribution tasks,\nperform reliably, and use hardware resources efficiently during training and\ninference, among other requirements. Several methods, such as reinforcement\nlearning and imitation learning, are commonly used to tackle these problems,\neach with different trade-offs. However, there is a lack of benchmarking suites\nthat define the environments, datasets, and metrics which can be used to\nprovide a meaningful way for the community to compare progress on applying\nthese methods to real-world problems. We introduce A2Perf--a benchmark with\nthree environments that closely resemble real-world domains: computer chip\nfloorplanning, web navigation, and quadruped locomotion. A2Perf provides\nmetrics that track task performance, generalization, system resource\nefficiency, and reliability, which are all critical to real-world applications.\nUsing A2Perf, we demonstrate that web navigation agents can achieve latencies\ncomparable to human reaction times on consumer hardware, reveal reliability\ntrade-offs between algorithms for quadruped locomotion, and quantify the energy\ncosts of different learning approaches for computer chip-design. In addition,\nwe propose a data cost metric to account for the cost incurred acquiring\noffline data for imitation learning and hybrid algorithms, which allows us to\nbetter compare these approaches. A2Perf also contains several standard\nbaselines, enabling apples-to-apples comparisons across methods and\nfacilitating progress in real-world autonomy. As an open-source benchmark,\nA2Perf is designed to remain accessible, up-to-date, and useful to the research\ncommunity over the long term.",
        "Model monitoring involves analyzing AI algorithms once they have been\ndeployed and detecting changes in their behaviour. This thesis explores machine\nlearning model monitoring ML before the predictions impact real-world decisions\nor users. This step is characterized by one particular condition: the absence\nof labelled data at test time, which makes it challenging, even often\nimpossible, to calculate performance metrics.\n  The thesis is structured around two main themes: (i) AI alignment, measuring\nif AI models behave in a manner consistent with human values and (ii)\nperformance monitoring, measuring if the models achieve specific accuracy goals\nor desires.\n  The thesis uses a common methodology that unifies all its sections. It\nexplores feature attribution distributions for both monitoring dimensions.\nUsing these feature attribution explanations, we can exploit their theoretical\nproperties to derive and establish certain guarantees and insights into model\nmonitoring.",
        "In multi-objective black-box optimization, the goal is typically to find\nsolutions that optimize a set of T black-box objective functions, $f_1$, ...,\n$f_T$, simultaneously. Traditional approaches often seek a single\nPareto-optimal set that balances trade-offs among all objectives. In this work,\nwe introduce a novel problem setting that departs from this paradigm: finding a\nsmaller set of K solutions, where K < T, that collectively \"covers\" the T\nobjectives. A set of solutions is defined as \"covering\" if, for each objective\n$f_1$, ..., $f_T$, there is at least one good solution. A motivating example\nfor this problem setting occurs in drug design. For example, we may have T\npathogens and aim to identify a set of K < T antibiotics such that at least one\nantibiotic can be used to treat each pathogen. To address this problem, we\npropose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled\nalgorithm designed to efficiently find a covering set. We validate our approach\nthrough extensive experiments on challenging high-dimensional tasks, including\napplications in peptide and molecular design. Experiments demonstrate MOCOBO's\nability to find high-performing covering sets of solutions. Additionally, we\nshow that the small sets of K < T solutions found by MOCOBO can match or nearly\nmatch the performance of T individually optimized solutions for the same\nobjectives. Our results highlight MOCOBO's potential to tackle complex\nmulti-objective problems in domains where finding at least one high-performing\nsolution for each objective is critical.",
        "Estimating the conditional average treatment effect (CATE) from observational\ndata plays a crucial role in areas such as e-commerce, healthcare, and\neconomics. Existing studies mainly rely on the strong ignorability assumption\nthat there are no hidden confounders, whose existence cannot be tested from\nobservational data and can invalidate any causal conclusion. In contrast, data\ncollected from randomized controlled trials (RCT) do not suffer from\nconfounding but are usually limited by a small sample size. To avoid\noverfitting caused by the small-scale RCT data, we propose a novel two-stage\npretraining-finetuning (TSPF) framework with a partial parameter initialization\nstrategy to estimate the CATE in the presence of hidden confounding. In the\nfirst stage, a foundational representation of covariates is trained to estimate\ncounterfactual outcomes through large-scale observational data. In the second\nstage, we propose to train an augmented representation of the covariates, which\nis concatenated with the foundational representation obtained in the first\nstage to adjust for the hidden confounding. Rather than training a separate\nnetwork from scratch, part of the prediction heads are initialized from the\nfirst stage. The superiority of our approach is validated on two datasets with\nextensive experiments.",
        "While recent advancements in foundation models have significantly impacted\nmachine learning, rigorous tests on the performance of time series foundation\nmodels (TSFMs) remain largely underexplored. This paper presents an empirical\nstudy evaluating the zero-shot, long-horizon forecasting abilities of several\nleading TSFMs over two synthetic datasets constituting noisy periodic time\nseries. We assess model efficacy across different noise levels, underlying\nfrequencies, and sampling rates. As benchmarks for comparison, we choose two\nstatistical techniques: a Fourier transform (FFT)-based approach and a linear\nautoregressive (AR) model. Our findings demonstrate that while for time series\nwith bounded periods and higher sampling rates, TSFMs can match or outperform\nthe statistical approaches, their forecasting abilities deteriorate with longer\nperiods, higher noise levels, lower sampling rates and more complex shapes of\nthe time series.",
        "The state-of-the-art online learning models generally conduct a single online\ngradient descent when a new sample arrives and thus suffer from suboptimal\nmodel weights. To this end, we introduce an online broad learning system\nframework with closed-form solutions for each online update. Different from\nemploying existing incremental broad learning algorithms for online learning\ntasks, which tend to incur degraded accuracy and expensive online update\noverhead, we design an effective weight estimation algorithm and an efficient\nonline updating strategy to remedy the above two deficiencies, respectively.\nSpecifically, an effective weight estimation algorithm is first developed by\nreplacing notorious matrix inverse operations with Cholesky decomposition and\nforward-backward substitution to improve model accuracy. Second, we devise an\nefficient online updating strategy that dramatically reduces online update\ntime. Theoretical analysis exhibits the splendid error bound and low time\ncomplexity of our model. The most popular test-then-training evaluation\nexperiments on various real-world datasets prove its superiority and\nefficiency. Furthermore, our framework is naturally extended to data stream\nscenarios with concept drift and exceeds state-of-the-art baselines.",
        "Sepsis is a life-threatening syndrome with high morbidity and mortality in\nhospitals. Early prediction of sepsis plays a crucial role in facilitating\nearly interventions for septic patients. However, early sepsis prediction\nsystems with uncertainty quantification and adaptive learning are scarce. This\npaper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis\nprediction by integrating conformal prediction for uncertainty quantification\nand Bayesian bandits for adaptive decision-making. By combining the robustness\nof Bayesian models with the statistical uncertainty guarantees of conformal\nprediction methodologies, this algorithm delivers accurate and trustworthy\npredictions, addressing the critical need for reliable and adaptive systems in\nhigh-stakes healthcare applications such as early sepsis prediction. We\nevaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit\nsetting, the area under the receiver operating characteristic curve (AUROC),\nand F-measure. Our results show that Sepsyn-OLCP outperforms existing\nindividual models, increasing AUROC of a neural network from 0.64 to 0.73\nwithout retraining and high computational costs. And the model selection policy\nconverges to the optimal strategy in the long run. We propose a novel\nreinforcement learning-based framework integrated with conformal prediction\ntechniques to provide uncertainty quantification for early sepsis prediction.\nThe proposed methodology delivers accurate and trustworthy predictions,\naddressing a critical need in high-stakes healthcare applications like early\nsepsis prediction.",
        "Soft Actor-Critic (SAC) has achieved notable success in continuous control\ntasks but struggles in sparse reward settings, where infrequent rewards make\nefficient exploration challenging. While novelty-based exploration methods\naddress this issue by encouraging the agent to explore novel states, they are\nnot trivial to apply to SAC. In particular, managing the interaction between\nnovelty-based exploration and SAC's stochastic policy can lead to inefficient\nexploration and redundant sample collection. In this paper, we propose KEA\n(Keeping Exploration Alive) which tackles the inefficiencies in balancing\nexploration strategies when combining SAC with novelty-based exploration. KEA\nintroduces an additional co-behavior agent that works alongside SAC and a\nswitching mechanism to facilitate proactive coordination between exploration\nstrategies from novelty-based exploration and stochastic policy. This\ncoordination allows the agent to maintain stochasticity in high-novelty\nregions, enhancing exploration efficiency and reducing repeated sample\ncollection. We first analyze this potential issue in a 2D navigation task and\nthen evaluate KEA on sparse reward control tasks from the DeepMind Control\nSuite. Compared to state-of-the-art novelty-based exploration baselines, our\nexperiments show that KEA significantly improves learning efficiency and\nrobustness in sparse reward setups.",
        "Health outcomes depend on complex environmental and sociodemographic factors\nwhose effects change over location and time. Only recently has fine-grained\nspatial and temporal data become available to study these effects, namely the\nMEDSAT dataset of English health, environmental, and sociodemographic\ninformation. Leveraging this new resource, we use a variety of variable\nimportance techniques to robustly identify the most informative predictors\nacross multiple health outcomes. We then develop an interpretable machine\nlearning framework based on Generalized Additive Models (GAMs) and Multiscale\nGeographically Weighted Regression (MGWR) to analyze both local and global\nspatial dependencies of each variable on various health outcomes. Our findings\nidentify NO2 as a global predictor for asthma, hypertension, and anxiety,\nalongside other outcome-specific predictors related to occupation, marriage,\nand vegetation. Regional analyses reveal local variations with air pollution\nand solar radiation, with notable shifts during COVID. This comprehensive\napproach provides actionable insights for addressing health disparities, and\nadvocates for the integration of interpretable machine learning in public\nhealth.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Let $G$ be a nontrivial graph with minimum degree $\\delta$ and $k$ an integer\nwith $k\\ge 2$. In the literature, there are eigenvalue conditions that imply\n$G$ contains $k$ edge-disjoint spanning trees. We give eigenvalue conditions\nthat imply $G$ contains $k$ edge-disjoint spanning trees and another forest $F$\nwith $|E(F)|>\\frac{\\delta-1}{\\delta}(|V(G)|-1)$, and if $F$ is not a spanning\ntree, then $F$ has a component with at least $\\delta$ edges.",
        "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.",
        "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
        "In the realm of online advertising, optimizing conversions is crucial for\ndelivering relevant products to users and enhancing business outcomes.\nPredicting conversion events is challenging due to variable delays between user\ninteractions, such as impressions or clicks, and the actual conversions. These\ndelays differ significantly across various advertisers and products,\nnecessitating distinct optimization time windows for targeted conversions. To\naddress this, we introduce a novel approach named the \\textit{Personalized\nInterpolation} method, which innovatively builds upon existing fixed conversion\nwindow models to estimate flexible conversion windows. This method allows for\nthe accurate estimation of conversions across a variety of delay ranges, thus\nmeeting the diverse needs of advertisers without increasing system complexity.\nTo validate the efficacy of our proposed method, we conducted comprehensive\nexperiments using ads conversion model. Our experiments demonstrate that this\nmethod not only achieves high prediction accuracy but also does so more\nefficiently than other existing solutions. This validation underscores the\npotential of our Personalized Interpolation method to significantly enhance\nconversion optimization in real-world online advertising systems, promising\nimproved targeting and effectiveness in advertising strategies.",
        "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$).",
        "The stress interval $S(u,v)$ between $u,v\\in V(G)$ is the set of all vertices\nin a graph $G$ that lie on every shortest $u,v$-path. A set $U \\subseteq V(G)$\nis stress convex if $S(u,v) \\subseteq U$ for any $u,v\\in U$. A vertex $v \\in\nV(G)$ is s-extreme if $V(G)-v$ is a stress convex set in $G$. The stress number\n$sn(G)$ of $G$ is the minimum cardinality of a set $U$ where $\\bigcup_{u,v \\in\nU}S(u,v)=V(G)$. The stress hull number $sh(G)$ of $G$ is the minimum\ncardinality of a set whose stress convex hull is $V(G)$. In this paper, we\npresent many basic properties of stress intervals. We characterize s-extreme\nvertices of a graph $G$ and construct graphs $G$ with arbitrarily large\ndifference between the number of s-extreme vertices, $sh(G)$ and $sn(G)$. Then\nwe study these three invariants for some special graph families, such as graph\nproducts, split graphs, and block graphs. We show that in any split graph $G$,\n$sh(G)=sn(G)=|Ext_s(G)|$, where $Ext_s(G)$ is the set of s-extreme vertices of\n$G$. Finally, we show that for $k \\in \\mathbb{N}$, deciding whether $sn(G) \\leq\nk$ is NP-complete problem, even when restricted to bipartite graphs.",
        "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
        "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.",
        "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image\/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code\/checkpoints are available at\nhttps:\/\/tulip-berkeley.github.io",
        "Existing vehicle trajectory prediction models struggle with generalizability,\nprediction uncertainties, and handling complex interactions. It is often due to\nlimitations like complex architectures customized for a specific dataset and\ninefficient multimodal handling. We propose Perceiver with Register queries\n(PerReg+), a novel trajectory prediction framework that introduces: (1)\nDual-Level Representation Learning via Self-Distillation (SD) and Masked\nReconstruction (MR), capturing global context and fine-grained details.\nAdditionally, our approach of reconstructing segmentlevel trajectories and lane\nsegments from masked inputs with query drop, enables effective use of\ncontextual information and improves generalization; (2) Enhanced Multimodality\nusing register-based queries and pretraining, eliminating the need for\nclustering and suppression; and (3) Adaptive Prompt Tuning during fine-tuning,\nfreezing the main architecture and optimizing a small number of prompts for\nefficient adaptation. PerReg+ sets a new state-of-the-art performance on\nnuScenes [1], Argoverse 2 [2], and Waymo Open Motion Dataset (WOMD) [3].\nRemarkable, our pretrained model reduces the error by 6.8% on smaller datasets,\nand multi-dataset training enhances generalization. In cross-domain tests,\nPerReg+ reduces B-FDE by 11.8% compared to its non-pretrained variant.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "Non-smooth and non-convex global optimization poses significant challenges\nacross various applications, where standard gradient-based methods often\nstruggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or\nBall Point Method (BPM) for short - a novel algorithmic framework inspired by\nthe classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we\nshow, sheds new light on several foundational optimization paradigms and\nphenomena, including non-convex and non-smooth optimization, acceleration,\nsmoothing, adaptive stepsize selection, and trust-region methods. At the core\nof BPM lies the ball-proximal (\"broximal\") operator, which arises from the\nclassical proximal operator by replacing the quadratic distance penalty by a\nball constraint. Surprisingly, and in sharp contrast with the sublinear rate of\nPPM in the nonsmooth convex regime, we prove that BPM converges linearly and in\na finite number of steps in the same regime. Furthermore, by introducing the\nconcept of ball-convexity, we prove that BPM retains the same global\nconvergence guarantees under weaker assumptions, making it a powerful tool for\na broader class of potentially non-convex optimization problems. Just like PPM\nplays the role of a conceptual method inspiring the development of practically\nefficient algorithms and algorithmic elements, e.g., gradient descent, adaptive\nstep sizes, acceleration (Ahn & Sra, 2020), and \"W\" in AdamW (Zhuang et al.,\n2022), we believe that BPM should be understood in the same manner: as a\nblueprint and inspiration for further development.",
        "In this paper, we study additional aspects of the capacity distribution on\nthe set $\\mathcal{B}_n$ of compositions of $n$ consisting of $1$'s and $2$'s.\nAmong our results are further recurrences for this distribution as well as\nformulas for the total capacity and sign balance on $\\mathcal{B}_n$. We provide\nalgebraic and combinatorial proofs of our results. We also give combinatorial\nexplanations of some prior results where such a proof was requested. Finally,\nthe joint distribution of the capacity statistic with two further parameters on\n$\\mathcal{B}_n$ is briefly considered.",
        "We introduce a pioneering autoregressive generative model for 3D point cloud\ngeneration. Inspired by visual autoregressive modeling (VAR), we conceptualize\npoint cloud generation as an autoregressive up-sampling process. This leads to\nour novel model, PointARU, which progressively refines 3D point clouds from\ncoarse to fine scales. PointARU follows a two-stage training paradigm: first,\nit learns multi-scale discrete representations of point clouds, and then it\ntrains an autoregressive transformer for next-scale prediction. To address the\ninherent unordered and irregular structure of point clouds, we incorporate\nspecialized point-based up-sampling network modules in both stages and\nintegrate 3D absolute positional encoding based on the decoded point cloud at\neach scale during the second stage. Our model surpasses state-of-the-art (SoTA)\ndiffusion-based approaches in both generation quality and parameter efficiency\nacross diverse experimental settings, marking a new milestone for\nautoregressive methods in 3D point cloud generation. Furthermore, PointARU\ndemonstrates exceptional performance in completing partial 3D shapes and\nup-sampling sparse point clouds, outperforming existing generative models in\nthese tasks.",
        "In this paper, we introduce rerailing automata for $\\omega$-regular\nlanguages. They generalize both deterministic parity (DPW) and minimized\nhistory-deterministic co-B\\\"uchi automata (with transition based acceptance,\nHdTbcBW) while combining their favorable properties. In particular, rerailing\nautomata can represent arbitrary $\\omega$-regular languages while allowing for\npolynomial-time minimization, just as HdTbcBW do. Since DPW are a special case\nof rerailing automata, a minimized rerailing automaton is never larger than the\nsmallest deterministic parity automaton for the same language. We also show\nthat rerailing automata can be used as a replacement for deterministic parity\nautomata for the realizability check of open systems.\n  The price to be paid to obtain the useful properties of rerailing automata is\nthat the acceptance condition in such automata refers to the dominating colors\nalong all runs for a given word, where just as in parity automata, the\ndominating color along a run is the lowest one occurring infinitely often along\nit. A rerailing automaton accepts those words for which the greatest of the\ndominating colors along the runs is even. Additionally, rerailing automata\nguarantee that every prefix of a run for a word can be extended to eventually\nreach a point from which all runs for the word extending the prefix have the\nsame dominating color, and it is even if and only if the word is in the\nlanguage of the automaton. We show that these properties together allow\ncharacterizing the role of each state in such an automaton in a way that\nrelates it to state combinations in a sequence of co-B\\\"uchi automata for the\nrepresented language. This characterization forms the basis of the\npolynomial-time minimization approach in this paper."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Additive manufacturing and sustainability: an exploratory study of the advantages and challenges",
    "start_abstract":"The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Machine learning in additive manufacturing: State-of-the-art and perspectives"
      ],
      "abstract":[
        "Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "No-Regret Linear Bandits under Gap-Adjusted Misspecification",
        "Continually Evolved Multimodal Foundation Models for Cancer Prognosis",
        "Mechanistic PDE Networks for Discovery of Governing Equations",
        "Fewer May Be Better: Enhancing Offline Reinforcement Learning with\n  Reduced Dataset",
        "Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as\n  hyperelastic constitutive artificial neural networks (CANs)",
        "Comparative Study of Deep Learning Architectures for Textual Damage\n  Level Classification",
        "Inorganic Catalyst Efficiency Prediction Based on EAPCR Model: A Deep\n  Learning Solution for Multi-Source Heterogeneous Data",
        "KNN and K-means in Gini Prametric Spaces",
        "Value Gradient Sampler: Sampling as Sequential Decision Making",
        "BOPO: Neural Combinatorial Optimization via Best-anchored and\n  Objective-guided Preference Optimization",
        "Model Monitoring in the Absence of Labeled Data via Feature Attributions\n  Distributions",
        "Preconditioned Inexact Stochastic ADMM for Deep Model",
        "Graph Augmentation for Cross Graph Domain Generalization",
        "RF Desense significance and its impact on the EVM at Signal Near the\n  Noise Floor",
        "Concentration phenomena for a mixed local\/nonlocal Schr\\\"{o}dinger\n  equation with Dirichlet datum",
        "Toward a Flexible Framework for Linear Representation Hypothesis Using\n  Maximum Likelihood Estimation",
        "Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based\n  Perspective",
        "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and\n  Deployment",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
        "Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum",
        "Disharmony: Forensics using Reverse Lighting Harmonization",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge",
        "Large language models streamline automated systematic review: A\n  preliminary study"
      ],
      "abstract":[
        "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1\/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1\/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign.",
        "Cancer prognosis is a critical task that involves predicting patient outcomes\nand survival rates. To enhance prediction accuracy, previous studies have\nintegrated diverse data modalities, such as clinical notes, medical images, and\ngenomic data, leveraging their complementary information. However, existing\napproaches face two major limitations. First, they struggle to incorporate\nnewly arrived data with varying distributions into training, such as patient\nrecords from different hospitals, thus rendering sub-optimal generalizability\nand limited utility in real-world applications. Second, most multimodal\nintegration methods rely on simplistic concatenation or task-specific\npipelines, which fail to capture the complex interdependencies across\nmodalities. To address these, we propose a continually evolving multi-modal\nfoundation model. Extensive experiments on the TCGA dataset demonstrate the\neffectiveness of our approach, highlighting its potential to advance cancer\nprognosis by enabling robust and adaptive multimodal integration.",
        "We present Mechanistic PDE Networks -- a model for discovery of governing\npartial differential equations from data. Mechanistic PDE Networks represent\nspatiotemporal data as space-time dependent linear partial differential\nequations in neural network hidden representations. The represented PDEs are\nthen solved and decoded for specific tasks. The learned PDE representations\nnaturally express the spatiotemporal dynamics in data in neural network hidden\nspace, enabling increased power for dynamical modeling. Solving the PDE\nrepresentations in a compute and memory-efficient way, however, is a\nsignificant challenge. We develop a native, GPU-capable, parallel, sparse, and\ndifferentiable multigrid solver specialized for linear partial differential\nequations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE\nsolver, we propose a discovery architecture that can discover nonlinear PDEs in\ncomplex settings while also being robust to noise. We validate PDE discovery on\na number of PDEs, including reaction-diffusion and Navier-Stokes equations.",
        "Offline reinforcement learning (RL) represents a significant shift in RL\nresearch, allowing agents to learn from pre-collected datasets without further\ninteraction with the environment. A key, yet underexplored, challenge in\noffline RL is selecting an optimal subset of the offline dataset that enhances\nboth algorithm performance and training efficiency. Reducing dataset size can\nalso reveal the minimal data requirements necessary for solving similar\nproblems. In response to this challenge, we introduce ReDOR (Reduced Datasets\nfor Offline RL), a method that frames dataset selection as a gradient\napproximation optimization problem. We demonstrate that the widely used\nactor-critic framework in RL can be reformulated as a submodular optimization\nobjective, enabling efficient subset selection. To achieve this, we adapt\northogonal matching pursuit (OMP), incorporating several novel modifications\ntailored for offline RL. Our experimental results show that the data subsets\nidentified by ReDOR not only boost algorithm performance but also do so with\nsignificantly lower computational complexity.",
        "Traditional constitutive models rely on hand-crafted parametric forms with\nlimited expressivity and generalizability, while neural network-based models\ncan capture complex material behavior but often lack interpretability. To\nbalance these trade-offs, we present Input-Convex Kolmogorov-Arnold Networks\n(ICKANs) for learning polyconvex hyperelastic constitutive laws. ICKANs\nleverage the Kolmogorov-Arnold representation, decomposing the model into\ncompositions of trainable univariate spline-based activation functions for rich\nexpressivity. We introduce trainable input-convex splines within the KAN\narchitecture, ensuring physically admissible polyconvex hyperelastic models.\nThe resulting models are both compact and interpretable, enabling explicit\nextraction of analytical constitutive relationships through an input-convex\nsymbolic regression techinque. Through unsupervised training on full-field\nstrain data and limited global force measurements, ICKANs accurately capture\nnonlinear stress-strain behavior across diverse strain states. Finite element\nsimulations of unseen geometries with trained ICKAN hyperelastic constitutive\nmodels confirm the framework's robustness and generalization capability.",
        "Given the paramount importance of safety in the aviation industry, even minor\noperational anomalies can have significant consequences. Comprehensive\ndocumentation of incidents and accidents serves to identify root causes and\npropose safety measures. However, the unstructured nature of incident event\nnarratives poses a challenge for computer systems to interpret. Our study aimed\nto leverage Natural Language Processing (NLP) and deep learning models to\nanalyze these narratives and classify the aircraft damage level incurred during\nsafety occurrences. Through the implementation of LSTM, BLSTM, GRU, and sRNN\ndeep learning models, our research yielded promising results, with all models\nshowcasing competitive performance, achieving an accuracy of over 88%\nsignificantly surpassing the 25% random guess threshold for a four-class\nclassification problem. Notably, the sRNN model emerged as the top performer in\nterms of recall and accuracy, boasting a remarkable 89%. These findings\nunderscore the potential of NLP and deep learning models in extracting\nactionable insights from unstructured text narratives, particularly in\nevaluating the extent of aircraft damage within the realm of aviation safety\noccurrences.",
        "The design of inorganic catalysts and the prediction of their catalytic\nefficiency are fundamental challenges in chemistry and materials science.\nTraditional catalyst evaluation methods primarily rely on machine learning\ntechniques; however, these methods often struggle to process multi-source\nheterogeneous data, limiting both predictive accuracy and generalization. To\naddress these limitations, this study introduces the\nEmbedding-Attention-Permutated CNN-Residual (EAPCR) deep learning model. EAPCR\nconstructs a feature association matrix using embedding and attention\nmechanisms and enhances predictive performance through permutated CNN\narchitectures and residual connections. This approach enables the model to\naccurately capture complex feature interactions across various catalytic\nconditions, leading to precise efficiency predictions. EAPCR serves as a\npowerful tool for computational researchers while also assisting domain experts\nin optimizing catalyst design, effectively bridging the gap between data-driven\nmodeling and experimental applications. We evaluate EAPCR on datasets from TiO2\nphotocatalysis, thermal catalysis, and electrocatalysis, demonstrating its\nsuperiority over traditional machine learning methods (e.g., linear regression,\nrandom forest) as well as conventional deep learning models (e.g., ANN, NNs).\nAcross multiple evaluation metrics (MAE, MSE, R2, and RMSE), EAPCR consistently\noutperforms existing approaches. These findings highlight the strong potential\nof EAPCR in inorganic catalytic efficiency prediction. As a versatile deep\nlearning framework, EAPCR not only improves predictive accuracy but also\nestablishes a solid foundation for future large-scale model development in\ninorganic catalysis.",
        "This paper introduces innovative enhancements to the K-means and K-nearest\nneighbors (KNN) algorithms based on the concept of Gini prametric spaces.\nUnlike traditional distance metrics, Gini-based measures incorporate both\nvalue-based and rank-based information, improving robustness to noise and\noutliers. The main contributions of this work include: proposing a Gini-based\nmeasure that captures both rank information and value distances; presenting a\nGini K-means algorithm that is proven to converge and demonstrates resilience\nto noisy data; and introducing a Gini KNN method that performs competitively\nwith state-of-the-art approaches such as Hassanat's distance in noisy\nenvironments. Experimental evaluations on 14 datasets from the UCI repository\ndemonstrate the superior performance and efficiency of Gini-based algorithms in\nclustering and classification tasks. This work opens new avenues for leveraging\nrank-based measures in machine learning and statistical analysis.",
        "We propose the Value Gradient Sampler (VGS), a trainable sampler based on the\ninterpretation of sampling as discrete-time sequential decision-making. VGS\ngenerates samples from a given unnormalized density (i.e., energy) by drifting\nand diffusing randomly initialized particles. In VGS, finding the optimal drift\nis equivalent to solving an optimal control problem where the cost is the upper\nbound of the KL divergence between the target density and the samples. We\nemploy value-based dynamic programming to solve this optimal control problem,\nwhich gives the gradient of the value function as the optimal drift vector. The\nconnection to sequential decision making allows VGS to leverage extensively\nstudied techniques in reinforcement learning, making VGS a fast, adaptive, and\naccurate sampler that achieves competitive results in various sampling\nbenchmarks. Furthermore, VGS can replace MCMC in contrastive divergence\ntraining of energy-based models. We demonstrate the effectiveness of VGS in\ntraining accurate energy-based models in industrial anomaly detection\napplications.",
        "Neural Combinatorial Optimization (NCO) has emerged as a promising approach\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\nsample efficiency due to sparse rewards and underused solutions. We propose\nPreference Optimization for Combinatorial Optimization (POCO), a training\nparadigm that leverages solution preferences via objective values. It\nintroduces: (1) an efficient preference pair construction for better explore\nand exploit solutions, and (2) a novel loss function that adaptively scales\ngradients via objective differences, removing reliance on reward models or\nreference policies. Experiments on Job-Shop Scheduling (JSP), Traveling\nSalesman (TSP), and Flexible Job-Shop Scheduling (FJSP) show POCO outperforms\nstate-of-the-art neural methods, reducing optimality gaps impressively with\nefficient inference. POCO is architecture-agnostic, enabling seamless\nintegration with existing NCO models, and establishes preference optimization\nas a principled framework for combinatorial optimization.",
        "Model monitoring involves analyzing AI algorithms once they have been\ndeployed and detecting changes in their behaviour. This thesis explores machine\nlearning model monitoring ML before the predictions impact real-world decisions\nor users. This step is characterized by one particular condition: the absence\nof labelled data at test time, which makes it challenging, even often\nimpossible, to calculate performance metrics.\n  The thesis is structured around two main themes: (i) AI alignment, measuring\nif AI models behave in a manner consistent with human values and (ii)\nperformance monitoring, measuring if the models achieve specific accuracy goals\nor desires.\n  The thesis uses a common methodology that unifies all its sections. It\nexplores feature attribution distributions for both monitoring dimensions.\nUsing these feature attribution explanations, we can exploit their theoretical\nproperties to derive and establish certain guarantees and insights into model\nmonitoring.",
        "The recent advancement of foundation models (FMs) has brought about a\nparadigm shift, revolutionizing various sectors worldwide. The popular\noptimizers used to train these models are stochastic gradient descent-based\nalgorithms, which face inherent limitations, such as slow convergence and\nstringent assumptions for convergence. In particular, data heterogeneity\narising from distributed settings poses significant challenges to their\ntheoretical and numerical performance. This paper develops an algorithm, PISA\n({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of\nMultipliers), which enables scalable parallel computing and supports various\nsecond-moment schemes. Grounded in rigorous theoretical guarantees, the\nalgorithm converges under the sole assumption of Lipschitz continuity of the\ngradient, thereby removing the need for other conditions commonly imposed by\nstochastic methods. This capability enables PISA to tackle the challenge of\ndata heterogeneity effectively. Comprehensive experimental evaluations for\ntraining or fine-tuning diverse FMs, including vision models, large language\nmodels, reinforcement learning models, generative adversarial networks, and\nrecurrent neural networks, demonstrate its superior numerical performance\ncompared to various state-of-the-art optimizers.",
        "Cross-graph node classification, utilizing the abundant labeled nodes from\none graph to help classify unlabeled nodes in another graph, can be viewed as a\ndomain generalization problem of graph neural networks (GNNs) due to the\nstructure shift commonly appearing among various graphs. Nevertheless, current\nendeavors for cross-graph node classification mainly focus on model training.\nData augmentation approaches, a simple and easy-to-implement domain\ngeneralization technique, remain under-explored. In this paper, we develop a\nnew graph structure augmentation for the crossgraph domain generalization\nproblem. Specifically, low-weight edgedropping is applied to remove potential\nnoise edges that may hinder the generalization ability of GNNs, stimulating the\nGNNs to capture the essential invariant information underlying different\nstructures. Meanwhile, clustering-based edge-adding is proposed to generate\ninvariant structures based on the node features from the same distribution.\nConsequently, with these augmentation techniques, the GNNs can maintain the\ndomain invariant structure information that can improve the generalization\nability. The experiments on out-ofdistribution citation network datasets verify\nour method achieves state-of-the-art performance among conventional\naugmentations.",
        "Hardware impairments and system non-linearities impacting communication\nsignal is one of key aspect for having harmonics and RF desense which overall\ncausing the lower quality and integrity of the modulated signal, resulting in\nI\/Q imbalance, further bit error and spectral efficiency degradation. This\npresentation outlines the RF Desense results, EVM Measurement and it impact at\nthe almost noise floor with step size by 1 dB in QPSK at LTE Bands, Note for\nmmW 3GPP 38.521-2 clause 6.4.2.1 indicates single polarization.",
        "We consider the mixed local\/nonlocal semilinear equation\n  \\begin{equation*}\n  -\\epsilon^{2}\\Delta u +\\epsilon^{2s}(-\\Delta)^s u +u=u^p\\qquad \\text{in }\n\\Omega\n  \\end{equation*} with zero Dirichlet datum, where $\\epsilon>0$ is a small\nparameter, $s\\in(0,1)$, $p\\in(1,\\frac{n+2}{n-2})$ and $\\Omega$ is a smooth,\nbounded domain. We construct a family of solutions that concentrate, as\n$\\epsilon\\rightarrow 0$, at an interior point of $\\Omega$ having uniform\ndistance to $\\partial\\Omega$ (this point can also be characterized as a local\nminimum of a nonlocal functional).\n  In spite of the presence of the Laplace operator, the leading order of the\nrelevant reduced energy functional in the Lyapunov-Schmidt procedure is\npolynomial rather than exponential in the distance to the boundary, in light of\nthe nonlocal effect at infinity. A delicate analysis is required to establish\nsome uniform estimates with respect to $\\epsilon$, due to the difficulty caused\nby the different scales coming from the mixed operator.",
        "Linear representation hypothesis posits that high-level concepts are encoded\nas linear directions in the representation spaces of LLMs. Park et al. (2024)\nformalize this notion by unifying multiple interpretations of linear\nrepresentation, such as 1-dimensional subspace representation and\ninterventions, using a causal inner product. However, their framework relies on\nsingle-token counterfactual pairs and cannot handle ambiguous contrasting\npairs, limiting its applicability to complex or context-dependent concepts. We\nintroduce a new notion of binary concepts as unit vectors in a canonical\nrepresentation space, and utilize LLMs' (neural) activation differences along\nwith maximum likelihood estimation (MLE) to compute concept directions (i.e.,\nsteering vectors). Our method, Sum of Activation-base Normalized Difference\n(SAND), formalizes the use of activation differences modeled as samples from a\nvon Mises-Fisher (vMF) distribution, providing a principled approach to derive\nconcept directions. We extend the applicability of Park et al. (2024) by\neliminating the dependency on unembedding representations and single-token\npairs. Through experiments with LLaMA models across diverse concepts and\nbenchmarks, we demonstrate that our lightweight approach offers greater\nflexibility, superior performance in activation engineering tasks like\nmonitoring and manipulation.",
        "Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as\nthe number of layers increases, node embeddings become increasingly similar,\nand model performance drops sharply. Traditionally, oversmoothing has been\nquantified using metrics that measure the similarity of neighbouring node\nfeatures, such as the Dirichlet energy. While these metrics are related to\noversmoothing, we argue they have critical limitations and fail to reliably\ncapture oversmoothing in realistic scenarios. For instance, they provide\nmeaningful insights only for very deep networks and under somewhat strict\nconditions on the norm of network weights and feature representations. As an\nalternative, we propose measuring oversmoothing by examining the numerical or\neffective rank of the feature representations. We provide theoretical support\nfor this approach, demonstrating that the numerical rank of feature\nrepresentations converges to one for a broad family of nonlinear activation\nfunctions under the assumption of nonnegative trained weights. To the best of\nour knowledge, this is the first result that proves the occurrence of\noversmoothing without assumptions on the boundedness of the weight matrices.\nAlong with the theoretical findings, we provide extensive numerical evaluation\nacross diverse graph architectures. Our results show that rank-based metrics\nconsistently capture oversmoothing, whereas energy-based metrics often fail.\nNotably, we reveal that a significant drop in the rank aligns closely with\nperformance degradation, even in scenarios where energy metrics remain\nunchanged.",
        "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "Adaptive teaming, the ability to collaborate with unseen teammates without\nprior coordination, remains an underexplored challenge in multi-robot\ncollaboration. This paper focuses on adaptive teaming in multi-drone\ncooperative pursuit, a critical task with real-world applications such as\nborder surveillance, search-and-rescue, and counter-terrorism. We first define\nand formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone\n\\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a\ncomprehensive framework that integrates simulation, algorithm training and\nreal-world deployment. AT-MDP framework provides a flexible experiment\nconfigurator and interface for simulation, a distributed training framework\nwith an extensive algorithm zoo (including two newly proposed baseline methods)\nand an unseen drone zoo for evaluating adaptive teaming, as well as a\nreal-world deployment system that utilizes edge computing and Crazyflie drones.\nTo the best of our knowledge, AT-MDP framework is the first adaptive framework\nfor continuous-action decision-making in complex real-world drone tasks,\nenabling multiple drones to coordinate effectively with unseen teammates.\nExtensive experiments in four multi-drone pursuit environments of increasing\ndifficulty confirm the effectiveness of AT-MDP framework, while real-world\ndeployments further validate its feasibility in physical systems. Videos and\ncode are available at https:\/\/sites.google.com\/view\/at-mdp.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
        "Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps:\/\/anonymous.4open.science\/r\/momentum-increasing-batch-size-888C\/.",
        "Content generation and manipulation approaches based on deep learning methods\nhave seen significant advancements, leading to an increased need for techniques\nto detect whether an image has been generated or edited. Another area of\nresearch focuses on the insertion and harmonization of objects within images.\nIn this study, we explore the potential of using harmonization data in\nconjunction with a segmentation model to enhance the detection of edited image\nregions. These edits can be either manually crafted or generated using deep\nlearning methods. Our findings demonstrate that this approach can effectively\nidentify such edits. Existing forensic models often overlook the detection of\nharmonized objects in relation to the background, but our proposed Disharmony\nNetwork addresses this gap. By utilizing an aggregated dataset of harmonization\ntechniques, our model outperforms existing forensic networks in identifying\nharmonized objects integrated into their backgrounds, and shows potential for\ndetecting various forms of edits, including virtual try-on tasks.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.",
        "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Machine learning in additive manufacturing: State-of-the-art and perspectives",
    "start_abstract":"Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Additive manufacturing and sustainability: an exploratory study of the advantages and challenges"
      ],
      "abstract":[
        "The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Fine-tuning foundation models of materials interatomic potentials with\n  frozen transfer learning",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Chiral Altermagnon in MnTe",
        "From High-Entropy Alloys to Alloys with High Entropy: A New Paradigm in\n  Materials Science and Engineering for Advancing Sustainable Metallurgy",
        "Microscopic origin of magnetoferroelectricity in monolayer NiBr$_{2}$\n  and NiI$_{2}$",
        "In situ growth and magnetic characterization of Cr Chloride monolayers",
        "Goldstone-mediated polar instability in hexagonal barium titanate",
        "Compositionally Grading Alloy Stacking Fault Energy using Autonomous\n  Path Planning and Additive Manufacturing with Elemental Powders",
        "Intercalated structures formed by platinum on epitaxial graphene on\n  SiC(0001)",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals",
        "Floquet optical selection rules in black phosphorus",
        "Magnon-phonon interactions from first principles",
        "A full breakthrough in vacuum ultraviolet nonlinear optical performance\n  of NH4B4O6F",
        "Modified FOX Optimizer for Solving optimization problems",
        "On the minimum cut-sets of the power graph of a finite cyclic group, II",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Bridging statistical mechanics and thermodynamics away from equilibrium:\n  a data-driven approach for learning internal variables and their dynamics",
        "Scalar probability density function mixing models need not comply with\n  the linearity and independence hypothesis",
        "The entropy profiles of a definable set over finite fields",
        "Optimal Functional $2^{s-1}$-Batch Codes: Exploring New Sufficient\n  Conditions",
        "On the ascent of almost and quasi-atomicity to monoid semidomains",
        "Refined curve counting with descendants and quantum mirrors",
        "Dual Control for Interactive Autonomous Merging with Model Predictive\n  Diffusion",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "The influence of missing data mechanisms and simple missing data\n  handling techniques on fairness",
        "From Mutation to Degradation: Predicting Nonsense-Mediated Decay with\n  NMDEP",
        "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)",
        "Zubarev response approach to polarization phenomena in local equilibrium"
      ],
      "abstract":[
        "Machine-learned interatomic potentials are revolutionising atomistic\nmaterials simulations by providing accurate and scalable predictions within the\nscope covered by the training data. However, generation of an accurate and\nrobust training data set remains a challenge, often requiring thousands of\nfirst-principles calculations to achieve high accuracy. Foundation models have\nstarted to emerge with the ambition to create universally applicable potentials\nacross a wide range of materials. While foundation models can be robust and\ntransferable, they do not yet achieve the accuracy required to predict reaction\nbarriers, phase transitions, and material stability. This work demonstrates\nthat foundation model potentials can reach chemical accuracy when fine-tuned\nusing transfer learning with partially frozen weights and biases. For two\nchallenging datasets on reactive chemistry at surfaces and stability and\nelastic properties of tertiary alloys, we show that frozen transfer learning\nwith 10-20% of the data (hundreds of datapoints) achieves similar accuracies to\nmodels trained from scratch (on thousands of datapoints). Moreover, we show\nthat an equally accurate, but significantly more efficient surrogate model can\nbe built using the transfer learned potential as the ground truth. In\ncombination, we present a simulation workflow for machine learning potentials\nthat improves data efficiency and computational efficiency.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "Altermagnetism has surfaced as a novel magnetic phase, bridging the\nproperties of ferro- and anti-ferromagnetism. The momentum-dependent\nspin-splitting observed in these materials reflects their unique symmetry\ncharacteristics which also establish the conditions for chiral magnons to\nemerge. Here we provide the first direct experimental evidence for a chiral\nmagnon in the altermagnetic candidate MnTe, revealed by circular-dichroism\nresonant inelastic X-ray scattering (CD-RIXS). This mode which we term chiral\naltermagnon exhibits a distinct momentum dependence consistent with the\nproposed altermagnetic $g-$wave symmetry of MnTe. Our results reveal a new\nclass of magnetic excitations, demonstrating how altermagnetic order shapes\nspin dynamics and paves the way for advances in spintronic and quantum\ntechnologies.",
        "The development of high-entropy alloys (HEAs) has marked a paradigm shift in\nalloy design, moving away from traditional methods that prioritize a dominant\nbase metal enhanced by minor elements. HEAs instead incorporate multiple\nalloying elements with no single dominant component, broadening the scope of\nalloy design. This shift has led to the creation of diverse alloys with high\nentropy (AHEs) families, including high-entropy steels, superalloys, and\nintermetallics, each highlighting the need to consider additional factors such\nas stacking fault energy (SFE), lattice misfit, and anti-phase boundary energy\n(APBE) due to their significant influence on microstructure and performance.\nLeveraging multiple elements in alloying opens up promising possibilities for\ndeveloping new alloys from multi-component scrap and electronic waste, reducing\nreliance on critical metals and emphasizing the need for advanced data\ngeneration techniques. With the vast possibilities offered by these\nmulti-component feedstocks, modelling and Artificial Intelligence based tools\nare essential to efficiently explore and optimize new alloys, supporting\nsustainable progress in metallurgy. These advancements call for a reimagined\nalloy design framework, emphasizing robust data acquisition, alternative design\nparameters, and advanced computational tools over traditional\ncomposition-focused methodologies.",
        "We investigate the magnetoelectric properties of the monolayer NiX$_{2}$ (X =\nBr, I) through first-principles calculations. Our calculations predict that the\nNiBr$_{2}$ monolayer exhibits a cycloidal magnetic ground state. For the\nNiI$_{2}$ monolayer, a proper-screw helical magnetic ground state with\nmodulation vector \\(\\boldsymbol{Q} = (q, 0, 0)\\) is adopted, approximated based\non experimental observations. The electric polarization in NiBr$_{2}$ shows a\nlinear dependence on the spin-orbit coupling strength \\(\\lambda_{\\text{SOC}}\\),\nwhich can be adequately described by the generalized Katsura-Nagaosa-Balatsky\n(gKNB) model, considering contributions from up to the third nearest-neighbor\nspin pairs. In contrast, the electric polarization in NiI$_{2}$ exhibits a\ndistinct dependence on \\(q\\) and \\(\\lambda_{\\text{SOC}}\\), which cannot be\nfully explained by the gKNB mechanism alone. To address this, the \\(p\\)-\\(d\\)\nhybridization mechanism is extended to NiI$_{2}$ to explain the observed\nbehavior. The respective contributions from the \\(p\\)-\\(d\\) hybridization and\nthe gKNB mechanism in NiI$_{2}$ are then quantitatively evaluated. Overall, our\nwork elucidates the microscopic mechanisms underlying multiferroicity in\nNiBr$_{2}$ and NiI$_{2}$ monolayers, with the conclusions readily applicable to\ntheir bulk forms.",
        "Monolayer Chromium Dihalides and Trihalides materials can be grown on a\nvariety of substrates by molecular beam epitaxy regardless of the lattice\nmismatch thanks to the van der Waals epitaxy. In this work, we studied the\nmagnetic nature of Cr Chloride monolayers grown on Au(111), Ni(111) and\ngraphene-passivated Ni(111) from the evaporation in ultra-high vacuum of the\nsame halide precursor. Structural, morphological and magnetic characterizations\nwere conducted in situ by low energy electron diffraction (LEED), scanning\ntunneling microscopy (STM) and X-ray magnetic circular dichroism (XMCD). Owing\nto opposite chemical behaviour, Au(111) and Ni(111) promote the formation of\ntwo different valence compounds, i.e. CrCl$_3$ and CrCl$_2$, showing distinct\nmagnetic properties at 4 K. When graphene is used to passivate the Ni(111)\nsurface, the formation of CrCl$_3$ becomes allowed also on this substrate. The\ncoexistence of CrCl$_3$ and CrCl$_2$, both showing few nm lateral size and\nsuper-paramagnetic properties, is demonstrated by XMCD spectra displaying two\ndichroic peaks at the characteristic Cr$^{3+}$ and Cr$^{2+}$ energies.\nSite-selective magnetization measurements performed with the photon energy\ntuned on the two absorption edges show reversed magnetization of some of the\nCrCl$_2$ islands with respect to the CrCl3 domains, which is interpreted in\nterms of magnetic frustration.",
        "We discover a rare structural manifestation of the Goldstone paradigm in a\nhexagonal polytype of the archetypal ferroelectric BaTiO3. First-principles\ncalculations confirm the Goldstone character of the order parameter, and\nhigh-resolution diffraction measurements link this to a quasi-continuous domain\ntexture in the vicinity of the low-temperature phase transitions. Our findings\nhighlight how changes in structural topology may be exploited to realize rich\npolar topologies in bulk ferroelectric perovskites.",
        "Compositionally graded alloys (CGAs) are often proposed for use in structural\ncomponents where the combination of two or more alloys within a single part can\nyield substantial enhancement in performance and functionality. For these\napplications, numerous design methodologies have been developed, one of the\nmost sophisticated being the application of path planning algorithms originally\ndesigned for robotics to solve CGA design problems. In addition to the\ntraditional application to structural components, this work proposes and\ndemonstrates the application of this CGA design framework to rapid alloy\ndesign, synthesis, and characterization. A composition gradient in the CoCrFeNi\nalloy space was planned between the maximum and minimum stacking fault energy\n(SFE) as predicted by a previously developed model in a face-centered cubic\n(FCC) high entropy alloy (HEA) space. The path was designed to be monotonic in\nSFE and avoid regions that did not meet FCC phase fraction and solidification\nrange constraints predicted by CALculation of PHAse Diagrams (CALPHAD).\nCompositions from the path were selected to produce a linear gradient in SFE,\nand the CGA was built using laser directed energy deposition (L-DED). The\nresulting gradient was characterized for microstructure and mechanical\nproperties, including hardness, elastic modulus, and strain rate sensitivity.\nDespite being predicted to contain a single FCC phase throughout the gradient,\npart of the CGA underwent a martensitic transformation, thereby demonstrating a\nlimitation of using equilibrium CALPHAD calculations for phase stability\npredictions. More broadly, this demonstrates the ability of the methods\nemployed to bring attention to blind spots in alloy models.",
        "Graphene on SiC intercalated with two-dimensional metal layers, such as Pt,\noffers a versatile platform for applications in spintronics, catalysis, and\nbeyond. Recent studies have demonstrated that Pt atoms can intercalate at the\nheterointerface between SiC(0001) and the C-rich\n$(6\\sqrt{3}\\times6\\sqrt{3})$R30{\\deg} reconstructed surface (hereafter referred\nas the buffer layer). However, key aspects such as intercalated phase structure\nand intercalation mechanisms remain unclear. In this work, we investigate\nchanges in morphology, chemistry, and electronic structure for both buffer\nlayer and monolayer graphene grown on SiC(0001) following Pt deposition and\nannealing cycles, which eventually led to Pt intercalation at temperatures\nabove 500{\\deg}C. Atomic-resolution imaging of the buffer layer reveals a\nsingle intercalated Pt layer that removes the periodic corrugation of the\nbuffer layer, arising from partial bonding of C-atoms with Si-atoms of the\nsubstrate. In monolayer graphene, the Pt-intercalated regions exhibit a\ntwo-level structure: the first level corresponds to a Pt layer intercalated\nbelow the buffer layer, while the second level contains a second Pt layer,\ngiving rise to a $(12\\times12)$ superstructure relative to graphene. Upon\nintercalation, Pt atoms appear as silicides, indicating a reaction with Si\natoms from the substrate. Additionally, charge neutral $\\pi$-bands\ncorresponding to quasi-free-standing monolayer and bilayer graphene emerge.\nAnalysis of multiple samples, coupled with a temperature-dependent study of the\nintercalation rate, demonstrates the pivotal role of buffer layer regions in\nfacilitating the Pt intercalation in monolayer graphene. These findings provide\nvaluable insight into Pt intercalation, advancing the potential for\napplications.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation.",
        "The optical selection rules endorsed by symmetry are crucial for\nunderstanding the optical properties of quantum materials and the associated\nultrafast spectral phenomena. Herein, we introduce momentum-resolved Floquet\noptical selection rules using the group theory to elucidate the pump-probe\nphotoemission spectral distributions of monolayer black phosphorus (BP), which\nare governed by the symmetries of both the material and the lasers. Using\ntime-dependent density functional theory (TDDFT), we further investigate the\ndynamical evolution of Floquet(-Volkov) states in the photoemission spectra of\nmonolayer BP, revealing their spectral weights at specific momenta for each\nsideband. These observations are comprehensively explained by the proposed\nFloquet optical selection rules. Our framework not only clarifies experimental\nphotoemission spectra but also uncovers novel characteristics under different\npump-probe configurations. Our results are expected to deepen the understanding\nof light-induced ultrafast spectra in BP and can be further extended to other\nFloquet systems.",
        "Modeling spin-wave (magnon) dynamics in novel materials is important to\nadvance spintronics and spin-based quantum technologies. The interactions\nbetween magnons and lattice vibrations (phonons) limit the length scale for\nmagnon transport. However, quantifying these interactions remains challenging.\nHere we show many-body calculations of magnon-phonon (mag-ph) coupling based on\nthe ab initio Bethe-Salpeter equation. We derive expressions for mag-ph\ncoupling matrices and compute them in 2D ferromagnets, focusing on hydrogenated\ngraphene and monolayer CrI3. Our analysis shows that electron-phonon (e-ph) and\nmag-ph interactions differ significantly, where modes with weak e-ph coupling\ncan exhibit strong mag-ph coupling (and vice versa), and reveals which phonon\nmodes couple more strongly with magnons. In both materials studied here, the\ninelastic magnon relaxation time is found to decrease abruptly above the\nthreshold for emission of strongly coupled phonons, thereby defining a\nlow-energy window for efficient magnon transport. By averaging in this window,\nwe compute the temperature-dependent magnon mean-free path, a key figure of\nmerit for spintronics, entirely from first principles. The theory and\ncomputational tools shown in this work enable studies of magnon interactions,\nscattering, and dynamics in generic materials, advancing the design of magnetic\nsystems and magnon- and spin-based devices.",
        "The lack of suitable vacuum ultraviolet (VUV) nonlinear optical (NLO)\ncrystals has hindered the development of compact, high-power VUV sources via\nsecond harmonic generation (SHG). Here, we report on the development of the\nfluorooxoborate crystal NH4B4O6F (ABF) as a promising material for VUV light\ngeneration. For the first time, devices with specific phase-matching angles\nwere constructed, achieving a record 158.9 nm VUV light through phase-matching\nSHG and a maximum nanosecond pulse energy of 4.8 mJ at 177.3 nm with a\nconversion efficiency of 5.9 %. The enhanced NLO performance is attributed to\noptimized arrangements of fluorine-based units creating asymmetric sublattices.\nThis work marks a significant milestone in the field of NLO materials,\nfacilitating the future applications of compact, high-power VUV lasers\nutilizing ABF.",
        "The FOX optimizer, inspired by red fox hunting behavior, is a powerful\nalgorithm for solving real-world and engineering problems. However, despite\nbalancing exploration and exploitation, it can prematurely converge to local\noptima, as agent positions are updated solely based on the current best-known\nposition, causing all agents to converge on one location. This study proposes\nthe modified FOX optimizer (mFOX) to enhance exploration and balance\nexploration and exploitation in three steps. First, the Oppositional-Based\nLearning (OBL) strategy is used to improve the initial population. Second,\ncontrol parameters are refined to achieve a better balance between exploration\nand exploitation. Third, a new update equation is introduced, allowing agents\nto adjust their positions relative to one another rather than relying solely on\nthe best-known position. This approach improves exploration efficiency without\nadding complexity. The mFOX algorithm's performance is evaluated against 12\nwell-known algorithms on 23 classical benchmark functions, 10 CEC2019\nfunctions, and 12 CEC2022 functions. It outperforms competitors in 74% of the\nclassical benchmarks, 60% of the CEC2019 benchmarks, and 58% of the CEC2022\nbenchmarks. Additionally, mFOX effectively addresses four engineering problems.\nThese results demonstrate mFOX's strong competitiveness in solving complex\noptimization tasks, including unimodal, constrained, and high-dimensional\nproblems.",
        "The power graph $\\mathcal{P}(G)$ of a finite group $G$ is the simple graph\nwith vertex set $G$ and two distinct vertices are adjacent if one of them is a\npower of the other. Let $n=p_1^{n_1}p_2^{n_2}\\cdots p_r^{n_r},$ where\n$p_1,p_2,\\ldots,p_r$ are primes with $p_1<p_2<\\cdots <p_r$ and $n_1,n_2,\\ldots,\nn_r$ are positive integers. For the cyclic group $C_n$ of order $n$, the\nminimum cut-sets of $\\mathcal{P}(C_n)$ are characterized in \\cite{cps} for\n$r\\leq 3$. Recently, in \\cite{MPS}, certain cut-sets of $\\mathcal{P}(C_n)$ are\nidentified such that any minimum cut-set of $\\mathcal{P}(C_n)$ must be one of\nthem. In this paper, for $r\\geq 4$, we explicitly determine the minimum\ncut-sets, in particular, the vertex connectivity of $\\mathcal{P}(C_n)$ when:\n(i) $n_r\\geq 2$, (ii) $r=4$ and $n_r=1$, and (iii) $r=5$, $n_r=1$, $p_1\\geq 3$.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Thermodynamics with internal variables is a common approach in continuum\nmechanics to model inelastic (i.e., non-equilibrium) material behavior. While\nthis approach is computationally and theoretically attractive, it currently\nlacks a well-established statistical mechanics foundation. As a result,\ninternal variables are typically chosen phenomenologically and lack a direct\nlink to the underlying physics which hinders the predictability of the theory.\nTo address these challenges, we propose a machine learning approach that is\nconsistent with the principles of statistical mechanics and thermodynamics. The\nproposed approach leverages the following techniques (i) the information\nbottleneck (IB) method to ensure that the learned internal variables are\nfunctions of the microstates and are capable of capturing the salient feature\nof the microscopic distribution; (ii) conditional normalizing flows to\nrepresent arbitrary probability distributions of the microscopic states as\nfunctions of the state variables; and (iii) Variational Onsager Neural Networks\n(VONNs) to guarantee thermodynamic consistency and Markovianity of the learned\nevolution equations. The resulting framework, called IB-VONNs, is tested on two\nproblems of colloidal systems, governed at the microscale by overdamped\nLangevin dynamics. The first one is a prototypical model for a colloidal\nparticle in an optical trap, which can be solved analytically, and thus ideal\nto verify the framework. The second problem is a one-dimensional\nphase-transforming system, whose macroscopic description still lacks a\nstatistical mechanics foundation under general conditions. The results in both\ncases indicate that the proposed machine learning strategy can indeed bridge\nstatistical mechanics and thermodynamics with internal variables away from\nequilibrium.",
        "In a mixture of scalar fields undergoing diffusive processes governed by\nFick's law, the concentration at each point evolves linearly in the\nconcentrations at all points and independently from the other concentrations,\nwhen one considers a finite differences integration of their evolution\nequations. However, these properties must not necessarily be enforced in\nprobability density function models, since they are relaxed when conditional\nexpected values are taken.",
        "A definable set $X$ in the first-order language of rings defines a family of\nrandom vectors: for each finite field $\\mathbb{F}_q$, let the distribution be\nsupported and uniform on the $\\mathbb{F}_q$-rational points of $X$. We employ\nresults from the model theory of finite fields to show that their entropy\nprofiles settle into one of finitely many stable asymptotic behaviors as $q$\ngrows. The attainable asymptotic entropy profiles and their dominant terms as\nfunctions of $q$ are computable. This generalizes a construction of Mat\\'u\\v{s}\nwhich gives an information-theoretic interpretation to algebraic matroids.",
        "A functional $k$-batch code of dimension $s$ consists of $n$ servers storing\nlinear combinations of $s$ linearly independent information bits. These codes\nare designed to recover any multiset of $k$ requests, each being a linear\ncombination of the information bits, by $k$ disjoint subsets of servers. A\nrecent conjecture suggests that for any set of $k = 2^{s-1}$ requests, the\noptimal solution requires $2^s-1$ servers. This paper shows that the problem of\nfunctional $k$-batch codes is equivalent to several other problems. Using these\nequivalences, we derive sufficient conditions that improve understanding of the\nproblem and enhance the ability to find the optimal solution.",
        "A commutative monoid is atomic if every non-invertible element factors into\nirreducibles (also called atoms), while an integral (semi)domain is atomic if\nits multiplicative monoid is atomic. Notions weaker than atomicity have been\nintroduced and studied during the past decade, including almost atomicity and\nquasi-atomicity, which were coined and first investigated by Boynton and\nCoykendall in their study of graphs of divisibility of integral domains. The\nascent of atomicity to polynomial extensions was settled by Roitman back in\n1993 while the ascent of atomicity to monoid domains was settled by Coykendall\nand the second author in 2019 (in both cases the answer was negative). The main\npurpose of this paper is to study the ascent of almost atomicity and\nquasi-atomicity to polynomial extensions and monoid domains. Under certain\nreasonable conditions, we establish the ascent of both properties to polynomial\nextensions (over semidomains). Then we construct an explicit example\nillustrating that, with no extra conditions, quasi-atomicity does not ascend to\npolynomial extensions. Finally, we show that, in general, neither almost\natomicity nor quasi-atomicity ascend to monoid domains, improving upon a\nconstruction first provided by Coykendall and the second author for the\nnon-ascent of atomicity.",
        "We establish a formula for structure constants of the quantum mirror to a log\nCalabi Yau surface $(Y,D)$ in terms of descendent logarithmic Gromov--Witten\ninvariants of $(Y,D)$. Our result generalises the weak Frobenius structure\nconjecture for surfaces to the $q$-refined setting, and is proved by relating\nthese invariants to counts of quantum broken lines in the associated quantum\nscattering diagram.",
        "Interactive decision-making is essential in applications such as autonomous\ndriving, where the agent must infer the behavior of nearby human drivers while\nplanning in real-time. Traditional predict-then-act frameworks are often\ninsufficient or inefficient because accurate inference of human behavior\nrequires a continuous interaction rather than isolated prediction. To address\nthis, we propose an active learning framework in which we rigorously derive\npredicted belief distributions. Additionally, we introduce a novel model-based\ndiffusion solver tailored for online receding horizon control problems,\ndemonstrated through a complex, non-convex highway merging scenario. Our\napproach extends previous high-fidelity dual control simulations to hardware\nexperiments, which may be viewed at https:\/\/youtu.be\/Q_JdZuopGL4, and verifies\nbehavior inference in human-driven traffic scenarios, moving beyond idealized\nmodels. The results show improvements in adaptive planning under uncertainty,\nadvancing the field of interactive decision-making for real-world applications.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "Fairness of machine learning algorithms is receiving increasing attention, as\nsuch algorithms permeate the day-to-day aspects of our lives. One way in which\nbias can manifest in a dataset is through missing values. If data are missing,\nthese data are often assumed to be missing completely randomly; in reality the\npropensity of data being missing is often tied to the demographic\ncharacteristics of individuals. There is limited research into how missing\nvalues and the handling thereof can impact the fairness of an algorithm. Most\nresearchers either apply listwise deletion or tend to use the simpler methods\nof imputation (e.g. mean or mode) compared to the more advanced ones (e.g.\nmultiple imputation); we therefore study the impact of the simpler methods on\nthe fairness of algorithms. The starting point of the study is the mechanism of\nmissingness, leading into how the missing data are processed and finally how\nthis impacts fairness. Three popular datasets in the field of fairness are\namputed in a simulation study. The results show that under certain scenarios\nthe impact on fairness can be pronounced when the missingness mechanism is\nmissing at random. Furthermore, elementary missing data handling techniques\nlike listwise deletion and mode imputation can lead to higher fairness compared\nto more complex imputation methods like k-nearest neighbour imputation, albeit\noften at the cost of lower accuracy.",
        "Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional\nsurveillance mechanism that degrades transcripts with premature termination\ncodons, safeguarding transcriptome integrity and shaping disease phenotypes.\nHowever, accurately predicting NMD efficiency remains challenging, as existing\nmodels often rely on simplistic rule-based heuristics or limited feature sets,\nconstraining their accuracy and generalizability. Using paired DNA and RNA data\nfrom The Cancer Genome Atlas, we benchmark embedding-only models and\ndemonstrate that they underperform compared to a simple rule-based approach. To\naddress this, we develop NMDEP (NMD Efficiency Predictor), an integrative\nframework that combines optimized rule-based methods, sequence embeddings, and\ncurated biological features, achieving state-of-the-art predictive performance.\nThrough explainable AI, we identify key NMD determinants, reaffirming\nestablished factors such as variant position while uncovering novel\ncontributors like ribosome loading. Applied to over 2.9 million simulated\nstop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments,\nadvancing variant interpretation and disease research.",
        "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps:\/\/github.com\/m4opt\/m4opt.",
        "Using the expansion of Zubarev's density operator, we develop a linear\nresponse approach to study various spin physics in a locally equilibrated\nmedium, particularly focusing on various polarization phenomena in heavy-ion\ncollisions. Specifically, we connect familiar correlation functions and\ndiagrammatic methods to the Zubarev formalism, enabling the use of established\ntechniques like the Matsubara\/imaginary time formalism to facilitate\ncalculations. For a spin-1\/2 particle, we re-derive its vector polarization\nusing this Zubarev response approach, which exactly reproduces with our\nprevious results based on Luttinger's method. For a spin-1 particle, we\ncalculate the vector polarization and find the expected contributions from\nvorticity, temperature gradients, and shear, which are identical to those for\nspin-1\/2 particles except for a factor of 4\/3 as expected. For the tensor\npolarization and spin alignment of a spin-1 boson, we explicitly prove that the\nnon-dissipative contribution is zero at leading order in gradients, and briefly\nreiterate our previous findings for the dissipative contribution with further\ndiscussions on several concerns. Additionally, we discuss several relevant\nsubtleties and questions, including an alternative derivation for Zubarev\nresponse approach, the covariance issues of different spin density matrix\ndefinitions, a further explanation of slow and fast modes, the mode selection\nscheme, etc. We also discuss skeleton expansions, higher-order contributions,\nand non-perturbative methods, particularly their potential connection to\nlattice field theory. In summary, this work discusses the foundations and\nsubtleties of Zubarev response approach, with specific examples from spin\nphysics in heavy-ion collisions."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
    "start_abstract":"This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals"
      ],
      "abstract":[
        "Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli)."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Functionalized Cr$_2$C MXenes: Novel Magnetic Semiconductors",
        "Uncertainty Quantification for Misspecified Machine Learned Interatomic\n  Potentials",
        "Ferromagnetism in LaFeO3\/LaNiO3 Superlattices with High Curie\n  Temperature",
        "Harnessing Layer-Controlled Two-dimensional Semiconductors for\n  Photoelectrochemical Energy Storage via Quantum Capacitance and Band Nesting",
        "Magnetotransport evidence of a potential low-lying Dirac node in\n  NbAl$_3$",
        "Stability of the long-range corrected exchange-correlation functional in\n  time-dependent density-functional theory",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Structural Modulation and Enhanced Magnetic Ordering in Incommensurate\n  K$_{1-{x}}$CrSe$_2$ Crystals",
        "Double-Crucible Vertical Bridgman Technique for Stoichiometry-Controlled\n  Chalcogenide Crystal Growth",
        "Curie temperature study of the Y(Fe$_{1-x}$Co$_x$)$_2$ and\n  Zr(Fe$_{1-x}$Co$_x$)$_2$ systems using mean-field theory and Monte Carlo\n  method",
        "Spin-reorientation driven topological Hall effect in Fe4GeTe2",
        "A full breakthrough in vacuum ultraviolet nonlinear optical performance\n  of NH4B4O6F",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "Anomalous Dynamics of a Liquid Corner Film",
        "HST Observations within the Sphere of Influence of the Powerful\n  Supermassive Black Hole in PKS0745-191",
        "Global bifurcations of nodal solutions for coupled elliptic equations",
        "A new convection scheme for GCMs of temperate sub-Neptunes",
        "Singularity of compound stationary measures",
        "The Stability and Accuracy of The Adams-Bashforth-type Integrator",
        "Characterizing Continuous Gravitational Waves from Supermassive Black\n  Hole Binaries in Realistic Pulsar Timing Array Data",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Theory of quantum-geometric charge and spin Josephson diode effects in\n  strongly spin-polarized hybrid structures with noncoplanar spin textures",
        "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Indirect reciprocity as a dynamics for weak balance",
        "Simple games with minimum",
        "Fully viable DHOST bounce with extra scalar",
        "Thermodynamic properties and Joule-Thomson expansion of AdS black hole\n  with Gaussian distribution in non-commutative geometry"
      ],
      "abstract":[
        "We report an \\textit{ab initio} investigation of functionalized and\n3$d$-electrons doped Cr$_2$C MXenes. Upon functionalization, the Cr$_2$C\nbecomes chemically, dynamically, and mechanically stable, and it exhibits\nmagnetic semiconducting behavior. Cr$_2$CF$_2$ stands out as a wide band gap\nsemiconductor, possessing super exchange interaction mediated by F atoms within\nthe layer, however, the applied strain transforms it from an indirect to a\ndirect band gap semiconductor. Strong spin-phonon coupling found in\nCr$_2$CH$_2$ is supported by the distorted Cr spin density due to hydrogen\nenvironment. Two magnon branches, associated with two sub-lattice spins, are\nfound in the ferromagnetic Cr$_2$CO$_2$ and antiferromagnetic Cr$_2$CF$_2$.\nDepending on the types of 3$d$-electron dopants and functionalization, Cr$_2$C\nMXenes (except for Cr$_2$CO$_2$) change from the indirect band gap magnetic\nsemiconductor to different states of electronic and magnetic matter including\nexotic direct band gap magnetic bipolar semiconductor. In addition, we reveal a\nband inversion between the two highest valence bands in the Fe-doped\nCr$_2$CCl$_2$.",
        "The use of high-dimensional regression techniques from machine learning has\nsignificantly improved the quantitative accuracy of interatomic potentials.\nAtomic simulations can now plausibly target quantitative predictions in a\nvariety of settings, which has brought renewed interest in robust means to\nquantify uncertainties on simulation results. In many practical settings,\nencompassing both classical and a large class of machine learning potentials,\nthe dominant form of uncertainty is currently not due to lack of training data\nbut to misspecification, namely the inability of any one choice of model\nparameters to exactly match all ab initio training data. However, Bayesian\ninference, the most common formal tool used to quantify uncertainty, is known\nto ignore misspecification and thus significantly underestimates parameter\nuncertainties. Here, we employ a recent misspecification-aware regression\ntechnique to quantify parameter uncertainties, which is then propagated to a\nbroad range of phase and defect properties in tungsten via brute force\nresampling or implicit differentiation. The propagated misspecification\nuncertainties robustly envelope errors to direct \\textit{ab initio} calculation\nof material properties outside of the training dataset, an essential\nrequirement for any quantitative multi-scale modeling scheme. Finally, we\ndemonstrate application to recent foundational machine learning interatomic\npotentials, accurately predicting and bounding errors in MACE-MPA-0 energy\npredictions across the diverse materials project database. Perspectives for the\napproach in multiscale simulation workflows are discussed.",
        "Interfacing complex oxides in atomically engineered layered structures can\ngive rise to a wealth of exceptional electronic and magnetic properties that\nsurpass those of the individual building blocks. Herein, we demonstrate a\nferromagnetic spin order with a high Curie temperature of 608 K in\nsuperlattices consisting of otherwise paramagnetic perovskite LaNiO3 (LNO) and\nantiferromagnetic LaFeO3 (LFO). The extraordinary ferromagnetism likely results\nfrom the covalent exchange due to interfacial charge transfer from Fe to Ni\ncations. By deliberately controlling the thickness of the LNO sublayers thus\nthe amount of charge transfer, a robust ferromagnetism of 4 uB is realized for\na stacking periodicity consisting of one single unit cell of both LNO and LFO,\nan emergent double perovskite phase of La2FeNiO6 with B-site layered ordering\nconfigurations. The ferromagnetic LFO\/LNO superlattices offer great potential\nfor the search of emergent magnetodielectric and\/or multiferroic properties as\nwell as applications in spintronics and electrocatalysts.",
        "Two-dimensional (2D) transition metal dichalcogenides like molybdenum\ndiselenide (MoSe$_2$) have shown great potential in optoelectronics and energy\nstorage due to their layer-dependent bandgap. However, producing high-quality\n2D MoSe$_2$ layers in a scalable and controlled manner remains challenging.\nTraditional methods, such as hydrothermal and liquid-phase exfoliation, lack\nprecision and understanding at the nanoscale, limiting further applications.\nAtmospheric pressure chemical vapor deposition (APCVD) offers a scalable\nsolution for growing high-quality, large-area, layer-controlled 2D MoSe$_2$.\nDespite this, the photoelectrochemical performance of APCVD-grown 2D MoSe$_2$,\nparticularly in energy storage, has not been extensively explored. This study\naddresses this by examining MoSe$_2$'s layer-dependent quantum capacitance and\nphoto-induced charge storage properties. Using a three-electrode setup in 0.5M\nH$_2$SO$_4$, we observed a layer-dependent increase in areal capacitance under\nboth dark and illuminated conditions. A six-layer MoSe$_2$ film exhibited the\nhighest capacitance, reaching $96 \\mu\\mathrm{F\/cm^2}$ in the dark and $115\n\\mu\\mathrm{F\/cm^2}$ under illumination at a current density of $5\n\\mu\\mathrm{A\/cm^2}$. Density Functional Theory (DFT) and Many-Body Perturbation\nTheory calculations reveal that Van Hove singularities and band nesting\nsignificantly enhance optical absorption and quantum capacitance. These results\nhighlight APCVD-grown 2D MoSe$_2$'s potential as light-responsive,\nhigh-performance energy storage electrodes, paving the way for innovative\nenergy storage systems.",
        "NbAl$_3$ is a novel semimetal with a type-II Dirac node ~230 meV above the\nFermi energy. We have performed both out-of-plane ($B\\parallel c$) and in-plane\nmagnetotransport measurements ($B\\perp c$) on single-crystalline NbAl$_3$. In\nour out-of-plane data, we observe an interesting linear component in the\ntransverse magnetoresistance, and the mobility spectrum analysis of the\nout-of-plane data reveals an emergence of high-mobility electrons at low\ntemperatures. Near $B\\parallel c$, Shubnikov-de Haas oscillations are discerned\nin the magnetoresistance. The oscillation frequencies agree with the density\nfunctional theory calculation, the same theory that shows that the Dirac node\nis far above the Fermi energy. Therefore, the out-of-plane results cannot be\nattributed to the type-II Dirac node but suggest NbAl$_3$ has additional Dirac\nor Weyl nodes close to the Fermi energy. To support this, we examine the\nin-plane data obtained with the magnetic field perpendicular to the tilting\ndirection of the type-II Dirac cone. Such field direction excludes the\npossibility of chiral anomaly from the predicted type-II Dirac node.\nRemarkably, we observe the planar Hall effect, anisotropic magnetoresistance,\nand negative longitudinal magnetoresistance. These in-plane results are a\nstrong indication of chiral anomaly unrelated to the previously established\ntype-II Dirac node, pointing to the presence of additional Dirac or Weyl nodes\nnear the Fermi energy. Our new density functional theory calculation reveals a\ntype-I Dirac node ~50 meV below the Fermi energy that has previously been\noverlooked. We argue that the exotic transport phenomena observed in NbAl$_3$\ncan be attributed to the newly identified type-I Dirac node.",
        "Excitonic effects in the optical absorption spectra of solids can be\ndescribed with time-dependent density-functional theory (TDDFT) in the\nlinear-response regime, using a simple class of approximate, long-range\ncorrected (LRC) exchange-correlation functionals. It was recently demonstrated\nthat the LRC approximation can also be employed in real-time TDDFT to describe\nexciton dynamics. Here, we investigate the numerical stability of the\ntime-dependent LRC approach using a two-dimensional model solid. It is found\nthat the time-dependent Kohn-Sham equation with an LRC vector potential becomes\nmore and more prone to instabilities for increasing exciton binding energies.\nThe origin of these instabilities is traced back to time-averaged violations of\nthe zero-force theorem, which leads to a simple and robust numerical\nstabilization scheme. This explains and justifies a recently proposed method by\nDewhurst et al., arXiv:2401.16140.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "Layered delafossite-type compounds and related transition metal\ndichalcogenides, characterized by their triangular net structures, serve as\nprototypical systems for exploring the intricate interplay between crystal\nstructure and magnetic behavior. Herein, we report on the discovery of the\ncompound K$_{1-x}$CrSe$_2$ ($x \\approx$ 0.13), an incommensurately modulated\nphase. Single crystals of this compound were grown for the first time using a\nK\/Se self-flux. We find a monoclinic crystal structure with incommensurate\nmodulation, that can be rationalized by a 3+1 dimensional model. This\nmodulation compensates for the under-stoichiometry of K cations, creating\npronounced undulations in the CrSe$_2$ layers. Our anisotropic magnetization\nmeasurements reveal that K$_{1-x}$CrSe$_2$ undergoes a transition to a\nlong-range magnetically ordered state below $T_{\\mathrm N}$ = 133 K, a\ntemperature 1.6 to 3.3 times higher than in earlier reported KCrSe$_2$\ncompounds. Our findings open new avenues for tuning the magnetic properties of\nthese layered materials through structural modulation.",
        "Precise stoichiometry control in single-crystal growth is essential for both\ntechnological applications and fundamental research. However, conventional\ngrowth methods often face challenges such as non-stoichiometry, compositional\ngradients, and phase impurities, particularly in non-congruent melting systems.\nEven in congruent melting systems like Bi2Se3, deviations from the ideal\nstoichiometric composition can lead to significant property degradation, such\nas excessive bulk conductivity, which limits its topological applications. In\nthis study, we introduce the double-crucible vertical Bridgman (DCVB) method, a\nnovel approach that enhances stoichiometry control through the combined use of\ncontinuous source material feeding, traveling-solvent growth, and liquid\nencapsulation, which suppresses volatile element loss under high pressure.\nUsing Bi2Se3 as a model system, we demonstrate that crystals grown via DCVB\nexhibit enhanced stoichiometric control, significantly reducing defect density\nand achieving much lower carrier concentrations compared to those produced by\nconventional Bridgman techniques. Moreover, the continuous feeding of source\nmaterial enables the growth of large crystals. This approach presents a\npromising strategy for synthesizing high-quality, large-scale crystals,\nparticularly for metal chalcogenides and pnictides that exhibit challenging\nnon-congruent melting behaviors.",
        "The cubic Laves phases including YFe$_2$, YCo$_2$, ZrFe$_2$, and ZrCo$_2$ are\nconsidered as promising candidates for application in hydrogen storage and\nmagnetic refrigeration. While YFe$_2$ and ZrFe$_2$ are ferromagnets, alloying\nwith Co decreases magnetic moments and Curie temperatures ($T_\\mathrm{C}$) of\npseudobinary Zr(Fe$_{1-x}$Co$_x$)$_2$ and Y(Fe$_{1-x}$Co$_x$)$_2$ systems,\nleading to the paramagnetic states of YCo$_2$ and ZrCo$_2$. The following study\nfocus on the investigation of Curie temperature of the Y(Fe$_{1-x}$Co$_x$)$_2$\nand Zr(Fe$_{1-x}$Co$_x$)$_2$ system from first principles. To do it, the Monte\nCarlo (MC) simulations and the mean field theory (MFT) based on the disordered\nlocal moments (DLM) calculations are used. The DLM-MFT results agree\nqualitatively with the experiment and preserve the characteristic features of\n$T_\\mathrm{C}(x)$ dependencies for both Y(Fe$_{1-x}$Co$_x$)$_2$ and\nZr(Fe$_{1-x}$Co$_x$)$_2$. However, we have encountered complications in the\nCo-rich regions due to failure of the local density approximation (LDA) in\ndescribing the Co magnetic moment in the DLM state. The analysis of Fe-Fe\nexchange couplings for YFe$_2$ and ZrFe$_2$ phases indicates that the\nnearest-neighbor interactions play the main role in the formation of\n$T_{\\mathrm{C}}$.",
        "Iron-based van der Waals (vdW) ferromagnets with relatively high ordering\ntemperatures are a current research focus due to their significance in\nfundamental physics and potential applications in spintronics. Competing\nmagnetic interactions and anisotropies can give rise to nontrivial spin\ntextures in these materials, resulting in novel topological features. Fe4GeTe2\n(F4GT) is a nearly room-temperature vdW ferromagnet, well known for hosting a\nspin-reorientation transition (SRT) arising from the interplay of perpendicular\nmagnetic anisotropy (PMA) and shape anisotropy. In this work, we investigate\nthe angle-dependent magneto-transport properties of F4GT single crystals. We\nreport a large topological Hall effect (THE) in a multi-layer F4GT originating\nfrom the SRT-driven non-coplanar spin textures. The THE appears at the in-plane\norientation of the external magnetic field and persists over a wide range of\ntemperatures around SRT. Additionally, we find a thickness-sensitive THE signal\nfor the c axis orientation of the magnetic field at a low-temperature regime\nwhich is associated with a reentrant Lifshitz transition.",
        "The lack of suitable vacuum ultraviolet (VUV) nonlinear optical (NLO)\ncrystals has hindered the development of compact, high-power VUV sources via\nsecond harmonic generation (SHG). Here, we report on the development of the\nfluorooxoborate crystal NH4B4O6F (ABF) as a promising material for VUV light\ngeneration. For the first time, devices with specific phase-matching angles\nwere constructed, achieving a record 158.9 nm VUV light through phase-matching\nSHG and a maximum nanosecond pulse energy of 4.8 mJ at 177.3 nm with a\nconversion efficiency of 5.9 %. The enhanced NLO performance is attributed to\noptimized arrangements of fluorine-based units creating asymmetric sublattices.\nThis work marks a significant milestone in the field of NLO materials,\nfacilitating the future applications of compact, high-power VUV lasers\nutilizing ABF.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Measuring the rheology of liquids typically requires precise control over\nshear rates and stresses. However, we demonstrate that the features of a\npower-law fluid can be predicted by simply observing the capillary spreading\ndynamics of viscous droplets within a wedge-shaped geometry. By considering the\ninfluence of capillary and viscous forces within this geometry, we show that\nthe spreading dynamics can be described by a nonlinear diffusion equation.\nAnalytical predictions indicate subdiffusive behavior, establishing a direct\nrelationship between the diffusion exponent and the rheological exponent, which\nis also corroborated by experimental results. Since this relationship is\nindependent of flow details, it provides robust predictions for the rheological\nproperties of power-law fluids.",
        "We present Space Telescope Imaging Spectrograph observations from the Hubble\nSpace Telescope of the supermassive black hole (SMBH) at the center of\nPKS0745-191, a brightest cluster galaxy (BCG) undergoing powerful radio-mode\nAGN feedback ($P_{\\rm cav}\\sim5\\times10^{45}$ erg s$^{-1}$). These\nhigh-resolution data offer the first spatially resolved map of gas dynamics\nwithin a SMBHs sphere of influence under such powerful feedback. Our results\nreveal the presence of highly chaotic, non-rotational ionized gas flows on\nsub-kpc scales, in contrast to the more coherent flows observed on larger\nscales. While radio-mode feedback effectively thermalizes hot gas in galaxy\nclusters on kiloparsec scales, within the core, the hot gas flow may decouple,\nleading to a reduction in angular momentum and supplying ionized gas through\ncooling, which could enhance accretion onto the SMBH. This process could, in\nturn, lead to a self-regulating feedback loop. Compared to other BCGs with\nweaker radio-mode feedback, where rotation is more stable, intense feedback may\nlead to more chaotic flows, indicating a stronger coupling between jet activity\nand gas dynamics. Additionally, we observe a sharp increase in velocity\ndispersion near the nucleus, consistent with a very massive $M_{\\rm\nBH}\\sim1.5\\times10^{10} M_\\odot$ SMBH. The density profile of the ionized gas\nis also notably flat, paralleling the profiles observed in X-ray gas around\ngalaxies where the Bondi radius is resolved. These results provide valuable\ninsights into the complex mechanisms driving galaxy evolution, highlighting the\nintricate relationship between SMBH fueling and AGN feedback within the host\ngalaxy.",
        "We investigate the global bifurcation structure of the radial nodal solutions\nto the coupled elliptic equations \\begin{equation}\n  \\left\\{\n  \\begin{array}{lr}\n  -{\\Delta}u+u=u^3+\\beta uv^2\\mbox{ in }B_1 ,\\nonumber\n  -{\\Delta}v+v=v^3+\\beta u^2v\\mbox{ in }B_1 ,\\nonumber\n  u,v\\in H_{0,r}^1(B_1).\\nonumber\n  \\end{array}\n  \\right. \\end{equation} Here $B_1$ is a unit ball in $\\mathbb{R}^3$ and\n$\\beta\\in\\mathbb{R}$ the coupling constant is used as bifurcation parameter.\nFor each $k$, the unique pair of nodal solutions $\\pm w_k$ with exactly $k-1$\nzeroes to the scalar field equation $-\\Delta w + w=w^3$ generate exactly four\nsynchronized solution curves and exactly four semi-trivial solution curves to\nthe above system. We obtain a fairly complete global bifurcation structure of\nall bifurcating branches emanating from these eight solution curves of the\nsystem, and show that for different $k$ these bifurcation structures are\ndisjoint. We obtain exact and distinct nodal information for each of the\nbifurcating branches, thus providing a fairly complete characterization of\nnodal solutions of the system in terms of the coupling.",
        "Atmospheric characterisation of temperate sub-Neptunes is the new frontier of\nexoplanetary science with recent JWST observations of possible Hycean world\nK2-18b. Accurate modelling of atmospheric processes is essential to\ninterpreting high-precision spectroscopic data given the wide range of possible\nconditions in the sub-Neptune regime, including on potentially habitable\nplanets. Notably, convection is an important process which can operate in\ndifferent modes across sub-Neptune conditions. Convection can act very\ndifferently in atmospheres with a high condensible mass fraction (non-dilute\natmospheres) or with a lighter background gas, e.g. water convection in a\nH$_2$-rich atmosphere, and can be much weaker or even shut down entirely in the\nlatter case. We present a new mass-flux scheme which can capture these\nvariations and simulate convection over a wide range of parameter space for use\nin 3D general circulation models (GCMs). We validate our scheme for two\nrepresentative cases, a terrestrial-like atmosphere and a mini-Neptune\natmosphere. In the terrestrial case, considering TRAPPIST-1e with an Earth-like\natmosphere, the model performs near-identically to Earth-tuned models in an\nEarth-like convection case. In the mini-Neptune case, considering the bulk\nproperties of K2-18b and assuming a deep H$_2$-rich atmosphere, we demonstrate\nthe capability of the scheme to reproduce non-condensing convection. We find\nconvection occurring at pressures greater than 0.3 bar and the dynamical\nstructure shows high-latitude prograde jets. Our convection scheme will aid in\nthe 3D climate modelling of a wide range of exoplanet atmospheres, and enable\nfurther exploration of temperate sub-Neptune atmospheres.",
        "We show that the product or convex combination of two Markov operators with\nequivalent stationary measures need not have a stationary measure from the same\nmeasure class. More specifically, we exhibit examples of a hitherto undescribed\nphenomenon: maximal entropy random walks for which the resulting compound\nrandom walks no longer have maximal entropy. The underlying group in these\nexamples is $PSL(2,\\mathbb Z)\\cong{{\\mathbb Z}_2}*{{\\mathbb Z}_3}$, and the\nassociated harmonic measures belong to the canonical Minkowski and Denjoy\nmeasure classes on the boundary. These examples also demonstrate that a number\nof other natural families of random walks are not closed under convolutions or\nconvex combinations of step distributions.",
        "This paper presents stability and accuracy analysis of a high-order explicit\ntime stepping scheme introduced by \\cite[Section 2.2]{Buvoli2019}, which\nexhibits superior stability compared to classical Adams-Bashforth. A conjecture\nthat is supported by several numerical phenomena in \\cite[Figure\n2.5]{Buvoli2018}, the method appears to remain stable when the accuracy\napproaches infinity, although it is not yet proven. It is regrettable that this\nhypothesis has been refuted from a fundamental perspective in harmonic\nanalysis. Notwithstanding the aforementioned, this method displays considerably\nenhanced stability in comparison to conventional explicit schemes. Furthermore,\nwe present a criterion for ascertaining the maximum permissible accuracy for a\ngiven specific parabolic stability radius. Conversely, the original method will\nlose one order associated with the expected accuracy, which can be recovered\nwith a slight modification. Consequently, a unified analysis strategy for the\n\\( L^2 \\)-stability will be presented for extensional PDEs under the CFL\ncondition. Finally, a selection of representative numerical examples will be\nshown in order to substantiate the theoretical analysis.",
        "Pulsar timing arrays recently found evidence for a gravitational wave\nbackground (GWB), likely the stochastic overlap of GWs from many supermassive\nblack hole binaries. Anticipating a continuous gravitational wave (CW)\ndetection from a single binary soon to follow, we examine how well current\nBayesian methods can detect CWs and characterize their binary properties by\nmodeling the response of the NANOGrav 15-year pulsar timing array to simulated\nbinary populations. We run Markov Chain Monte Carlo searches for CWs in these\ndatasets and compare them to quicker detection statistics including the optimal\nsignal-to-noise ratio, matched filter detection statistic, and reduced\nlog-likelihood ratio between the signal and noise models calculated at the\ninjected parameters. The latter is the best proxy for Bayesian detection\nfractions, corresponding to a 50% detection fraction (by Bayes factors >10\nfavoring a CW detection over noise-only model) at a signal-to-noise ratio of\n4.6. Source confusion between the GWB and a CW, or between multiple CWs, can\ncause false detections and unexpected dismissals. 53% of realistic binary\npopulations consistent with the recently observed GWB have successful CW\ndetections. 82% of these CWs are in the 4th or 5th frequency bin of the 16.03\nyr dataset (6.9 nHz and 10.8 nHz), with 95 percentile regions spanning\n4nHz-12nHz frequencies, $7-20\\times10^9 M_\\odot$ chirp masses, 60Mpc-8Gpc\nluminosity distances, and 18-13,000 sq. deg 68% confidence localization areas.\nThese successful detections often poorly recover the chirp mass, with only 29%\nidentifying the chirp mass accurately to within 1 dex with a 68% posterior\nwidth also narrower than 1 dex.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We present a systematic study of the spin-resolved Josephson diode effect\n(JDE) in strongly spin-polarized ferromagnets (sFM) coupled to singlet\nsuperconductors (SC) via ferromagnetic insulating interfaces (FI). All metallic\nparts are described in the framework of the quasiclassical Usadel Green's\nfunction theory applicable to diffusive systems. The interfaces are\ncharacterized by an S-matrix obtained for a model potential with exchange\nvectors pointing in an arbitrary direction with respect to the magnetization in\nthe sFM. Our theory predicts a large charge Josephson diode effect with an\nefficiency exceeding $33\\%$ and a perfect spin diode effect with $100\\%$\nefficiency. To achieve these the following conditions are necessary: (i) a\nnoncoplanar profile of the three magnetization vectors in the system and (ii)\ndifferent densities of states of spin-$\\uparrow$ and spin-$\\downarrow$ bands in\nthe sFM achieved by a strong spin polarization. The former gives rise to the\nquantum-geometric phase, $\\Delta\\varphi$, that enters the theory in a very\nsimilar manner as the superconducting phase difference across the junction,\n$\\Delta\\chi$. We perform a harmonic analysis of the Josephson current in both\nvariables and find symmetries between Fourier coefficients allowing an\ninterpretation in terms of transfer processes of multiple equal-spin Cooper\npairs across the two ferromagnetic spin bands. We point out the importance of\ncrossed pair transmission processes. Finally, we study a spin-switching effect\nof an equal-spin supercurrent by reversing the magnetic flux in a SQUID device\nincorporating the mentioned junction and propose a way for measuring it.",
        "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} \/ 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z\/(1 +\nz) = 0.14$.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "A social network is often divided into many factions. People are friends\nwithin each faction, while they are enemies of the other factions, and even my\nenemy's enemy is not necessarily my friend. This configuration can be described\nin terms of a weak form of structural balance. Although weak balance explains a\nnumber of real social networks, which dynamical rule achieves it has remained\nunexplored. In this work, we show that the answer can be found in the field of\nindirect reciprocity, which assumes that people assess each other's behavior\nand choose how to behave to others based on the assessment according to a\nsocial norm. We begin by showing that weak structural balance is equivalent to\nstationarity when the rule is given by a norm called `judging'. By analyzing\nits cluster dynamics of merging, division, and migration induced by assessment\nerror in complete graphs, we obtain the cluster size distribution in a steady\nstate, which shows the coexistence of a giant cluster and smaller ones. We\ncompare this shape with the distributions of seats among the parties in the\nparliaments of Germany, the United Kingdom, and Spain. This study suggests that\nindirect reciprocity can provide insight into the interplay between a norm that\nindividuals abide by and the macroscopic group structure in society.",
        "Every simple game is a monotone Boolean function. For the other direction we\njust have to exclude the two constant functions. The enumeration of monotone\nBoolean functions with distinguishable variables is also known as the\nDedekind's problem. The corresponding number for nine variables was determined\njust recently by two disjoint research groups. Considering permutations of the\nvariables as symmetries we can also speak about non-equivalent monotone Boolean\nfunctions (or simple games). Here we consider simple games with minimum, i.e.,\nsimple games with a unique minimal winning vector. A closed formula for the\nnumber of such games is found as well as its dimension in terms of the number\nof players and equivalence classes of players.",
        "In this paper we construct a class of Degenerate Higher-Order Scalar-Tensor\n(DHOST) theories with an extra scalar field, which admits viable solutions of\nbouncing universe satisfying the following requirements: (i) absence of\nBelinski-Khalatnikov-Lifshitz (BKL) instability, ghost and gradient\ninstability, (ii) absence of superluminality, (iii) generation of nearly\nscale-invariant curvature perturbations and very small tensor-to-scalar ratio,\nand (iv) conventional asymptotics in the distant past and future, where gravity\nsector is described by General Relativity and the DHOST scalar has a canonical\nform of Lagrangian. We also expect our models to have sufficiently small\nnon-Gaussianities of primordial curvature perturbations to be compatible with\nobservations. As such, this work exemplifies for the first time the fully\nviable two-field DHOST bouncing cosmology, which is free of instability and\nsuperluminality problems as well as compatible with observations.",
        "The thermodynamics and Joule-Thomson expansion of anti-de Sitter black hole\n(AdS BH) with Gaussian distribution in non-commutative geometry is\nsystematically studied. The metric of Gaussian-distributed BH is obtained,\nshowing a dS geometry at the core of BH. The research indicates that the BH\ncharacterized by a Gaussian distribution exhibit thermodynamic properties that\nare remarkably similar to those of BH with a Lorentzian distribution in\nnon-commutative geometry. This similarity is specifically manifested in the\nsmall BH-large BH phase transition, the corrected first law of thermodynamics,\nthe criticality, the heat capacity, the zeroth-order phase transition and the\nJoule-Thomson process. Notably, the critical ratio of Gaussian-distributed BH\n(0.46531) is significantly larger than those observed in Van der Waals fluids\n(0.375), and indeed, it is also substantially exceed those of\nLorentzian-distributed BH (0.36671). Moreover, compared to the case of\nLorentzian source, the zeroth-order phase transition effect in\nGaussian-distributed BH is exceedingly subtle (accompanied by a relative\nincrease in the Gibbs free energy on the order of $10^{-3}\\!\\sim\\!\\!10^{-2}$)\nand is difficult to detect distinctly."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
    "start_abstract":"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli).",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation"
      ],
      "abstract":[
        "This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Models That Are Interpretable But Not Transparent",
        "Dissecting the Impact of Model Misspecification in Data-driven\n  Optimization",
        "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos",
        "Lifelong Learning with Task-Specific Adaptation: Addressing the\n  Stability-Plasticity Dilemma",
        "SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with\n  Pre-trained Large Language Model",
        "Saliency Maps are Ambiguous: Analysis of Logical Relations on First and\n  Second Order Attributions",
        "General Time-series Model for Universal Knowledge Representation of\n  Multivariate Time-Series data",
        "Learning from Reward-Free Offline Data: A Case for Planning with Latent\n  Dynamics Models",
        "The Relationship Between Head Injury and Alzheimer's Disease: A Causal\n  Analysis with Bayesian Networks",
        "Fixed-sized clusters $k$-Means",
        "Robust and Efficient Writer-Independent IMU-Based Handwriting\n  Recognization",
        "Preconditioned Inexact Stochastic ADMM for Deep Model",
        "Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data\n  Generation",
        "Bridging Classical and Modern Approaches to Thales' Theorem",
        "S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time\n  Series Forecasting",
        "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery",
        "Family-wise Error Rate Control with E-values",
        "GRNFormer: A Biologically-Guided Framework for Integrating Gene\n  Regulatory Networks into RNA Foundation Models",
        "Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and\n  Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS",
        "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments",
        "The H\\\"{o}lder regularity of div-curl system with anisotropic\n  coefficients",
        "A Semi-Orthogonal Decomposition Theorem for Weighted Blowups",
        "Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions",
        "Algorithmic Data Minimization for Machine Learning over\n  Internet-of-Things Data Streams",
        "Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference",
        "Medial Axis in Pseudo-Euclidean Spaces",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling"
      ],
      "abstract":[
        "Faithful explanations are essential for machine learning models in\nhigh-stakes applications. Inherently interpretable models are well-suited for\nthese applications because they naturally provide faithful explanations by\nrevealing their decision logic. However, model designers often need to keep\nthese models proprietary to maintain their value. This creates a tension: we\nneed models that are interpretable--allowing human decision-makers to\nunderstand and justify predictions, but not transparent, so that the model's\ndecision boundary is not easily replicated by attackers. Shielding the model's\ndecision boundary is particularly challenging alongside the requirement of\ncompletely faithful explanations, since such explanations reveal the true logic\nof the model for an entire subspace around each query point. This work provides\nan approach, FaithfulDefense, that creates model explanations for logical\nmodels that are completely faithful, yet reveal as little as possible about the\ndecision boundary. FaithfulDefense is based on a maximum set cover formulation,\nand we provide multiple formulations for it, taking advantage of submodularity.",
        "Data-driven optimization aims to translate a machine learning model into\ndecision-making by optimizing decisions on estimated costs. Such a pipeline can\nbe conducted by fitting a distributional model which is then plugged into the\ntarget optimization problem. While this fitting can utilize traditional methods\nsuch as maximum likelihood, a more recent approach uses estimation-optimization\nintegration that minimizes decision error instead of estimation error. Although\nintuitive, the statistical benefit of the latter approach is not well\nunderstood yet is important to guide the prescriptive usage of machine\nlearning. In this paper, we dissect the performance comparisons between these\napproaches in terms of the amount of model misspecification. In particular, we\nshow how the integrated approach offers a ``universal double benefit'' on the\ntop two dominating terms of regret when the underlying model is misspecified,\nwhile the traditional approach can be advantageous when the model is nearly\nwell-specified. Our comparison is powered by finite-sample tail regret bounds\nthat are derived via new higher-order expansions of regrets and the leveraging\nof a recent Berry-Esseen theorem.",
        "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.",
        "Lifelong learning (LL) aims to continuously acquire new knowledge while\nretaining previously learned knowledge. A central challenge in LL is the\nstability-plasticity dilemma, which requires models to balance the preservation\nof previous knowledge (stability) with the ability to learn new tasks\n(plasticity). While parameter-efficient fine-tuning (PEFT) has been widely\nadopted in large language models, its application to lifelong learning remains\nunderexplored. To bridge this gap, this paper proposes AdaLL, an adapter-based\nframework designed to address the dilemma through a simple, universal, and\neffective strategy. AdaLL co-trains the backbone network and adapters under\nregularization constraints, enabling the backbone to capture task-invariant\nfeatures while allowing the adapters to specialize in task-specific\ninformation. Unlike methods that freeze the backbone network, AdaLL\nincrementally enhances the backbone's capabilities across tasks while\nminimizing interference through backbone regularization. This architectural\ndesign significantly improves both stability and plasticity, effectively\neliminating the stability-plasticity dilemma. Extensive experiments demonstrate\nthat AdaLL consistently outperforms existing methods across various\nconfigurations, including dataset choices, task sequences, and task scales.",
        "Recent advances in deep learning have revolutionized seismic monitoring, yet\ndeveloping a foundation model that performs well across multiple complex tasks\nremains challenging, particularly when dealing with degraded signals or data\nscarcity. This work presents SeisMoLLM, the first foundation model that\nutilizes cross-modal transfer for seismic monitoring, to unleash the power of\nlarge-scale pre-training from a large language model without requiring direct\npre-training on seismic datasets. Through elaborate waveform tokenization and\nfine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-art\nperformance on the DiTing and STEAD datasets across five critical tasks:\nback-azimuth estimation, epicentral distance estimation, magnitude estimation,\nphase picking, and first-motion polarity classification. It attains 36 best\nresults out of 43 task metrics and 12 top scores out of 16 few-shot\ngeneralization metrics, with many relative improvements ranging from 10% to\n50%. In addition to its superior performance, SeisMoLLM maintains efficiency\ncomparable to or even better than lightweight models in both training and\ninference. These findings establish SeisMoLLM as a promising foundation model\nfor practical seismic monitoring and highlight cross-modal transfer as an\nexciting new direction for earthquake studies, showcasing the potential of\nadvanced deep learning techniques to propel seismology research forward.",
        "Recent work uncovered potential flaws in \\eg attribution or heatmap based\nsaliency methods. A typical flaw is a confirmations bias, where the scores are\ncompared to human expectation. Since measuring the quality of saliency methods\nis hard due to missing ground truth model reasoning, finding general\nlimitations is also hard. This is further complicated, because masking-based\nevaluation on complex data can easily introduce a bias, as most methods cannot\nfully ignore inputs. In this work, we extend our previous analysis on the\nlogical dataset framework ANDOR, where we showed that all analysed saliency\nmethods fail to grasp all needed classification information for all possible\nscenarios. Specifically, this paper extends our previous work using analysis on\nmore datasets, in order to better understand in which scenarios the saliency\nmethods fail. Further, we apply the Global Coherence Representation as an\nadditional evaluation method in order to enable actual input omission.",
        "Universal knowledge representation is a central problem for multivariate time\nseries(MTS) foundation models and yet remains open. This paper investigates\nthis problem from the first principle and it makes four folds of contributions.\nFirst, a new empirical finding is revealed: time series with different time\ngranularities (or corresponding frequency resolutions) exhibit distinct joint\ndistributions in the frequency domain. This implies a crucial aspect of\nlearning universal knowledge, one that has been overlooked by previous studies.\nSecond, a novel Fourier knowledge attention mechanism is proposed to enable\nlearning time granularity-aware representations from both the temporal and\nfrequency domains. Third, an autoregressive blank infilling pre-training\nframework is incorporated to time series analysis for the first time, leading\nto a generative tasks agnostic pre-training strategy. To this end, we develop\nthe General Time-series Model (GTM), a unified MTS foundation model that\naddresses the limitation of contemporary time series models, which often\nrequire token, pre-training, or model-level customizations for downstream tasks\nadaption. Fourth, extensive experiments show that GTM outperforms\nstate-of-the-art (SOTA) methods across all generative tasks, including\nlong-term forecasting, anomaly detection, and imputation.",
        "A long-standing goal in AI is to build agents that can solve a variety of\ntasks across different environments, including previously unseen ones. Two\ndominant approaches tackle this challenge: (i) reinforcement learning (RL),\nwhich learns policies through trial and error, and (ii) optimal control, which\nplans actions using a learned or known dynamics model. However, their relative\nstrengths and weaknesses remain underexplored in the setting where agents must\nlearn from offline trajectories without reward annotations. In this work, we\nsystematically analyze the performance of different RL and control-based\nmethods under datasets of varying quality. On the RL side, we consider\ngoal-conditioned and zero-shot approaches. On the control side, we train a\nlatent dynamics model using the Joint Embedding Predictive Architecture (JEPA)\nand use it for planning. We study how dataset properties-such as data\ndiversity, trajectory quality, and environment variability-affect the\nperformance of these approaches. Our results show that model-free RL excels\nwhen abundant, high-quality data is available, while model-based planning\nexcels in generalization to novel environment layouts, trajectory stitching,\nand data-efficiency. Notably, planning with a latent dynamics model emerges as\na promising approach for zero-shot generalization from suboptimal data.",
        "This study examines the potential causal relationship between head injury and\nthe risk of developing Alzheimer's disease (AD) using Bayesian networks and\nregression models. Using a dataset of 2,149 patients, we analyze key medical\nhistory variables, including head injury history, memory complaints,\ncardiovascular disease, and diabetes. Logistic regression results suggest an\nodds ratio of 0.88 for head injury, indicating a potential but statistically\ninsignificant protective effect against AD. In contrast, memory complaints\nexhibit a strong association with AD, with an odds ratio of 4.59. Linear\nregression analysis further confirms the lack of statistical significance for\nhead injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive\nimportance of memory complaints. These findings highlight the complex interplay\nof medical history factors in AD risk assessment and underscore the need for\nfurther research utilizing larger datasets and advanced causal modeling\ntechniques.",
        "We present a $k$-means-based clustering algorithm, which optimizes the mean\nsquare error, for given cluster sizes. A straightforward application is\nbalanced clustering, where the sizes of each cluster are equal. In the\n$k$-means assignment phase, the algorithm solves an assignment problem using\nthe Hungarian algorithm. This makes the assignment phase time complexity\n$O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
        "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of high-quality annotated datasets. Traditional models\noften struggle to recognize handwriting from unseen writers, making\nwriter-independent (WI) recognition a crucial but difficult problem. This paper\npresents an HWR model with an encoder-decoder structure for IMU data, featuring\na CNN-based encoder for feature extraction and a BiLSTM decoder for sequence\nmodeling, which supports inputs of varying lengths. Our approach demonstrates\nstrong robustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own dataset.\nExtensive evaluations show that our model maintains high accuracy across\ndifferent age groups and writing conditions while effectively learning from\nlimited data. Through comprehensive ablation studies, we analyze key design\nchoices, achieving a balance between accuracy and efficiency. These findings\ncontribute to the development of more adaptable and scalable HWR systems for\nreal-world applications.",
        "The recent advancement of foundation models (FMs) has brought about a\nparadigm shift, revolutionizing various sectors worldwide. The popular\noptimizers used to train these models are stochastic gradient descent-based\nalgorithms, which face inherent limitations, such as slow convergence and\nstringent assumptions for convergence. In particular, data heterogeneity\narising from distributed settings poses significant challenges to their\ntheoretical and numerical performance. This paper develops an algorithm, PISA\n({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of\nMultipliers), which enables scalable parallel computing and supports various\nsecond-moment schemes. Grounded in rigorous theoretical guarantees, the\nalgorithm converges under the sole assumption of Lipschitz continuity of the\ngradient, thereby removing the need for other conditions commonly imposed by\nstochastic methods. This capability enables PISA to tackle the challenge of\ndata heterogeneity effectively. Comprehensive experimental evaluations for\ntraining or fine-tuning diverse FMs, including vision models, large language\nmodels, reinforcement learning models, generative adversarial networks, and\nrecurrent neural networks, demonstrate its superior numerical performance\ncompared to various state-of-the-art optimizers.",
        "Current evaluations of synthetic tabular data mainly focus on how well joint\ndistributions are modeled, often overlooking the assessment of their\neffectiveness in preserving realistic event sequences and coherent entity\nrelationships across columns.This paper proposes three evaluation metrics\ndesigned to assess the preservation of logical relationships among columns in\nsynthetic tabular data. We validate these metrics by assessing the performance\nof both classical and state-of-the-art generation methods on a real-world\nindustrial dataset.Experimental results reveal that existing methods often fail\nto rigorously maintain logical consistency (e.g., hierarchical relationships in\ngeography or organization) and dependencies (e.g., temporal sequences or\nmathematical relationships), which are crucial for preserving the fine-grained\nrealism of real-world tabular data. Building on these insights, this study also\ndiscusses possible pathways to better capture logical relationships while\nmodeling the distribution of synthetic tabular data.",
        "In this paper, we reconstruct Euclid's theory of similar triangles, as\ndeveloped in Book VI of the \\textit{Elements}, along with its 20th-century\ncounterparts, formulated within the systems of Hilbert, Birkhoff, Borsuk and\nSzmielew, Millman and Parker, as well as Hartshorne. In the final sections, we\npresent recent developments concerning non-Archimedean fields and mechanized\nproofs. Thales' theorem (VI.2) serves as the reference point in our\ncomparisons. It forms the basis of Euclid's system and follows from VI.1 the\nonly proposition within the theory of similar triangles that explicitly applies\nthe definition of proportion. Instead of the ancient proportion, modern systems\nadopt the arithmetic of line segments or real numbers. Accordingly, they adopt\nother propositions from Euclid's Book VI, such as VI.4, VI.6, or VI.9, as a\nbasis. In {\\S}\\,10, we present a system that, while meeting modern criteria of\nrigor, reconstructs Euclid's theory and mimics its deductive structure,\nbeginning with VI.1. This system extends to automated proofs of Euclid's\npropositions from Book VI. Systems relying on real numbers provide the\nfoundation for trigonometry as applied in modern mathematics. In {\\S}\\,9, we\nprove Thales' theorem in geometry over the hyperreal numbers. Just as Hilbert\nmanaged to prove Thales' theorem without referencing the Archimedean axiom, so\ndo we by applying the arithmetic of the non-Archimedean field of hyperreal\nnumbers.",
        "Time series forecasting has recently achieved significant progress with\nmulti-scale models to address the heterogeneity between long and short range\npatterns. Despite their state-of-the-art performance, we identify two potential\nareas for improvement. First, the variates of the multivariate time series are\nprocessed independently. Moreover, the multi-scale (long and short range)\nrepresentations are learned separately by two independent models without\ncommunication. In light of these concerns, we propose State Space Transformer\nwith cross-attention (S2TX). S2TX employs a cross-attention mechanism to\nintegrate a Mamba model for extracting long-range cross-variate context and a\nTransformer model with local window attention to capture short-range\nrepresentations. By cross-attending to the global context, the Transformer\nmodel further facilitates variate-level interactions as well as local\/global\ncommunications. Comprehensive experiments on seven classic long-short range\ntime-series forecasting benchmark datasets demonstrate that S2TX can achieve\nhighly robust SOTA results while maintaining a low memory footprint.",
        "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.",
        "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
        "Foundation models for single-cell RNA sequencing (scRNA-seq) have shown\npromising capabilities in capturing gene expression patterns. However, current\napproaches face critical limitations: they ignore biological prior knowledge\nencoded in gene regulatory relationships and fail to leverage multi-omics\nsignals that could provide complementary regulatory insights. In this paper, we\npropose GRNFormer, a new framework that systematically integrates multi-scale\nGene Regulatory Networks (GRNs) inferred from multi-omics data into RNA\nfoundation model training. Our framework introduces two key innovations. First,\nwe introduce a pipeline for constructing hierarchical GRNs that capture\nregulatory relationships at both cell-type-specific and cell-specific\nresolutions. Second, we design a structure-aware integration framework that\naddresses the information asymmetry in GRNs through two technical advances: (1)\nA graph topological adapter using multi-head cross-attention to weight\nregulatory relationships dynamically, and (2) a novel edge perturbation\nstrategy that perturb GRNs with biologically-informed co-expression links to\naugment graph neural network training. Comprehensive experiments have been\nconducted on three representative downstream tasks across multiple model\narchitectures to demonstrate the effectiveness of GRNFormer. It achieves\nconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$\nincrease in drug response prediction correlation, $9.6\\%$ improvement in\nsingle-cell drug classification AUC, and $1.1\\%$ average gain in gene\nperturbation prediction accuracy.",
        "This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary\nsearch (BS) locking, designed to cover a broad frequency range from 533 MHz to\n4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a\nlinear to a logarithmic function, completing in B+1 cycles, where B represents\nthe digital-to-analog (DAC) resolution controlling the voltage-controlled delay\nline (VCDL). At the start of the BS process, large step sizes can cause\nsignificant bias overshoots, potentially leading to clock failure conditions\n(i.e., clocks fail to propagate through the VCDL). To address this issue, a\ntoggle detector is introduced to monitor clock activity and adjust the binary\nsearch controller. Upon detecting a stalled clock, the controller reverts the\nDAC code to the previous working code and resumes the BS with a reduced step\nsize. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a\nlocking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at\n4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with\na static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter\nof 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit\n(FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.",
        "Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.",
        "This research examines the regularity of weak solutions to the Div-Curl\nsystem with low regularity anisotropic coefficients. The H\\\"older regularity of\nthe Div-Curl system with one anisotropic coefficient was an unresolved problem\nraised by Yin in 2016. We have addressed the open problem, and the findings\nextend to the scenario involving two anisotropic coefficients. We establish the\nH\\\"{o}lder regularity of the solution when the coefficients is H\\\"{o}lder\ncontinuous. Moreover, the degree of H\\\"{o}lder regularity of the solution can\nbe improved if the coefficient has a greater degree of H\\\"{o}lder regularity.",
        "We establish a semi-orthogonal decomposition for the weighted blowup of an\nalgebraic stack along a Koszul-regular weighted centre, generalising the\nclassic result of Orlov. Our approach is based on the work of Bergh-Schn\\\"urer.",
        "Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment\/emotion detection of a given input as a sentence\/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions.",
        "Machine learning can analyze vast amounts of data generated by IoT devices to\nidentify patterns, make predictions, and enable real-time decision-making. By\nprocessing sensor data, machine learning models can optimize processes, improve\nefficiency, and enhance personalized user experiences in smart systems.\nHowever, IoT systems are often deployed in sensitive environments such as\nhouseholds and offices, where they may inadvertently expose identifiable\ninformation, including location, habits, and personal identifiers. This raises\nsignificant privacy concerns, necessitating the application of data\nminimization -- a foundational principle in emerging data regulations, which\nmandates that service providers only collect data that is directly relevant and\nnecessary for a specified purpose. Despite its importance, data minimization\nlacks a precise technical definition in the context of sensor data, where\ncollections of weak signals make it challenging to apply a binary \"relevant and\nnecessary\" rule. This paper provides a technical interpretation of data\nminimization in the context of sensor streams, explores practical methods for\nimplementation, and addresses the challenges involved. Through our approach, we\ndemonstrate that our framework can reduce user identifiability by up to 16.7%\nwhile maintaining accuracy loss below 1%, offering a viable path toward\nprivacy-preserving IoT data processing.",
        "Multi-armed bandits (MABs) are frequently used for online sequential\ndecision-making in applications ranging from recommending personalized content\nto assigning treatments to patients. A recurring challenge in the applicability\nof the classic MAB framework to real-world settings is ignoring\n\\textit{interference}, where a unit's outcome depends on treatment assigned to\nothers. This leads to an exponentially growing action space, rendering standard\napproaches computationally impractical. We study the MAB problem under network\ninterference, where each unit's reward depends on its own treatment and those\nof its neighbors in a given interference graph. We propose a novel algorithm\nthat uses the local structure of the interference graph to minimize regret. We\nderive a graph-dependent upper bound on cumulative regret showing that it\nimproves over prior work. Additionally, we provide the first lower bounds for\nbandits with arbitrary network interference, where each bound involves a\ndistinct structural property of the interference graph. These bounds\ndemonstrate that when the graph is either dense or sparse, our algorithm is\nnearly optimal, with upper and lower bounds that match up to logarithmic\nfactors. We complement our theoretical results with numerical experiments,\nwhich show that our approach outperforms baseline methods.",
        "We investigate the notion of the medial axis for pseudo-Euclidean spaces. For\nmost of the article, we follow the path of Birbrair and Denkowski's article\n\"Medial Axis and Singularities\", checking its feasibility in the new context.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE"
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window",
    "start_abstract":"SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Fast fit-free analysis of fluorescence lifetime imaging via deep learning"
      ],
      "abstract":[
        "Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Adding numbers with spiking neural circuits on neuromorphic hardware",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Evolving Form and Function: Dual-Objective Optimization in Neural\n  Symbolic Regression Networks",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Hardware-In-The-Loop Training of a 4f Optical Correlator with\n  Logarithmic Complexity Reduction for CNNs",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Testing the limits of ITkPixV2: the ATLAS inner tracker pixel detector\n  readout chip",
        "Analysis of pitchfork bifurcations and symmetry breaking in the elliptic\n  restricted three-body problem",
        "Range-Only Localization System for Small-Scale Flapping-Wing Robots",
        "Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding",
        "A Novel Interpretation of the Radon Transform's Ray- and Pixel-Driven\n  Discretizations under Balanced Resolutions",
        "ADAPT: An Autonomous Forklift for Construction Site Operation",
        "Perimeter length of the convex hull of Brownian motion in the hyperbolic\n  plane",
        "Post-disaster building indoor damage and survivor detection using\n  autonomous path planning and deep learning with unmanned aerial vehicles",
        "On families of strongly divisible modules of rank 2",
        "On-demand storage and retrieval of single photons from a semiconductor\n  quantum dot in a room-temperature atomic vapor memory",
        "High Energy Jet Emission from GRS 1758-258 & 1E 1740.7-2942 with\n  INTEGRAL?",
        "Emotional Multifaceted Feedback on AI Tool Use in EFL Learning\n  Initiation: Chain-Mediated Effects of Motivation and Metacognitive Strategies\n  in an Optimized TAM Model",
        "Derivation of the Planck Units Based in a Membranes Model",
        "Nonparametric Smoothing of Directional and Axial Data",
        "Carbonic anhydrase II simulated with a universal neural network\n  potential"
      ],
      "abstract":[
        "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "Data increasingly abounds, but distilling their underlying relationships down\nto something interpretable remains challenging. One approach is genetic\nprogramming, which `symbolically regresses' a data set down into an equation.\n  However, symbolic regression (SR) faces the issue of requiring training from\nscratch for each new dataset. To generalize across all datasets, deep learning\ntechniques have been applied to SR.\n  These networks, however, are only able to be trained using a symbolic\nobjective: NN-generated and target equations are symbolically compared. But\nthis does not consider the predictive power of these equations, which could be\nmeasured by a behavioral objective that compares the generated equation's\npredictions to actual data.\n  Here we introduce a method that combines gradient descent and evolutionary\ncomputation to yield neural networks that minimize the symbolic and behavioral\nerrors of the equations they generate from data.\n  As a result, these evolved networks are shown to generate more symbolically\nand behaviorally accurate equations than those generated by networks trained by\nstate-of-the-art gradient based neural symbolic regression methods.\n  We hope this method suggests that evolutionary algorithms, combined with\ngradient descent, can improve SR results by yielding equations with more\naccurate form and function.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "This work evaluates a forward-only learning algorithm on the MNIST dataset\nwith hardware-in-the-loop training of a 4f optical correlator, achieving 87.6%\naccuracy with O(n2) complexity, compared to backpropagation, which achieves\n88.8% accuracy with O(n2 log n) complexity.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "The ITkPixV2 chip is the final production readout chip for the ATLAS Phase 2\nInner Tracker (ITk) upgrade at the upcoming High-Luminosity LHC (HL-LHC). Due\nto the extraordinarily high peak luminosity at the HL-LHC of $5 \\times 10^{34}$\ncm$^{-1}$s$^{-1}$, ITkPixV2 must meet significant increases in nearly all\ndesign requirements, including a 10x increase in trigger rate, a 7.5x increase\nin hit rate, a 3x increase in radiation tolerance, and a 12.5x decrease in\npixel current draw per unit area, all while maintaining a similar power per\nunit area as present pixel detectors. Here we present the first measurements of\nthe ITkPixV2 chip operated at the limits of the full chip design requirements,\nincluding in particular a measurement of the activity-induced current of the\nchip as a function of increasing hit rate.",
        "A unified framework is proposed to quantitatively characterize pitchfork\nbifurcations and associated symmetry breaking in the elliptic restricted\nthree-body problem (ERTBP). It is known that planar\/vertical Lyapunov orbits\nand Lissajous orbits near the collinear libration points undergo pitchfork\nbifurcations with varying orbital energy. These bifurcations induce symmetry\nbreaking, generating bifurcated families including halo\/quasi-halo orbits,\naxial\/quasi-axial orbits, and their corresponding invariant manifolds.\nTraditional semi-analytical methods for constructing halo orbits, based on\nresonant bifurcation mechanisms, have obstacles in fully exploiting the\nintrinsic symmetry breaking characteristics in pitchfork bifurcations. In this\npaper, we propose a unified trigonometric series-based framework to analyze\nthese bifurcated families from the perspective of coupling-induced bifurcation\nmechanisms. By introducing a coupling coefficient and various bifurcation\nequations into the ERTBP, different symmetry breaking is achieved when the\ncoupling coefficient is non-zero. This unified semi-analytical framework\ncaptures bifurcations of both periodic\/quasi-periodic and transit\/non-transit\norbits. Furthermore, it reveals that pitchfork bifurcation solutions in the\nERTBP fundamentally depend solely on the orbital eccentricity and three\namplitude parameters of the system's degrees of freedom, governing both the\nelliptic direction and the hyperbolic one.",
        "The design of localization systems for small-scale flapping-wing aerial\nrobots faces relevant challenges caused by the limited payload and onboard\ncomputational resources. This paper presents an ultra-wideband localization\nsystem particularly designed for small-scale flapping-wing robots. The solution\nrelies on custom 5 grams ultra-wideband sensors and provides robust, very\nefficient (in terms of both computation and energy consumption), and accurate\n(mean error of 0.28 meters) 3D position estimation. We validate our system\nusing a Flapper Nimble+ flapping-wing robot.",
        "We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.",
        "Tomographic investigations are a central tool in medical applications,\nallowing doctors to image the interior of patients. The corresponding\nmeasurement process is commonly modeled by the Radon transform. In practice,\nthe solution of the tomographic problem requires discretization of the Radon\ntransform and its adjoint (called the backprojection). There are various\ndiscretization schemes; often structured around three discretization\nparameters: spatial-, detector-, and angular resolutions. The most widespread\napproach uses the ray-driven Radon transform and the pixel-driven\nbackprojection in a balanced resolution setting, i.e., the spatial resolution\nroughly equals the detector resolution. The use of these particular\ndiscretization approaches is based on anecdotal reports of their approximation\nperformance, but there is little rigorous analysis of these methods'\napproximation errors. This paper presents a novel interpretation of ray-driven\nand pixel-driven methods as convolutional discretizations, illustrating that\nfrom an abstract perspective these methods are similar. Moreover, we announce\nstatements concerning the convergence of the ray-driven Radon transform and the\npixel-driven backprojection under balanced resolutions. Our considerations are\nsupported by numerical experiments highlighting aspects of the discussed\nmethods.",
        "Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics.",
        "We relate the expected hyperbolic length of the perimeter of the convex hull\nof the trajectory of Brownian motion in the hyperbolic plane to an expectation\nof a certain exponential functional of a one-dimensional real-valued Brownian\nmotion, and hence derive small- and large-time asymptotics for the expected\nhyperbolic perimeter. In contrast to the case of Euclidean Brownian motion with\nnon-zero drift, the large-time asymptotics are a factor of two greater than the\nlower bound implied by the fact that the convex hull includes the hyperbolic\nline segment from the origin to the endpoint of the hyperbolic Brownian motion.\nWe also obtain an exact expression for the expected perimeter length after an\nindependent exponential random time.",
        "Rapid response to natural disasters such as earthquakes is a crucial element\nin ensuring the safety of civil infrastructures and minimizing casualties.\nTraditional manual inspection is labour-intensive, time-consuming, and can be\ndangerous for inspectors and rescue workers. This paper proposed an autonomous\ninspection approach for structural damage inspection and survivor detection in\nthe post-disaster building indoor scenario, which incorporates an autonomous\nnavigation method, deep learning-based damage and survivor detection method,\nand a customized low-cost micro aerial vehicle (MAV) with onboard sensors.\nExperimental studies in a pseudo-post-disaster office building have shown the\nproposed methodology can achieve high accuracy in structural damage inspection\nand survivor detection. Overall, the proposed inspection approach shows great\npotential to improve the efficiency of existing manual post-disaster building\ninspection.",
        "Let $p$ be an odd prime, and $\\mathbf{Q}_{p^f}$ the unramified extension of\n$\\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of\nconstructing strongly divisible modules for $2$-dimensional semi-stable\nnon-crystalline representations of\n$\\mathrm{Gal}(\\overline{\\mathbf{Q}}_p\/\\mathbf{Q}_{p^f})$ with Hodge--Tate\nweights in the Fontaine--Laffaille range to solving systems of linear equations\nand inequalities. We also determine the Breuil modules corresponding to the\nmod-$p$ reduction of the strongly divisible modules. We expect our method to\nproduce at least one Galois-stable lattice in each such representation for\ngeneral $f$. Moreover, when the mod-$p$ reduction is an extension of distinct\ncharacters, we further expect our method to provide the two non-homothetic\nlattices. As applications, we show that our approach recovers previously known\nresults for $f=1$ and determine the mod-$p$ reduction of the semi-stable\nrepresentations with some small Hodge--Tate weights when $f=2$.",
        "Interfacing light from solid-state single-photon sources with scalable and\nrobust room-temperature quantum memories has been a long-standing challenge in\nphotonic quantum information technologies due to inherent noise processes and\ntime-scale mismatches between the operating conditions of solid-state and\natomic systems. Here, we demonstrate on-demand storage and retrieval of single\nphotons from a semiconductor quantum dot device in a room-temperature atomic\nvapor memory. A deterministically fabricated InGaAs quantum dot light source\nemits single photons at the wavelength of the cesium D1 line at 895\\,nm which\nexhibit an inhomogeneously broadened linewidth of 5.1(7)\\,GHz and are\nsubsequently stored in a low-noise ladder-type cesium vapor memory. We show\ncontrol over the interaction between the single photons and the atomic vapor,\nallowing for variable retrieval times of up to 19.8(3)\\,ns at an internal\nefficiency of $\\eta_\\mathrm{int}=0.6(1)\\%$. Our results significantly expand\nthe application space of both room-temperature vapor memories and semiconductor\nquantum dots in future quantum network architectures.",
        "GRS 1758-258 and 1E 1740.7-2942 are two long-known persistent black hole\nbinaries in the Galactic Center region. Using INTEGRAL's extensive monitoring\nof the Galactic Center and Bulge, we studied their temporal and spectral\nevolutions in the 30-610 keV energy range from March 2003 through April 2022\nwith the IBIS\/ISGRI gamma-ray telescope. Our analyses found that the sources\ntypically had Comptonized spectra, though not always with the same parameters.\nThe spectral states with more than 8 Ms of observation time show deviations\nfrom a Comptonized spectrum above ~200 keV or a \"hard tail\" that extends up to\nat least 600 keV. The origin of this component remains debated with the most\npopular scenarios being synchrotron emission from the jet or Comptonization in\na hybrid thermal\/non-thermal plasma. Anyway, the GRS 1758-258 and 1E\n1740.7-2942 spectra are acceptably described by CompTT+po (jet) and Eqpair\n(hybrid Comptonization) scenarios. To differentiate between the two scenarios,\nwe calculated the Spearman correlation coefficient comparing 30-50 keV count\nrates with those in higher energy bands (50-100, 100-300, and 300-600 keV). The\ncount rates below 300 keV are strongly correlated, indicating those photons\narise from the same physical process. Above 300 keV the count rates are either\nanti-correlated or not correlated with the 30-50 keV count rates for GRS\n1758-258, which suggests that the photons originate from a different physical\nprocess. For 1E 1740.7-2942, the level of correlation is unclear due to scatter\nin the data points. However, the 300-600 keV count rates are consistent with a\nconstant value. This disfavors the hybrid Comptonization scenario for both\nsources.",
        "This study specifically investigates the initiation phase of EFL learners'\nengagement with AI tools, focusing on how technology acceptance constructs\nperceived usefulness (PU), perceived ease of use (PEOU), and perceived\nself-efficacy (PSE) influence learning resilience. Drawing on an optimized\nTechnology Acceptance Model (TAM) and integrating constructs from positive\npsychology, the study examines the chain-mediated effects of learning\nmotivation (LM) and metacognitive strategies (MS) on resilience outcomes,\noperationalized through optimism (OP), psychological resilience (PR), and\ngrowth mindset (GM). A survey of first-year English majors (N = 730) was\nconducted, and structural equation modeling was employed to analyze the data.\nThe findings indicate that favorable perceptions of AI tools are significantly\nassociated with enhanced LM and MS, which in turn positively impact resilience\nmeasures. These results suggest that the interplay between technology\nacceptance and internal regulatory processes is vital in shaping EFL learners'\nearly experiences with AI-assisted learning. Practical implications for\neducators and researchers are discussed, with an emphasis on promoting\nuser-friendly and effective AI environments to support the development of\nadaptive learning behaviors.",
        "In this study, the Planck units (mass, time and length) have only been\nderived, explained and attributed a physical meaning when they were deduced\nbased on the concept of interacting membranes (membranes instead of strings of\nstring theory). For this purpose, a set of five assumptions were proposed: (a)\nthe existence of the interacting membranes; (b) the curvatures of the membranes\noscillate according to the classical wave equation; (c) the spatial period of\nthe wave that arise when the membranes oscillate is given by $\\lambda =\n{\\xi}{\\pi}\/k$; (d) the membranes oscillate with wavelength given by de Broglie\nrelation and (e) $x=ct$ holds. The parameter $\\xi$ determines the period of\noscillation of the given membranes. In deriving the Planck units in this work,\n$\\xi$ must take the value 2 and determines a period 2$\\pi$, closely to minimum\nvalue 1 or to fundamental period $\\pi$, respectively. In this context, Planck\nunits must be fundamental. Moreover, the parameter $\\xi$ was reported as a\nunification parameter between the formulas for the Coulomb$^{\\prime}$s law and\nNewton$^{\\prime}$s law of universal gravitation linking the forces of\nmicroworld and macroworld. Depending on the value $\\xi$ takes, one force or\nanother will be had. It is also shown that the potential $V = hc\/{\\xi}{\\pi}x$\ndeduced from the above assumptions and which contributes to deduce the Planck\nunits, can be derived from Yukawa$^{\\prime}$s equation. Hence, the present work\nwould be contributing to theoretical physics, since at the Planck scale\npredictions of some theories like Standard Model, quantum field theory and\ngeneral relativity are not expected to be valid.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "The carbonic anhydrase II enzyme (CA II) is one of the most significant\nenzymes in nature, reversibly converting CO$_2$ to bicarbonate at a remarkable\nrate. The precise mechanism it uses to achieve this rapid turnover remains\nunclear due to our inability to directly observe or simulate the full process\ndynamically. Here, we use a recently developed universal neural network\npotential (Orb) to simulate the active site of CA II. We reproduce several\nknown features of the reaction mechanism, including the proton relay that\nconducts protons out of the active site to the His64 residue. Additionally, we\nobserve a new reaction pathway where CO$_2$ reacts with a water molecule in the\nactive site, which donates a proton to the zinc-bound hydroxide. This differs\nfrom the established mechanism where CO$_2$ directly reacts with hydroxide.\nExisting experimental data and independent quantum chemistry calculations are\nused to support the plausibility of this new mechanism. This demonstrates the\npotential of Orb to efficiently generate novel insights into important\nmolecular scale processes that can potentially be harnessed to improve CO$_2$\ncapture technologies and drug design."
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Fast fit-free analysis of fluorescence lifetime imaging via deep learning",
    "start_abstract":"Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window"
      ],
      "abstract":[
        "SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "Progress of the anti-obesity of Berberine",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "COLOR: A compositional linear operation-based representation of protein\n  sequences for identification of monomer contributions to properties",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Mechanism of Electricacupuncture Treating Detrusor Bladder Neck\n  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Short Paths in the Planar Graph Product Structure Theorem",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law",
        "Coherent dynamics of flavor mode entangled neutrinos",
        "On the Use of WGANs for Super Resolution in Dark-Matter Simulations",
        "Relationship between the $\\gamma-$ray variability and the pc-scale jet\n  in the blazar 3C 454.3",
        "Gas Perturbations in Hot Smooth Atmospheres X-ray Surface Brightness\n  Fluctuations in Smooth Galaxy Cluster Atmospheres",
        "Causal Inference on Outcomes Learned from Text",
        "Propagation of extreme events in multiplex neuronal networks",
        "The Energy Cascade Rate in Supersonic Magnetohydrodynamic Turbulence",
        "Self-organized institutions in evolutionary dynamical-systems game",
        "Identifying Flare Locations Through Exoplanet Transit Occultations",
        "Effect of 3d Transition Metal Doping (Mn, Fe, Co, Ni) on the Electronic\n  and Magnetic Properties of Pd Alloys at Low Impurity Concentrations: An Ab\n  initio Study",
        "Resonant current from singlet-triplet state mixing in coupled quantum\n  dots",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone"
      ],
      "abstract":[
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "The properties of biological materials like proteins and nucleic acids are\nlargely determined by their primary sequence. While certain segments in the\nsequence strongly influence specific functions, identifying these segments, or\nso-called motifs, is challenging due to the complexity of sequential data.\nWhile deep learning (DL) models can accurately capture sequence-property\nrelationships, the degree of nonlinearity in these models limits the assessment\nof monomer contributions to a property - a critical step in identifying key\nmotifs. Recent advances in explainable AI (XAI) offer attention and\ngradient-based methods for estimating monomeric contributions. However, these\nmethods are primarily applied to classification tasks, such as binding site\nidentification, where they achieve limited accuracy (40-45%) and rely on\nqualitative evaluations. To address these limitations, we introduce a DL model\nwith interpretable steps, enabling direct tracing of monomeric contributions.\nWe also propose a metric ($\\mathcal{I}$), inspired by the masking technique in\nthe field of image analysis and natural language processing, for quantitative\nanalysis on datasets mainly containing distinct properties of anti-cancer\npeptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits\n22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that\nsignificantly destabilize ACPs, and identifies motifs in AMPs that are 50% more\neffective in converting non-AMPs to AMPs. These findings highlight the\npotential of our model in guiding mutation strategies for designing\nprotein-based biomaterials.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Objectives This study aimed to elucidate the potential mechanisms of\nelectroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)\nfollowing suprasacral spinal cord injury.\n  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned\nto either a sham group (n=12) or a spinal cord injury model group (n=40). In\nthe model group, DBND was induced in 40 rats through Hassan Shaker spinal cord\ntransection, with 24 rats surviving spinal shock and subsequently randomized\ninto two groups: a model-only group (DBND, n=12) and an EA intervention group\n(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and\nSanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10\nconsecutive days. On day 29 post-injury, all rats underwent urodynamic\nassessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag\n(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder\nneck tissues.\n  Results Urodynamic evaluation demonstrated that EA intervention enhanced\nbladder function in DBND rats. HE staining indicated reduced fibroplasia in the\ndetrusor muscle and alleviated inflammation in the bladder neck following EA.\nTMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in\nthe detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results\ncorroborated these TMT findings.\n  Conclusion EA effectively promotes synergy between the detrusor muscle and\nbladder neck in DBND, likely by enhancing detrusor contractility and\nfacilitating bladder neck relaxation during urination. This study provides\nmechanistic insights into the therapeutic role of EA in managing DBND.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "The Planar Graph Product Structure Theorem of Dujmovi\\'c et al. [J. ACM '20]\nsays that every planar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_3$\nfor some planar graph $H$ with treewidth at most 3 and some path $P$. This\nresult has been the key to solving several old open problems. Several people\nhave asked whether the Planar Graph Product Structure Theorem can be proved\nwith good upper bounds on the length of $P$. No $o(n)$ upper bound was\npreviously known for $n$-vertex planar graphs. We answer this question in the\naffirmative, by proving that for any $\\epsilon\\in (0,1)$ every $n$-vertex\nplanar graph is contained in $H\\boxtimes P\\boxtimes K_{O(1\/\\epsilon)}$, for\nsome planar graph $H$ with treewidth 3 and for some path $P$ of length\n$O(\\frac{1}{\\epsilon}n^{(1+\\epsilon)\/2})$. This bound is almost tight since\nthere is a lower bound of $\\Omega(n^{1\/2})$ for certain $n$-vertex planar\ngraphs. In fact, we prove a stronger result with $P$ of length\n$O(\\frac{1}{\\epsilon}\\,\\textrm{tw}(G)\\,n^{\\epsilon})$, which is tight up to the\n$O(\\frac{1}{\\epsilon}\\,n^{\\epsilon})$ factor for every $n$-vertex planar graph\n$G$. Finally, taking $\\epsilon=\\frac{1}{\\log n}$, we show that every $n$-vertex\nplanar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ for some\nplanar graph $H$ with treewidth at most 3 and some path $P$ of length\n$O(\\textrm{tw}(G)\\,\\log n)$. This result is particularly attractive since the\ntreewidth of the product $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ is within a\n$O(\\log^2n)$ factor of the treewidth of $G$.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings.",
        "As the lynchpin of all quantum correlations, quantum coherence is fundamental\nfor distinguishing quantum systems from classical ones and is essential for\nrealizing quantum advantages in areas such as computation, communication, and\nmetrology. In this study, we investigate the relationship between quantum\ncoherence and neutrino oscillations by mapping the neutrino state as a\nmulti-mode quantum system into qubit and qutrit frameworks. Our analysis\nextends beyond the commonly used $l_1$-norm and relative entropy of coherence\nto include all relevant measures of coherence such as robustness of coherence,\ncoherence concurrence, trace-norm distance measure of coherence, coherence of\nformation, Schatten-$p$-norm-based functionals, geometric coherence and\nlogarithmic coherence rank, each offering unique insights into the quantum\ncorrelations in these systems. Notably, while the $l_1$-norm and relative\nentropy-based measures apply to general quantum states, the other measures are\nparticularly relevant for entangled systems, highlighting the critical role of\nentanglement in neutrino oscillations. We present a detailed methodology for\ncalculating coherence measures in both two-flavor and three-flavor mixing\nscenarios, contributing to a deeper understanding of how quantum coherence\nmanifests and evolves in mode-entangled neutrino systems. Our findings\nemphasize the potential of these systems as robust candidates for quantum\ninformation tasks, facilitated by the weak interaction nature of neutrinos.",
        "Super-resolution techniques have the potential to reduce the computational\ncost of cosmological and astrophysical simulations. This can be achieved by\nenabling traditional simulation methods to run at lower resolution and then\nefficiently computing high-resolution data corresponding to the simulated\nlow-resolution data. In this work, we investigate the application of a\nWasserstein Generative Adversarial Network (WGAN) to increase the particle\nresolution of dark-matter-only simulations, reproducing and building on prior\nresults. Our WGAN models successfully generate high-resolution data with\nsummary statistics, including the power spectrum and halo mass function, that\nclosely match those of true high-resolution simulations. We also identify a\nlimitation of the WGAN model in the form of smeared features in the generated\nhigh-resolution snapshots, particularly in the shapes of dark-matter halos.",
        "3C 454.3 is a flat spectrum radio quasar (FSRQ) known for its high\nvariability across the electromagnetic spectrum, showing structural and flux\nvariability in its pc-scale jet, and correlated variability among frequency\nbands. This study aims to identify the structure, dynamics, and radiative\nprocesses common to the innermost regions of the blazar 3C 454.3. We\ninvestigate whether any jet component can be associated with $\\gamma-$ray\nemission and variability. We analyze the relationship between the variable\n$\\gamma-$ray emission and pc-scale jet properties in 3C 454.3 by combining\n$\\gamma-$ray data spanning twelve years with contemporaneous VLBA multi-epoch\nimages at 15 and 43 GHz. Spearman rank correlation tests are conducted to\ndetermine if the flux variability of any jet component is associated with\n$\\gamma-$ray variability. Core emission at 43 and 15 GHz strongly correlates\nwith $\\gamma-$ray emission. The 43 GHz core (Q0) contributes around 37$\\%$ of\nthe observed $\\gamma-$ray variability, while the 15 GHz core (K0) accounts for\n30$\\%$. A quasi-stationary component at 43 GHz, at a projected distance of 4.6\npc, correlates with the $\\gamma-$ray flux, accounting for 20$\\%$ of its\nemission between 2016 and 2021. We found a mobile component (Q3 between 2010.18\nand 2011.16) at 43 GHz with a projected distance between 0.8 and 2.3 pc and\napparent velocity of $\\beta_{app} = 9.9 \\pm 1.1$ c, accounting for\napproximately 28% of the $\\gamma-$ray emission. The observed simultaneous\nvariability in emission regions beyond the central parsec strongly suggests\nsynchrotron self-Compton (SSC) as the primary mechanism for $\\gamma-$ray\nproduction in these regions. Our findings demonstrate the existence of multiple\n$\\gamma-$ray emission regions within the blazar jet but also suggest that some\nof these regions are non-stationary over time.",
        "We measure surface brightness fluctuations in Chandra X-ray images of the\ncores of the galaxy clusters Abell 2029, Abell 2151, Abell 2107, RBS0533, and\nRBS0540. Their relatively structureless X-ray atmospheres exhibit the\nthermodynamic properties of cool cores including short central cooling times\nand low entropy. However, unlike typical cool-core clusters, molecular gas,\nstar formation, and bubbles associated with radio jets are faint or absent near\ntheir central galaxies. Four clusters show typical gas density fluctuation\namplitudes of $\\sim$ 10 per cent on the scales probed, apart from RBS0540,\nwhich exhibits lower amplitudes, suggesting that its gas is mildly disturbed.\nUnder the assumption that gas density fluctuations are indicative of random gas\nvelocities, we estimate scale-dependent velocity amplitudes of gas motions\nacross all studied clusters, which range from 100 km\/s to 200 km\/s in Abell\n2029, Abell 2151, and Abell 2107. These velocity estimates are comparable to\nthe atmospheric velocity dispersion in the Perseus cluster measured by the\nHitomi X-ray Observatory. The turbulent heating rates implied by our\nmeasurements are of the same order as the radiative cooling rates. Our results\nsuggest that atmospheric sloshing and perhaps turbulent motion may aid radio\njets in stabilizing atmospheric cooling.",
        "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "Three-dimensional direct numerical simulations (DNS) are implemented to\ninvestigate the energy cascade rate in compressible isothermal\nmagnetohydrodynamic (MHD) turbulence. Utilizing an exact law derived from the\nK\\'arm\\'an-Howarth equation, we examine the contributions of flux and non-flux\nterms to the cascade rate across a broad range of sonic and Alfv\\'enic Mach\nnumbers, from subsonic to supersonic regimes and varying mean magnetic fields.\nCascade rates are computed using on-grid 3-D decomposition and two plasma\nincrement approaches: signed and absolute values. Anisotropy induced by strong\nmagnetic fields is analyzed through angular-dependent scaling of the cascade\nterms. Moreover, the increment calculation method significantly influences the\nrelative contributions of flux and non-flux terms, with absolute methods\ntending to overestimate the latter. These findings extend current studies of\ncompressible turbulence and offer critical insights into energy transfer\nmechanisms relevant to many astrophysical phenomena.",
        "Social institutions are systems of shared norms and rules that regulate\npeople's behaviors, often emerging without external enforcement. They provide\ncriteria to distinguish cooperation from defection and establish rules to\nsustain cooperation, shaped through long-term trial and error. While principles\nfor successful institutions have been proposed, the mechanisms underlying their\nemergence remain poorly understood. Here, we introduce the evolutionary\ndynamical-systems game, a framework that couples game actions with\nenvironmental dynamics and explores the evolution of cognitive frameworks for\ndecision-making. We analyze a minimal model of common-pool resource management,\nwhere resources grow naturally and are harvested. Players use decision-making\nfunctions to determine whether to harvest at each step, based on environmental\nand peer monitoring. As these functions evolve, players detect selfish\nharvesting and punish it by degrading the environment through harvesting. This\nprocess leads to the self-organization of norms that classify harvesting\nactions as cooperative, defective, or punitive. The emergent norms for\n``cooperativeness'' and rules of punishment serve as institutions. The\nenvironmental and players' states converge to distinct modes characterized by\nlimit-cycles, representing temporal regularities in socio-ecological systems.\nThese modes remain stable despite slight variations in decision-making,\nillustrating the stability of institutions. The evolutionary robustness of\ndecision-making functions serves as a measure of the evolutionary favorability\nof institutions, highlighting the role of plasticity in responding to diverse\nopponents. This work introduces foundational concepts in evolutionary\ndynamical-systems games and elucidates the mechanisms underlying the\nself-organization of institutions by modeling the interplay between ecological\ndynamics and human decision-making.",
        "M dwarfs are the most common stars in the galaxy, with long lifespans, a high\noccurrence rate of rocky planets, and close-in habitable zones. However, high\nstellar activity in the form of frequent flaring and any associated coronal\nmass ejections may drive atmospheric escape with the bombardment of radiation\nand high-energy particles, drastically impacting the habitability of these\nsystems. The stellar latitude where flares and coronal mass ejections occur\ndetermines the space weather that exoplanets are subject to, with high-energy\nparticle events associated with equatorial flares producing significant\natmospheric erosion. However, the flaring latitudes for M dwarfs remain largely\nunconstrained. To aid in the effort to locate these flaring regions we explore\nthe applicability of flare occultations using optical photometry to identify\nthe latitudes of flares. As a planet transits in front of an ongoing flare the\ntiming and geometry of the transit can be used to constrain the latitude and\nlongitude of the flare. We predict the probability of detecting an occultation\nfor known transiting planets and eclipsing binaries. From this, we estimate\n3-22 detectable occultations exist within the TESS primary mission photometry,\nwith the majority occurring in eclipsing binary observations. To demonstrate\nthis technique, we analyze a candidate flare occultation event for the\neclipsing binary CM Draconis.",
        "The nature of low-impurity ferromagnetism remains a challenging problem in\nthe solid-state community. Despite initial experiments dating back to the\nmid-20th century, a comprehensive theoretical explanation and reliable ab\ninitio evaluations have remained elusive. The present research aims to bridge\nthis gap by refining first-principle calculations by elucidating the magnetic\nand electronic behavior of Pd1-xMx alloys (where M = Mn, Fe, Co, Ni). Our study\nincludes calculations of magnetic properties throughout the range of impurity\nconcentrations, from 1 to 100 atomic percent (at.%), where we estimate critical\nconcentrations and perform a comparative analysis for the listed alloys.\nFurthermore, electronic structure was analyzed, including the calculations of\natomic, spin, and orbital-resolved states density, and exploration of the\nspatial formation of magnetic clusters containing ferromagnetic impurities\nacross all concentration ranges.",
        "Electrically driven spin resonances in double quantum dots can lift the spin\nblockade and give rise to a resonant current. This current can probe the\nproperties of coupled two-spin states for different quantum dot configurations.\nUsing a Floquet-Markov quantum transport model we compute the resonant current\nfor different driving amplitudes and ac field frequencies in spin-orbit coupled\nquantum dots. We show that the resonant current has a very rich interference\npattern which can give valuable insight into the singlet-triplet state mixing.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Pathology image analysis using segmentation deep learning algorithms",
    "start_abstract":"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Comparative gene expression profiles of intestinal transporters in mice, rats and humans"
      ],
      "abstract":[
        "We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Viscoelastic tensor and hydrodynamics of altermagnets",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Euclid Quick Data Release (Q1). Combined Euclid and Spitzer galaxy\n  density catalogues at $z>$ 1.3 and detection of significant Euclid passive\n  galaxy overdensities in Spitzer overdense regions",
        "Vacuum axisymmetric gravitational collapse revisited: preliminary\n  investigation",
        "Asymptotic integrability and its consequences",
        "Bidirectional controlled quantum state preparation in high-dimensional\n  quantum system",
        "Multiple orthogonal polynomial ensembles of derivative type",
        "Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm",
        "Scaffold-Assisted Window Junctions for Superconducting Qubit Fabrication",
        "A benchmark analysis of saliency-based explainable deep learning methods\n  for the morphological classification of radio galaxies",
        "Simulations of Magnetic Monopole Collisions",
        "First-principles study of dielectric properties of ferroelectric\n  perovskite oxides with on-site and inter-site Hubbard interactions",
        "When Less is More: Evolutionary Dynamics of Deception in a\n  Sender-Receiver Game"
      ],
      "abstract":[
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "We calculate the viscoelasticity tensor for altermagnets and formulate the\ncorresponding hydrodynamic equations. The anisotropy of altermagnetic Fermi\nsurfaces allows for additional terms in the viscoelasticity tensor and is\nmanifested in transport properties including electron and spin flows in a\nchannel and nonlocal responses. In the channel geometry, the altermagnetic spin\nsplitting leads to nontrivial spin density and spin current. Like the electric\ncurrent, the spin current acquires a Poiseuille profile for no-slip boundary\nconditions. In nonlocal responses, the altermagnetic anisotropy affects current\nstreamlines and electric potential distributions in the viscous regime. Our\nresults provide signatures of the hydrodynamic transport regime in altermagnets\npotentially facilitating its experimental studies and discovery.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Euclid will detect tens of thousands of clusters and protoclusters at\n$z$>1.3. With a total coverage of 63.1deg$^2$, the Euclid Quick Data Release 1\n(Q1) is large enough to detect tens of clusters and hundreds of protoclusters\nat these early epochs. The Q1 photometric redshift catalogue enables us to\ndetect clusters out to $z$ < 1.5; however, infrared imaging from Spitzer\nextends this limit to higher redshifts by using high local projected densities\nof Spitzer-selected galaxies as signposts for cluster and protocluster\ncandidates. We use Spitzer imaging of the Euclid Deep Fields (EDFs) to derive\ndensities for a sample of Spitzer-selected galaxies at redshifts $z$ > 1.3,\nbuilding Spitzer IRAC1 and IRAC2 photometric catalogues that are 95% complete\nat a magnitude limit of IRAC2=22.2, 22.6, and 22.8 for the EDF-S, EDF-F, and\nEDF-N, respectively. We apply two complementary methods to calculate galaxy\ndensities: (1) aperture and surface density; and (2) the Nth-nearest-neighbour\nmethod. When considering a sample selected at a magnitude limit of IRAC2 <\n22.2, at which all three EDFs are 95% complete, our surface density\ndistributions are consistent among the three EDFs and with the SpUDS blank\nfield survey. We also considered a deeper sample (IRAC2 < 22.8), finding that\n2% and 3% of the surface densities in the North and Fornax fields are 3$\\sigma$\nhigher than the average field distribution and similar to densities found in\nthe CARLA cluster survey. Our surface densities are also consistent with\npredictions from the GAEA semi-analytical model. Using combined Euclid and\nground-based i-band photometry we show that our highest Spitzer-selected galaxy\noverdense regions, found at $z$~1.5, also host high densities of passive\ngalaxies. This means that we measure densities consistent with those found in\nclusters and protoclusters at $z$>1.3.",
        "Validating the results of [A.M. Abrahams and C.R. Evans, Phys. Rev. Lett. 70,\n2980] poses a numerical challenge and has been inspiring a lot of research. We\njoin these efforts and present our first steps to achieve this goal: we discuss\na formulation of Einstein equations for a vacuum axisymmetric spacetime with\nvanishing twist in spherical-polar coordinates, its linearised approximation,\nand identify some problems in achieving numerically stable evolution at the\nthreshold of a black hole formation.",
        "We give a brief review of the concept of asymptotic integrability, which\nmeans that the Hamilton equations for the propagation of short-wavelength\npackets along a smooth, large-scale background wave have an integral\nindependent of the initial conditions. The existence of such an integral leads\nto a number of important consequences, which include, besides the direct\napplication to the packets propagation problems, Hamiltonian theory of narrow\nsolitons motion and generalized Bohr-Sommerfeld rule for parameters of solitons\nproduced from an intensive initial pulse. We show that in the case of systems\nwith two wave variables and exact fulfillment of the asymptotic integrability\ncondition, the `quantization' of mechanical systems, associated with the\nadditional integrals, yields the Lax pairs for a number of typical completely\nintegrable equations, and this sheds new light on the origin of the complete\nintegrability in nonlinear wave physics.",
        "High-dimensional quantum system exhibits unique advantages over the qubit\nsystem in some quantum information processing tasks. We present a program for\nimplementing deterministic bidirectional controlled remote quantum state\npreparation (BCRSP) in arbitrary $N$-dimensional (quNit) system. By introducing\ntwo generalized Greenberger-Horne-Zeilinger (GHZ) states as quantum channels,\ntwo communication parties can simultaneously prepare a single-particle\nhigh-dimensional state at each other's site under the control of Charlie.\nCompared with the previous counterparts, the significant advantage of our\nscheme is that the high-dimensional CNOT operations are not required. Moreover,\nthe performance our scheme are evaluated. The evaluation of the performance\nshows that if the quNit is encoded in the spatial mode of single photons, our\nscheme can be accomplished solely using only linear optical elements.",
        "We characterize the biorthogonal ensembles that are both a multiple\northogonal polynomial ensemble and a polynomial ensemble of derivative type\n(also called a P\\'olya ensemble). We focus on the two notions of derivative\ntype that typically appear in connection with the squared singular values of\nproducts of invertible random matrices and the eigenvalues of sums of Hermitian\nrandom matrices. Essential in the characterization is the use of the Mellin and\nLaplace transform: we show that the derivative type structure, which is a\npriori analytic in nature, becomes algebraic after applying the appropriate\ntransform. Afterwards, we explain how these notions of derivative type can be\nused to provide a partial solution to an open problem related to orthogonality\nof the finite finite free multiplicative and additive convolution from finite\nfree probability. In particular, we obtain families of multiple orthogonal\npolynomials that (de)compose naturally using these convolutions.",
        "Decision making often uses complex computer codes run at the exa-scale (10e18\nflops). Such computer codes or models are often run in a hierarchy of different\nlevels of fidelity ranging from the basic to the very sophisticated. The top\nlevels in this hierarchy are expensive to run, limiting the number of possible\nruns. To make use of runs over all levels, and crucially improve emulation at\nthe top level, we use multi-level Gaussian process emulators (GPs). We will\npresent a new method of building GP emulators from hierarchies of models. In\norder to share information across the different levels, l=1,...,L, we define\nthe form of the prior of the l+1th level to be the posterior of the lth level,\nhence building a Bayesian hierarchical structure for the top Lth level. This\nenables us to not only learn about the GP hyperparameters as we move up the\nmulti-level hierarchy, but also allows us to limit the total number of\nparameters in the full model, whilst maintaining accuracy.",
        "The superconducting qubit is one of the promising directions in realizing\nfault-tolerant quantum computing (FTQC), which requires many high-quality\nqubits. To achieve this, it is desirable to leverage modern semiconductor\nindustry technology to ensure quality, uniformity, and reproducibility.\nHowever, conventional Josephson junction fabrication relies mainly on\nresist-assistant double-angle evaporation, posing integration challenges. Here,\nwe demonstrate a lift-off-free qubit fabrication that integrates seamlessly\nwith existing industrial technologies. This method employs a silicon oxide\n(SiO$_2$) scaffold to define an etched window with a well-controlled size to\nform a Josephson junction. The SiO$_2$, which has a large dielectric loss, is\netched away in the final step using vapor HF leaving little residue. This\nWindow junction (WJ) process mitigates the degradation of qubit quality during\nfabrication and allows clean removal of the scaffold. The WJ process is\nvalidated by inspection and Josephson junction measurement. The scaffold\nremoval process is verified by measuring the quality factor of the resonators.\nFurthermore, compared to scaffolds fabricated by plasma-enhanced chemical vapor\ndeposition (PECVD), qubits made by WJ through physical vapor deposition (PVD)\nachieve relaxation time up to $57\\,\\mu\\text{s}$. Our results pave the way for a\nlift-off-free qubit fabrication process, designed to be compatible with modern\nfoundry tools and capable of minimizing damage to the substrate and material\nsurfaces.",
        "This work proposes a saliency-based attribution framework to evaluate and\ncompare 10 state-of-the-art explainability methods for deep learning models in\nastronomy, focusing on the classification of radio galaxy images. While\nprevious work has primarily emphasized classification accuracy, we prioritize\nmodel interpretability. Qualitative assessments reveal that Score-CAM,\nGrad-CAM, and Grad-CAM++ consistently produce meaningful attribution maps,\nhighlighting the brightest regions of FRI and FRII galaxies in alignment with\nknown astrophysical features. In contrast, other methods often emphasize\nirrelevant or noisy areas, reducing their effectiveness.",
        "In this paper, we investigate the scattering of BPS magnetic monopoles\nthrough numerical simulations. We present an ansatz for various multi-monopole\nconfigurations suitable for analyzing monopole scattering processes. Our study\nincludes planar scattering scenarios involving two, three, and four monopoles,\nas well as non-planar processes where three and four monopoles form\nintermediate tetrahedral and cubic states, respectively. Our observations align\nwith the theoretical predictions of the moduli space approximation.\nFurthermore, we extend our analysis to relativistic velocities and explore\nparameters beyond the BPS limit.",
        "We study the atomic and electronic structures of ferroelectric perovskite\noxides, BaTiO$_3$, LiNbO$_3$, and PbTiO$_3$ using ab initio extended Hubbard\nfunctionals in which the on-site and inter-site Hubbard interactions are\ndetermined self-consistently, adapted from the pseudohybrid density functional\nproposed by Agapito-Curtarolo-Buongiorno Nardelli. Band structures,\nferroelectric distortions, polarization, Born effective charges, and switching\nbarriers are calculated with extended Hubbard functionals, that are compared\nwith those using local density approximation (LDA), generalized gradient\napproximation (GGA), and Hybrid (HSE06) functionals. The properties of all\nthree compounds calculated by extended Hubbard functionals are in good\nagreement with experimental data. We find a substantial increase in band gaps\ndue to the inter-site Coulomb interactions, which show better agreement with\n$GW$ results compared to those from LDA and GGA functionals. The crucial role\nof the inter-site Coulomb interactions in restoring the suppressed polar\ninstability, which is computed when only the on-site Hubbard interactions are\nconsidered, is also highlighted. Overall, we find that the properties\ncalculated using our extended Hubbard functionals exhibit trends similar to\nthose obtained with the HSE06 functional, while reducing computational costs by\nover an order of magnitude. Thus, we propose that the current method is\nwell-suited for high-throughput calculations for perovskite oxides, offering\nsignificantly improved accuracy in computing band gap and other related\nphysical properties such as the shift current photovoltaic effect and band\nalignments in ferroelectric heterostructures.",
        "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Comparative gene expression profiles of intestinal transporters in mice, rats and humans",
    "start_abstract":"We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Pathology image analysis using segmentation deep learning algorithms"
      ],
      "abstract":[
        "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Backpropagation through Soft Body: Investigating Information Processing\n  in Brain-Body Coupling Systems",
        "A Weight Adaptation Trigger Mechanism in Decomposition-based\n  Evolutionary Multi-Objective Optimisation",
        "Abnormal Mutations: Evolution Strategies Don't Require Gaussianity",
        "Pareto Optimization with Robust Evaluation for Noisy Subset Selection",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Was Tournament Selection All We Ever Needed? A Critical Reflection on\n  Lexicase Selection",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Quantum Simplicial Neural Networks",
        "Sustainable AI: Mathematical Foundations of Spiking Neural Networks",
        "Aerial Reliable Collaborative Communications for Terrestrial Mobile\n  Users via Evolutionary Multi-Objective Deep Reinforcement Learning",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments",
        "Global Hall-magnetohydrodynamic simulations of transition disks",
        "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly",
        "MIGHTEE: exploring the relationship between spectral index, redshift and\n  radio luminosity",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Intrinsic charm and $D^+D^-$ asymmetry produced in proton-proton\n  collisions",
        "A note on partial polynomial functions, in memory of Marek Jarnicki",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Random Dynamical Systems on the circle without a finite orbit",
        "First photometric investigation of V517 Cam combined with ground-based\n  and TESS data",
        "Enhancing the charging performance of an atomic quantum battery",
        "BEARCUBS: A benchmark for computer-using web agents",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Practical Spoofing Attacks on Galileo Open Service Navigation Message\n  Authentication",
        "Intelligent Gradient Boosting Algorithms for Estimating Strength of\n  Modified Subgrade Soil"
      ],
      "abstract":[
        "Animals achieve sophisticated behavioral control through dynamic coupling of\nthe brain, body, and environment. Accordingly, the co-design approach, in which\nboth the controllers and the physical properties are optimized simultaneously,\nhas been suggested for generating refined agents without designing each\ncomponent separately. In this study, we aim to reveal how the function of the\ninformation processing is distributed between brains and bodies while applying\nthe co-design approach. Using a framework called ``backpropagation through soft\nbody,\" we developed agents to perform specified tasks and analyzed their\nmechanisms. The tasks included classification and corresponding behavioral\nassociation, nonlinear dynamical system emulation, and autonomous behavioral\ngeneration. In each case, our analyses revealed reciprocal relationships\nbetween the brains and bodies. In addition, we show that optimized brain\nfunctionalities can be embedded into bodies using physical reservoir computing\ntechniques. Our results pave the way for efficient designs of brain--body\ncoupling systems.",
        "Decomposition-based multi-objective evolutionary algorithms (MOEAs) are\nwidely used for solving multi-objective optimisation problems. However, their\neffectiveness depends on the consistency between the problems Pareto front\nshape and the weight distribution. Decomposition-based MOEAs, with uniformly\ndistributed weights (in a simplex), perform well on problems with a regular\n(simplex-like) Pareto front, but not on those with an irregular Pareto front.\nPrevious studies have focused on adapting the weights to approximate the\nirregular Pareto front during the evolutionary process. However, these\nadaptations can actually harm the performance on the regular Pareto front via\nchanging the weights during the search process that are eventually the best fit\nfor the Pareto front. In this paper, we propose an algorithm called the weight\nadaptation trigger mechanism for decomposition-based MOEAs (ATM-MOEA\/D) to\ntackle this issue. ATM-MOEA\/D uses an archive to gradually approximate the\nshape of the Pareto front during the search. When the algorithm detects\nevolution stagnation (meaning the population no longer improves significantly),\nit compares the distribution of the population with that of the archive to\ndistinguish between regular and irregular Pareto fronts. Only when an irregular\nPareto front is identified, the weights are adapted. Our experimental results\nshow that the proposed algorithm not only performs generally better than seven\nstate-of-the-art weight-adapting methods on irregular Pareto fronts but also is\nable to achieve the same results as fixed-weight methods like MOEA\/D on regular\nPareto fronts.",
        "The mutation process in evolution strategies has been interlinked with the\nnormal distribution since its inception. Many lines of reasoning have been\ngiven for this strong dependency, ranging from maximum entropy arguments to the\nneed for isotropy. However, some theoretical results suggest that other\ndistributions might lead to similar local convergence properties. This paper\nempirically shows that a wide range of evolutionary strategies, from the\n(1+1)-ES to CMA-ES, show comparable optimization performance when using a\nmutation distribution other than the standard Gaussian. Replacing it with,\ne.g., uniformly distributed mutations, does not deteriorate the performance of\nES, when using the default adaptation mechanism for the strategy parameters. We\nobserve that these results hold not only for the sphere model but also for a\nwider range of benchmark problems.",
        "Subset selection is a fundamental problem in combinatorial optimization,\nwhich has a wide range of applications such as influence maximization and\nsparse regression. The goal is to select a subset of limited size from a ground\nset in order to maximize a given objective function. However, the evaluation of\nthe objective function in real-world scenarios is often noisy. Previous\nalgorithms, including the greedy algorithm and multi-objective evolutionary\nalgorithms POSS and PONSS, either struggle in noisy environments or consume\nexcessive computational resources. In this paper, we focus on the noisy subset\nselection problem with a cardinality constraint, where the evaluation of a\nsubset is noisy. We propose a novel approach based on Pareto Optimization with\nRobust Evaluation for noisy subset selection (PORE), which maximizes a robust\nevaluation function and minimizes the subset size simultaneously. PORE can\nefficiently identify well-structured solutions and handle computational\nresources, addressing the limitations observed in PONSS. Our experiments,\nconducted on real-world datasets for influence maximization and sparse\nregression, demonstrate that PORE significantly outperforms previous methods,\nincluding the classical greedy algorithm, POSS, and PONSS. Further validation\nthrough ablation studies confirms the effectiveness of our robust evaluation\nfunction.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "The success of lexicase selection has led to various extensions, including\nits combination with down-sampling, which further increased performance.\nHowever, recent work found that down-sampling also leads to significant\nimprovements in the performance of tournament selection. This raises the\nquestion of whether tournament selection combined with down-sampling is the\nbetter choice, given its faster running times. To address this question, we run\na set of experiments comparing epsilon-lexicase and tournament selection with\ndifferent down-sampling techniques on synthetic problems of varying noise\nlevels and problem sizes as well as real-world symbolic regression problems.\nOverall, we find that down-sampling improves generalization and performance\neven when compared over the same number of generations. This means that\ndown-sampling is beneficial even with way fewer fitness evaluations.\nAdditionally, down-sampling successfully reduces code growth. We observe that\npopulation diversity increases for tournament selection when combined with\ndown-sampling. Further, we find that tournament selection and epsilon-lexicase\nselection with down-sampling perform similar, while tournament selection is\nsignificantly faster. We conclude that tournament selection should be further\nanalyzed and improved in future work instead of only focusing on the\nimprovement of lexicase variants.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "Deep learning's success comes with growing energy demands, raising concerns\nabout the long-term sustainability of the field. Spiking neural networks,\ninspired by biological neurons, offer a promising alternative with potential\ncomputational and energy-efficiency gains. This article examines the\ncomputational properties of spiking networks through the lens of learning\ntheory, focusing on expressivity, training, and generalization, as well as\nenergy-efficient implementations while comparing them to artificial neural\nnetworks. By categorizing spiking models based on time representation and\ninformation encoding, we highlight their strengths, challenges, and potential\nas an alternative computational paradigm.",
        "Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base\nstations (BSs) to improve terrestrial communications. However, the limited\nonboard energy and antenna power of a UAV restrict its communication range and\ntransmission capability. To address these limitations, this work employs\ncollaborative beamforming through a UAV-enabled virtual antenna array to\nimprove transmission performance from the UAV to terrestrial mobile users,\nunder interference from non-associated BSs and dynamic channel conditions.\nSpecifically, we introduce a memory-based random walk model to more accurately\ndepict the mobility patterns of terrestrial mobile users. Following this, we\nformulate a multi-objective optimization problem (MOP) focused on maximizing\nthe transmission rate while minimizing the flight energy consumption of the UAV\nswarm. Given the NP-hard nature of the formulated MOP and the highly dynamic\nenvironment, we transform this problem into a multi-objective Markov decision\nprocess and propose an improved evolutionary multi-objective reinforcement\nlearning algorithm. Specifically, this algorithm introduces an evolutionary\nlearning approach to obtain the approximate Pareto set for the formulated MOP.\nMoreover, the algorithm incorporates a long short-term memory network and\nhyper-sphere-based task selection method to discern the movement patterns of\nterrestrial mobile users and improve the diversity of the obtained Pareto set.\nSimulation results demonstrate that the proposed method effectively generates a\ndiverse range of non-dominated policies and outperforms existing methods.\nAdditional simulations demonstrate the scalability and robustness of the\nproposed CB-based method under different system parameters and various\nunexpected circumstances.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.",
        "Context. Transition disks (TDs) are a type of protoplanetary disk\ncharacterized by a central dust and gas cavity. The processes behind how these\ncavities are formed and maintained, along with their observed high accretion\nrates of $10^{-8} -10^{-7} \\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, continue to be\nsubjects of active research. Aims. This work aims to investigate how the\ninclusion of the Hall effect (HE) alongside Ohmic resistivity (OR) and\nambipolar diffusion (AD) affects the structure of the TD. Of key interest is\nthe dynamical evolution of the cavity and whether it can indeed produce\ntransonic accretion, as predicted by theoretical models in order to account for\nthe observed high accretion rates despite the inner disk's low density.\nMethods. We present our results of 2D axisymmetric global radiation\nmagnetohydrodynamic (MHD) simulations of TDs for which all three non-ideal MHD\neffects are accounted. We used the NIRVANA-III fluid code and initialized our\nmodel with a disk cavity reaching up to $R=8~\\mathrm{au}$ with a density\ncontrast of $10^5$. We performed three runs, one with only OR and AD, and one\nfor each of the two configurations that arise when additionally including the\nHE, that is, with the field aligned (anti-aligned) with respect to the rotation\naxis. Results. For all three runs, our models maintain an intact inner cavity\nand an outer standard disk. MHD winds are launched both from the cavity and\nfrom the disk. Notably, when the HE is included, ring-like structures develop\nwithin the cavity. We moreover obtain accretion rates of $3 - 8 \\times 10^{-8}\n\\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, comparable to typical values seen in full\ndisks. Importantly, we clearly observe transonic accretion ($v_{\\mathrm{acc}}\n\\gtrsim c_{s}$) in the cavity. Additionally, outward magnetic flux transport\noccurs in all three runs.",
        "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.",
        "It has been known for many years that there is an apparent trend for the\nspectral index ({\\alpha}) of radio sources to steepen with redshift z, which\nhas led to attempts to select high-redshift objects by searching for radio\nsources with steep spectra. In this study we use data from the MeerKAT, LOFAR,\nGMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE\nsurveys, to select compact sources over a wide range of redshifts and\nluminosities. We investigate the relationship between spectral index,\nluminosity and redshift and compare our results to those of previous studies.\nAlthough there is a correlation between {\\alpha} and z in our sample for some\ncombinations of frequency where good data are available, there is a clear\noffset between the {\\alpha}-z relations in our sample and those derived\npreviously from samples of more luminous objects; in other words, the\n{\\alpha}-z relation is different for low and high luminosity sources. The\nrelationships between {\\alpha} and luminosity are also weak in our sample but\nin general the most luminous sources are steeper-spectrum and this trend is\nextended by samples from previous studies. In detail, we argue that both a\n{\\alpha}-luminosity relation and an {\\alpha}-z relation can be found in the\ndata, but it is the former that drives the apparent {\\alpha}-z relation\nobserved in earlier work, which only appears because of the strong\nredshift-luminosity relation in bright, flux density-limited samples.\nSteep-spectrum selection should be applied with caution in searching for high-z\nsources in future deep surveys.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "We investigate the contribution of the charm-anticharrm ($c{\\bar c}$)\nasymmetry of the proton eigenstate obtained from QCD lattice gauge to the\nasymmetry of $D^+D^-$ and $D^0{\\bar D}^0$ mesons produced in $pp$ collisions at\nlarge Feynman variables $x$. It is shown that an important tool for the\nestablishing the intrinsic charm (IC) content of the proton is the charm\nhadron-antihadron asymmetry formed in $pp$ collisions. Predictions for the\nasymmetry as function of $x$ for different IC probabilities are presented. We\nshow that the interference of the intrinsic $|uud c{\\bar c}>$ Fock state with\nthe standard contribution from the PQCD evolution leads to a large $D^+D^-$\nasymmetry at large Feynman $x$.",
        "We present an extension theorem for a separately holomorphic function which\nis polynomial\/rational in some variables.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "In this paper, we study Random Dynamical Systems (RDSs) of homeomorphisms on\nthe circle without a finite orbit. We characterize the topological dynamics of\nthe associated semigroup by identifying the existence of invariant sets which\nare finite unions of intervals. We describe the accumulation points of the\naverage orbit of the transfer operator. For each ergodic stationary measure, we\ndemonstrate interesting properties of its weight function on the circle.\nRelationships between the minimal sets of an RDS and its inverse RDS are also\nestablished.",
        "The observations of eclipsing binary systems are of great importance in\nastrophysics, as they allow direct measurements of fundamental stellar\nparameters. By analysing high-quality space-based observations with\nground-based photometric data, it becomes possible to detect these fundamental\nparameters with greater precision using multicolour photometry. Here, we report\nthe first photometric analysis results of the V517 Cam eclipsing binary system\nby combining the Transiting Exoplanet Survey Satellite (TESS) light curve and\nnew CCD observations in BVRI filters, obtained with a 60 cm robotic telescope\n(T60) at the T\\\"UB\\.ITAK National Observatory. By means of photometric\nanalyses, the masses and radii of the primary and secondary stars were\ncarefully determined to be $M_{1}= 1.47\\pm 0.06\\,M_\\odot$, $M_{2}=\n0.79\\pm0.05\\,M_\\odot$, and $R_{1}=1.43\\pm 0.03\\,R_\\odot$, $R_{2}= 0.75\\pm\n0.04\\,R_\\odot$, respectively. Furthermore, the distance to V517 Cam was\ncalculated to be $284\\pm20$ pc. The overall age of the system is estimated to\nbe around $63\\pm15$ Myr. At this age, the primary component stands near the\nonset of its main-sequence evolution, near the ZAMS, whereas the secondary\ncomponent remains in the pre-main-sequence evolutionary phase. To better\nunderstand the evolutionary status and nature of V517 Cam, the mass ratio and\ntemperature values, obtained with relatively low sensitivity by photometric\nmeasurements, need to be confirmed by spectral analysis.",
        "We study a quantum battery (QB) model composed of two atoms, where the\ncharger and battery elements are coupled to a multimode vacuum field that\nserves as a mediator for energy transfer. Different figures of merit such as\nergotropy, charging time, and charging efficiency are analyzed, putting\nemphasis on the role of various control parameters on the charging performance.\nIt is found that there is a range of angle between the transition dipole\nmoments and interatomic axis in which the QB can be charged. The optimal\ncharging performance is achieved if the atomic dipole moments are perpendicular\nor parallel to the interatomic axis. The charging performance also improves\nwith the decrease of the interatomic distance. Besides, the charged ergotropy\ncan be enhanced by increasing the initial ergotropy of the charger and it is\nbeneficial to charge the QB starting from a passive state.",
        "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing search inefficiencies and domain knowledge gaps as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n24.3% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and\/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations.",
        "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties."
      ]
    }
  }
]