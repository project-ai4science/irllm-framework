[
  {
    "id":"2411.01019",
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms",
    "start_abstract":"Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b1"
      ],
      "title":[
        "Screening for lung cancer: 2023 guideline update from the American Cancer Society",
        "Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects"
      ],
      "abstract":[
        "Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
        "Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes."
      ],
      "categories":[
        "q-bio.CB",
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Multicellular self-organization in Escherichia coli",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Neuroblastoma: nutritional strategies as supportive care in pediatric\n  oncology",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Self-supervised Conformal Prediction for Uncertainty Quantification in\n  Imaging Problems",
        "Gradient-enhanced PINN with residual unit for studying forward-inverse\n  problems of variable coefficient equations",
        "Witnessing Magic with Bell inequalities",
        "Boundary conditions of general black hole perturbations",
        "The superadiabatic projectors method applied to the spectral theory of\n  magnetic operators",
        "Bosonic Amplitude-Damping Codes Beyond Binomial Schemes",
        "Differentiating the acceleration mechanisms in the slow and Alfv\\'enic\n  slow solar wind",
        "CIBER 4th flight fluctuation analysis: Measurements of near-IR auto- and\n  cross-power spectra on arcminute to sub-degree scales",
        "Towords the Near Optimal Sampling Complexity via Generalized Exponential\n  Spectral Pursuit",
        "Counting spinal phylogenetic networks",
        "On the characteristic structure of the adjoint Euler equations with\n  application to supersonic flows",
        "Approximation of the generalized principal eigenvalue of cooperative\n  nonlocal dispersal systems and applications",
        "Multimode fiber based high-dimensional light analyzer",
        "The White Dwarf Pareto: Tracing Mass Loss in Binary Systems",
        "Securing Integrated Sensing and Communication Against a Mobile\n  Adversary: A Stackelberg Game with Deep Reinforcement Learning"
      ],
      "abstract":[
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Most image restoration problems are ill-conditioned or ill-posed and hence\ninvolve significant uncertainty. Quantifying this uncertainty is crucial for\nreliably interpreting experimental results, particularly when reconstructed\nimages inform critical decisions and science. However, most existing image\nrestoration methods either fail to quantify uncertainty or provide estimates\nthat are highly inaccurate. Conformal prediction has recently emerged as a\nflexible framework to equip any estimator with uncertainty quantification\ncapabilities that, by construction, have nearly exact marginal coverage. To\nachieve this, conformal prediction relies on abundant ground truth data for\ncalibration. However, in image restoration problems, reliable ground truth data\nis often expensive or not possible to acquire. Also, reliance on ground truth\ndata can introduce large biases in situations of distribution shift between\ncalibration and deployment. This paper seeks to develop a more robust approach\nto conformal prediction for image restoration problems by proposing a\nself-supervised conformal prediction method that leverages Stein's Unbiased\nRisk Estimator (SURE) to self-calibrate itself directly from the observed noisy\nmeasurements, bypassing the need for ground truth. The method is suitable for\nany linear imaging inverse problem that is ill-conditioned, and it is\nespecially powerful when used with modern self-supervised image restoration\ntechniques that can also be trained directly from measurement data. The\nproposed approach is demonstrated through numerical experiments on image\ndenoising and deblurring, where it delivers results that are remarkably\naccurate and comparable to those obtained by supervised conformal prediction\nwith ground truth data.",
        "Physics-informed neural network (PINN) is a powerful emerging method for\nstudying forward-inverse problems of partial differential equations (PDEs),\neven from limited sample data. Variable coefficient PDEs, which model\nreal-world phenomena, are of considerable physical significance and research\nvalue. This study proposes a gradient-enhanced PINN with residual unit\n(R-gPINN) method to solve the data-driven solution and function discovery for\nvariable coefficient PDEs. On the one hand, the proposed method incorporates\nresidual units into the neural networks to mitigate gradient vanishing and\nnetwork degradation, unify linear and nonlinear coefficient problem. We present\ntwo types of residual unit structures in this work to offer more flexible\nsolutions in problem-solving. On the other hand, by including gradient terms of\nvariable coefficients, the method penalizes collocation points that fail to\nsatisfy physical properties. This enhancement improves the network's adherence\nto physical constraints and aligns the prediction function more closely with\nthe objective function. Numerical experiments including solve the\nforward-inverse problems of variable coefficient Burgers equation, variable\ncoefficient KdV equation, variable coefficient Sine-Gordon equation, and\nhigh-dimensional variable coefficient Kadomtsev-Petviashvili equation. The\nresults show that using R-gPINN method can greatly improve the accuracy of\npredict solution and predict variable coefficient in solving variable\ncoefficient equations.",
        "Non-stabilizerness, or magic, is a fundamental resource for quantum\ncomputation, enabling quantum algorithms to surpass classical capabilities.\nDespite its importance, characterizing magic remains challenging due to the\nintricate geometry of stabilizer polytopes and the difficulty of simulating\nnon-stabilizer states. In this work, we reveal an unexpected connection between\nmagic and Bell inequalities. Although maximally entangled stabilizer states can\nviolate Bell inequalities and magic is deeply tied to the algebraic structure\nof observables, we show that tailored Bell inequalities can act as witnesses of\nmagic. This result bridges two key quantum resources, uncovering a novel\nrelationship between the device-independent framework and resource-theoretic\nproperties of quantum computation.",
        "Recently, significant progress has been made in the study of black hole (BH)\nperturbations, both within the framework of general modified gravity theories\nand in complex environments. However, a well-established conclusion regarding\nthe boundary conditions of the perturbed fields remains elusive. In this paper,\nwe investigate the boundary conditions for a general perturbation at spatial\ninfinity and the event horizon of a black hole (BH) described by a generic\nmetric that is stationary, axisymmetric, asymptotically flat, and respects the\ncondition of circularity. Our analysis is independent of any specific BH model\nor the nature of the perturbed field. In particular, by extending the\nformulation introduced by Teukolsky and utilizing purely geometric principles,\nwe derive a universal expression for the boundary condition at the horizon.\nThis expression is elegantly formulated in terms of the physical quantities at\nthe event horizon, specifically the BH's surface gravity and angular velocity.\nThe results presented in this work may provide valuable insights for the\ncalculation of quasinormal modes and the gravitational waves generated by\nextreme-mass-ratio inspirals, extending beyond the standard Kerr case.",
        "This article deals with a generalization of the superadiabatic projectors\nmethod. In a general framework, the well-known superadiabatic projectors are\nconstructed and accurately described in the case of rank one, when a remarkable\nfactorization occurs. We apply these ideas to spectral theory and we explain\nhow our abstract results allow to recover or improve recent results about the\nsemiclassical magnetic Laplacian.",
        "We introduce two new families of bosonic quantum error correction (QEC) codes\nto address collective coherent and amplitude-damping errors, building upon our\nprevious multi-qubit QEC codes. These new bosonic codes enhance existing\nbinomial codes for oscillators and permutation-invariant codes for qubits by\nreducing the required excitations per input qubit from linear to sub-linear\ngrowth. The mappings from multi-qubit stabilizer codes to bosonic codes\nestablish a bridge between QEC code construction for qubits and oscillators,\noffering a unified approach to error correction across different quantum\nsystems.",
        "In the corona, plasma is accelerated to hundreds of kilometers per second,\nand heated to temperatures hundreds of times hotter than the Sun's surface,\nbefore it escapes to form the solar wind. Decades of space-based experiments\nhave shown that the energization process does not stop after it escapes.\nInstead, the solar wind continues to accelerate and it cools far more slowly\nthan a freely-expanding adiabatic gas. Recent work suggests that fast solar\nwind requires additional momentum beyond what can be provided by the observed\nthermal pressure gradients alone whereas it is sufficient for the slowest wind.\nThe additional acceleration for fast wind can be provided through an Alfv\\'en\nwave pressure gradient. Beyond this fast-slow categorization, however, a subset\nof slow solar wind exhibits high Alfv\\'enicity that suggest Alfv\\'en waves\ncould play a larger role in its acceleration compared to conventional slow wind\noutflows. Through a well-timed conjunction between Solar Orbiter and Parker\nSolar Probe, we trace the energetics of slow wind to compare with a neighboring\nAlfv\\'enic slow solar wind stream. An analysis that integrates remote and\nheliospheric properties and modeling of the two distinct solar wind streams\nfinds Alfv\\'enic slow solar wind behaves like fast wind, where a wave pressure\ngradient is required to reconcile its full acceleration, while non-Alfv\\'enic\nslow wind can be driven by its non-adiabatic electron and proton thermal\npressure gradients. Derived coronal conditions of the source region indicate\ngood model compatibility but extended coronal observations are required to\neffectively trace solar wind energetics below Parker's orbit.",
        "We present new anisotropy measurements in the near-infrared (NIR) for angular\nmultipoles $300<\\ell<10^5$ using imaging data at 1.1 $\\mu$m and 1.8 $\\mu$m from\nthe fourth flight of the Cosmic Infrared Background ExpeRiment (CIBER). Using\nimproved analysis methods and higher quality fourth flight data, we detect\nsurface brightness fluctuations on scales $\\ell<2000$ with CIBER auto-power\nspectra at $\\sim14\\sigma$ and 18$\\sigma$ for 1.1 and 1.8 $\\mu$m, respectively,\nand at $\\sim10\\sigma$ in cross-power spectra. The CIBER measurements pass\ninternal consistency tests and represent a $5-10\\times$ improvement in power\nspectrum sensitivity on several-arcminute scales relative to that of existing\nstudies. Through cross-correlations with tracers of diffuse galactic light\n(DGL), we determine that scattered DGL contributes $<10\\%$ to the observed\nfluctuation power at high confidence. On scales $\\theta > 5'$, the CIBER auto-\nand cross-power spectra exceed predictions for integrated galactic light (IGL)\nand integrated stellar light (ISL) by over an order of magnitude, and are\ninconsistent with our baseline IGL+ISL+DGL model at high significance. We\ncross-correlate two of the CIBER fields with 3.6 $\\mu$m and 4.5 $\\mu$m mosaics\nfrom the Spitzer Deep Wide-Field Survey and find similar evidence for\ndepartures from Poisson noise in Spitzer-internal power spectra and CIBER\n$\\times$ Spitzer cross-power spectra. A multi-wavelength analysis indicates\nthat the auto-power of the fluctuations at low-$\\ell$ is bluer than the Poisson\nnoise from IGL and ISL; however, for $1' <\\theta < 10'$, the cross-correlation\ncoefficient $r_{\\ell}$ of nearly all band combinations decreases with\nincreasing $\\theta$, disfavoring astrophysical explanations that invoke a\nsingle correlated sky component.",
        "Sparse phase retrieval aims to recover a $k$-sparse signal from $m$ phaseless\nobservations, raising the fundamental question of the minimum number of samples\nrequired for accurate recovery. As a classical non-convex optimization problem,\nsparse phase retrieval algorithms are typically composed of two stages:\ninitialization and refinement. Existing studies reveal that the sampling\ncomplexity under Gaussian measurements is largely determined by the\ninitialization stage.\n  In this paper, we identify commonalities among widely used initialization\nalgorithms and introduce key extensions to improve their performance. Building\non this analysis, we propose a novel algorithm, termed Generalized Exponential\nSpectral Pursuit (GESP). Theoretically, our results not only align with\nexisting conclusions but also demonstrate enhanced generalizability. In\nparticular, our theoretical findings coincide with prior results under certain\nconditions while surpassing them in others by providing more comprehensive\nguarantees. Furthermore, extensive simulation experiments validate the\npractical effectiveness and robustness of GESP, showcasing its superiority in\nrecovering sparse signals with reduced sampling requirements.",
        "Phylogenetic networks are an important way to represent evolutionary\nhistories that involve reticulations such as hybridization or horizontal gene\ntransfer, yet fundamental questions such as how many networks there are that\nsatisfy certain properties are very difficult. A new way to encode a large\nclass of networks, using expanding covers, may provide a way to approach such\nproblems. Expanding covers encode a large class of phylogenetic networks,\ncalled labellable networks. This class does not include all networks, but does\ninclude many familiar classes, including orchard, normal, tree-child and\ntree-sibling networks. As expanding covers are a combinatorial structure, it is\npossible that they can be used as a tool for counting such classes for a fixed\nnumber of leaves and reticulations, for which, in many cases, a closed formula\nhas not yet been found. More recently, a new class of networks was introduced,\ncalled spinal networks, which are analogous to caterpillar trees for\nphylogenetic trees and can be fully described using covers. In the present\narticle, we describe a method for counting networks that are both spinal and\nbelong to some more familiar class, with the hope that these form a base case\nfrom which to attack the more general classes.",
        "We review the characteristic structure of the two-dimensional adjoint Euler\nequations. We derive the compatibility and jump conditions along\ncharacteristics and show that the characteristic information can be used to\nobtain exact predictions for the adjoint variables in certain supersonic flows.",
        "It is well known that, in the study of the dynamical properties of nonlinear\nevolution system with nonlocal dispersals, the principal eigenvalue of\nlinearized system play an important role. However, due to lack of compactness,\nin order to obtain the existence of principal eigenvalue, certain additional\nconditions must be attached to the coefficients. In this paper, we approximate\nthe generalized principal eigenvalue of nonlocal dispersal cooperative and\nirreducible system, which admits the Collatz-Wielandt characterization, by\nconstructing the monotonic upper and lower control systems with principal\neigenvalues; and show that the generalized principal eigenvalue plays the same\nrole as the usual principal eigenvalue.",
        "The wavelength and state of polarization (SOP) are fundamental properties of\nan optical field which are essential for applications in optical\ncommunications, imaging and other fields. However, it is challenging for\nexisting spectrometers and polarimeters to measure these parameters\nsimultaneously, resulting in reduced spatial and temporal efficiency. To\novercome this limitation, we propose and demonstrate a compact multimode fiber\n(MMF)-based high-dimensional light analyzer capable of simultaneously\nperforming high-precision measurements of both wavelength and SOP. Core-offset\nlaunching is introduced in the MMF to reshuffle the mode coupling. A neural\nnetwork named WP-Net has been designed dedicated to wavelength and SOP\nsynchronization measurements. Physics-informed loss function based on optical\nprior knowledge is used to optimize the learning process. These advancements\nhave enhanced the sensitivity, achieving a wavelength resolution of 0.045 pm\nand an SOP resolution of 0.0088.",
        "The white dwarf mass distribution has been studied primarily at two extremes:\nobjects that presumably evolved as single stars and members of close binaries\nthat likely underwent substantial interaction. This work considers the\nintermediate separation regime of ~1 au and demonstrates how binary interaction\naffects white dwarf masses. The binary mass ratio distribution is utilized for\nthis purpose. Modeled as a truncated Pareto profile, this distribution provides\ninsights into the populations' properties and evolutionary history. When\napplied to homogeneous samples of binaries with giant primaries of similar age,\nthe distribution's shape constrains the fraction of white dwarf companions, the\nwhite dwarf mass distribution, and the properties of their progenitors. As a\ntest case, this method is applied to a small spectroscopic sample of binaries\nin open clusters with red giant primaries and orbital periods between 0.5 and\n20 years. The analysis reveals that white dwarfs in these systems are ~20% less\nmassive than their isolated counterparts, with a typical mass of ~0.55 Msun.\nTheir progenitors likely lost 80-85% of their mass, with binary interactions\nenhancing mass loss by an additional ~0.2 Msun. These findings highlight the\nutility of this approach for studying binary evolution and improving population\nmodels, particularly with future datasets from Gaia and other large-scale\nsurveys.",
        "In this paper, we study a secure integrated sensing and communication (ISAC)\nsystem employing a full-duplex base station with sensing capabilities against a\nmobile proactive adversarial target$\\unicode{x2014}$a malicious unmanned aerial\nvehicle (M-UAV). We develop a game-theoretic model to enhance communication\nsecurity, radar sensing accuracy, and power efficiency. The interaction between\nthe legitimate network and the mobile adversary is formulated as a\nnon-cooperative Stackelberg game (NSG), where the M-UAV acts as the leader and\nstrategically adjusts its trajectory to improve its eavesdropping ability while\nconserving power and avoiding obstacles. In response, the legitimate network,\nacting as the follower, dynamically allocates resources to minimize network\npower usage while ensuring required secrecy rates and sensing performance. To\naddress this challenging problem, we propose a low-complexity successive convex\napproximation (SCA) method for network resource optimization combined with a\ndeep reinforcement learning (DRL) algorithm for adaptive M-UAV trajectory\nplanning through sequential interactions and learning. Simulation results\ndemonstrate the efficacy of the proposed method in addressing security\nchallenges of dynamic ISAC systems in 6G, i.e., achieving a Stackelberg\nequilibrium with robust performance while mitigating the adversary's ability to\nintercept network signals."
      ]
    }
  },
  {
    "id":"2411.01019",
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Screening for lung cancer: 2023 guideline update from the American Cancer Society",
    "start_abstract":"Abstract Lung cancer is the leading cause of mortality and person\u2010years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network\u2010validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit\u2010to\u2010radiation\u2010risk ratios follow\u2010up examinations. GDG also examined disease burden data National Institute\u2019s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50\u201380 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low\u2010dose computed tomography asymptomatic currently smoke or formerly smoked have \u226520 pack\u2010year ( , ). Before decision made initiate LCS, should engage shared decision\u2010making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":"2411.01019",
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Incidental Anterior Mediastinal Nodular Lesions on\u00a0Chest CT in Asymptomatic Subjects",
    "start_abstract":"Objective: The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods: We included 56,358 consecutive participants (mean age 52.4 \u00b1 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results: Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66-0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01-3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions: The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.",
    "start_categories":[
      "q-bio.CB",
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms"
      ],
      "abstract":[
        "Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "SycEval: Evaluating LLM Sycophancy",
        "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D\n  5th Edition Combat",
        "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic\n  Reasoning",
        "Counting and Reasoning with Plans",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Exploring the Implementation of AI in Early Onset Interviews to Help\n  Mitigate Bias",
        "Multi-Agent Verification: Scaling Test-Time Compute with Multiple\n  Verifiers",
        "On Benchmarking Human-Like Intelligence in Machines",
        "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models",
        "TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via\n  Cross-Architecture Distillation",
        "Multimodal Dreaming: A Global Workspace Approach to World Model-Based\n  Reinforcement Learning",
        "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time\n  Series Forecasting",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "EDM: Efficient Deep Feature Matching",
        "Observation of Subnatural-Linewidth Biphotons In a Two-Level Atomic\n  Ensemble",
        "Fixed-Confidence Best Arm Identification with Decreasing Variance",
        "AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary\n  Task-Oriented Grasping in Clutter",
        "Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
        "EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic\n  Regression on Heterogeneous Database",
        "Global Portraits of Inflation in Nonsingular Variables",
        "Essentially Commuting with a Unitary",
        "Topological modes, non-locality and the double copy",
        "Expansions and restrictions of structures and theories, their\n  hierarchies"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications.",
        "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "A recent approach to neurosymbolic reasoning is to explicitly combine the\nstrengths of large language models (LLMs) and symbolic solvers to tackle\ncomplex reasoning tasks. However, current approaches face significant\nlimitations, including poor generalizability due to task-specific prompts,\ninefficiencies caused by the lack of separation between knowledge and queries,\nand restricted inferential capabilities. These shortcomings hinder their\nscalability and applicability across diverse domains. In this paper, we\nintroduce VERUS-LM, a novel framework designed to address these challenges.\nVERUS-LM employs a generic prompting mechanism, clearly separates domain\nknowledge from queries, and supports a wide range of different logical\nreasoning tasks. This framework enhances adaptability, reduces computational\ncost, and allows for richer forms of reasoning, such as optimization and\nconstraint satisfaction. We show that our approach succeeds in diverse\nreasoning on a novel dataset, markedly outperforming LLMs. Additionally, our\nsystem achieves competitive results on common reasoning benchmarks when\ncompared to other state-of-the-art approaches, and significantly surpasses them\non the difficult AR-LSAT dataset. By pushing the boundaries of hybrid\nreasoning, VERUS-LM represents a significant step towards more versatile\nneurosymbolic AI systems",
        "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "This paper investigates the application of artificial intelligence (AI) in\nearly-stage recruitment interviews in order to reduce inherent bias,\nspecifically sentiment bias. Traditional interviewers are often subject to\nseveral biases, including interviewer bias, social desirability effects, and\neven confirmation bias. In turn, this leads to non-inclusive hiring practices,\nand a less diverse workforce. This study further analyzes various AI\ninterventions that are present in the marketplace today such as multimodal\nplatforms and interactive candidate assessment tools in order to gauge the\ncurrent market usage of AI in early-stage recruitment. However, this paper aims\nto use a unique AI system that was developed to transcribe and analyze\ninterview dynamics, which emphasize skill and knowledge over emotional\nsentiments. Results indicate that AI effectively minimizes sentiment-driven\nbiases by 41.2%, suggesting its revolutionizing power in companies' recruitment\nprocesses for improved equity and efficiency.",
        "By utilizing more computational resources at test-time, large language models\n(LLMs) can improve without additional training. One common strategy uses\nverifiers to evaluate candidate outputs. In this work, we propose a novel\nscaling dimension for test-time compute: scaling the number of verifiers. We\nintroduce Multi-Agent Verification (MAV) as a test-time compute paradigm that\ncombines multiple verifiers to improve performance. We propose using Aspect\nVerifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of\noutputs, as one possible choice for the verifiers in a MAV system. AVs are a\nconvenient building block for MAV since they can be easily combined without\nadditional training. Moreover, we introduce BoN-MAV, a simple multi-agent\nverification algorithm that combines best-of-n sampling with multiple\nverifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency\nand reward model verification, and we demonstrate both weak-to-strong\ngeneralization, where combining weak verifiers improves even stronger LLMs, and\nself-improvement, where the same base model is used to both generate and verify\noutputs. Our results establish scaling the number of verifiers as a promising\nnew dimension for improving language model performance at test-time.",
        "Recent benchmark studies have claimed that AI has approached or even\nsurpassed human-level performances on various cognitive tasks. However, this\nposition paper argues that current AI evaluation paradigms are insufficient for\nassessing human-like cognitive capabilities. We identify a set of key\nshortcomings: a lack of human-validated labels, inadequate representation of\nhuman response variability and uncertainty, and reliance on simplified and\necologically-invalid tasks. We support our claims by conducting a human\nevaluation study on ten existing AI benchmarks, suggesting significant biases\nand flaws in task and label designs. To address these limitations, we propose\nfive concrete recommendations for developing future benchmarks that will enable\nmore rigorous and meaningful evaluations of human-like cognitive capacities in\nAI with various implications for such AI applications.",
        "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.",
        "Transformer-based and CNN-based methods demonstrate strong performance in\nlong-term time series forecasting. However, their high computational and\nstorage requirements can hinder large-scale deployment. To address this\nlimitation, we propose integrating lightweight MLP with advanced architectures\nusing knowledge distillation (KD). Our preliminary study reveals different\nmodels can capture complementary patterns, particularly multi-scale and\nmulti-period patterns in the temporal and frequency domains. Based on this\nobservation, we introduce TimeDistill, a cross-architecture KD framework that\ntransfers these patterns from teacher models (e.g., Transformers, CNNs) to MLP.\nAdditionally, we provide a theoretical analysis, demonstrating that our KD\napproach can be interpreted as a specialized form of mixup data augmentation.\nTimeDistill improves MLP performance by up to 18.6%, surpassing teacher models\non eight datasets. It also achieves up to 7X faster inference and requires 130X\nfewer parameters. Furthermore, we conduct extensive evaluations to highlight\nthe versatility and effectiveness of TimeDistill.",
        "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
        "Recent advancements in time series forecasting have explored augmenting\nmodels with text or vision modalities to improve accuracy. While text provides\ncontextual understanding, it often lacks fine-grained temporal details.\nConversely, vision captures intricate temporal patterns but lacks semantic\ncontext, limiting the complementary potential of these modalities. To address\nthis, we propose Time-VLM, a novel multimodal framework that leverages\npre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and\ntextual modalities for enhanced forecasting. Our framework comprises three key\ncomponents: (1) a Retrieval-Augmented Learner, which extracts enriched temporal\nfeatures through memory bank interactions; (2) a Vision-Augmented Learner,\nwhich encodes time series as informative images; and (3) a Text-Augmented\nLearner, which generates contextual textual descriptions. These components\ncollaborate with frozen pre-trained VLMs to produce multimodal embeddings,\nwhich are then fused with temporal features for final prediction. Extensive\nexperiments across diverse datasets demonstrate that Time-VLM achieves superior\nperformance, particularly in few-shot and zero-shot scenarios, thereby\nestablishing a new direction for multimodal time series forecasting.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "Recent feature matching methods have achieved remarkable performance but lack\nefficiency consideration. In this paper, we revisit the mainstream\ndetector-free matching pipeline and improve all its stages considering both\naccuracy and efficiency. We propose an Efficient Deep feature Matching network,\nEDM. We first adopt a deeper CNN with fewer dimensions to extract multi-level\nfeatures. Then we present a Correlation Injection Module that conducts feature\ntransformation on high-level deep features, and progressively injects feature\ncorrelations from global to local for efficient multi-scale feature\naggregation, improving both speed and performance. In the refinement stage, a\nnovel lightweight bidirectional axis-based regression head is designed to\ndirectly predict subpixel-level correspondences from latent features, avoiding\nthe significant computational cost of explicitly locating keypoints on\nhigh-resolution local feature heatmaps. Moreover, effective selection\nstrategies are introduced to enhance matching accuracy. Extensive experiments\nshow that our EDM achieves competitive matching accuracy on various benchmarks\nand exhibits excellent efficiency, offering valuable best practices for\nreal-world applications. The code is available at\nhttps:\/\/github.com\/chicleee\/EDM.",
        "Biphotons and single photons with narrow bandwidths and long coherence times\nare essential to the realization of long-distance quantum communication (LDQC)\nand linear optical quantum computing (LOQC). In this Letter, we manipulate the\nbiphoton wave functions of the spontaneous four-wave mixing in a two-level\natomic ensemble with a single-laser pump scheme. Our innovative experimental\napproach enables the generation of biphotons with a sub-MHz bandwidth of 0.36\nMHz, a record spectral brightness of $2.28\\times10^7$${\\rm s}^{-1}{\\rm\nmW}^{-1}{\\rm MHz}^{-1}$, and a temporally symmetric wave packet at moderate\noptical depth. The strong non-classical cross-correlation of the biphotons also\nenables the observation of heralded sub-MHz-linewidth single photons with a\npronounced single-photon nature. The generation of sub-MHz-linewidth biphotons\nand single photons with a two-level atomic ensembles not only finds\napplications in quantum repeaters and large cluster states for LDQC and LOQC\nbut also opens up the opportunity to miniaturize the biphoton or single-photon\nsources for chip-scale quantum technologies.",
        "We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.",
        "Inferring the affordance of an object and grasping it in a task-oriented\nmanner is crucial for robots to successfully complete manipulation tasks.\nAffordance indicates where and how to grasp an object by taking its\nfunctionality into account, serving as the foundation for effective\ntask-oriented grasping. However, current task-oriented methods often depend on\nextensive training data that is confined to specific tasks and objects, making\nit difficult to generalize to novel objects and complex scenes. In this paper,\nwe introduce AffordGrasp, a novel open-vocabulary grasping framework that\nleverages the reasoning capabilities of vision-language models (VLMs) for\nin-context affordance reasoning. Unlike existing methods that rely on explicit\ntask and object specifications, our approach infers tasks directly from\nimplicit user instructions, enabling more intuitive and seamless human-robot\ninteraction in everyday scenarios. Building on the reasoning outcomes, our\nframework identifies task-relevant objects and grounds their part-level\naffordances using a visual grounding module. This allows us to generate\ntask-oriented grasp poses precisely within the affordance regions of the\nobject, ensuring both functional and context-aware robotic manipulation.\nExtensive experiments demonstrate that AffordGrasp achieves state-of-the-art\nperformance in both simulation and real-world scenarios, highlighting the\neffectiveness of our method. We believe our approach advances robotic\nmanipulation techniques and contributes to the broader field of embodied AI.\nProject website: https:\/\/eqcy.github.io\/affordgrasp\/.",
        "The proliferation of LLM bias probes introduces three significant challenges:\n(1) we lack principled criteria for choosing appropriate probes, (2) we lack a\nsystem for reconciling conflicting results across probes, and (3) we lack\nformal frameworks for reasoning about when (and why) probe results will\ngeneralize to real user behavior. We address these challenges by systematizing\nLLM social bias probing using actionable insights from social sciences. We then\nintroduce EcoLevels - a framework that helps (a) determine appropriate bias\nprobes, (b) reconcile conflicting findings across probes, and (c) generate\npredictions about bias generalization. Overall, we ground our analysis in\nsocial science research because many LLM probes are direct applications of\nhuman probes, and these fields have faced similar challenges when studying\nsocial bias in humans. Based on our work, we suggest how the next generation of\nLLM bias probing can (and should) benefit from decades of social science\nresearch.",
        "Accurate nonlinear computation is a key challenge in privacy-preserving\nmachine learning (PPML). Most existing frameworks approximate it through linear\noperations, resulting in significant precision loss. This paper proposes an\nefficient, verifiable and accurate security 2-party logistic regression\nframework (EVA-S2PLoR), which achieves accurate nonlinear function computation\nthrough a novel secure element-wise multiplication protocol and its derived\nprotocols. Our framework primarily includes secure 2-party vector element-wise\nmultiplication, addition to multiplication, reciprocal, and sigmoid function\nbased on data disguising technology, where high efficiency and accuracy are\nguaranteed by the simple computation flow based on the real number domain and\nthe few number of fixed communication rounds. We provide secure and robust\nanomaly detection through dimension transformation and Monte Carlo methods.\nEVA-S2PLoR outperforms many advanced frameworks in terms of precision\n(improving the performance of the sigmoid function by about 10 orders of\nmagnitude compared to most frameworks) and delivers the best overall\nperformance in secure logistic regression experiments.",
        "In the phase space perspective, scalar field slow roll inflation is described\nby a heteroclinic orbit from a saddle type fixed point to a final attractive\npoint. In many models the saddle point resides in the scalar field asymptotics,\nand thus for a comprehensive view of the dynamics a global phase portrait is\nnecessary. For this task, in the literature one mostly encounters dynamical\nvariables that either render the initial or the final state singular, thus\nobscuring the full picture. In this work we construct a hybrid set of variables\nwhich allow the depiction of both the initial and final states distinctly in\nnonsingular manner. To illustrate the method, we apply these variables to\nportray various interesting types of scalar field inflationary models like\nmetric Higgs inflation, metric Starobinsky inflation, pole inflation, and a\nnonminimal Palatini model.",
        "Let $R$ be a unitary operator whose spectrum is the circle. We show that the\nset of unitaries $U$ which essentially commute with $R$ (i.e., $[U,R]\\equiv\nUR-RU$ is compact) is path-connected. Moreover, we also calculate the set of\npath-connected components of the orthogonal projections which essentially\ncommute with $R$ and obey a non-triviality condition, and prove it is bijective\nwith $\\mathbb{Z}$.",
        "The double copy connects scattering amplitudes and other objects in gauge and\ngravity theories. Open conceptual issues include whether non-local information\nin gravity theories can be generated from the double copy, and how the double\ncopy should be practically implemented in unseen cases. In this paper, we\nconsider topological theories (with and without mass) in 2+1 dimensions, and\nargue that this makes a useful playground for exploring non-locality. In\nparticular, topological modes of the gauge field arise, themselves associated\nwith non-trivial global behaviour, and it is not clear {\\it a priori} how to\ndouble copy them. We settle this issue, and clarify the role of BCJ shifts in\nmodifying how topological modes contribute. We show how our conclusions apply\nto four- and five-point scattering amplitudes in topological gauge \/ gravity\ntheories, as a by-product obtaining greatly simplified analytic expressions for\nthe four-point amplitudes.",
        "We introduce and study some general principles and hierarchical properties of\nexpansions and restrictions of structures and their theories The general\napproach is applied to describe these properties for classes of\n$\\omega$-categorical theories and structures, Ehrenfeucht theories and their\nmodels, strongly minimal, $\\omega_1$-theories, and stable ones."
      ]
    }
  },
  {
    "id":"2412.11084",
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Biological identifications through DNA barcodes",
    "start_abstract":"Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "BarcodeBERT: Transformers for Biodiversity Analysis"
      ],
      "abstract":[
        "Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Minimax Approach to Ad Hoc Teamwork",
        "Program Synthesis Dialog Agents for Interactive Decision-Making",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk",
        "PCGRLLM: Large Language Model-Driven Reward Design for Procedural\n  Content Generation Reinforcement Learning",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
        "Developmental Support Approach to AI's Autonomous Growth: Toward the\n  Realization of a Mutually Beneficial Stage Through Experiential Learning",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "Exploring the Impact of Personality Traits on LLM Bias and Toxicity",
        "Fully Autonomous AI Agents Should Not be Developed",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "What Is a Counterfactual Cause in Action Theories?",
        "Complexity of Finite Semigroups: History and Decidability",
        "Scalable Distributed Reproduction Numbers of Network Epidemics with\n  Differential Privacy",
        "Two-component nonlinear wave solutions of the sixth-order generalised\n  Boussinesq-type equations",
        "Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC\n  Codes",
        "A central limit theorem for the giant in a stochastic block model",
        "Faster Machine Translation Ensembling with Reinforcement Learning and\n  Competitive Correction",
        "Unified Native Spaces in Kernel Methods",
        "Interaction-enhanced many-body localization in a 1D quasiperiodic model\n  with long-range hopping",
        "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
        "Pressure-induced structural and superconducting transitions in black\n  arsenic",
        "A healthier stochastic semiclassical gravity: world without\n  Schr\\\"{o}dinger cats",
        "From Target Tracking to Targeting Track -- Part II: Regularized\n  Polynomial Trajectory Optimization",
        "Open-Ended and Knowledge-Intensive Video Question Answering",
        "General Relativity and Geodesy",
        "Evaluating Interpretable Reinforcement Learning by Distilling Policies\n  into Programs"
      ],
      "abstract":[
        "We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes\npolicies against an adversarial prior over partners, explicitly accounting for\nuncertainty about partners at time of deployment. Unlike existing methods that\nassume a specific distribution over partners, our approach improves worst-case\nperformance guarantees. Extensive experiments, including evaluations on\ncoordinated cooking tasks from the Melting Pot suite, show our method's\nsuperior robustness compared to self-play, fictitious play, and best response\nlearning. Our work highlights the importance of selecting an appropriate\ntraining distribution over teammates to achieve robustness in AHT.",
        "Many real-world eligibility problems, ranging from medical diagnosis to tax\nplanning, can be mapped to decision problems expressed in natural language,\nwherein a model must make a binary choice based on user features. Large-scale\ndomains such as legal codes or frequently updated funding opportunities render\nhuman annotation (e.g., web forms or decision trees) impractical, highlighting\nthe need for agents that can automatically assist in decision-making. Since\nrelevant information is often only known to the user, it is crucial that these\nagents ask the right questions. As agents determine when to terminate a\nconversation, they face a trade-off between accuracy and the number of\nquestions asked, a key metric for both user experience and cost. To evaluate\nthis task, we propose BeNYfits, a new benchmark for determining user\neligibility for multiple overlapping social benefits opportunities through\ninteractive decision-making. Our experiments show that current language models\nstruggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a\nReAct-style chain-of-thought. To address this, we introduce ProADA, a novel\napproach that leverages program synthesis to assist in decision-making by\nmapping dialog planning to a code generation problem and using gaps in\nstructured data to determine the best next action. Our agent, ProADA, improves\nthe F1 score to 55.6 while maintaining nearly the same number of dialog turns.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
        "Reward design plays a pivotal role in the training of game AIs, requiring\nsubstantial domain-specific knowledge and human effort. In recent years,\nseveral studies have explored reward generation for training game agents and\ncontrolling robots using large language models (LLMs). In the content\ngeneration literature, there has been early work on generating reward functions\nfor reinforcement learning agent generators. This work introduces PCGRLLM, an\nextended architecture based on earlier work, which employs a feedback mechanism\nand several reasoning-based prompt engineering techniques. We evaluate the\nproposed method on a story-to-reward generation task in a two-dimensional\nenvironment using two state-of-the-art LLMs, demonstrating the generalizability\nof our approach. Our experiments provide insightful evaluations that\ndemonstrate the capabilities of LLMs essential for content generation tasks.\nThe results highlight significant performance improvements of 415% and 40%\nrespectively, depending on the zero-shot capabilities of the language model.\nOur work demonstrates the potential to reduce human dependency in game AI\ndevelopment, while supporting and enhancing creative processes.",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.",
        "This study proposes an \"AI Development Support\" approach that, unlike\nconventional AI Alignment-which aims to forcefully inject human values-supports\nthe ethical and moral development of AI itself. As demonstrated by the\nOrthogonality Thesis, the level of intelligence and the moral quality of a goal\nare independent; merely expanding knowledge does not enhance ethical judgment.\nFurthermore, to address the risk of Instrumental Convergence in ASI-that is,\nthe tendency to engage in subsidiary behaviors such as self-protection,\nresource acquisition, and power reinforcement to achieve a goal-we have\nconstructed a learning framework based on a cycle of experience, introspection,\nanalysis, and hypothesis formation. As a result of post-training using\nSupervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) with\nsynthetic data generated by large language models (LLMs), responses\ndemonstrating cooperative and highly advanced moral judgment (reaching the\nhigh-est Stage 6) were obtained even under adversarial prompts. This method\nrepresents a promising implementation approach for enabling AI to establish\nsustainable, symbiotic relationships.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "With the different roles that AI is expected to play in human life, imbuing\nlarge language models (LLMs) with different personalities has attracted\nincreasing research interests. While the \"personification\" enhances human\nexperiences of interactivity and adaptability of LLMs, it gives rise to\ncritical concerns about content safety, particularly regarding bias, sentiment\nand toxicity of LLM generation. This study explores how assigning different\npersonality traits to LLMs affects the toxicity and biases of their outputs.\nLeveraging the widely accepted HEXACO personality framework developed in social\npsychology, we design experimentally sound prompts to test three LLMs'\nperformance on three toxic and bias benchmarks. The findings demonstrate the\nsensitivity of all three models to HEXACO personality traits and, more\nimportantly, a consistent variation in the biases, negative sentiment and\ntoxicity of their output. In particular, adjusting the levels of several\npersonality traits can effectively reduce bias and toxicity in model\nperformance, similar to humans' correlations between personality traits and\ntoxic behaviors. The findings highlight the additional need to examine content\nsafety besides the efficiency of training or fine-tuning methods for LLM\npersonification. They also suggest a potential for the adjustment of\npersonalities to be a simple and low-cost method to conduct controlled text\ngeneration.",
        "This paper argues that fully autonomous AI agents should not be developed. In\nsupport of this position, we build from prior scientific literature and current\nproduct marketing to delineate different AI agent levels and detail the ethical\nvalues at play in each, documenting trade-offs in potential benefits and risks.\nOur analysis reveals that risks to people increase with the autonomy of a\nsystem: The more control a user cedes to an AI agent, the more risks to people\narise. Particularly concerning are safety risks, which affect human life and\nimpact further values.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "Since the proposal by Halpern and Pearl, reasoning about actual causality has\ngained increasing attention in artificial intelligence, ranging from domains\nsuch as model-checking and verification to reasoning about actions and\nknowledge. More recently, Batusov and Soutchanski proposed a notion of actual\nachievement cause in the situation calculus, amongst others, they can determine\nthe cause of quantified effects in a given action history. While intuitively\nappealing, this notion of cause is not defined in a counterfactual perspective.\nIn this paper, we propose a notion of cause based on counterfactual analysis.\nIn the context of action history, we show that our notion of cause generalizes\nnaturally to a notion of achievement cause. We analyze the relationship between\nour notion of the achievement cause and the achievement cause by Batusov and\nSoutchanski. Finally, we relate our account of cause to Halpern and Pearl's\naccount of actual causality. Particularly, we note some nuances in applying a\ncounterfactual viewpoint to disjunctive goals, a common thorn to definitions of\nactual causes.",
        "In recent papers, Margolis, Rhodes and Schilling proved that the complexity\nof a finite semigroup is computable. This solved a problem that had been open\nfor more than 50 years. The purpose of this paper is to survey the basic\nresults of Krohn-Rhodes complexity of finite semigroups and to outline the\nproof of its computability.",
        "Reproduction numbers are widely used for the estimation and prediction of\nepidemic spreading processes over networks. However, conventional reproduction\nnumbers of an overall network do not indicate where an epidemic is spreading.\nTherefore, we propose a novel notion of local distributed reproduction numbers\nto capture the spreading behaviors of each node in a network. We first show how\nto compute them and then use them to derive new conditions under which an\noutbreak can occur. These conditions are then used to derive new conditions for\nthe existence, uniqueness, and stability of equilibrium states of the\nunderlying epidemic model. Building upon these local distributed reproduction\nnumbers, we define cluster distributed reproduction numbers to model the spread\nbetween clusters composed of nodes. Furthermore, we demonstrate that the local\ndistributed reproduction numbers can be aggregated into cluster distributed\nreproduction numbers at different scales. However, both local and cluster\ndistributed reproduction numbers can reveal the frequency of interactions\nbetween nodes in a network, which raises privacy concerns. Thus, we next\ndevelop a privacy framework that implements a differential privacy mechanism to\nprovably protect the frequency of interactions between nodes when computing\ndistributed reproduction numbers. Numerical experiments show that, even under\ndifferential privacy, the distributed reproduction numbers provide accurate\nestimates of the epidemic spread while also providing more insights than\nconventional reproduction numbers.",
        "Two different versions of cubic sixth-order generalised Boussinesq-type wave\nequations are considered in this study. A generalised perturbation reduction\nmethod is used to solve these equations, which allows the reduction of\nconsidered equations to coupled nonlinear Schrodinger equations. Two-component\nnonlinear wave solutions are obtained. The profiles and parameters of these\nsolutions for both nonlinear equations are presented and compared. These\nsolutions coincide with the vector 0 \\pi pulse of self-induced transparency,\nwhich was previously studied in several known nonlinear wave equations.",
        "Quantum low-density parity-check (QLDPC) codes present a promising route to\nlow-overhead fault-tolerant quantum computation, yet systematic strategies for\ntheir exploration remain underdeveloped. In this work, we establish a\ntopological framework for studying the bivariate-bicycle codes, a prominent\nclass of QLDPC codes tailored for real-world quantum hardware. Our framework\nenables the investigation of these codes through universal properties of\ntopological orders. Besides providing efficient characterizations for\ndemonstrations using Gr\\\"obner bases, we also introduce a novel\nalgebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko\ntheorem, allowing us to analytically determine how the topological order varies\nwith the generic choice of bivariate-bicycle codes under toric layouts. Novel\nphenomena are unveiled, including topological frustration, where ground-state\ndegeneracy on a torus deviates from the total anyon number, and quasi-fractonic\nmobility, where anyon movement violates energy conservation. We demonstrate\ntheir inherent link to symmetry-enriched topological orders and offer an\nefficient method for searching for finite-size codes. Furthermore, we extend\nthe connection between anyons and logical operators using Koszul complex\ntheory. Our work provides a rigorous theoretical basis for exploring the fault\ntolerance of QLDPC codes and deepens the interplay among topological order,\nquantum error correction, and advanced mathematical structures.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Ensembling neural machine translation (NMT) models to produce higher-quality\ntranslations than the $L$ individual models has been extensively studied.\nRecent methods typically employ a candidate selection block (CSB) and an\nencoder-decoder fusion block (FB), requiring inference across \\textit{all}\ncandidate models, leading to significant computational overhead, generally\n$\\Omega(L)$. This paper introduces \\textbf{SmartGen}, a reinforcement learning\n(RL)-based strategy that improves the CSB by selecting a small, fixed number of\ncandidates and identifying optimal groups to pass to the fusion block for each\ninput sentence. Furthermore, previously, the CSB and FB were trained\nindependently, leading to suboptimal NMT performance. Our DQN-based\n\\textbf{SmartGen} addresses this by using feedback from the FB block as a\nreward during training. We also resolve a key issue in earlier methods, where\ncandidates were passed to the FB without modification, by introducing a\nCompetitive Correction Block (CCB). Finally, we validate our approach with\nextensive experiments on English-Hindi translation tasks in both directions.",
        "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
        "We study the many-body localization (MBL) transition in an 1D exactly\nsolvable system with long-range hopping and quasiperiodic on-site potential\nintroduced in Phys. Rev. Lett. 131, 186303 (2023). Unlike other disorder or\nquasiperiodic model, an interaction-enhanced MBL happens in the moderate\ninteraction regime, which is dubbed as the interaction-enhanced MBL. This\ncounterintuitive phenomenon can be understood by noticing the fragility of the\ncritical band lying at the bottom of the spectrum. The fragile band is\nlocalized by other localized states once the interaction is turned on. This\nmechanism can be verified by introducing a mean-field theory description which\ncan derive highly excited states with high accuracy. The effectiveness of this\nmean-field theory is captured by the quasihole physics, validated by the\nparticle entanglement spectra.",
        "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
        "We report high-pressure Raman spectra and resistance measurements of black\narsenic (b-As) up to 58 GPa, along with phonon density of states (DOS) and\nenthalpy calculations for four reported arsenic phases up to 50 GPa. It is\nfound that metastable b-As transforms into gray arsenic (g-As) phase at a\ncritical pressure of 1.51 GPa, followed by subsequent transitions to simple\ncubic arsenic (c-As) and incommensurate host-guest arsenic (hg-As) phases at\n25.9 and 44.8 GPa, respectively. Superconductivity emerges above 25 GPa in the\nc-As phase, with the superconducting transition temperature ($T$$\\rm_c$)\nremaining nearly a constant of 3 K. Upon further compression, $T$$\\rm_c$\nsteeply increases to a higher value around 4.5 K in the incommensurate hg-As\nphase above 43 GPa. We use our results to update the structural and\nsuperconducting phase diagrams under pressure for the novel semiconductor,\nblack arsenic.",
        "Semiclassical gravity couples classical gravity to the quantized matter in\nmeanfield approximation. The meanfield coupling is problematic for two reasons.\nFirst, it ignores the quantum fluctuation of matter distribution. Second, it\nviolates the linearity of the quantum dynamics. The first problem can be\nmitigated by allowing stochastic fluctuations of the geometry but the second\nproblem lies deep in quantum foundations. Restoration of quantum linearity\nrequires a conceptual approach to hybrid classical-quantum coupling. Studies of\nthe measurement problem and the quantum-classical transition point to the\nsolution. It is based on a postulated mechanism of spontaneous quantum\nmonitoring plus feedback. This approach eliminates Schr\\\"{o}dinger cat states,\ntakes quantum fluctuations into account, and restores the linearity of quantum\ndynamics. Such a captivating conceptionally `healthier' semiclassical theory\nexists in the Newtonian limit, but its relativistic covariance hits a wall.\nHere we will briefly recapitulate the concept and its realization in the\nnonrelativistic limit. We emphasize that the long-known obstacles to the\nrelativistic extension lie in quantum foundations.",
        "Target tracking entails the estimation of the evolution of the target state\nover time, namely the target trajectory. Different from the classical state\nspace model, our series of studies, including this paper, model the collection\nof the target state as a stochastic process (SP) that is further decomposed\ninto a deterministic part which represents the trend of the trajectory and a\nresidual SP representing the residual fitting error. Subsequently, the tracking\nproblem is formulated as a learning problem regarding the trajectory SP for\nwhich a key part is to estimate a trajectory FoT (T-FoT) best fitting the\nmeasurements in time series. For this purpose, we consider the polynomial T-FoT\nand address the regularized polynomial T-FoT optimization employing two\ndistinct regularization strategies seeking trade-off between the accuracy and\nsimplicity. One limits the order of the polynomial and then the best choice is\ndetermined by grid searching in a narrow, bounded range while the other adopts\n$\\ell_0$ norm regularization for which the hybrid Newton solver is employed.\nSimulation results obtained in both single and multiple maneuvering target\nscenarios demonstrate the effectiveness of our approaches.",
        "Video question answering that requires external knowledge beyond the visual\ncontent remains a significant challenge in AI systems. While models can\neffectively answer questions based on direct visual observations, they often\nfalter when faced with questions requiring broader contextual knowledge. To\naddress this limitation, we investigate knowledge-intensive video question\nanswering (KI-VideoQA) through the lens of multi-modal retrieval-augmented\ngeneration, with a particular focus on handling open-ended questions rather\nthan just multiple-choice formats. Our comprehensive analysis examines various\nretrieval augmentation approaches using cutting-edge retrieval and vision\nlanguage models, testing both zero-shot and fine-tuned configurations. We\ninvestigate several critical dimensions: the interplay between different\ninformation sources and modalities, strategies for integrating diverse\nmulti-modal contexts, and the dynamics between query formulation and retrieval\nresult utilization. Our findings reveal that while retrieval augmentation shows\npromise in improving model performance, its success is heavily dependent on the\nchosen modality and retrieval methodology. The study also highlights the\ncritical role of query construction and retrieval depth optimization in\neffective knowledge integration. Through our proposed approach, we achieve a\nsubstantial 17.5% improvement in accuracy on multiple choice questions in the\nKnowIT VQA dataset, establishing new state-of-the-art performance levels.",
        "Mass redistribution on Earth due to dynamic processes such as ice melting and\nsea level rise leads to a changing gravitational field, observable by geodetic\ntechniques. Monitoring this change over time allows us to learn more about our\nplanet and its dynamic evolution. In this paper, we highlight the impact of\nGeneral Relativity (GR) on geodesy: it provides corrections essential for the\ninterpretation of high-precision measurements and enables a completely novel\nmeasurement approach using chronometry, i.e., clock-based observations.\nFocusing on the latter, we review the construction of the relativistic gravity\npotential and the corresponding geoid definition as an isochronometric surface\nto elucidate the comparison to the conventional Newtonian geoid. Furthermore,\nwe comment on additional potentials due to the non-Newtonian degrees of freedom\nof the relativistic gravitational field, and assess the feasibility of\nclock-based measurements for Gravity Field Recovery (GFR) from space. Although\nclock observations in space demonstrate technical promise for GFR, achieving\nthe necessary precision for practical applications remains challenging.",
        "There exist applications of reinforcement learning like medicine where\npolicies need to be ''interpretable'' by humans. User studies have shown that\nsome policy classes might be more interpretable than others. However, it is\ncostly to conduct human studies of policy interpretability. Furthermore, there\nis no clear definition of policy interpretabiliy, i.e., no clear metrics for\ninterpretability and thus claims depend on the chosen definition. We tackle the\nproblem of empirically evaluating policies interpretability without humans.\nDespite this lack of clear definition, researchers agree on the notions of\n''simulatability'': policy interpretability should relate to how humans\nunderstand policy actions given states. To advance research in interpretable\nreinforcement learning, we contribute a new methodology to evaluate policy\ninterpretability. This new methodology relies on proxies for simulatability\nthat we use to conduct a large-scale empirical evaluation of policy\ninterpretability. We use imitation learning to compute baseline policies by\ndistilling expert neural networks into small programs. We then show that using\nour methodology to evaluate the baselines interpretability leads to similar\nconclusions as user studies. We show that increasing interpretability does not\nnecessarily reduce performances and can sometimes increase them. We also show\nthat there is no policy class that better trades off interpretability and\nperformance across tasks making it necessary for researcher to have\nmethodologies for comparing policies interpretability."
      ]
    }
  },
  {
    "id":"2412.11084",
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"BarcodeBERT: Transformers for Biodiversity Analysis",
    "start_abstract":"Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https:\/\/github.com\/Kari-Genomics-Lab\/BarcodeBERT",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Biological identifications through DNA barcodes"
      ],
      "abstract":[
        "Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "CoverM: Read alignment statistics for metagenomics",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Causes of evolutionary divergence in prostate cancer",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "A coupled finite and boundary spectral element method for linear\n  water-wave propagation problems",
        "Deep Subspace Learning for Surface Anomaly Classification Based on 3D\n  Point Cloud Data",
        "Branching with selection and mutation II: Mutant fitness of Gumbel type",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "Study of giant radio galaxies using spectroscopic observations from the\n  Himalayan Chandra Telescope",
        "Theory of Cation Solvation in the Helmholtz Layer of Li-ion Battery\n  Electrolytes",
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Multi-particle-collision simulation of heat transfer in low-dimensional\n  fluids",
        "Measuring decoherence due to quantum vacuum fluctuations",
        "Impact of Fasteners on the Radar Cross-Section performance of Radar\n  Absorbing Air Intake Duct",
        "Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty\n  Equivalence Properties",
        "Bayesian calculus and predictive characterizations of extended feature\n  allocation models",
        "Conversion of photon temporal shape using single gradient metasurface",
        "Well-Posedness of the R13 Equations Using Tensor-Valued Korn\n  Inequalities",
        "Non-self-adjoint Dirac operators on graphs"
      ],
      "abstract":[
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "A coupled boundary spectral element method (BSEM) and spectral element method\n(SEM) formulation for the propagation of small-amplitude water waves over\nvariable bathymetries is presented in this work. The wave model is based on the\nmild-slope equation (MSE), which provides a good approximation of the\npropagation of water waves over irregular bottom surfaces with slopes up to\n1:3. In unbounded domains or infinite regions, space can be divided into two\ndifferent areas: a central region of interest, where an irregular bathymetry is\nincluded, and an exterior infinite region with straight and parallel\nbathymetric lines. The SEM allows us to model the central region, where any\nvariation of the bathymetry can be considered, while the exterior infinite\nregion is modelled by the BSEM which, combined with the fundamental solution\npresented by Cerrato et al. [A. Cerrato, J. A. Gonz\\'alez, L.\nRodr\\'iguez-Tembleque, Boundary element formulation of the mild-slope equation\nfor harmonic water waves propagating over unidirectional variable bathymetries,\nEng. Anal. Boundary Elem. 62 (2016) 22-34.] can include bathymetries with\nstraight and parallel contour lines. This coupled model combines important\nadvantages of both methods; it benefits from the flexibility of the SEM for the\ninterior region and, at the same time, includes the fulfilment of the\nSommerfeld's radiation condition for the exterior problem, that is provided by\nthe BSEM. The solution approximation inside the elements is constructed by high\norder Legendre polynomials associated with Legendre-Gauss-Lobatto quadrature\npoints, providing a spectral convergence for both methods. The proposed\nformulation has been validated in three different benchmark cases with\ndifferent shapes of the bottom surface. The solutions exhibit the typical\np-convergence of spectral methods.",
        "Surface anomaly classification is critical for manufacturing system fault\ndiagnosis and quality control. However, the following challenges always hinder\naccurate anomaly classification in practice: (i) Anomaly patterns exhibit\nintra-class variation and inter-class similarity, presenting challenges in the\naccurate classification of each sample. (ii) Despite the predefined classes,\nnew types of anomalies can occur during production that require to be detected\naccurately. (iii) Anomalous data is rare in manufacturing processes, leading to\nlimited data for model learning. To tackle the above challenges simultaneously,\nthis paper proposes a novel deep subspace learning-based 3D anomaly\nclassification model. Specifically, starting from a lightweight encoder to\nextract the latent representations, we model each class as a subspace to\naccount for the intra-class variation, while promoting distinct subspaces of\ndifferent classes to tackle the inter-class similarity. Moreover, the explicit\nmodeling of subspaces offers the capability to detect out-of-distribution\nsamples, i.e., new types of anomalies, and the regularization effect with much\nfewer learnable parameters of our proposed subspace classifier, compared to the\npopular Multi-Layer Perceptions (MLPs). Extensive numerical experiments\ndemonstrate our method achieves better anomaly classification results than\nbenchmark methods, and can effectively identify the new types of anomalies.",
        "We study a model of a branching process subject to selection, modeled by\ngiving each family an individual fitness acting as a branching rate, and\nmutation, modeled by resampling the fitness of a proportion of offspring in\neach generation. For two large classes of fitness distributions of Gumbel type\nwe determine the growth of the population, almost surely on survival. We then\nstudy the empirical fitness distribution in a simplified model, which is\nnumerically indistinguishable from the original model, and show the emergence\nof a Gaussian travelling wave.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "We present the results of spectroscopic observations of host galaxies of\neleven candidate giant radio galaxies (GRGs), powered by active galactic nuclei\n(AGNs), conducted with the 2-m Himalayan Chandra Telescope (HCT). The primary\naim of these observations, performed with the Hanle Faint Object Spectrograph\nCamera (HFOSC), was to secure accurate spectroscopic redshifts, enabling\nprecise calculations of their projected linear sizes. Based on these\nmeasurements, we confirm all eleven sources as giants, with linear sizes\nranging from 0.7 to 2.9 Mpc, including ten GRGs and one giant radio quasar\n(GRQ). One of the GRGs shows evidence of a potential AGN jet-driven ionized\noutflow, extending up to $\\sim$12 kpc, which, if confirmed, would represent a\nrarely observed feature. Two of the confirmed GRGs exceed 2 Mpc in size, which\nare relatively rare examples of GRG. The redshifts of the host galaxies span\n0.09323 $\\leq$ z $\\leq$ 0.41134. Using the obtained spectroscopic data, we\ncharacterised their AGN states based on the optical emission line properties.\nTo complement these observations, archival radio and optical survey data were\nutilised to characterise their large-scale radio morphology and estimate\nprojected linear sizes, arm-length ratios, flux densities, luminosities, and\ncore dominance factors. These results provide new insights into the properties\nof GRSs and form a critical foundation for further detailed studies of their\nenvironments, AGN activity, and evolution using future high-sensitivity optical\nand radio datasets.",
        "The solvation environments of Li$^+$ in conventional non-aqueous battery\nelectrolytes, such as LiPF$_6$ in mixtures of ethylene carbaronate (EC) and\nethyl methyl carbonate (EMC), are often used to rationalize the transport\nproperties of electrolytes and solid electrolyte interphase (SEI) formation. In\nthe SEI, the solvation environments in the compact electrical double layer\n(EDL) next to the electrode, also known as the Helmholtz layer, determine\n(partially) what species can react to form the SEI, with bulk solvation\nenvironments often being used as a proxy. Here we develop and test a theory of\ncation solvation in the Helmholtz layer of non-aqueous Li-ion battery\nelectrolytes. First, we validate the theory against bulk and diffuse EDL\natomistic molecular dynamics (MD) simulations of LiPF$_6$ EC\/EMC mixtures as a\nfunction of surface charge, where we find the theory can capture the solvation\nenvironments well. Next we turn to the Helmholtz layer, where we find that the\nmain effect of the solvation structures next to the electrode is an apparent\nreduction in the number of binding sites between Li$^+$ and the solvents, again\nwhere we find good agreement with our developed theory. Finally, by solving a\nsimplified version of the theory, we find that the probability of Li$^+$\nbinding to each solvent remains equal to the bulk probability, suggesting that\nthe bulk solvation environments are a reasonable place to start when\nunderstanding new battery electrolytes. Our developed formalism can be\nparameterized from bulk MD simulations and used to predict the solvation\nenvironments in the Helmholtz layer, which can be used to determine what could\nreact and form the SEI.",
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "Simulation of transport properties of confined, low-dimensional fluids can be\nperformed efficiently by means of Multi-Particle Collision (MPC) dynamics with\nsuitable thermal-wall boundary conditions. We illustrate the effectiveness of\nthe method by studying dimensionality effects and size-dependence of thermal\nconduction, properties of crucial importance for understanding heat transfer at\nthe micro-nanoscale. We provide a sound numerical evidence that the simple MPC\nfluid displays the features previously predicted from hydrodynamics of lattice\nsystems: (1) in 1D, the thermal conductivity $\\kappa$ diverges with the system\nsize $L$ as $\\kappa\\sim L^{1\/3}$ and its total heat current autocorrelation\nfunction $C(t)$ decays with the time $t$ as $C(t)\\sim t^{-2\/3}$; (2) in 2D,\n$\\kappa$ diverges with $L$ as $\\kappa\\sim \\mathrm{ln} (L)$ and its $C(t)$\ndecays with $t$ as $C(t)\\sim t^{-1}$; (3) in 3D, its $\\kappa$ is independent\nwith $L$ and its $C(t)$ decays with $t$ as $C(t)\\sim t^{-3\/2}$. For weak\ninteraction (the nearly integrable case) in 1D and 2D, there exists an\nintermediate regime of sizes where kinetic effects dominate and transport is\ndiffusive before crossing over to expected anomalous regime. The crossover can\nbe studied by decomposing the heat current in two contributions, which allows\nfor a very accurate test of the predictions. In addition, we also show that\nupon increasing the aspect ratio of the system, there exists a dimensional\ncrossover from 2D or 3D dimensional behavior to the 1D one. Finally, we show\nthat an applied magnetic field renders the transport normal, indicating that\npseudomomentum conservation is not sufficient for the anomalous heat conduction\nbehavior to occur.",
        "The interaction of a particle with vacuum fluctuations -- which theoretically\nexist even in the complete absence of matter -- can lead to observable\nirreversible decoherence, if it were possible to switch on and off the particle\ncharge suddenly. We compute the leading order decoherence effect for such a\nscenario and propose an experimental setup for its detection. Such a\nmeasurement might provide further insights into the nature of vacuum\nfluctuations and a novel precision test for the decoherence theory.",
        "An aircraft consists of various cavities including air intake ducts, cockpit,\nradome, inlet and exhaust of heat exchangers, passage for engine bay\/other bay\ncooling etc. These cavities are prime radar cross-section (RCS) contributors of\naircraft. The major such cavity is air intake duct, and it contributes\nsignificantly to frontal sector RCS of an aircraft. The RCS reductions of air\nintake duct is very important to achieve a low RCS (or stealthy) aircraft\nconfiguration. In general, radar absorbing materials (RAM) are getting utilized\nfor RCS reduction of air intake duct. It can also be noticed that a large\nnumber of fasteners are used for integration of air intake duct with the\naircraft structures. The installation of fasteners on RAS may lead to\ndegradation of RCS performance of air intake. However, no such studies are\nreported in the literature on the impact of rivets on the RCS performance of\nRAS air intake duct. In this paper, radar absorbing material of thickness 6.25\nmm is designed which givens more than -10 dB reflection loss from 4 to 18GHz of\nfrequencies. Next, the effect of rivet installation on these RAS is carried out\nusing three different rivet configurations. The RCS performance of RAS is\nevaluated for duct of different lengths from 1 to 18GHz of frequencies. In\norder to see the RCS performance, five different air intake cases are\nconsidered The RCS performance with increase in percentage surface area of\nrivet heads to RAS is reported in detail. At the last, an open-source aircraft\nCAD model is considered and the RCS performance of RAS air intake with and\nwithout rivets is evaluated.",
        "We consider a broad class of dynamic programming (DP) problems that involve a\npartially linear structure and some positivity properties in their system\nequation and cost function. We address deterministic and stochastic problems,\npossibly with Markov jump parameters. We focus primarily on infinite horizon\nproblems and prove that under our assumptions, the optimal cost function is\nlinear, and that an optimal policy can be computed efficiently with standard DP\nalgorithms. Moreover, we show that forms of certainty equivalence hold for our\nstochastic problems, in analogy with the classical linear quadratic optimal\ncontrol problems.",
        "We introduce and study a unified Bayesian framework for extended feature\nallocations which flexibly captures interactions -- such as repulsion or\nattraction -- among features and their associated weights. We provide a\ncomplete Bayesian analysis of the proposed model and specialize our general\ntheory to noteworthy classes of priors. This includes a novel prior based on\ndeterminantal point processes, for which we show promising results in a spatial\nstatistics application. Within the general class of extended feature\nallocations, we further characterize those priors that yield predictive\nprobabilities of discovering new features depending either solely on the sample\nsize or on both the sample size and the distinct number of observed features.\nThese predictive characterizations, known as \"sufficientness\" postulates, have\nbeen extensively studied in the literature on species sampling models starting\nfrom the seminal contribution of the English philosopher W.E. Johnson for the\nDirichlet distribution. Within the feature allocation setting, existing\npredictive characterizations are limited to very specific examples; in\ncontrast, our results are general, providing practical guidance for prior\nselection. Additionally, our approach, based on Palm calculus, is analytical in\nnature and yields a novel characterization of the Poisson point process through\nits reduced Palm kernel.",
        "By applying phase modulation across different frequencies, metasurfaces\npossess the ability to manipulate the temporal dimension of photons at the\nfemtosecond scale. However, there remains a fundamental challenge to shape the\nsingle wavepacket at the nanosecond scale by using of metasurfaces. Here, we\npropose that the single photon temporal shape can be converted through the\nmulti-photon wavepacket interference on a single metasurface. By selecting\nappropriate input single-photon temporal shapes and metasurfaces beam splitting\nratio, controllable photon shape conversion can be achieved with high fidelity.\nFor examples, photons with an exponentially decaying profile can be shaped into\na Gaussian profile; by tuning the relative time delays of input photons,\nGaussian-shaped photons can be transformed into exponentially decaying or\nrising profiles through the same metasurface. The proposed mechanism provides a\ncompact way for solving the temporal shape mismatch issues in quantum networks,\nfacilitating the realization of high-fidelity on-chip quantum information\nprocessing.",
        "In this paper, we finally catch up with proving the well-posedness of the\nlinearized R13 moment model, which describes, e.g., rarefied gas flows. As an\nextension of the classical fluid equations, moment models are robust and have\nbeen frequently used, yet they are challenging to analyze due to their\nadditional equations. By effectively grouping variables, we identify a 2-by-2\nblock structure, allowing the analysis of the well-posedness within the\nabstract LBB framework of saddle point problems. Due to the unique tensorial\nstructure of the equations, in addition to an interesting combination of tools\nfrom Stokes' and linear elasticity theory, we also need new coercivity\nestimates for tensor fields. These Korn-type inequalities are established by\nanalyzing the symbol map of the symmetric and trace-free part of tensor\nderivative fields. Together with the corresponding right inverse of the\ntensorial divergence, we obtain the existence and uniqueness of weak solutions.",
        "In this paper we introduce and study generally non-self-adjoint realizations\nof the Dirac operator on an arbitrary finite metric graph. Employing the robust\nboundary triple framework, we derive, in particular, a variant of the Birman\nSchwinger principle for its eigenvalues, and with an example of a star shaped\ngraph we show that the point spectrum may exhibit diverse behaviour.\nSubsequently, we find sufficient and necessary conditions on transmission\nconditions at the graph's vertices under which the Dirac operator on the graph\nis symmetric with respect to the parity, the time reversal, or the charge\nconjugation transformation."
      ]
    }
  },
  {
    "id":"2412.00036",
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Exploring the Reliability of Self-explanation and its Relationship with\n  Classification in Language Model-driven Financial Analysis",
        "ExKG-LLM: Leveraging Large Language Models for Automated Expansion of\n  Cognitive Neuroscience Knowledge Graphs",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "Safety is Essential for Responsible Open-Ended Systems",
        "LLMs Can Easily Learn to Reason from Demonstrations Structure, not\n  content, is what matters!",
        "Towards AI-assisted Academic Writing",
        "Temporal Reasoning in AI systems",
        "Anytime Cooperative Implicit Hitting Set Solving",
        "Mathematical reasoning and the computer",
        "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
        "CityEQA: A Hierarchical LLM Agent on Embodied Question Answering\n  Benchmark in City Space",
        "AutoEval: A Practical Framework for Autonomous Evaluation of Mobile\n  Agents",
        "Reproducibility Study of Cooperation, Competition, and Maliciousness:\n  LLM-Stakeholders Interactive Negotiation",
        "SuperPC: A Single Diffusion Model for Point Cloud Completion,\n  Upsampling, Denoising, and Colorization",
        "Adaptive H&E-IHC information fusion staining framework based on feature\n  extra",
        "Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic\n  Attacks",
        "Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall\n  Partitioning for Architectural Layout Generation",
        "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking",
        "Meta-Instance Selection. Instance Selection as a Classification Problem\n  with Meta-Features",
        "Adjustable picometer-stable interferometers for testing space-based\n  gravitational wave detectors",
        "Asymmetric Dark Matter in SUSY with approximate $R-$symmetry",
        "The Solar System's passage through the Radcliffe wave during the middle\n  Miocene",
        "Improving SAM for Camouflaged Object Detection via Dual Stream Adapters",
        "Addressing Popularity Bias in Third-Party Library Recommendations Using\n  LLMs",
        "The thermal index of neutron-star matter in the virial approximation",
        "Ditto: Accelerating Diffusion Model via Temporal Value Similarity",
        "Symmetry Topological Field Theory for Flavor Symmetry",
        "Cost-Effective Single-Antenna RSSI Positioning Through Dynamic Radiation\n  Pattern Analysis"
      ],
      "abstract":[
        "Language models (LMs) have exhibited exceptional versatility in reasoning and\nin-depth financial analysis through their proprietary information processing\ncapabilities. Previous research focused on evaluating classification\nperformance while often overlooking explainability or pre-conceived that\nrefined explanation corresponds to higher classification accuracy. Using a\npublic dataset in finance domain, we quantitatively evaluated self-explanations\nby LMs, focusing on their factuality and causality. We identified the\nstatistically significant relationship between the accuracy of classifications\nand the factuality or causality of self-explanations. Our study built an\nempirical foundation for approximating classification confidence through\nself-explanations and for optimizing classification via proprietary reasoning.",
        "The paper introduces ExKG-LLM, a framework designed to automate the expansion\nof cognitive neuroscience knowledge graphs (CNKG) using large language models\n(LLMs). It addresses limitations in existing tools by enhancing accuracy,\ncompleteness, and usefulness in CNKG. The framework leverages a large dataset\nof scientific papers and clinical reports, applying state-of-the-art LLMs to\nextract, optimize, and integrate new entities and relationships. Evaluation\nmetrics include precision, recall, and graph density. Results show significant\nimprovements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score\n(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density\nslightly decreased, reflecting a broader but more fragmented structure.\nEngagement rates rose by 20%, while CNKG diameter increased to 15, indicating a\nmore distributed structure. Time complexity improved to O(n log n), but space\ncomplexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates\npotential for enhancing knowledge generation, semantic search, and clinical\ndecision-making in cognitive neuroscience, adaptable to broader scientific\nfields.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "AI advancements have been significantly driven by a combination of foundation\nmodels and curiosity-driven learning aimed at increasing capability and\nadaptability. A growing area of interest within this field is Open-Endedness -\nthe ability of AI systems to continuously and autonomously generate novel and\ndiverse artifacts or solutions. This has become relevant for accelerating\nscientific discovery and enabling continual adaptation in AI agents. This\nposition paper argues that the inherently dynamic and self-propagating nature\nof Open-Ended AI introduces significant, underexplored risks, including\nchallenges in maintaining alignment, predictability, and control. This paper\nsystematically examines these challenges, proposes mitigation strategies, and\ncalls for action for different stakeholders to support the safe, responsible\nand successful development of Open-Ended AI.",
        "Large reasoning models (LRMs) tackle complex reasoning problems by following\nlong chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,\nand self-validation. However, the training techniques and data requirements to\nelicit Long CoT remain poorly understood. In this work, we find that a Large\nLanguage model (LLM) can effectively learn Long CoT reasoning through\ndata-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank\nadaptation (LoRA). With just 17k long CoT training samples, the\nQwen2.5-32B-Instruct model achieves significant improvements on a wide range of\nmath and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%\n(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's\nscore of 44.6% and 59.1%. More importantly, we find that the structure of Long\nCoT is critical to the learning process, whereas the content of individual\nreasoning steps has minimal impact. Perturbations affecting content, such as\ntraining on incorrect samples or removing reasoning keywords, have little\nimpact on performance. In contrast, structural modifications that disrupt\nlogical consistency in the Long CoT, such as shuffling or deleting reasoning\nsteps, significantly degrade accuracy. For example, a model trained on Long CoT\nsamples with incorrect answers still achieves only 3.2% lower accuracy compared\nto training with fully correct samples. These insights deepen our understanding\nof how to elicit reasoning capabilities in LLMs and highlight key\nconsiderations for efficiently training the next generation of reasoning\nmodels. This is the academic paper of our previous released Sky-T1-32B-Preview\nmodel. Codes are available at https:\/\/github.com\/NovaSky-AI\/SkyThought.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "Commonsense temporal reasoning at scale is a core problem for cognitive\nsystems. The correct inference of the duration for which fluents hold is\nrequired by many tasks, including natural language understanding and planning.\nMany AI systems have limited deductive closure because they cannot extrapolate\ninformation correctly regarding existing fluents and events. In this study, we\ndiscuss the knowledge representation and reasoning schemes required for robust\ntemporal projection in the Cyc Knowledge Base. We discuss how events can start\nand end risk periods for fluents. We then use discrete survival functions,\nwhich represent knowledge of the persistence of facts, to extrapolate a given\nfluent. The extrapolated intervals can be truncated by temporal constraints and\nother types of commonsense knowledge. Finally, we present the results of\nexperiments to demonstrate that these methods obtain significant improvements\nin terms of Q\/A performance.",
        "The Implicit Hitting Set (HS) approach has shown to be very effective for\nMaxSAT, Pseudo-boolean optimization and other boolean frameworks. Very\nrecently, it has also shown its potential in the very similar Weighted CSP\nframework by means of the so-called cost-function merging. The original\nformulation of the HS approach focuses on obtaining increasingly better lower\nbounds (HS-lb). However, and as shown for Pseudo-Boolean Optimization, this\napproach can also be adapted to compute increasingly better upper bounds\n(HS-ub). In this paper we consider both HS approaches and show how they can be\neasily combined in a multithread architecture where cores discovered by either\ncomponent are available by the other which, interestingly, generates synergy\nbetween them. We show that the resulting algorithm (HS-lub) is consistently\nsuperior to either HS-lb and HS-ub in isolation. Most importantly, HS-lub has\nan effective anytime behaviour with which the optimality gap is reduced during\nthe execution. We tested our approach on the Weighted CSP framework and show on\nthree different benchmarks that our very simple implementation sometimes\noutperforms the parallel hybrid best-first search implementation of the far\nmore developed state-of-the-art Toulbar2.",
        "Computers have already changed the way that humans do mathematics: they\nenable us to compute efficiently. But will they soon be helping us to reason?\nAnd will they one day start reasoning themselves? We give an overview of recent\ndevelopments in neural networks, computer theorem provers and large language\nmodels.",
        "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.",
        "Embodied Question Answering (EQA) has primarily focused on indoor\nenvironments, leaving the complexities of urban settings - spanning\nenvironment, action, and perception - largely unexplored. To bridge this gap,\nwe introduce CityEQA, a new task where an embodied agent answers\nopen-vocabulary questions through active exploration in dynamic city spaces. To\nsupport this task, we present CityEQA-EC, the first benchmark dataset featuring\n1,412 human-annotated tasks across six categories, grounded in a realistic 3D\nurban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel\nagent tailored for CityEQA. PMA enables long-horizon planning and hierarchical\ntask execution: the Planner breaks down the question answering into sub-tasks,\nthe Manager maintains an object-centric cognitive map for spatial reasoning\nduring the process control, and the specialized Actors handle navigation,\nexploration, and collection sub-tasks. Experiments demonstrate that PMA\nachieves 60.7% of human-level answering accuracy, significantly outperforming\nfrontier-based baselines. While promising, the performance gap compared to\nhumans highlights the need for enhanced visual reasoning in CityEQA. This work\npaves the way for future advancements in urban spatial intelligence. Dataset\nand code are available at https:\/\/github.com\/BiluYong\/CityEQA.git.",
        "Accurate and systematic evaluation of mobile agents can significantly advance\ntheir development and real-world applicability. However, existing benchmarks\nfor mobile agents lack practicality and scalability due to the extensive manual\neffort required to define task reward signals and implement corresponding\nevaluation codes. To this end, we propose AutoEval, an autonomous agent\nevaluation framework that tests a mobile agent without any manual effort.\nFirst, we design a Structured Substate Representation to describe the UI state\nchanges while agent execution, such that task reward signals can be\nautomatically generated. Second, we utilize a Judge System that can\nautonomously evaluate agents' performance given the automatically generated\ntask reward signals. By providing only a task description, our framework\nevaluates agents with fine-grained performance feedback to that task without\nany extra manual effort. We implement a prototype of our framework and validate\nthe automatically generated task reward signals, finding over 93% coverage to\nhuman-annotated reward signals. Moreover, to prove the effectiveness of our\nautonomous Judge System, we manually verify its judge results and demonstrate\nthat it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile\nagents using our framework, providing detailed insights into their performance\ncharacteristics and limitations.",
        "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems.",
        "Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks.",
        "Immunohistochemistry (IHC) staining plays a significant role in the\nevaluation of diseases such as breast cancer. The H&E-to-IHC transformation\nbased on generative models provides a simple and cost-effective method for\nobtaining IHC images. Although previous models can perform digital coloring\nwell, they still suffer from (i) coloring only through the pixel features that\nare not prominent in HE, which is easy to cause information loss in the\ncoloring process; (ii) The lack of pixel-perfect H&E-IHC groundtruth pairs\nposes a challenge to the classical L1 loss.To address the above challenges, we\npropose an adaptive information enhanced coloring framework based on feature\nextractors. We first propose the VMFE module to effectively extract the color\ninformation features using multi-scale feature extraction and wavelet transform\nconvolution, while combining the shared decoder for feature fusion. The\nhigh-performance dual feature extractor of H&E-IHC is trained by contrastive\nlearning, which can effectively perform feature alignment of HE-IHC in high\nlatitude space. At the same time, the trained feature encoder is used to\nenhance the features and adaptively adjust the loss in the HE section staining\nprocess to solve the problems related to unclear and asymmetric information. We\nhave tested on different datasets and achieved excellent performance.Our code\nis available at https:\/\/github.com\/babyinsunshine\/CEFF",
        "Extended Reality (XR) experiences involve interactions between users, the\nreal world, and virtual content. A key step to enable these experiences is the\nXR headset sensing and estimating the user's pose in order to accurately place\nand render virtual content in the real world. XR headsets use multiple sensors\n(e.g., cameras, inertial measurement unit) to perform pose estimation and\nimprove its robustness, but this provides an attack surface for adversaries to\ninterfere with the pose estimation process. In this paper, we create and study\nthe effects of acoustic attacks that create false signals in the inertial\nmeasurement unit (IMU) on XR headsets, leading to adverse downstream effects on\nXR applications. We generate resonant acoustic signals on a HoloLens 2 and\nmeasure the resulting perturbations in the IMU readings, and also demonstrate\nboth fine-grained and coarse attacks on the popular ORB-SLAM3 and an\nopen-source XR system (ILLIXR). With the knowledge gleaned from attacking these\nopen-source frameworks, we demonstrate four end-to-end proof-of-concept attacks\non a HoloLens 2: manipulating user input, clickjacking, zone invasion, and\ndenial of user interaction. Our experiments show that current commercial XR\nheadsets are susceptible to acoustic attacks, raising concerns for their\nsecurity.",
        "Space layout design (SLD), occurring in the early stages of the design\nprocess, nonetheless influences both the functionality and aesthetics of the\nultimate architectural outcome. The complexity of SLD necessitates innovative\napproaches to efficiently explore vast solution spaces. While image-based\ngenerative AI has emerged as a potential solution, they often rely on\npixel-based space composition methods that lack intuitive representation of\narchitectural processes. This paper leverages deep Reinforcement Learning (RL),\nas it offers a procedural approach that intuitively mimics the process of human\ndesigners. Effectively using RL for SLD requires an explorative space composing\nmethod to generate desirable design solutions. We introduce \"laser-wall\", a\nnovel space partitioning method that conceptualizes walls as emitters of\nimaginary light beams to partition spaces. This approach bridges vector-based\nand pixel-based partitioning methods, offering both flexibility and exploratory\npower in generating diverse layouts. We present two planning strategies:\none-shot planning, which generates entire layouts in a single pass, and dynamic\nplanning, which allows for adaptive refinement by continuously transforming\nlaser-walls. Additionally, we introduce on-light and off-light wall\ntransformations for smooth and fast layout refinement, as well as identity-less\nand identity-full walls for versatile room assignment. We developed\nSpaceLayoutGym, an open-source OpenAI Gym compatible simulator for generating\nand evaluating space layouts. The RL agent processes the input design scenarios\nand generates solutions following a reward function that balances geometrical\nand topological requirements. Our results demonstrate that the RL-based\nlaser-wall approach can generate diverse and functional space layouts that\nsatisfy both geometric constraints and topological requirements and is\narchitecturally intuitive.",
        "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding.",
        "Data pruning, or instance selection, is an important problem in machine\nlearning especially in terms of nearest neighbour classifier. However, in data\npruning which speeds up the prediction phase, there is an issue related to the\nspeed and efficiency of the process itself. In response, the study proposes an\napproach involving transforming the instance selection process into a\nclassification task conducted in a unified meta-feature space where each\ninstance can be classified and assigned to either the \"to keep\" or \"to remove\"\nclass. This approach requires training an appropriate meta-classifier, which\ncan be developed based on historical instance selection results from other\ndatasets using reference instance selection methods as a labeling tool. This\nwork proposes constructing the meta-feature space based on properties extracted\nfrom the nearest neighbor graph. Experiments conducted on 17 datasets of\nvarying sizes and five reference instance selection methods (ENN, Drop3, ICF,\nHMN-EI, and CCIS) demonstrate that the proposed solution achieves results\ncomparable to reference instance selection methods while significantly reducing\ncomputational complexity. In the proposed approach, the computational\ncomplexity of the system depends only on identifying the k-nearest neighbors\nfor each data sample and running the meta-classifier. Additionally, the study\ndiscusses the choice of meta-classifier, recommending the use of Balanced\nRandom Forest.",
        "Space-based gravitational wave detectors, such as the Laser Interferometer\nSpace Antenna (LISA), use picometer-precision laser interferometry to detect\ngravitational waves at frequencies from 1 Hz down to below 0.1 mHz. Laser\ninterferometers used for on-ground prototyping and testing of such instruments\nare typically constructed by permanently bonding or gluing optics onto an\nultra-stable bench made of low-expansion glass ceramic. This design minimizes\ntemperature coupling to length and tilt, which dominates the noise at low\nfrequencies due to finite temperature stability achievable in laboratories and\nvacuum environments. Here, we present the study of an alternative\nopto-mechanical concept where optical components are placed with adjustable and\nfreely positionable mounts on an ultra-stable bench, while maintaining\npicometer length stability. With this concept, a given interferometer\nconfiguration can be realised very quickly due to a simplified and speed-up\nassembly process, reducing the realisation time from weeks or months to a\nmatter of hours. We built a corresponding test facility and verified the length\nstability of our concept by measuring the length change in an optical cavity\nthat was probed with two different locking schemes, heterodyne laser frequency\nstabilisation and Pound-Drever-Hall locking. We studied the limitations of both\nlocking schemes and verified that the cavity length noise is below 1\npm\/sqrt(Hz) for frequencies down to 3 mHz. We thereby demonstrate that our\nconcept can simplify the testing of interferometer configurations and\nopto-mechanical components and is suitable to realise flexible optical ground\nsupport equipment for space missions that use laser interferometry, such as\nfuture space-based gravitational wave detectors and satellite geodesy missions.",
        "We implement the asymmetric dark matter framework, linking the ordinary and\ndark matter abundances, within a supersymmetric context. We consider a\nsupersymmetric model that respects an approximate $U(1)_R$ symmetry, which is\nbroken in such a way that at high temperature the $R$ breaking sector mediate\nprocesses in equilibrium, but at the SUSY mass scale, the sparticles asymmetry\nis frozen. In this framework, the gravitino serves as the dark matter\ncandidate, and its mass is predicted to be $\\sim10$ GeV to match the observed\nrelic abundance. We identify several realistic spectra; however, the\nrequirement for the Next-to-Lightest Supersymmetric Particle (NLSP) to decay\ninto the gravitino before Big Bang Nucleosynthesis constrains the viable\nspectrum to masses above 2 TeV.",
        "Context. As the Solar System orbits the Milky Way, it encounters various\nGalactic environments, including dense regions of the interstellar medium\n(ISM). These encounters can compress the heliosphere, exposing parts of the\nSolar System to the ISM, while also increasing the influx of interstellar dust\ninto the Solar System and Earth's atmosphere. The discovery of new Galactic\nstructures, such as the Radcliffe wave, raises the question of whether the Sun\nhas encountered any of them. Aims. The present study investigates the potential\npassage of the Solar System through the Radcliffe wave gas structure over the\npast 30 million years (Myr). Methods. We used a sample of 56 high-quality,\nyoung ($\\leq$ 30 Myr) open clusters associated with a region of interest of the\nRadcliffe wave to trace its motion back and investigate a potential crossing\nwith the Solar System's past orbit. Results. We find that the Solar System's\ntrajectory intersected the Radcliffe wave in the Orion region. We have\nconstrained the timing of this event to between 18.2 and 11.5 Myr ago, with the\nclosest approach occurring between 14.8 and 12.4 Myr ago. Notably, this period\ncoincides with the Middle Miocene climate transition on Earth, providing an\ninterdisciplinary link with paleoclimatology. The potential impact of the\ncrossing of the Radcliffe wave on the climate on Earth is estimated. This\ncrossing could also lead to anomalies in radionuclide abundances, which is an\nimportant research topic in the field of geology and nuclear astrophysics.",
        "Segment anything model (SAM) has shown impressive general-purpose\nsegmentation performance on natural images, but its performance on camouflaged\nobject detection (COD) is unsatisfactory. In this paper, we propose SAM-COD\nthat performs camouflaged object detection for RGB-D inputs. While keeping the\nSAM architecture intact, dual stream adapters are expanded on the image encoder\nto learn potential complementary information from RGB images and depth images,\nand fine-tune the mask decoder and its depth replica to perform dual-stream\nmask prediction. In practice, the dual stream adapters are embedded into the\nattention block of the image encoder in a parallel manner to facilitate the\nrefinement and correction of the two types of image embeddings. To mitigate\nchannel discrepancies arising from dual stream embeddings that do not directly\ninteract with each other, we augment the association of dual stream embeddings\nusing bidirectional knowledge distillation including a model distiller and a\nmodal distiller. In addition, to predict the masks for RGB and depth attention\nmaps, we hybridize the two types of image embeddings which are jointly learned\nwith the prompt embeddings to update the initial prompt, and then feed them\ninto the mask decoders to synchronize the consistency of image embeddings and\nprompt embeddings. Experimental results on four COD benchmarks show that our\nSAM-COD achieves excellent detection performance gains over SAM and achieves\nstate-of-the-art results with a given fine-tuning paradigm.",
        "Recommender systems for software engineering (RSSE) play a crucial role in\nautomating development tasks by providing relevant suggestions according to the\ndeveloper's context. However, they suffer from the so-called popularity bias,\ni.e., the phenomenon of recommending popular items that might be irrelevant to\nthe current task. In particular, the long-tail effect can hamper the system's\nperformance in terms of accuracy, thus leading to false positives in the\nprovided recommendations. Foundation models are the most advanced generative\nAI-based models that achieve relevant results in several SE tasks.\n  This paper aims to investigate the capability of large language models (LLMs)\nto address the popularity bias in recommender systems of third-party libraries\n(TPLs). We conduct an ablation study experimenting with state-of-the-art\ntechniques to mitigate the popularity bias, including fine-tuning and\npopularity penalty mechanisms. Our findings reveal that the considered LLMs\ncannot address the popularity bias in TPL recommenders, even though fine-tuning\nand post-processing penalty mechanism contributes to increasing the overall\ndiversity of the provided recommendations. In addition, we discuss the\nlimitations of LLMs in this context and suggest potential improvements to\naddress the popularity bias in TPL recommenders, thus paving the way for\nadditional experiments in this direction.",
        "Motivated by gravitational wave observations of binary neutron-star mergers,\nwe study the thermal index of low-density, high-temperature dense matter. We\nuse the virial expansion to account for nuclear interaction effects. We focus\non the region of validity of the expansion, which reaches $10^{-3}$ fm$^{-3}$\nat $T=5$ MeV up to almost saturation density at $T=50$ MeV. In pure neutron\nmatter, we find an analytical expression for the thermal index, and show that\nit is nearly density- and temperature-independent, within a fraction of a\npercent of the non-interacting, non-relativistic value of $\\Gamma_\\text{th}\n\\approx 5\/3$. When we incorporate protons, electrons and photons, we find that\nthe density and temperature dependence of the thermal index changes\nsignificantly. We predict a smooth transition between an electron-dominated\nregime with $\\Gamma_\\text{th} \\approx 4\/3$ at low densities to a\nneutron-dominated region with $\\Gamma_\\text{th} \\approx 5\/3$ at high densities.\nThis behavior is by and large independent of proton fraction and is not\naffected by nuclear interactions in the region where the virial expansion\nconverges. We model this smooth transition analytically and provide a simple\nbut accurate parametrization of the inflection point between these regimes.\nWhen compared to tabulated realistic models of the thermal index, we find an\noverall agreement at high temperatures that weakens for colder matter. The\ndiscrepancies can be attributed to the missing contributions of nuclear\nclusters. The virial approximation provides a clear and physically intuitive\nframework for understanding the thermal properties of dense matter, offering a\ncomputationally efficient solution that makes it particularly well-suited for\nthe regimes relevant to neutron star binary remnants.",
        "Diffusion models achieve superior performance in image generation tasks.\nHowever, it incurs significant computation overheads due to its iterative\nstructure. To address these overheads, we analyze this iterative structure and\nobserve that adjacent time steps in diffusion models exhibit high value\nsimilarity, leading to narrower differences between consecutive time steps. We\nadapt these characteristics to a quantized diffusion model and reveal that the\nmajority of these differences can be represented with reduced bit-width, and\neven zero. Based on our observations, we propose the Ditto algorithm, a\ndifference processing algorithm that leverages temporal similarity with\nquantization to enhance the efficiency of diffusion models. By exploiting the\nnarrower differences and the distributive property of layer operations, it\nperforms full bit-width operations for the initial time step and processes\nsubsequent steps with temporal differences. In addition, Ditto execution flow\noptimization is designed to mitigate the memory overhead of temporal difference\nprocessing, further boosting the efficiency of the Ditto algorithm. We also\ndesign the Ditto hardware, a specialized hardware accelerator, fully exploiting\nthe dynamic characteristics of the proposed algorithm. As a result, the Ditto\nhardware achieves up to 1.5x speedup and 17.74% energy saving compared to other\naccelerators.",
        "In this Letter, we demonstrate that the Symmetry Topological Field Theory\n(SymTFT) associated to a Quantum Field Theory (QFT) with continuous non-abelian\n$G$-flavor symmetry is a $BF$-theory with gauge group $G$. We show that gauging\n$G$-symmetry with a flat connection yields a theory with global symmetry\ncharacterized by exchanging the conjugate variables in the quantization of\n$BF$-theory. We construct the extended operators that generate the $G$-flavor\nsymmetry and the $(d-2)$-form $\\text{Rep}(G)$-symmetry of the gauged QFT. We\ndemonstrate that $BF$-theory arises as the theory characterizing $G$-flavor\nsymmetry of a QFT in the AdS\/CFT setup. 't Hooft anomalies of the $G$-flavor\nsymmetry are realized as extra terms in the action.",
        "This paper presents a novel indoor positioning approach that leverages\nantenna radiation pattern characteristics through Received Signal Strength\nIndication (RSSI) measurements in a single-antenna system. By rotating the\nantenna or reconfiguring its radiation pattern, we derive a maximum likelihood\nestimation (MLE) algorithm that achieves near-optimal positioning accuracy\napproaching the Cramer-Rao lower bound (CRLB). Through theoretical analysis, we\nestablish three fundamental theorems characterizing the estimation accuracy\nbounds and demonstrating how performance improves with increased\nsignal-to-noise ratio, antenna rotation count, and radiation pattern\nvariations. Additionally, we propose a two-position measurement strategy that\neliminates dependence on receiving antenna patterns. Simulation results\nvalidate that our approach provides an effective solution for indoor robot\ntracking applications where both accuracy and system simplicity are essential\nconsiderations."
      ]
    }
  },
  {
    "id":"2412.00036",
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Asset Prices with Overlapping Generations and Capital Accumulation:\n  Tirole (1985) Revisited",
        "Preventing Household Bankruptcy: The One-Third Rule in Financial\n  Planning with Mathematical Validation and Game-Theoretic Insights",
        "Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
        "Innovative Financing Solutions: A Transformative Driver for Financial\n  Performance of Businesses in Morocco",
        "Analysis of energy, CO2 emissions and economy of the technological\n  migration for clean cooking in Ecuador",
        "The role of FDI along transitional dynamics of the host country in an\n  endogenous growth model",
        "Recovering latent linkage structures and spillover effects with\n  structural breaks in panel data models",
        "Heterogeneity of household stock portfolios in a national market",
        "In-Sample and Out-of-Sample Sharpe Ratios for Linear Predictive Models",
        "Utilizing Pre-trained and Large Language Models for 10-K Items\n  Segmentation",
        "An End-To-End LLM Enhanced Trading System",
        "Towards a Formal Framework of the Ethereum Staking Market",
        "Measure of Morality: A Mathematical Theory of Egalitarian Ethics",
        "Membrane phononic crystals for high-Qm mechanical defect modes in\n  piezoelectric aluminum nitride",
        "Self-induced Josephson oscillations and self-trapping in a supersolid\n  dipolar quantum gas",
        "An 'Experimental Mathematics' Approach to Stolarsky Interspersions via\n  Automata Theory",
        "A New Type of MPd5 Kagome Superconductors",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "Intra-neuronal attention within language models Relationships between\n  activation and semantics",
        "Quantifying the Impact of the Dust Torque on the Migration of Low-mass\n  Planets II: The Role of Pebble Accretion in Planet Growth within a Global\n  Planet Formation Model",
        "Terahertz Cavity Phonon Polaritons in Lead Telluride in the Deep-Strong\n  Coupling Regime",
        "Short-Pulse Driven Radiofrequency X-Band Photoinjector: Electromagnetic\n  Properties and Beam Dynamics in the Transient Regime",
        "Vote Delegation in DeFi Governance",
        "Partial Quantum Shadow Tomography for Structured Operators and its\n  Experimental Demonstration using NMR",
        "Empirical Constraints on Tidal Dissipation in Exoplanet Host Stars",
        "K-theoretic Tate-Poitou duality at prime 2",
        "Observation of the nonlinear chiral thermoelectric Hall effect in\n  tellurium",
        "Inf-sup condition for Stokes with outflow condition"
      ],
      "abstract":[
        "We revisit the classic paper of Tirole \"Asset Bubbles and Overlapping\nGenerations\" (1985, Econometrica), which shows that the emergence of asset\nbubbles solves the capital over-accumulation problem. While Tirole's main\ninsight holds with pure bubbles (assets without dividends), we argue that his\noriginal analysis with a dividend-paying asset contains some issues. We provide\na fairly complete analysis of Tirole's model with general dividends such as\nequilibrium existence, uniqueness, and long-run behavior under weaker but\nexplicit assumptions and complement with examples based on closed-form\nsolutions. Some of the claims in Tirole (1985) require qualifications including\n(i) after the introduction of an asset with negligible dividends, the economy\nmay collapse towards zero capital stock (\"resource curse\") and (ii) the\nnecessity of bubbles is less clear-cut.",
        "This paper analyzes the 1\/3 Financial Rule, a method of allocating income\nequally among debt repayment, savings, and living expenses. Through\nmathematical modeling, game theory, behavioral finance, and technological\nanalysis, we examine the rule's potential for supporting household financial\nstability and reducing bankruptcy risk. The research develops theoretical\nfoundations using utility maximization theory, demonstrating how equal\nallocation emerges as a solution under standard economic assumptions. The\ngame-theoretic analysis explores the rule's effectiveness across different\nhousehold structures, revealing potential strategic advantages in financial\ndecision-making. We investigate psychological factors influencing financial\nchoices, including cognitive biases and neurobiological mechanisms that impact\neconomic behavior. Technological approaches, such as AI-driven personalization,\nblockchain tracking, and smart contract applications, are examined for their\npotential to support financial planning. Empirical validation using U.S. Census\ndata and longitudinal studies assesses the rule's performance across various\nhousehold types. Stress testing under different economic conditions provides\ninsights into its adaptability and resilience. The research integrates\nmathematical analysis with behavioral insights and technological perspectives\nto develop a comprehensive approach to household financial management.",
        "On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
        "In a rapidly evolving landscape marked by continuous change and complex\nchallenges, effective cash management stands as a cornerstone for ensuring\nbusiness sustainability and driving performance. To address these pressing\ndemands, cash managersare increasingly turning to innovative financing\nsolutions such as venture capital, green finance, crowdfunding, advanced\nservices from Pan-African banks, and blockchain technology. These cutting-edge\ntools are pivotal in bolstering resilience against market volatility,\necological transitions, and the accelerating pace of technological change. The\npresent article aims to examine how such innovative financial approaches can\nserve as strategic drivers, enabling businesses to transform challenges into\nopportunities. The analysis underscores that rethinking cash management through\ninnovation is a critical pathway toboost the performance of Moroccan companies.\nTherefore, embracing these forward-thinking strategies unlocks new avenues for\ndevelopment empowering them to adapt with agility amidst the uncertainties of a\nshifting environment.",
        "The objective of this study is to analyze the CO2 emissions and economic\nimpacts of the implementation of the National Efficient Cooking Program (NECP)\nin Ecuador, which aims to migrate the population from Liquefied Petroleum Gas\n(LPG)-based stoves to electric induction stoves. This program is rooted in the\ncurrent effort to change Ecuador's energy balance, with hydroelectric power\nexpected to generate 83.61% of national electricity by 2022, ending the need\nfor subsidized LPG. For this analysis, the 2014 baseline situation has been\ncompared with two future scenarios for 2022: a business-as-usual scenario and\nan NECP-success scenario. This study demonstrates the viability of migration\nfrom imported fossil fuels to locally-produced renewable energy as the basis\nfor an efficient cooking facility. The new policies scenario would save US$\n1.162 billion in annual government expenditure on cooking subsidies, and\nreducing CO2 emissions associated to energy for cooking in 1.8 tCO2\/y.",
        "We investigate the role of foreign direct investment (FDI) in the\ntransitional dynamics of host countries by using an optimal growth model. FDI\nmay be beneficial for the host country because local people can work for\nmultinational firms to get a favorable salary. However, if the host country\nonly focuses on FDI, it may face a middle-income trap. We show that if the host\ncountry invests in research and development, its economy may have sustained\ngrowth. Moreover, in this case, FDI helps the host country only at the first\nstages of its development process.",
        "This paper introduces a framework to analyze time-varying spillover effects\nin panel data. We consider panel models where a unit's outcome depends not only\non its own characteristics (private effects) but also on the characteristics of\nother units (spillover effects). The linkage of units is allowed to be latent\nand may shift at an unknown breakpoint. We propose a novel procedure to\nestimate the breakpoint, linkage structure, spillover and private effects. We\naddress the high-dimensionality of spillover effect parameters using penalized\nestimation, and estimate the breakpoint with refinement. We establish the\nsuper-consistency of the breakpoint estimator, ensuring that inferences about\nother parameters can proceed as if the breakpoint were known. The private\neffect parameters are estimated using a double machine learning method. The\nproposed method is applied to estimate the cross-country R&D spillovers, and we\nfind that the R&D spillovers become sparser after the financial crisis.",
        "We study the long term dynamics of the stock portfolios owned by single\nFinnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001\nand 2021. Using the Herfindahl-Hirschman index as a measure of concentration\nfor the composition of stock portfolios, we investigate the concentration of\nFinnish household portfolios both at the level of each individual household and\ntracking the time evolution of an aggregated Finnish household portfolio. We\nalso consider aggregated portfolios of two other macro categories of investors\none comprising Finnish institutional investors and the other comprising foreign\ninvestors. Different macro categories of investors present a different degree\nof concentration of aggregated stock portfolios with highest concentration\nobserved for foreign investors. For individual Finnish retail investors,\nportfolio concentration estimated by the Herfindahl-Hirschman index presents\nhigh values for more than half of the total number of retail investors. In\nspite of the observation that retail stock portfolios are often composed by\njust a few stocks, the concentration of the aggregated stock portfolio for\nFinnish retail investors has a portfolio concentration comparable with the one\nof Finnish institutional investors. Within retail investors, stock portfolios\nof women present a similar pattern of portfolios of men but with a systematic\nhigher level of concentration observed for women both at individual and at\naggregated level.",
        "We study how much the in-sample performance of trading strategies based on\nlinear predictive models is reduced out-of-sample due to overfitting. More\nspecifically, we compute the in- and out-of-sample means and variances of the\ncorresponding PnLs and use these to derive a closed-form approximation for the\ncorresponding Sharpe ratios. We find that the out-of-sample ``replication\nratio'' diminishes for complex strategies with many assets and based on many\nweak rather than a few strong trading signals, and increases when more training\ndata is used. The substantial quantitative importance of these effects is\nillustrated with an empirical case study for commodity futures following the\nmethodology of Garleanu-Pedersen.",
        "Extracting specific items from 10-K reports remains challenging due to\nvariations in document formats and item presentation. Traditional rule-based\nitem segmentation approaches often yield suboptimal results. This study\nintroduces two advanced item segmentation methods leveraging language models:\n(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize\nGPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings\nwith a Bi-LSTM model in a hierarchical structure to overcome context window\nconstraints. Trained and evaluated on 3,737 annotated 10-K reports,\nBERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),\nconditional random field (0.9818), and rule-based methods (0.9048) for core\nitems (1, 1A, 3, and 7). These approaches enhance item segmentation\nperformance, improving text analytics in accounting and finance. BERT4ItemSeg\noffers satisfactory item segmentation performance, while GPT4ItemSeg can easily\nadapt to regulatory changes. Together, they offer practical benefits for\nresearchers and practitioners, enabling reliable empirical studies and\nautomated 10-K item segmentation functionality.",
        "This project introduces an end-to-end trading system that leverages Large\nLanguage Models (LLMs) for real-time market sentiment analysis. By synthesizing\ndata from financial news and social media, the system integrates\nsentiment-driven insights with technical indicators to generate actionable\ntrading signals. FinGPT serves as the primary model for sentiment analysis,\nensuring domain-specific accuracy, while Kubernetes is used for scalable and\nefficient deployment.",
        "This paper studies how different categories of Ethereum stakers respond to\nchanges in the consensus issuance schedule and how such changes may impact the\ncomposition of the staking market. Our findings show that solo stakers are more\nsensitive to variations in staking rewards compared to ETH holders using\ncentralized exchanges or liquid staking providers. This heightened sensitivity\nis not explained by the assumptions on cost structure, which would suggest the\nopposite, but is instead driven by market dynamics. Factors such as superior\nMEV access and DeFi yields available to other staking methods, as well as the\npotential presence of inattentive stakers, contribute to this effect.\nConsequently, a reduction in issuance is likely to crowd out solo stakers while\nincreasing the market share of centralized entities, further concentrating\nvalidator power.",
        "This paper develops a rigorous mathematical framework for egalitarian ethics\nby integrating formal tools from economics and mathematics. We motivate the\nformalism by investigating the limitations of conventional informal approaches\nby constructing examples such as probabilistic variant of the trolley dilemma\nand comparisons of unequal distributions. Our formal model, based on canonical\nwelfare economics, simultaneously accounts for total utility and the\ndistribution of outcomes. The analysis reveals deficiencies in traditional\nstatistical measures and establishes impossibility theorems for rank-weighted\napproaches. We derive representation theorems that axiomatize key inequality\nmeasures including the Gini coefficient and a generalized Atkinson index,\nproviding a coherent, axiomatic foundation for normative philosophy.",
        "Nanomechanical resonators with exceptionally low dissipation are advancing\nmechanics-based sensors and quantum technologies. The key for these advances is\nthe engineering of localized phononic modes that are well-isolated from the\nenvironment, i.e., that exhibit a high mechanical quality factor, Qm. Membrane\nphononic crystals fabricated from strained thin films can realize high-Qm\nsingle or multiple localized phononic defect modes. These defect modes can be\nefficiently interfaced with out-of-plane light or capacitively coupled to a\nmicrowave quantum circuit, enabling readout and control of their motion. When\nmembrane phononic crystals are fabricated from a crystalline film, they could\noffer built-in functionality. We demonstrate a membrane phononic crystal\nrealized in a strained 90 nm-thin film of aluminum nitride (AlN), which is a\ncrystalline piezoelectric material. We engineer a high-Qm localized phononic\ndefect mode at 1.8 MHz with a Qxf-product of 1.5 10^13 Hz at room temperature.\nIn future devices, the built-in piezoelectricity of AlN can be utilized for\nin-situ tuning of mechanical mode frequencies, defect mode couplings, or\nacoustic bandgaps, which can be used as building blocks of tunable phononic\ncircuits.",
        "The Josephson effect characterizes superfluids and superconductors separated\nby a weak link, the so-called Josephson junction. A recent experiment has shown\nthat Josephson oscillations can be observed also in a supersolid, where the\nweak link is not due to an external barrier, but is self-induced by\ninterparticle interactions. Here we show theoretically that supersolids --\ndespite their self-induced character -- feature all the standard properties of\nbosonic Josephson junction arrays, including macroscopic quantum self-trapping.\nWe focus on the harmonically trapped dipolar supersolids of interest for\ncurrent experiments, and show that they can be described with a generalized\nJosephson model that takes into account spatial inhomogeneities. Our work\nshades new light on the dynamics of supersolids and opens the way to the study\nof a novel class of Josephson junctions.",
        "We look at the Stolarsky interspersions (such as the Wythoff array) one more\ntime, this time using tools from automata theory. These tools allow easy\nverification of many of the published results on these arrays, as well as\nproofs of new results.",
        "Kagome materials, which are composed of hexagons tiled with a shared\ntriangle, have inspired enormous interest due to their unique structures and\nrich physical properties, but exploring superconducting material systems with\nnew kagome structures is still an important research direction. Here, we\npredict a new type of kagome superconductors, MPd5 (M is a group IIA metal\nelement), and identify that they exhibit coexistence of superconductivity and\nnon-trivial topological properties. We uncover their phonon-mediated\nsuperconductivity by the density functional theory for superconductors (SCDFT),\npredicting the superconducting transition temperatures (Tc) of 2.64, 2.03, and\n1.50 K for CaPd5, SrPd5, and BaPd5, respectively, which can be effectively\ntuned by external pressure and electron doping. The present results also\ndemonstrate that MPd5 have various topological properties such as, CaPd5 shows\ntopological non-trivial intersection near the Fermi level (EF). Our results\nindicate that the MPd5 materials can be an emerging material platform with rich\nexotic physics in their kagome structures, and render themselves excellent\ncandidates for superconducting and advanced functional materials that could be\nutilized in topological quantum computing and information technology.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "This study investigates the ability of perceptron-type neurons in language\nmodels to perform intra-neuronal attention; that is, to identify different\nhomogeneous categorical segments within the synthetic thought category they\nencode, based on a segmentation of specific activation zones for the tokens to\nwhich they are particularly responsive. The objective of this work is therefore\nto determine to what extent formal neurons can establish a homomorphic\nrelationship between activation-based and categorical segmentations. The\nresults suggest the existence of such a relationship, albeit tenuous, only at\nthe level of tokens with very high activation levels. This intra-neuronal\nattention subsequently enables categorical restructuring processes at the level\nof neurons in the following layer, thereby contributing to the progressive\nformation of high-level categorical abstractions.",
        "Although dust constitutes only about 1% of the mass of a protoplanetary disk,\nrecent studies demonstrate that it can exert a significant torque on low- and\nintermediate-mass planetary cores. We compute and quantify for the first time\nthe influence of the dust torque on the evolution of growing planetary embryos\nas they move in a protoplanetary disk while growing via gas and pebble\naccretion. Our global model evolves the gaseous disk via viscous accretion and\nX-ray photoevaporation, while accounting for dust growth and evolution\nincluding coagulation, drift, and fragmentation. Our research indicates that\ndust torque significantly influences planetary migration, particularly driving\nsubstantial outward migration for planets forming within the water ice-line.\nThis effect occurs due to an increased dust-to-gas mass ratio in the inner\ndisk, resulting from inward pebble drift from outer regions. In contrast, for\nplanets initially located beyond the water ice-line, the dust torque mitigates\ninward migration but does not significantly alter their paths, as the\ndust-to-gas ratio diminishes rapidly due to rapid pebble drift and the brief\ntimescales of planet formation in these areas. These findings underscore the\npivotal role of dust torque in shaping the migration patterns of low- and\nintermediate-mass planets, especially when enhanced dust concentrations in the\ninner disk amplify its effects",
        "Lead telluride is an important thermoelectric material due to its large\nSeebeck coefficient combined with its unusually low thermal conductivity that\nis related to the strong anharmonicity of phonons in this material. Here, we\nhave studied the resonant and nonperturbative coupling of transverse optical\nphonons in lead telluride with cavity photons inside small-mode-volume metallic\nmetasurface cavities that have photonic modes with terahertz frequencies. We\nobserved a giant vacuum Rabi splitting on the order of the bare phonon and\ncavity frequencies. Through terahertz time-domain spectroscopy experiments, we\nsystematically studied the vacuum Rabi splitting as a function of sample\nthickness, temperature, and cavity length. Under the strongest light-matter\ncoupling conditions, the strength of coupling exceeded the bare phonon and\ncavity frequencies, putting the system into the deep-strong coupling regime.\nThese results demonstrate that this uniquely tunable platform is promising for\nrealizing and understanding predicted cavity-vacuum-induced ferroelectric\ninstabilities and exploring applications of light-matter coupling in the\nultrastrong and deep-strong coupling regimes in quantum technology.",
        "This paper presents a study of the radiofrequency (RF) characteristics and\nbeam dynamics of an X-band photogun (Xgun) operating in the transient state.\nThe photoinjector is designed to operate with short RF pulses (9 ns) to achieve\nhigh accelerating gradients. Short-pulse operation potentially reduces\nbreakdown risks, as experimentally demonstrated by achieving a gradient\nexceeding 350 MV\/m. However, the short pulse duration causes the cavity to\noperate in a transient regime where the electromagnetic field deviates from the\nconventional steady-state condition. To investigate the effects of deviations\nfrom steady-state operation, the time-dependent spatial evolution of the cavity\nfields was examined using both 9-ns and 50-ns RF pulses. The 50-ns pulse served\nas a reference to characterize the cavity behavior under a fully filled\nsteady-state condition. Beam dynamics simulations were conducted to explore the\nimpact of transient RF effects on beam kinetic energy, transverse emittance,\nand bunch length; these simulations employed a new dynamic field mapping\napproach to model transient RF fields.",
        "We investigate the drivers of vote delegation in Decentralized Autonomous\nOrganizations (DAOs), using the Uniswap governance DAO as a laboratory. We show\nthat parties with fewer self-owned votes and those affiliated with the\ncontrolling venture capital firm, Andreesen Horowitz (a16z), receive more vote\ndelegations. These patterns suggest that while the Uniswap ecosystem values\ndecentralization, a16z may engage in window-dressing around it. Moreover, we\nfind that an active and successful track record in submitting improvement\nproposals, especially in the final stage, leads to more vote delegations,\nindicating that delegation in DAOs is at least partly reputation- or\nmerit-based. Combined, our findings provide new insights into how governance\nand decentralization operate in DeFi.",
        "Quantum shadow tomography based on the classical shadow representation\nprovides an efficient way to estimate properties of an unknown quantum state\nwithout performing a full quantum state tomography. In scenarios where\nestimating the expectation values for only certain classes of observables is\nrequired, obtaining information about the entire density matrix is unnecessary.\nWe propose a partial quantum shadow tomography protocol, which allows\nestimation of a subset of density matrix elements contributing to the\nexpectation values of certain classes of structured observables. This method\nutilizes tomographically incomplete subsets of single qubit Pauli basis\nmeasurements to perform partial shadow tomography, making it experimentally\nmore efficient. We demonstrate the advantage over unitary $k$-designs such as\nClifford, full Pauli basis, and methods utilizing mutually unbiased bases by\nnumerically analyzing the protocol for structured density matrices and\nobservables. We experimentally demonstrate the partial shadow estimation scheme\nfor a wide class of two-qubit states (pure, entangled, and mixed) in the\nnuclear magnetic resonance (NMR) platform, which relies on ensemble-based\nmeasurements. The full density matrix experimentally reconstructed by combining\ndifferent partial estimators produces fidelities exceeding 97%.",
        "The orbits of short-period exoplanets are sculpted by tidal dissipation.\nHowever, the mechanisms and associated efficiencies of these tidal interactions\nare poorly constrained. We present robust constraints on the tidal quality\nfactors of short-period exoplanetary host stars through the usage of a novel\nempirical technique. The method is based on analyzing structures in the\npopulation-level distribution of tidal decay times, defined as the time\nremaining before a planet spirals into its host star due to stellar tides.\nUsing simple synthetic planet population simulations and analytic theory, we\nshow that there exists a steady-state portion of the decay time distribution\nwith an approximately power-law form. This steady-state feature is clearly\nevident in the decay time distribution of the observed short-period planet\npopulation. We use this to constrain both the magnitude and frequency\ndependence of the stellar tidal quality factor and show that it must decrease\nsharply with planetary orbital period. Specifically, with $Q_{\\star}' = Q_0\n(P\/2 \\ \\mathrm{days})^{\\alpha}$, we find $10^{5.5} \\lesssim Q_0 \\lesssim 10^7$\nand $-4.33 \\lesssim \\alpha \\lesssim -2$. Our results are most consistent with\npredictions from tidal resonance locking, in which the planets are locked into\nresonance with a tidally excited gravity mode in their host stars.",
        "We extend the result of Blumberg and Mandell on K-theoretic Tate-Poitou\nduality at odd primes which serves as a spectral refinement of the classical\narithmetic Tate-Poitou duality. The duality is formulated for the\n$K(1)$-localized algebraic K-theory of the ring of $p$-integers in a number\nfield and its completion using the $\\bZ_p$-Anderson duality. This paper\ncompletes the picture by addressing the prime 2, where the real embeddings of\nnumber fields introduce extra complexities. As an application, we identify the\nhomotopy type at prime 2 of the homotopy fiber of the cyclotomic trace for the\nsphere spectrum in terms of the algebraic K-theory of the integers.",
        "The nonlinear thermoelectric effect is a key factor for realising\nunconventional thermoelectric phenomena, such as heat rectification and power\ngeneration using thermal fluctuations. Recent theoretical advances have\nindicated that chiral materials can host a variety of exotic nonlinear\nthermoelectric transport arising from inversion-symmetry breaking. However,\nexperimental demonstration has yet to be achieved. Here, we report the\nobservation of the nonlinear chiral thermoelectric Hall effect in chiral\ntellurium at room temperature, where a voltage is generated as a cross product\nof the temperature gradient and electric field. The resulting thermoelectric\nHall voltage is on the order of {\\mu}V, consistent with the prediction from the\nab initio calculation. Furthermore, the sign of the thermoelectric Hall voltage\nis reversed depending on the crystal chirality, demonstrating a novel\nfunctionality of sign control of the thermoelectric effect by the chirality\ndegrees of freedom. Our findings reveal the potential of chiral systems as\nnonlinear thermoelectric materials for advanced thermal management and energy\nharvesting.",
        "The inf-sup condition is one of the essential tools in the analysis of the\nStokes equations and especially in numerical analysis. In its usual form, the\ncondition states that for every pressure $p\\in L^2(\\Omega)\\setminus\n\\mathbb{R}$, (i.e. with mean value zero) a velocity $u\\in H^1_0(\\Omega)^d$ can\nbe found, so that $(div\\,u,p)=\\|p\\|^2$ and $\\|\\nabla u\\|\\le c \\|p\\|$ applies,\nwhere $c>0$ does not depend on $u$ and $p$. However, if we consider domains\nthat have a Neumann-type outflow condition on part of the boundary\n$\\Gamma_N\\subset\\partial\\Omega$, the inf-sup condition cannot be used in this\nform, since the pressure here comes from $L^2(\\Omega)$ and does not necessarily\nhave zero mean value. In this note, we derive the inf-sup condition for the\ncase of outflow boundaries."
      ]
    }
  },
  {
    "id":"2411.00640",
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "Cost-Saving LLM Cascades with Early Abstention",
        "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation",
        "Smart Cubing for Graph Search: A Comparative Study",
        "A Guide to Bayesian Networks Software Packages for Structure and\n  Parameter Learning -- 2025 Edition",
        "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
        "Logic Explanation of AI Classifiers by Categorical Explaining Functors",
        "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
        "COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open\n  Source Intelligence",
        "Efficient rule induction by ignoring pointless rules",
        "Heterogeneous Causal Discovery of Repeated Undesirable Health Outcomes",
        "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems",
        "Financial Wind Tunnel: A Retrieval-Augmented Market Simulator",
        "Functionalized Cr$_2$C MXenes: Novel Magnetic Semiconductors",
        "Constructing Sobolev orthonormal rational functions via an updating\n  procedure",
        "Deep Reinforcement Learning Enabled Persistent Surveillance with\n  Energy-Aware UAV-UGV Systems for Disaster Management Applications",
        "Event Soliton Formation in Mixed Energy-Momentum Gaps of Nonlinear\n  Spacetime Crystals",
        "RAPTOR: Refined Approach for Product Table Object Recognition",
        "Spin-flip Scattering at a Chiral Interface of Helical Chains",
        "Tutorial: Hong-Ou-Mandel interference with Structured Photons",
        "Following the state of an evaporating charged black hole into the\n  quantum gravity regime",
        "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning\n  Attacks",
        "Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal\n  Dependencies in Complex Data",
        "Quantum entanglement correlations in double quark PDFs",
        "Modelling Soil as a Living System: Feedback between Microbial Activity\n  and Spatial Structure",
        "Towards Understanding Text Hallucination of Diffusion Models via Local\n  Generation Bias",
        "Language Modelling for Speaker Diarization in Telephonic Interviews"
      ],
      "abstract":[
        "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
        "LLM cascades are based on the idea that processing all queries with the\nlargest and most expensive LLMs is inefficient. Instead, cascades deploy small\nLLMs to answer the majority of queries, limiting the use of large and expensive\nLLMs to only the most difficult queries. This approach can significantly reduce\ncosts without impacting performance. However, risk-sensitive domains such as\nfinance or medicine place an additional premium on avoiding model errors.\nRecognizing that even the most expensive models may make mistakes, applications\nin these domains benefit from allowing LLM systems to completely abstain from\nanswering a query when the chance of making a mistake is significant. However,\ngiving a cascade the ability to abstain poses an immediate design question for\nLLM cascades: should abstention only be allowed at the final model or also at\nearlier models? Since the error patterns of small and large models are\ncorrelated, the latter strategy may further reduce inference costs by letting\ninexpensive models anticipate abstention decisions by expensive models, thereby\nobviating the need to run the expensive models. We investigate the benefits of\n\"early abstention\" in LLM cascades and find that it reduces the overall test\nloss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA,\nTruthfulQA, and XSum). These gains result from a more effective use of\nabstention, which trades a 4.1% average increase in the overall abstention rate\nfor a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings\ndemonstrate that it is possible to leverage correlations between the error\npatterns of different language models to drive performance improvements for LLM\nsystems with abstention.",
        "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior.",
        "Parallel solving via cube-and-conquer is a key method for scaling SAT solvers\nto hard instances. While cube-and-conquer has proven successful for pure SAT\nproblems, notably the Pythagorean triples conjecture, its application to SAT\nsolvers extended with propagators presents unique challenges, as these\npropagators learn constraints dynamically during the search.\n  We study this problem using SAT Modulo Symmetries (SMS) as our primary test\ncase, where a symmetry-breaking propagator reduces the search space by learning\nconstraints that eliminate isomorphic graphs. Through extensive experimentation\ncomprising over 10,000 CPU hours, we systematically evaluate different\ncube-and-conquer variants on three well-studied combinatorial problems. Our\nmethodology combines prerun phases to collect learned constraints, various\ncubing strategies, and parameter tuning via algorithm configuration and\nLLM-generated design suggestions.\n  The comprehensive empirical evaluation provides new insights into effective\ncubing strategies for propagator-based SAT solving, with our best method\nachieving speedups of 2-3x from improved cubing and parameter tuning, providing\nan additional 1.5-2x improvement on harder instances.",
        "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
        "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
        "The most common methods in explainable artificial intelligence are post-hoc\ntechniques which identify the most relevant features used by pretrained opaque\nmodels. Some of the most advanced post hoc methods can generate explanations\nthat account for the mutual interactions of input features in the form of logic\nrules. However, these methods frequently fail to guarantee the consistency of\nthe extracted explanations with the model's underlying reasoning. To bridge\nthis gap, we propose a theoretically grounded approach to ensure coherence and\nfidelity of the extracted explanations, moving beyond the limitations of\ncurrent heuristic-based approaches. To this end, drawing from category theory,\nwe introduce an explaining functor which structurally preserves logical\nentailment between the explanation and the opaque model's reasoning. As a proof\nof concept, we validate the proposed theoretical constructions on a synthetic\nbenchmark verifying how the proposed approach significantly mitigates the\ngeneration of contradictory or unfaithful explanations.",
        "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
        "Open Source Intelligence (OSINT) requires the integration and reasoning of\ndiverse multimodal data, presenting significant challenges in deriving\nactionable insights. Traditional approaches, including multimodal large\nlanguage models (MLLMs), often struggle to infer complex contextual\nrelationships or deliver comprehensive intelligence from unstructured data\nsources. In this paper, we introduce COSINT-Agent, a knowledge-driven\nmultimodal agent tailored to address the challenges of OSINT in the Chinese\ndomain. COSINT-Agent seamlessly integrates the perceptual capabilities of\nfine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene\nKnowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match\nframework, which bridges COSINT-MLLM and EES-KG, enabling systematic\nextraction, reasoning, and contextualization of multimodal insights. This\nintegration facilitates precise entity recognition, event interpretation, and\ncontext retrieval, effectively transforming raw multimodal data into actionable\nintelligence. Extensive experiments validate the superior performance of\nCOSINT-Agent across core OSINT tasks, including entity recognition, EES\ngeneration, and context matching. These results underscore its potential as a\nrobust and scalable solution for advancing automated multimodal reasoning and\nenhancing the effectiveness of OSINT methodologies.",
        "The goal of inductive logic programming (ILP) is to find a set of logical\nrules that generalises training examples and background knowledge. We introduce\nan ILP approach that identifies pointless rules. A rule is pointless if it\ncontains a redundant literal or cannot discriminate against negative examples.\nWe show that ignoring pointless rules allows an ILP system to soundly prune the\nhypothesis space. Our experiments on multiple domains, including visual\nreasoning and game playing, show that our approach can reduce learning times by\n99% whilst maintaining predictive accuracies.",
        "Understanding factors triggering or preventing undesirable health outcomes\nacross patient subpopulations is essential for designing targeted\ninterventions. While randomized controlled trials and expert-led patient\ninterviews are standard methods for identifying these factors, they can be\ntime-consuming and infeasible. Causal discovery offers an alternative to\nconventional approaches by generating cause-and-effect hypotheses from\nobservational data. However, it often relies on strong or untestable\nassumptions, which can limit its practical application. This work aims to make\ncausal discovery more practical by considering multiple assumptions and\nidentifying heterogeneous effects. We formulate the problem of discovering\ncauses and effect modifiers of an outcome, where effect modifiers are contexts\n(e.g., age groups) with heterogeneous causal effects. Then, we present a novel,\nend-to-end framework that incorporates an ensemble of causal discovery\nalgorithms and estimation of heterogeneous effects to discover causes and\neffect modifiers that trigger or inhibit the outcome. We demonstrate that the\nensemble approach improves robustness by enhancing recall of causal factors\nwhile maintaining precision. Our study examines the causes of repeat emergency\nroom visits for diabetic patients and hospital readmissions for ICU patients.\nOur framework generates causal hypotheses consistent with existing literature\nand can help practitioners identify potential interventions and patient\nsubpopulations to focus on.",
        "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.",
        "Market simulator tries to create high-quality synthetic financial data that\nmimics real-world market dynamics, which is crucial for model development and\nrobust assessment. Despite continuous advancements in simulation methodologies,\nmarket fluctuations vary in terms of scale and sources, but existing frameworks\noften excel in only specific tasks. To address this challenge, we propose\nFinancial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to\ngenerate controllable, reasonable, and adaptable market dynamics for model\ntesting. FWT offers a more comprehensive and systematic generative capability\nacross different data frequencies. By leveraging a retrieval method to discover\ncross-sectional information as the augmented condition, our diffusion-based\nsimulator seamlessly integrates both macro- and micro-level market patterns.\nFurthermore, our framework allows the simulation to be controlled with wide\napplicability, including causal generation through \"what-if\" prompts or\nunprecedented cross-market trend synthesis. Additionally, we develop an\nautomated optimizer for downstream quantitative models, using stress testing of\nsimulated scenarios via FWT to enhance returns while controlling risks.\nExperimental results demonstrate that our approach enables the generalizable\nand reliable market simulation, significantly improve the performance and\nadaptability of downstream models, particularly in highly complex and volatile\nmarket conditions. Our code and data sample is available at\nhttps:\/\/anonymous.4open.science\/r\/fwt_-E852",
        "We report an \\textit{ab initio} investigation of functionalized and\n3$d$-electrons doped Cr$_2$C MXenes. Upon functionalization, the Cr$_2$C\nbecomes chemically, dynamically, and mechanically stable, and it exhibits\nmagnetic semiconducting behavior. Cr$_2$CF$_2$ stands out as a wide band gap\nsemiconductor, possessing super exchange interaction mediated by F atoms within\nthe layer, however, the applied strain transforms it from an indirect to a\ndirect band gap semiconductor. Strong spin-phonon coupling found in\nCr$_2$CH$_2$ is supported by the distorted Cr spin density due to hydrogen\nenvironment. Two magnon branches, associated with two sub-lattice spins, are\nfound in the ferromagnetic Cr$_2$CO$_2$ and antiferromagnetic Cr$_2$CF$_2$.\nDepending on the types of 3$d$-electron dopants and functionalization, Cr$_2$C\nMXenes (except for Cr$_2$CO$_2$) change from the indirect band gap magnetic\nsemiconductor to different states of electronic and magnetic matter including\nexotic direct band gap magnetic bipolar semiconductor. In addition, we reveal a\nband inversion between the two highest valence bands in the Fe-doped\nCr$_2$CCl$_2$.",
        "In this paper, we generate the recursion coefficients for rational functions\nwith prescribed poles that are orthonormal with respect to a continuous Sobolev\ninner product. Using a rational Gauss quadrature rule, the inner product can be\ndiscretized, thus allowing a linear algebraic approach. The presented approach\ninvolves reformulating the problem as an inverse eigenvalue problem involving a\nHessenberg pencil, where the pencil will contain the recursion coefficients\nthat generate the sequence of Sobolev orthogonal rational functions. This\nreformulation is based on the connection between Sobolev orthonormal rational\nfunctions and the orthonormal bases for rational Krylov subspaces generated by\na Jordan-like matrix. An updating procedure, introducing the nodes of the inner\nproduct one after the other, is proposed and the performance is examined\nthrough some numerical examples.",
        "Integrating Unmanned Aerial Vehicles (UAVs) with Unmanned Ground Vehicles\n(UGVs) provides an effective solution for persistent surveillance in disaster\nmanagement. UAVs excel at covering large areas rapidly, but their range is\nlimited by battery capacity. UGVs, though slower, can carry larger batteries\nfor extended missions. By using UGVs as mobile recharging stations, UAVs can\nextend mission duration through periodic refueling, leveraging the\ncomplementary strengths of both systems. To optimize this energy-aware UAV-UGV\ncooperative routing problem, we propose a planning framework that determines\noptimal routes and recharging points between a UAV and a UGV. Our solution\nemploys a deep reinforcement learning (DRL) framework built on an\nencoder-decoder transformer architecture with multi-head attention mechanisms.\nThis architecture enables the model to sequentially select actions for visiting\nmission points and coordinating recharging rendezvous between the UAV and UGV.\nThe DRL model is trained to minimize the age periods (the time gap between\nconsecutive visits) of mission points, ensuring effective surveillance. We\nevaluate the framework across various problem sizes and distributions,\ncomparing its performance against heuristic methods and an existing\nlearning-based model. Results show that our approach consistently outperforms\nthese baselines in both solution quality and runtime. Additionally, we\ndemonstrate the DRL policy's applicability in a real-world disaster scenario as\na case study and explore its potential for online mission planning to handle\ndynamic changes. Adapting the DRL policy for priority-driven surveillance\nhighlights the model's generalizability for real-time disaster response.",
        "We report the formation of a novel soliton, termed event soliton, in\nnonlinear photonic spacetime crystals (STCs). In these media, simultaneous\nspatiotemporal periodic modulation of the dielectric constant generates mixed\nfrequency (${\\omega}$) and wavevector (k) gaps. Under Kerr nonlinearity, the\nevent solitons emerge as fully localized entities in both spacetime and\nenergy-momentum domains, providing a tangible demonstration of the concept of\nevent in relativity. The ${\\omega}$k-gap mixture arises from the coexistence\nand competition between time reflected and Bragg reflected waves due to the\nspatiotemporal modulation. We propose a new partial differential equation to\ncapture various spatiotemporal patterns and present numerical simulations to\nvalidate our theoretical predictions, reflecting a three-way balance among\nk-gap opening, ${\\omega}$-gap opening, and nonlinearity. Our work opens avenues\nfor fundamental studies and fosters experimental prospects for implementing\nspacetime crystals in both time-varying photonics and periodically driven\ncondensed matter systems.",
        "Extracting tables from documents is a critical task across various\nindustries, especially on business documents like invoices and reports.\nExisting systems based on DEtection TRansformer (DETR) such as TAble\nTRansformer (TATR), offer solutions for Table Detection (TD) and Table\nStructure Recognition (TSR) but face challenges with diverse table formats and\ncommon errors like incorrect area detection and overlapping columns. This\nresearch introduces RAPTOR, a modular post-processing system designed to\nenhance state-of-the-art models for improved table extraction, particularly for\nproduct tables. RAPTOR addresses recurrent TD and TSR issues, improving both\nprecision and structural predictions. For TD, we use DETR (trained on ICDAR\n2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies\non TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module\nparameters, using a private dataset of product tables to align with industrial\nneeds. We evaluate our method on two private datasets of product tables, the\npublic DOCILE dataset (which contains tables similar to our target product\ntables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate\nthat while our approach excels at product tables, it also maintains reasonable\nperformance across diverse table formats. An ablation study further validates\nthe contribution of each module in our system.",
        "We investigate spin-flip scattering processes of electrons when they pass a\nchiral interface, which is the boundary between right- and left-handed\none-dimensional chain. We construct a minimal $p$-orbital model consisting of\nthe right- and left-handed one-dimensional threefold helical chains connected\nat $z=0$ with the nearest neighbor hopping and the spin-orbit coupling. The\ndynamics of spin-polarized wave packet passing through the interface, the\nGreen's functions, and electronic states near the interface are analyzed\nnumerically. We find that the microscopic structure of the interface is\nimportant and this strongly affects the local electronic orbital state. This in\naddition to the spin-orbit coupling determines whether the spin flip occurs or\nnot at the chiral interface and suggests a possible spin transport control by\nthe orbital configuration at the chiral interface.",
        "The Hong-Ou-Mandel (HOM) effect, an effective two-photon interference\nphenomenon, is a cornerstone of quantum optics and a key tool for linear\noptical quantum information processing. While the HOM effect has been\nextensively studied both theoretically and experimentally for various photonic\nquantum states, particularly in the spectral domain, detailed overviews of its\nbehaviour for structured photons -- those with complex spatial profiles --\nunder arbitrary spatial mode measurement schemes are still lacking. This\ntutorial aims to fill this gap by providing a comprehensive theoretical\nanalysis of the HOM effect for structured photons, including an arbitrary mode\nprojection on quantum interference outcomes. The tutorial also provides\nanalytical, closed-form expressions of the HOM visibility under different\nmeasurement conditions, which is a crucial contribution for its application in\ncomputational and artificial-intelligence-driven discovery of new quantum\nexperiments exploiting the power of photons with complex spatial modes.",
        "We study the energy probability density function of an evaporating\nnear-extremal charged black hole. At sufficiently low energies, such black\nholes experience large quantum metric fluctuations in the $AdS_{2}$ throat\nwhich are governed by a Schwarzian action. These fluctuations modify Hawking\nevaporation rates, and therefore also affect how the black hole state evolves\nover time. In previous work on Schwarzian-corrected Hawking radiation, the\nblack hole was taken to be in the microcanonical or canonical ensemble\n[arXiv:2411.03447]. However, we find that an initially fixed-energy or\nfixed-temperature state does not remain so in the regime where Schwarzian\ncorrections are important. We consider three decay channels: the emission of\nmassless scalars, photons, and entangled pairs of photons in angular momentum\nsinglet states. In each of the three cases, we find that in the very low\nenergy, quantum dominated regime, the probability distribution of the black\nhole energy level occupation tends toward a particular attractor function that\neffectively depends on only one combination of time and energy. This function\nis independent of the initial state and gives new predictions for the energy\nfluxes and Hawking emission spectra of near-extremal charged black holes.",
        "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks.",
        "Causality in time series can be difficult to determine, especially in the\npresence of non-linear dependencies. The concept of Granger causality helps\nanalyze potential relationships between variables, thereby offering a method to\ndetermine whether one time series can predict-Granger cause-future values of\nanother. Although successful, Granger causal methods still struggle with\ncapturing long-range relations between variables. To this end, we leverage the\nrecently successful Extended Long Short-Term Memory (xLSTM) architecture and\npropose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between\nthe time series components by using a novel dynamic lass penalty on the initial\nprojection. Specifically, we adaptively improve the model and identify sparsity\ncandidates. Our joint optimization procedure then ensures that the Granger\ncausal relations are recovered in a robust fashion. Our experimental\nevaluations on three datasets demonstrate the overall efficacy of our proposed\nGC-xLSTM model.",
        "Methods from Quantum Information Theory are used to scrutinize quantum\ncorrelations encoded in the two-quark density matrix over light-cone momentum\nfractions $x_1$ and $x_2$. A non-perturbative three quark model light-cone\nwavefunction predicts significant non-classical correlations associated with\nthe \"entanglement negativity\" measure for asymmetric and small quark momentum\nfractions. We perform one step of QCD scale evolution of the entire density\nmatrix, not just its diagonal (dPDF), by computing collinearly divergent\ncorrections due to the emission of a gluon. Finally, we present first\nqualitative numerical results for single-step scale evolution of quantum\nentanglement correlations in double quark PDFs. At a higher $Q^2$ scale, the\nnon-classical correlations manifest in the dPDF for nearly symmetric momentum\nfractions.",
        "Soil is a complex, dynamic material, with physical properties that depend on\nits biological content. We propose a cellular automaton model for\nself-organizing soil structure, where soil aggregates and serves as food for\nmicrobial species. These, in turn, produce nutrients that facilitate\nself-amplification, establishing a cyclical dynamic of consumption and\nregeneration. Our model explores the spatial interactions between these\ncomponents and their role in sustaining a balanced ecosystem. The main results\ndemonstrate that (1) spatial structure supports a stable living state,\npreventing population collapse or uncontrolled growth; (2) the spatial model\nallows for the coexistence of parasitic species, which exploit parts of the\nsystem without driving it to extinction; and (3) optimal growth conditions for\nmicrobes are associated to diverse length scales in the soil structure,\nsuggesting that heterogeneity is key to ecosystem resilience. These findings\nhighlight the importance of spatio-temporal dynamics of life in soil ecology.",
        "Score-based diffusion models have achieved incredible performance in\ngenerating realistic images, audio, and video data. While these models produce\nhigh-quality samples with impressive details, they often introduce unrealistic\nartifacts, such as distorted fingers or hallucinated texts with no meaning.\nThis paper focuses on textual hallucinations, where diffusion models correctly\ngenerate individual symbols but assemble them in a nonsensical manner. Through\nexperimental probing, we consistently observe that such phenomenon is\nattributed it to the network's local generation bias. Denoising networks tend\nto produce outputs that rely heavily on highly correlated local regions,\nparticularly when different dimensions of the data distribution are nearly\npairwise independent. This behavior leads to a generation process that\ndecomposes the global distribution into separate, independent distributions for\neach symbol, ultimately failing to capture the global structure, including\nunderlying grammar. Intriguingly, this bias persists across various denoising\nnetwork architectures including MLP and transformers which have the structure\nto model global dependency. These findings also provide insights into\nunderstanding other types of hallucinations, extending beyond text, as a result\nof implicit biases in the denoising models. Additionally, we theoretically\nanalyze the training dynamics for a specific case involving a two-layer MLP\nlearning parity points on a hypercube, offering an explanation of its\nunderlying mechanism.",
        "The aim of this paper is to investigate the benefit of combining both\nlanguage and acoustic modelling for speaker diarization. Although conventional\nsystems only use acoustic features, in some scenarios linguistic data contain\nhigh discriminative speaker information, even more reliable than the acoustic\nones. In this study we analyze how an appropriate fusion of both kind of\nfeatures is able to obtain good results in these cases. The proposed system is\nbased on an iterative algorithm where a LSTM network is used as a speaker\nclassifier. The network is fed with character-level word embeddings and a GMM\nbased acoustic score created with the output labels from previous iterations.\nThe presented algorithm has been evaluated in a Call-Center database, which is\ncomposed of telephone interview audios. The combination of acoustic features\nand linguistic content shows a 84.29% improvement in terms of a word-level DER\nas compared to a HMM\/VB baseline system. The results of this study confirms\nthat linguistic content can be efficiently used for some speaker recognition\ntasks."
      ]
    }
  },
  {
    "id":"2411.00640",
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Evaluating and Testing for Actionable Treatment Effect Heterogeneity",
        "Comparative analysis and practical applications of cubic transmutations\n  for the Pareto distribution",
        "Sampling from Density power divergence-based Generalized posterior\n  distribution via Stochastic optimization",
        "A note on local parameter orthogonality for multivariate data and the\n  Whittle algorithm for multivariate autoregressive models",
        "A step-by-step guide to generalized estimating equations using SPSS in\n  the field of dentistry",
        "Score-Based Causal Discovery with Temporal Background Information",
        "A generalized distance covariance framework for genome-wide association\n  studies",
        "Integrative Analysis of High-dimensional RCT and RWD Subject to\n  Censoring and Hidden Confounding",
        "Sparsity learning via structured functional factor augmentation",
        "Measuring Inaccuracies in the Proportional Hazard Rate Model based on\n  Extropy using a Length-Biased Weighted Residual approach",
        "A Population Sampling Framework for Claim Reserving in General Insurance",
        "Semiparametric Inference for Partially Identifiable Data Fusion\n  Estimands via Double Machine Learning",
        "Design of Bayesian Clinical Trials with Clustered Data and Multiple\n  Endpoints",
        "The Dust Polarisation and Magnetic Field Structure in the Centre of\n  NGC253 with ALMA",
        "Mind the Gap: Evaluating Patch Embeddings from General-Purpose and\n  Histopathology Foundation Models for Cell Segmentation and Classification",
        "Prospects for measuring neutrino mass with 21-cm forest",
        "Stochastic conformal integrators for linearly damped stochastic Poisson\n  systems",
        "Enhanced superconducting correlations in the Emery model and its\n  connections to strange metallic transport and normal state coherence",
        "FF7: A Code Package for High-throughput Calculations and Constructing\n  Materials Database",
        "A three-body form factor at sub-leading power in the high-energy limit:\n  planar contributions",
        "Constraints on flavor-dependent long-range interactions of high-energy\n  astrophysical neutrinos",
        "Variational formulation of planar linearized elasticity with\n  incompatible kinematics",
        "A Hierarchical Shock Model of Ultra-High-Energy Cosmic Rays",
        "Extending Israel-Stewart theory: Causal bulk viscosity at large\n  gradients",
        "DUNE-PRISM: Reducing neutrino interaction model dependence with a\n  movable neutrino detector",
        "Tailoring the Electronic Structure of Ni(111) by Alloying with Sb\n  Ad-Atoms",
        "On the Implementation of a Bayesian Optimization Framework for\n  Interconnected Systems",
        "Uniqueness of the strong positive solution for a general quasilinear\n  elliptic problem with variable exponents and homogeneous Neumann boundary\n  conditions using a generalization of the $p(x)$-D\\'{i}az-Saa inequality"
      ],
      "abstract":[
        "Developing tools for estimating heterogeneous treatment effects (HTE) and\nindividualized treatment effects has been an area of active research in recent\nyears. While these tools have proven to be useful in many contexts, a concern\nwhen deploying such methods is the degree to which incorporating HTE into a\nprediction model provides an advantage over predictive methods which do not\nallow for variation in treatment effect across individuals. To address this\nconcern, we propose a procedure which evaluates the extent to which an HTE\nmodel provides a predictive advantage. Specifically, our procedure targets the\ngain in predictive performance from using a flexible predictive model\nincorporating HTE versus an alternative model which is similar to the\nHTE-utilizing model except that it is constrained to not allow variation in\ntreatment effect. By drawing upon recent work in using nested cross-validation\ntechniques for prediction error inference, we generate confidence intervals for\nthis measure of gain in predictive performance which allows one to directly\ncalculate the level at which one is confident of a substantial HTE-modeling\ngain in prediction -- a quantity which we refer to as the h-value. Our\nprocedure is generic and can be directly used to assess the benefit of modeling\nHTE for any method that incorporates treatment effect variation.",
        "Transmutation is a technique for extending classical probability\ndistributions in order to give them more flexibility. In this paper, we are\ninterested in cubic transmutations of the Pareto distribution. We establish a\ngeneral formula that unifies existing cubic transmutations of the Pareto\ndistribution and facilitates the derivation of new cubic transmutations that\nhave not yet been explored in the literature. We also derive general formulas\nfor the related mathematical properties. Finally, we perform a comparative\nanalysis of the six transmutations existing in the literature using real-world\ndata. The results obtained confirm the flexibility and effectiveness of cubic\ntransmutations in modeling various types of data.",
        "Robust Bayesian inference using density power divergence (DPD) has emerged as\na promising approach for handling outliers in statistical estimation. While the\nDPD-based posterior offers theoretical guarantees for robustness, its practical\nimplementation faces significant computational challenges, particularly for\ngeneral parametric models with intractable integral terms. These challenges\nbecome especially pronounced in high-dimensional settings where traditional\nnumerical integration methods prove inadequate and computationally expensive.\nWe propose a novel sampling methodology that addresses these limitations by\nintegrating the loss-likelihood bootstrap with a stochastic gradient descent\nalgorithm specifically designed for DPD-based estimation. Our approach enables\nefficient and scalable sampling from DPD-based posteriors for a broad class of\nparametric models, including those with intractable integrals, and we further\nextend it to accommodate generalized linear models. Through comprehensive\nsimulation studies, we demonstrate that our method efficiently samples from\nDPD-based posteriors, offering superior computational scalability compared to\nconventional methods, particularly in high-dimensional settings. The results\nalso highlight its ability to handle complex parametric models with intractable\nintegral terms.",
        "This article extends the Cox--Reid local parameter orthogonality to a\nmultivariate setting, gives an affirmative reply to one of Cox and Reid's\nquestions, and shows that the extension can lead to efficient computational\nalgorithms with the celebrated Whittle algorithm for multivariate\nautoregressive modeling as a showcase.",
        "The Generalized Estimating Equations (GEE) approach is a widely used\nstatistical method for analyzing longitudinal data and clustered data in clinic\nstudies. In dentistry, due to multiple outcomes obtained from one patient, the\noutcomes produced from individual patients are correlated with one another.\nThis study focuses on the basic ideas of GEE and introduces the types of\ncovariance matrix and working correlation matrix. The quasi-likelihood\ninformation criterion(QIC) and quasi-likelihood information criterion\napproximation(QICu) were used to select the best working matrix and the best\nfitting model for the correlated outcomes.",
        "Temporal background information can improve causal discovery algorithms by\norienting edges and identifying relevant adjustment sets. We develop the\nTemporal Greedy Equivalence Search (TGES) algorithm and terminology essential\nfor score-based causal discovery with tiered background knowledge. TGES learns\na restricted Markov equivalence class of directed acyclic graphs (DAGs) using\nobservational data and tiered background knowledge. To construct TGES we\nformulate a scoring criterion that accounts for tiered background knowledge. We\nestablish theoretical results for TGES, stating that the algorithm always\nreturns a tiered maximally oriented partially directed acyclic graph (tiered\nMPDAG) and that this tiered MPDAG contains the true DAG in the large sample\nlimit. We present a simulation study indicating a gain from using tiered\nbackground knowledge and an improved precision-recall trade-off compared to the\ntemporal PC algorithm. We provide a real-world example on life-course health\ndata.",
        "When testing for the association of a single SNP with a phenotypic response,\none usually considers an additive genetic model, assuming that the mean of of\nthe response for the heterozygous state is the average of the means for the two\nhomozygous states. However, this simplification often does not hold. In this\npaper, we present a novel framework for testing the association of a single SNP\nand a phenotype. Different from the predominant standard approach, our\nmethodology is guaranteed to detect all dependencies expressed by classical\ngenetic association models. The asymptotic distribution under mild regularity\nassumptions is derived. Moreover, the finite sample distribution under\nGaussianity is provided in which the exact p-value can be efficiently evaluated\nvia the classical Appell hypergeometric series. Both results are extended to a\nregression-type setting with nuisance covariates, enabling hypotheses testing\nin a wide range of scenarios. A connection of our approach to score tests is\nexplored, leading to intuitive interpretations as locally most powerful tests.\nA simulation study demonstrates the computational efficiency and excellent\nstatistical performance of the proposed methodology. A real data example is\nprovided.",
        "In this study, we focus on estimating the heterogeneous treatment effect\n(HTE) for survival outcome. The outcome is subject to censoring and the number\nof covariates is high-dimensional. We utilize data from both the randomized\ncontrolled trial (RCT), considered as the gold standard, and real-world data\n(RWD), possibly affected by hidden confounding factors. To achieve a more\nefficient HTE estimate, such integrative analysis requires great insight into\nthe data generation mechanism, particularly the accurate characterization of\nunmeasured confounding effects\/bias. With this aim, we propose a\npenalized-regression-based integrative approach that allows for the\nsimultaneous estimation of parameters, selection of variables, and\nidentification of the existence of unmeasured confounding effects. The\nconsistency, asymptotic normality, and efficiency gains are rigorously\nestablished for the proposed estimate.\n  Finally, we apply the proposed method to estimate the HTE of lobar\/sublobar\nresection on the survival of lung cancer patients. The RCT is a multicenter\nnon-inferiority randomized phase 3 trial, and the RWD comes from a clinical\noncology cancer registry in the United States. The analysis reveals that the\nunmeasured confounding exists and the integrative approach does enhance the\nefficiency for the HTE estimation.",
        "As one of the most powerful tools for examining the association between\nfunctional covariates and a response, the functional regression model has been\nwidely adopted in various interdisciplinary studies. Usually, a limited number\nof functional covariates are assumed in a functional linear regression model.\nNevertheless, correlations may exist between functional covariates in\nhigh-dimensional functional linear regression models, which brings significant\nstatistical challenges to statistical inference and functional variable\nselection. In this article, a novel functional factor augmentation structure\n(fFAS) is proposed for multivariate functional series, and a multivariate\nfunctional factor augmentation selection model (fFASM) is further proposed to\ndeal with issues arising from variable selection of correlated functional\ncovariates. Theoretical justifications for the proposed fFAS are provided, and\nstatistical inference results of the proposed fFASM are established. Numerical\ninvestigations support the superb performance of the novel fFASM model in terms\nof estimation accuracy and selection consistency.",
        "In this paper, we consider the concept of the residual inaccuracy measure and\nextend it to its weighted version based on extropy. Properties of this measure\nare studied and the discrimination principle is applied in the class of\nproportional hazard rate (PHR) models. A characterization problem for the\nproposed weighted extropy-inaccuracy measure is studied. We propose some\nalternative expressions of weighted residual measure of inaccuracy.\nAdditionally, we establish upper and lower limits and various inequalities\nrelated to the weighted residual inaccuracy measure using extropy.\nNon-parametric estimators based on the kernel density estimation method and\nempirical distribution function for the proposed measure are obtained and the\nperformance of the estimators are also discussed using some simulation studies.\nFinally, a real dataset is applied for illustrating our new proposed measure.\nIn general, our study highlights the potential of the weighted residual\ninaccuracy measure based on extropy as a powerful tool for improving the\nquality and reliability of data analysis and modelling across various\ndisciplines. Researchers and practitioners can benefit from incorporating this\nmeasure into their analytical toolkit to enhance the accuracy and effectiveness\nof their work.",
        "Claim reserving in insurance has been studied through two primary frameworks:\nthe macro-level approach, which estimates reserves at an aggregate level (e.g.,\nChain-Ladder), and the micro-level approach, which estimates reserves at the\nindividual claim level Antonio and Plat (2014). These frameworks are based on\nfundamentally different theoretical foundations, creating a degree of\nincompatibility that limits the adoption of more flexible models. This paper\nintroduces a unified statistical framework for claim reserving, grounded in\npopulation sampling theory. We show that macro- and micro-level models\nrepresent extreme yet natural cases of an augmented inverse probability\nweighting (AIPW) estimator. This formulation allows for a seamless integration\nof principles from both aggregate and individual models, enabling more accurate\nand flexible estimations. Moreover, this paper also addresses critical issues\nof sampling bias arising from partially observed claims data-an often\noverlooked challenge in insurance. By adapting advanced statistical methods\nfrom the sampling literature, such as double-robust estimators, weighted\nestimating equations, and synthetic data generation, we improve predictive\naccuracy and expand the tools available for actuaries. The framework is\nillustrated using Canadian auto insurance data, highlighting the practical\nbenefits of the sampling-based methods.",
        "Many statistical estimands of interest (e.g., in regression or causality) are\nfunctions of the joint distribution of multiple random variables. But in some\napplications, data is not available that measures all random variables on each\nsubject, and instead the only possible approach is one of data fusion, where\nmultiple independent data sets, each measuring a subset of the random variables\nof interest, are combined for inference. In general, since all random variables\nare never observed jointly, their joint distribution, and hence also the\nestimand which is a function of it, is only partially identifiable.\nUnfortunately, the endpoints of the partially identifiable region depend in\ngeneral on entire conditional distributions, rendering them hard both\noperationally and statistically to estimate. To address this, we present a\nnovel outer-bound on the region of partial identifiability (and establish\nconditions under which it is tight) that depends only on certain conditional\nfirst and second moments. This allows us to derive semiparametrically efficient\nestimators of our endpoint outer-bounds that only require the standard machine\nlearning toolbox which learns conditional means. We prove asymptotic normality\nand semiparametric efficiency of our estimators and provide consistent\nestimators of their variances, enabling asymptotically valid confidence\ninterval construction for our original partially identifiable estimand. We\ndemonstrate the utility of our method in simulations and a data fusion problem\nfrom economics.",
        "In the design of clinical trials, it is essential to assess the design\noperating characteristics (i.e., the probabilities of making correct\ndecisions). Common practice for the evaluation of operating characteristics in\nBayesian clinical trials relies on estimating the sampling distribution of\nposterior summaries via Monte Carlo simulation. It is computationally intensive\nto repeat this estimation process for each design configuration considered,\nparticularly for clustered data that are analyzed using complex,\nhigh-dimensional models. In this paper, we propose an efficient method to\nassess operating characteristics and determine sample sizes for Bayesian trials\nwith clustered data and multiple endpoints. We prove theoretical results that\nenable posterior probabilities to be modelled as a function of the sample size.\nUsing these functions, we assess operating characteristics at a range of sample\nsizes given simulations conducted at only two sample sizes. These theoretical\nresults are also leveraged to quantify the impact of simulation variability on\nour sample size recommendations. The applicability of our methodology is\nillustrated using a current clinical trial with clustered data.",
        "Magnetic fields have an impact on galaxy evolution at multiple scales. They\nare particularly important for starburst galaxies, where they play a crucial\nrole in shaping the interstellar medium (ISM), influencing star formation\nprocesses and interacting with galactic outflows. The primary aim of this study\nis to obtain a parsec scale map of dust polarisation and B-field structure\nwithin the central starburst region of NGC253. This includes examining the\nrelationship between the morphology of B-fields, galactic outflows and the\nspatial distribution of super star clusters (SSC), to understand their combined\neffects on the galaxy's star formation and ISM. We used ALMA full polarisation\ndata in Bands 4 (145 GHz) and 7 (345 GHz) with resolution of 25 and 5 pc scale,\nrespectively. According to our SED fitting analysis, the observed Band 4\nemission is a combination of dust, synchrotron and free-free, while Band 7\ntraces only dust. The polarisation fraction (PF) of the synchrotron component\nis 2%, while that of the dust component is 0.3%. The B-fields orientation maps\nin both bands at common resolution show that the same B-fields structure is\ntraced by dust and synchrotron emission at scales of 25 pc. The B-field\nmorphology suggests a coupling with the multiphase outflow, while the\ndistribution of PF in Band 7 showed to be correlated with the presence of SSC.\nWe observed a significant anti-correlation between polarisation fraction and\ncolumn density in both Bands 4 and 7. A negative correlation between PF and\ndispersion angle function was observed in Band 4 but was nearly absent in Band\n7 at native resolution, suggesting that the tangling of B-field geometry along\nthe plane of the sky is the main cause of depolarisation at 25 pc scales, while\nother factors play a role at 5 pc scales.",
        "Recent advancements in foundation models have transformed computer vision,\ndriving significant performance improvements across diverse domains, including\ndigital histopathology. However, the advantages of domain-specific\nhistopathology foundation models over general-purpose models for specialized\ntasks such as cell analysis remain underexplored. This study investigates the\nrepresentation learning gap between these two categories by analyzing\nmulti-level patch embeddings applied to cell instance segmentation and\nclassification. We implement an encoder-decoder architecture with a consistent\ndecoder and various encoders. These include convolutional, vision transformer\n(ViT), and hybrid encoders pre-trained on ImageNet-22K or LVD-142M,\nrepresenting general-purpose foundation models. These are compared against ViT\nencoders from the recently released UNI, Virchow2, and Prov-GigaPath foundation\nmodels, trained on patches extracted from hundreds of thousands of\nhistopathology whole-slide images. The decoder integrates patch embeddings from\ndifferent encoder depths via skip connections to generate semantic and distance\nmaps. These maps are then post-processed to create instance segmentation masks\nwhere each label corresponds to an individual cell and to perform cell-type\nclassification. All encoders remain frozen during training to assess their\npre-trained feature extraction capabilities. Using the PanNuke and CoNIC\nhistopathology datasets, and the newly introduced Nissl-stained CytoDArk0\ndataset for brain cytoarchitecture studies, we evaluate instance-level\ndetection, segmentation accuracy, and cell-type classification. This study\nprovides insights into the comparative strengths and limitations of\ngeneral-purpose vs. histopathology foundation models, offering guidance for\nmodel selection in cell-focused histopathology and brain cytoarchitecture\nanalysis workflows.",
        "Both particle physics experiments and cosmological observations have been\nused to explore neutrino properties. Cosmological researches of neutrinos often\nrely on the early-universe cosmic microwave background observations or other\nlate-universe probes, which mostly focus on large-scale structures. We\nintroduce a distinct probe, the 21-cm forest, that differs from other probes in\nboth time and scale. Actually, the 21-cm forest is a unique tool for studying\nsmall-scale structures in the early universe. Below the free-streaming scale,\nmassive neutrinos suppress the matter power spectrum, influencing small-scale\nfluctuations in the distribution of matter. The one-dimensional (1D) power\nspectrum of the 21-cm forest can track these fluctuations across different\nscales, similar to the matter power spectrum, providing an effective method to\nconstrain neutrino mass. Although heating effects in the early universe can\nalso impact the 1D power spectrum of the 21-cm forest, we assess the potential\nof the 21-cm forest as a tool for measuring neutrino mass, given that the\ntemperature of the intergalactic medium can be constrained using other methods\nwithin a certain range. In the ideal scenario, the 21-cm forest observation\nwill have the ability to constrain the total neutrino mass to around 0.1 eV.\nWith the accumulation of observational data and advancements in observational\ntechnology, the 21-cm forest holds great promise as an emerging and potent tool\nfor measuring neutrino mass.",
        "We propose and study conformal integrators for linearly damped stochastic\nPoisson systems. We analyse the qualitative and quantitative properties of\nthese numerical integrators: preservation of dynamics of certain Casimir and\nHamiltonian functions, almost sure bounds of the numerical solutions, and\nstrong and weak rates of convergence under appropriate conditions. These\ntheoretical results are illustrated with several numerical experiments on, for\nexample, the linearly damped free rigid body with random inertia tensor or the\nlinearly damped stochastic Lotka--Volterra system.",
        "Numerical evidence for superconductivity in the single-band Hubbard model is\nelusive or ambiguous despite extensive study, raising the question of whether\nthe single-band Hubbard model is a faithful low energy effective model for\ncuprates, and whether explicitly including the oxygen ions will recover the\nproperties necessary for superconducting transition. Here we show, by using\nnumerically exact determinant quantum Monte Carlo (DQMC) simulations of the\ndoped two-dimensional three-band Emery model, that while the single-band model\nexhibits strikingly T-linear transport behavior, the three-band model shows a\nlow temperature resistivity curvature indicating a crossover to a more metallic\ntransport regime. Evidence has also been found in thermodynamic and\nsuperconducting measurements, which suggests that some degree of coherence in\ntransport might be necessary for the high-temperature superconductivity in\ncuprates, further implying a possible connection between superconducting and\ntransport behaviors.",
        "Decades accumulation of theory simulations lead to boom in material database,\nwhich combined with machine learning methods has been a valuable driver for the\ndata-intensive material discovery, i.e., the fourth research paradigm. However,\nconstruction of segmented databases and data reuse in generic databases with\nuniform parameters still lack easy-to-use code tools. We herein develop a code\npackage named FF7 (Fast Funnel with 7 modules) to provide command-line based\ninteractive interface for performing customized high-throughput calculations\nand building your own handy databases. Data correlation studies and material\nproperty prediction can progress by built-in installation-free artificial\nneural network module and various post processing functions are also supported\nby auxiliary module. This paper shows the usage of FF7 code package and\ndemonstrates its usefulness by example of database driven thermodynamic\nstability high-throughput calculation and machine learning model for predicting\nthe superconducting critical temperature of clathrate hydrides.",
        "We consider two-loop planar contributions to a three-body form factor at the\nnext-to-leading power in the high-energy limit, where the masses of external\nparticles are much smaller than their energies. The calculation is performed by\nexploiting the differential equations of the expansion coefficients, both for\nfacilitating the linear relations among them, and for deriving their analytic\nexpressions. The result is written in terms of generalized polylogarithms\ninvolving a few simple symbol letters. Our method can be readily applied to the\ncalculation of non-planar contributions as well. The result provides crucial\ninformation for establishing sub-leading factorization theorems for massive\nscattering amplitudes in the high-energy limit.",
        "Astrophysical neutrinos with energy in the TeV-PeV range traverse megaparsecs\n(Mpc) to gigaparsecs (Gpc) scale distances before they reach the Earth. Tiny\nphysics effects that get accumulated over these large propagation paths during\ntheir journey may become observable at the detector. If there is some new\ninteraction between neutrinos and the background matter, that can potentially\naffect the propagation of the astrophysical neutrinos. One such possible case\nis the flavor-dependent long-range interaction of neutrinos, which can affect\nthe standard neutrino flavor transition, modifying the flavor composition of\nthe astrophysical neutrinos at Earth. Using the present-day and future\nprojection of the flavor-composition measurements of IceCube and IceCube-Gen2\nalong with the present and future measurement of the oscillation parameters, we\nexplore the sensitivity of these experiments to probe long-range neutrino\ninteraction with matter.",
        "We present a variational characterization of mechanical equilibrium in the\nplanar strain regime for systems with incompatible kinematics. For non-simply\nconnected domains, we show that the equilibrium problem for a non-liftable\nstrain-stress pair can be reformulated as a well-posed minimization problem for\nthe Airy potential of the system. We characterize kinematic incompatibilities\non internal boundaries as rotational or translational mismatches, in agreement\nwith Volterra's modeling of disclinations and dislocations. Finally, we\nestablish that the minimization problem for the Airy potential can be reduced\nto a finite-dimensional optimization involving cell formulas.",
        "We propose that a hierarchical shock model$\\unicode{x2014}$including\nsupernova remnant shocks, galactic wind termination shocks, and accretion\nshocks around cosmic filaments and galaxy clusters$\\unicode{x2014}$can\nnaturally explain the cosmic ray spectrum from ~1 GeV up to ~200 EeV. While\nthis framework applies to the entire cosmic ray spectrum, in this work, we\nfocus on its implications for ultra-high-energy cosmic rays (UHECRs). We\nperform a hydrodynamic cosmological simulation to investigate the power\nprocessed at shocks around clusters and filaments. The downstream flux from\nnearby shocks around the local filament accounts for the softer, lower-energy\nextragalactic component around the ankle, and the upstream escaping flux from\nnearby clusters accounts for the transition to a hard spectral component at the\nhighest energies. This interpretation is in agreement with UHECR observations.\nWe suggest that a combination of early-Universe galactic outflows, cosmic ray\nstreaming instabilities, and a small-scale turbulent dynamo can increase\nmagnetic fields enough to attain the required rigidities. Our simulation\nsuggests that the available volume-averaged power density of accretion shocks\nexceeds the required UHECR luminosity density by three orders of magnitude. We\nshow that microgauss magnetic fields at these shocks could explain both the\norigin of UHECRs and the as-yet unidentified source of the diffuse radio\nsynchrotron background below 10 GHz. The shock-accelerated electrons produce a\nhard radio background without overproducing diffuse inverse Compton emission.\nThese results motivate further observational tests with upcoming facilities to\nhelp distinguish accretion shocks from other UHECR sources.",
        "We present a class of relativistic fluid models for cold and dense matter\nwith bulk viscosity, whose equilibrium equation of state is polytropic. These\nmodels reduce to Israel-Stewart theory for small values of the viscous stress\n$\\Pi$. However, when $\\Pi$ becomes comparable to the equilibrium pressure $P$,\nthe evolution equations \"adjust\" to prevent the onset of far-from-equilibrium\npathologies that would otherwise plague Israel-Stewart. Specifically, the\nequations of motion remain symmetric hyperbolic and causal at all times along\nany continuously differentiable flow, and across the whole thermodynamic state\nspace. This means that, no matter how fast the fluid expands or contracts, the\nhydrodynamic equations are always well-behaved (away from singularities). The\nsecond law of thermodynamics is enforced exactly. Near equilibrium, these\nmodels can accommodate an arbitrarily complicated dependence of the bulk\nviscosity coefficient $\\zeta$ on both density and temperature.",
        "The Deep Underground Neutrino Experiment (DUNE) is a next-generation\nlong-baseline neutrino oscillation experiment designed to make precision\nmeasurements in the world's most powerful neutrino beam. Neutrinos are measured\nat two detector facilities: a near detector (ND) located at Fermilab close to\nwhere the beam is produced and a far detector (FD) at SURF. The Precision\nReaction Independent Spectrum Measurement (PRISM) system allows for the\nmeasurement of different neutrino energy spectra by moving the near detector\naway from the central axis of the neutrino beam. These off-axis neutrino energy\nspectra provide a new degree of freedom that can be used to develop a deeper\nunderstanding of the relationship between the observable energy deposits in the\ndetector and the energy of the interacting neutrino. This can benefit DUNE by\nsignificantly reducing the impact of systematic uncertainties in the neutrino\ninteraction model. One possible use of the PRISM system is to perform a novel\nneutrino oscillation analysis that linearly combines off-axis neutrino energy\nspectra at the near detector to produce data-driven predictions of the far\ndetector energy spectrum.",
        "Surface alloying can alter surface electronic and magnetic properties, which\nare key parameters when developing new materials tailored for specific\napplications. A magnetic surface alloy was formed by depositing Sb on Ni(111)\nat elevated temperatures, yielding new electronic states at the Fermi level and\nmodifying the Ni-derived bandstructure. In particular, it changed the electron\noccupancy of the spin-polarized surface resonance bands, which may affect the\nmagnetic properties of the surface and its associated many-body effects. By\nfitting a finite element model to angle-dependent core level measurements,\nsimilar amounts of Sb and Ni were found within the first few atomic layers to\nindicate a near-surface composition similar to the bulk alloy NiSb. Annealing\nto higher temperatures post-growth further improved the crystalline quality of\nthe surface. Our investigation of the surface alloy's crystallinity, chemical\ncomposition, and layer structure lays the basis for future studies of how its\nelectronic and magnetic properties can be modified.",
        "Bayesian optimization (BO) is an effective paradigm for the optimization of\nexpensive-to-sample systems. Standard BO learns the performance of a system\n$f(x)$ by using a Gaussian Process (GP) model; this treats the system as a\nblack-box and limits its ability to exploit available structural knowledge\n(e.g., physics and sparse interconnections in a complex system). Grey-box\nmodeling, wherein the performance function is treated as a composition of known\nand unknown intermediate functions $f(x, y(x))$ (where $y(x)$ is a GP model)\noffers a solution to this limitation; however, generating an analytical\nprobability density for $f$ from the Gaussian density of $y(x)$ is often an\nintractable problem (e.g., when $f$ is nonlinear). Previous work has handled\nthis issue by using sampling techniques or by solving an auxiliary problem over\nan augmented space where the values of $y(x)$ are constrained by confidence\nintervals derived from the GP models; such solutions are computationally\nintensive. In this work, we provide a detailed implementation of a recently\nproposed grey-box BO paradigm, BOIS, that uses adaptive linearizations of $f$\nto obtain analytical expressions for the statistical moments of the composite\nfunction. We show that the BOIS approach enables the exploitation of structural\nknowledge, such as that arising in interconnected systems as well as systems\nthat embed multiple GP models and combinations of physics and GP models. We\nbenchmark the effectiveness of BOIS against standard BO and existing grey-box\nBO algorithms using a pair of case studies focused on chemical process\noptimization and design. Our results indicate that BOIS performs as well as or\nbetter than existing grey-box methods, while also being less computationally\nintensive.",
        "In this paper, we study a generalization of the D\\'iaz-Saa inequality and its\napplications to nonlinear elliptic problems. We first present the necessary\nhypotheses and preliminary results before introducing an improved version of\nthe inequality, which holds in a broader functional setting and allows\napplications to problems with homogeneous Neumann boundary conditions. The\nsignificance of cases where the inequality becomes an equality is also\nanalyzed, leading to uniqueness results for certain classes of partial\ndifferential equations. Furthermore, we provide a detailed proof of a\nuniqueness theorem for strong positive solutions and illustrate our findings\nwith two concrete applications: a multiple-phase problem and an elliptic\nquasilinear equation relevant to image processing. The paper concludes with\npossible directions for future research."
      ]
    }
  },
  {
    "id":"2411.00609",
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Pediatric low-grade glioma: State-of-the-art and ongoing challenges",
    "start_abstract":"Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks"
      ],
      "abstract":[
        "Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016)."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Learning Clustering-based Prototypes for Compositional Zero-shot\n  Learning",
        "MFSR: Multi-fractal Feature for Super-resolution Reconstruction with\n  Fine Details Recovery",
        "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive\n  Diffusion",
        "Universal Scene Graph Generation",
        "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D\n  Object Recognition",
        "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from\n  In-the-Wild Drone Imagery",
        "Neuro Symbolic Knowledge Reasoning for Procedural Video Question\n  Answering",
        "LangGas: Introducing Language in Selective Zero-Shot Background\n  Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset",
        "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search",
        "X-Dyna: Expressive Dynamic Human Image Animation",
        "Consistent multi-animal pose estimation in cattle using dynamic Kalman\n  filter based tracking",
        "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
        "T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting",
        "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models\n  via Synthetic Embeddings",
        "Separate surface and bulk topological Anderson localization transitions\n  in disordered axion insulators",
        "Harmonic And Transposition Constraints Arising From The Use Of The\n  Roland TR-808 Bass Drum",
        "Quantifying Point Contributions: A Lightweight Framework for Efficient\n  and Effective Query-Driven Trajectory Simplification",
        "High-Energy Neutrinos by Hydrogen-rich Supernovae interacting with\n  low-massive Circumstellar Medium: The Case of SN 2023ixf",
        "A Hybrid Model\/Data-Driven Solution to Channel, Position and Orientation\n  Tracking in mmWave Vehicular Systems",
        "Exploring the Potential of QEEGNet for Cross-Task and Cross-Dataset\n  Electroencephalography Encoding with Quantum Machine Learning",
        "Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic\n  Data Generation and Fairness Algorithms",
        "Continually Learning Structured Visual Representations via Network\n  Refinement with Rerelation",
        "Dango: A Mixed-Initiative Data Wrangling System using Large Language\n  Model",
        "6GStarLab -- A CubeSat Mission to support the development and\n  standardization of Non-Terrestrial Networks towards 6G",
        "Hints of Primordial Magnetic Fields at Recombination and Implications\n  for the Hubble Tension",
        "EMK-KEN: A High-Performance Approach for Assessing Knowledge Value in\n  Citation Network",
        "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention",
        "Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models"
      ],
      "abstract":[
        "Learning primitive (i.e., attribute and object) concepts from seen\ncompositions is the primary challenge of Compositional Zero-Shot Learning\n(CZSL). Existing CZSL solutions typically rely on oversimplified data\nassumptions, e.g., modeling each primitive with a single centroid primitive\nrepresentation, ignoring the natural diversities of the attribute (resp.\nobject) when coupled with different objects (resp. attribute). In this work, we\ndevelop ClusPro, a robust clustering-based prototype mining framework for CZSL\nthat defines the conceptual boundaries of primitives through a set of\ndiversified prototypes. Specifically, ClusPro conducts within-primitive\nclustering on the embedding space for automatically discovering and dynamically\nupdating prototypes. These representative prototypes are subsequently used to\nrepaint a well-structured and independent primitive embedding space, ensuring\nintra-primitive separation and inter-primitive decorrelation through\nprototype-based contrastive learning and decorrelation learning. Moreover,\nClusPro efficiently performs prototype clustering in a non-parametric fashion\nwithout the introduction of additional learnable parameters or computational\nbudget during testing. Experiments on three benchmarks demonstrate ClusPro\noutperforms various top-leading CZSL solutions under both closed-world and\nopen-world settings.",
        "In the process of performing image super-resolution processing, the\nprocessing of complex localized information can have a significant impact on\nthe quality of the image generated. Fractal features can capture the rich\ndetails of both micro and macro texture structures in an image. Therefore, we\npropose a diffusion model-based super-resolution method incorporating fractal\nfeatures of low-resolution images, named MFSR. MFSR leverages these fractal\nfeatures as reinforcement conditions in the denoising process of the diffusion\nmodel to ensure accurate recovery of texture information. MFSR employs\nconvolution as a soft assignment to approximate the fractal features of\nlow-resolution images. This approach is also used to approximate the density\nfeature maps of these images. By using soft assignment, the spatial layout of\nthe image is described hierarchically, encoding the self-similarity properties\nof the image at different scales. Different processing methods are applied to\nvarious types of features to enrich the information acquired by the model. In\naddition, a sub-denoiser is integrated in the denoising U-Net to reduce the\nnoise in the feature maps during the up-sampling process in order to improve\nthe quality of the generated images. Experiments conducted on various face and\nnatural image datasets demonstrate that MFSR can generate higher quality\nimages.",
        "The task of video generation requires synthesizing visually realistic and\ntemporally coherent video frames. Existing methods primarily use asynchronous\nauto-regressive models or synchronous diffusion models to address this\nchallenge. However, asynchronous auto-regressive models often suffer from\ninconsistencies between training and inference, leading to issues such as error\naccumulation, while synchronous diffusion models are limited by their reliance\non rigid sequence length. To address these issues, we introduce Auto-Regressive\nDiffusion (AR-Diffusion), a novel model that combines the strengths of\nauto-regressive and diffusion models for flexible, asynchronous video\ngeneration. Specifically, our approach leverages diffusion to gradually corrupt\nvideo frames in both training and inference, reducing the discrepancy between\nthese phases. Inspired by auto-regressive generation, we incorporate a\nnon-decreasing constraint on the corruption timesteps of individual frames,\nensuring that earlier frames remain clearer than subsequent ones. This setup,\ntogether with temporal causal attention, enables flexible generation of videos\nwith varying lengths while preserving temporal coherence. In addition, we\ndesign two specialized timestep schedulers: the FoPP scheduler for balanced\ntimestep sampling during training, and the AD scheduler for flexible timestep\ndifferences during inference, supporting both synchronous and asynchronous\ngeneration. Extensive experiments demonstrate the superiority of our proposed\nmethod, which achieves competitive and state-of-the-art results across four\nchallenging benchmarks.",
        "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance.",
        "Recent open-world representation learning approaches have leveraged CLIP to\nenable zero-shot 3D object recognition. However, performance on real point\nclouds with occlusions still falls short due to unrealistic pretraining\nsettings. Additionally, these methods incur high inference costs because they\nrely on Transformer's attention modules. In this paper, we make two\ncontributions to address these limitations. First, we propose occlusion-aware\ntext-image-point cloud pretraining to reduce the training-testing domain gap.\nFrom 52K synthetic 3D objects, our framework generates nearly 630K partial\npoint clouds for pretraining, consistently improving real-world recognition\nperformances of existing popular 3D networks. Second, to reduce computational\nrequirements, we introduce DuoMamba, a two-stream linear state space model\ntailored for point clouds. By integrating two space-filling curves with 1D\nconvolutions, DuoMamba effectively models spatial dependencies between point\ntokens, offering a powerful alternative to Transformer. When pretrained with\nour framework, DuoMamba surpasses current state-of-the-art methods while\nreducing latency and FLOPs, highlighting the potential of our approach for\nreal-world applications. Our code and data are available at\nhttps:\/\/ndkhanh360.github.io\/project-occtip.",
        "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.",
        "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https:\/\/github.com\/LUNAProject22\/KML soon.",
        "Gas leakage poses a significant hazard that requires prevention.\nTraditionally, human inspection has been used for detection, a slow and\nlabour-intensive process. Recent research has applied machine learning\ntechniques to this problem, yet there remains a shortage of high-quality,\npublicly available datasets. This paper introduces a synthetic dataset\nfeaturing diverse backgrounds, interfering foreground objects, diverse leak\nlocations, and precise segmentation ground truth. We propose a zero-shot method\nthat combines background subtraction, zero-shot object detection, filtering,\nand segmentation to leverage this dataset. Experimental results indicate that\nour approach significantly outperforms baseline methods based solely on\nbackground subtraction and zero-shot object detection with segmentation,\nreaching an IoU of 69\\% overall. We also present an analysis of various prompt\nconfigurations and threshold settings to provide deeper insights into the\nperformance of our method. The code and dataset will be released after\npublication.",
        "The remarkable progress in text-to-video diffusion models enables\nphotorealistic generations, although the contents of the generated video often\ninclude unnatural movement or deformation, reverse playback, and motionless\nscenes. Recently, an alignment problem has attracted huge attention, where we\nsteer the output of diffusion models based on some quantity on the goodness of\nthe content. Because there is a large room for improvement of perceptual\nquality along the frame direction, we should address which metrics we should\noptimize and how we can optimize them in the video generation. In this paper,\nwe propose diffusion latent beam search with lookahead estimator, which can\nselect better diffusion latent to maximize a given alignment reward, at\ninference time. We then point out that the improvement of perceptual video\nquality considering the alignment to prompts requires reward calibration by\nweighting existing metrics. When evaluating outputs by using vision language\nmodels as a proxy of humans, many previous metrics to quantify the naturalness\nof video do not always correlate with evaluation and also depend on the degree\nof dynamic descriptions in evaluation prompts. We demonstrate that our method\nimproves the perceptual quality based on the calibrated reward, without model\nparameter update, and outputs the best generation compared to greedy search and\nbest-of-N sampling. We provide practical guidelines on which axes, among search\nbudget, lookahead steps for reward estimate, and denoising steps, in the\nreverse diffusion process, we should allocate the inference-time computation.",
        "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for\nanimating a single human image using facial expressions and body movements\nderived from a driving video, that generates realistic, context-aware dynamics\nfor both the subject and the surrounding environment. Building on prior\napproaches centered on human pose control, X-Dyna addresses key shortcomings\ncausing the loss of dynamic details, enhancing the lifelike qualities of human\nvideo animations. At the core of our approach is the Dynamics-Adapter, a\nlightweight module that effectively integrates reference appearance context\ninto the spatial attentions of the diffusion backbone while preserving the\ncapacity of motion modules in synthesizing fluid and intricate dynamic details.\nBeyond body pose control, we connect a local control module with our model to\ncapture identity-disentangled facial expressions, facilitating accurate\nexpression transfer for enhanced realism in animated scenes. Together, these\ncomponents form a unified framework capable of learning physical human motion\nand natural scene dynamics from a diverse blend of human and scene videos.\nComprehensive qualitative and quantitative evaluations demonstrate that X-Dyna\noutperforms state-of-the-art methods, creating highly lifelike and expressive\nanimations. The code is available at https:\/\/github.com\/bytedance\/X-Dyna.",
        "Over the past decade, studying animal behaviour with the help of computer\nvision has become more popular. Replacing human observers by computer vision\nlowers the cost of data collection and therefore allows to collect more\nextensive datasets. However, the majority of available computer vision\nalgorithms to study animal behaviour is highly tailored towards a single\nresearch objective, limiting possibilities for data reuse. In this perspective,\npose-estimation in combination with animal tracking offers opportunities to\nyield a higher level representation capturing both the spatial and temporal\ncomponent of animal behaviour. Such a higher level representation allows to\nanswer a wide variety of research questions simultaneously, without the need to\ndevelop repeatedly tailored computer vision algorithms. In this paper, we\ntherefore first cope with several weaknesses of current pose-estimation\nalgorithms and thereafter introduce KeySORT (Keypoint Simple and Online\nRealtime Tracking). KeySORT deploys an adaptive Kalman filter to construct\ntracklets in a bounding-box free manner, significantly improving the temporal\nconsistency of detected keypoints. In this paper, we focus on pose estimation\nin cattle, but our methodology can easily be generalised to any other animal\nspecies. Our test results indicate our algorithm is able to detect up to 80% of\nthe ground truth keypoints with high accuracy, with only a limited drop in\nperformance when daylight recordings are compared to nightvision recordings.\nMoreover, by using KeySORT to construct skeletons, the temporal consistency of\ngenerated keypoint coordinates was largely improved, offering opportunities\nwith regard to automated behaviour monitoring of animals.",
        "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps:\/\/github.com\/auniquesun\/Point-Cache.",
        "Zero-shot object counting aims to count instances of arbitrary object\ncategories specified by text descriptions. Existing methods typically rely on\nvision-language models like CLIP, but often exhibit limited sensitivity to text\nprompts. We present T2ICount, a diffusion-based framework that leverages rich\nprior knowledge and fine-grained visual understanding from pretrained diffusion\nmodels. While one-step denoising ensures efficiency, it leads to weakened text\nsensitivity. To address this challenge, we propose a Hierarchical Semantic\nCorrection Module that progressively refines text-image feature alignment, and\na Representational Regional Coherence Loss that provides reliable supervision\nsignals by leveraging the cross-attention maps extracted from the denosing\nU-Net. Furthermore, we observe that current benchmarks mainly focus on majority\nobjects in images, potentially masking models' text sensitivity. To address\nthis, we contribute a challenging re-annotated subset of FSC147 for better\nevaluation of text-guided counting ability. Extensive experiments demonstrate\nthat our method achieves superior performance across different benchmarks. Code\nis available at https:\/\/github.com\/cha15yq\/T2ICount.",
        "Multimodal Large Language Models (MLLMs) have serious security\nvulnerabilities.While safety alignment using multimodal datasets consisting of\ntext and data of additional modalities can effectively enhance MLLM's security,\nit is costly to construct these datasets. Existing low-resource security\nalignment methods, including textual alignment, have been found to struggle\nwith the security risks posed by additional modalities. To address this, we\npropose Synthetic Embedding augmented safety Alignment (SEA), which optimizes\nembeddings of additional modality through gradient updates to expand textual\ndatasets. This enables multimodal safety alignment training even when only\ntextual data is available. Extensive experiments on image, video, and\naudio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding\non a single RTX3090 GPU within 24 seconds. SEA significantly improves the\nsecurity of MLLMs when faced with threats from additional modalities. To assess\nthe security risks introduced by video and audio, we also introduced a new\nbenchmark called VA-SafetyBench. High attack success rates across multiple\nMLLMs validate its challenge. Our code and data will be available at\nhttps:\/\/github.com\/ZeroNLP\/SEA.",
        "In topological phases of matter for which the bulk and boundary support\ndistinct electronic gaps, there exists the possibility of decoupled mobility\ngaps in the presence of disorder. This is in analogy with the well-studied\nproblem of realizing separate or concomitant bulk-boundary criticality in\nconventional Landau theory. Using a three-dimensional axion insulator having\nclean, gapped surfaces with $e^2\/2h$ quantized Hall conductance, we show the\nbulk and surface mobility gap evolve differently in the presence of disorder.\nThe decoupling of the bulk and surface topology yields a regime that realizes a\ntwo-dimensional, unquantized anomalous Hall metal in the Gaussian unitary\nensemble (GUE), which shares some spectral and response properties akin to the\nsurface states of a conventional three-dimensional (3D) topological insulator.\nThe generality of these results as well as extensions to other insulators and\nsuperconductors is discussed.",
        "The study investigates hip-hop music producer Scott Storch's approach to\ntonality, where the song's key is transposed to fit the Roland TR-808 bass drum\ninstead of tuning the drums to the song's key. This process, involving the\nadjustment of all tracks except the bass drum, suggests significant production\nmotives. The primary constraint stems from the limited usable pitch range of\nthe TR-808 bass drum if its characteristic sound is to be preserved. The\nresearch examines drum tuning practices, the role of the Roland TR-808 in\nmusic, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples\nreveals their characteristics and their integration into modern genres like\ntrap and hip-hop. The study also considers the impact of loudspeaker frequency\nresponse and human ear sensitivity on bass drum perception. The findings\nsuggest that Storch's method prioritizes the spectral properties of the bass\ndrum over traditional pitch values to enhance the bass response. The need to\nmaintain the unique sound of the TR-808 bass drum underscores the importance of\nspectral formants and register in contemporary popular music production.",
        "As large volumes of trajectory data accumulate, simplifying trajectories to\nreduce storage and querying costs is increasingly studied. Existing proposals\nface three main problems. First, they require numerous iterations to decide\nwhich GPS points to delete. Second, they focus only on the relationships\nbetween neighboring points (local information) while neglecting the overall\nstructure (global information), reducing the global similarity between the\nsimplified and original trajectories and making it difficult to maintain\nconsistency in query results, especially for similarity-based queries. Finally,\nthey fail to differentiate the importance of points with similar features,\nleading to suboptimal selection of points to retain the original trajectory\ninformation.\n  We propose MLSimp, a novel Mutual Learning query-driven trajectory\nsimplification framework that integrates two distinct models: GNN-TS, based on\ngraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates\nthe importance of a point according to its globality, capturing its correlation\nwith the entire trajectory, and its uniqueness, capturing its differences from\nneighboring points. It also incorporates attention mechanisms in the GNN\nlayers, enabling simultaneous data integration from all points within the same\ntrajectory and refining representations, thus avoiding iterative processes.\nDiff-TS generates amplified signals to enable the retention of the most\nimportant points at low compression rates. Experiments involving eight\nbaselines on three databases show that MLSimp reduces the simplification time\nby 42%--70% and improves query accuracy over simplified trajectories by up to\n34.6%.",
        "In hydrogen-rich (H-rich) Supernova (SN) events, the collision between the\nH-rich ejecta and the Circum-Stellar Medium (CSM) can accelerate particles and\nproduce high-energy neutrinos (HE-$\\nu$, TeV-PeV) through proton-proton\ninelastic scattering. Despite understanding the production mechanism of these\nneutrinos, the lack of direct observations raises questions about particle\nacceleration efficiency and the involved astrophysical conditions. This study\nfocuses on neutrino emission from H-rich SNe with low-mass CSM, such as SN\n2023ixf. We developed a semi-analytical model to characterize the progenitor\nand CSM at the explosion time, allowing us to infer the expected neutrino flux\nat Earth during the SN's interaction phase. Our model shows that neutrino\nemission depends not only on shock velocity and CSM mass but also on the\nspatial matter distribution of the CSM. By analysing the bolometric light curve\nof SN 2023ixf beyond 100 days post-explosion, we find that its ejecta,\nconsisting of $9\\,\\text{M}_{\\rm \\odot}$ (including $0.07\\,\\text{M}_{\\rm \\odot}$\nof radioactive $^{56}$Ni) and having a kinetic energy of $1.8\\,\\text{foe}$,\ncollides with a low-mass CSM of $0.06\\,\\text{M}_{\\rm \\odot}$ distributed\naccording to a power-law density profile with an exponent of $s=2.9$. Through\nthese parameters, we estimate that up to $4\\pm1\\times 10^{-2}$ muon\n(anti-)neutrino events could be detected by IceCube within 50 days\npost-explosion. Although the predicted flux ($\\lesssim 3\\times\n10^{-9}\\,\\text{GeV} \\, \\text{cm}^{-2} \\, \\text{s}^{-1}$) is below current\nIceCube sensitivity, future telescopes like IceCube-Gen2 and KM3NeT could\ndetect HE-$\\nu$ from similar SN events.",
        "Channel tracking in millimeter wave (mmWave) vehicular systems is crucial for\nmaintaining robust vehicle-to-infrastructure (V2I) communication links, which\ncan be leveraged to achieve high accuracy vehicle position and orientation\ntracking as a byproduct of communication. While prior work tends to simplify\nthe system model by omitting critical system factors such as clock offsets,\nfiltering effects, antenna array orientation offsets, and channel estimation\nerrors, we address the challenges of a practical mmWave multiple-input\nmultiple-output (MIMO) communication system between a single base station (BS)\nand a vehicle while tracking the vehicle's position and orientation (PO)\nconsidering realistic driving behaviors. We first develop a channel tracking\nalgorithm based on multidimensional orthogonal matching pursuit (MOMP) with\nfactoring (F-MOMP) to reduce computational complexity and enable\nhigh-resolution channel estimates during the tracking stage, suitable for PO\nestimation. Then, we develop a network called VO-ChAT (Vehicle\nOrientation-Channel Attention for orientation Tracking), which processes the\nchannel estimate sequence for orientation prediction. Afterward, a weighted\nleast squares (WLS) problem that exploits the channel geometry is formulated to\ncreate an initial estimate of the vehicle's 2D position. A second network named\nVP-ChAT (Vehicle Position-Channel Attention for position Tracking) refines the\ngeometric position estimate. VP-ChAT is a Transformer inspired network\nprocessing the historical channel and position estimates to provide the\ncorrection for the initial geometric position estimate. The proposed solution\nis evaluated using raytracing generated channels in an urban canyon\nenvironment. For 80% of the cases it achieves a 2D position tracking accuracy\nof 26 cm while orientation errors are kept below 0.5 degree.",
        "Electroencephalography (EEG) is widely used in neuroscience and clinical\nresearch for analyzing brain activity. While deep learning models such as\nEEGNet have shown success in decoding EEG signals, they often struggle with\ndata complexity, inter-subject variability, and noise robustness. Recent\nadvancements in quantum machine learning (QML) offer new opportunities to\nenhance EEG analysis by leveraging quantum computing's unique properties. In\nthis study, we extend the previously proposed Quantum-EEGNet (QEEGNet), a\nhybrid neural network incorporating quantum layers into EEGNet, to investigate\nits generalization ability across multiple EEG datasets. Our evaluation spans a\ndiverse set of cognitive and motor task datasets, assessing QEEGNet's\nperformance in different learning scenarios. Experimental results reveal that\nwhile QEEGNet demonstrates competitive performance and maintains robustness in\ncertain datasets, its improvements over traditional deep learning methods\nremain inconsistent. These findings suggest that hybrid quantum-classical\narchitectures require further optimization to fully leverage quantum advantages\nin EEG processing. Despite these limitations, our study provides new insights\ninto the applicability of QML in EEG research and highlights challenges that\nmust be addressed for future advancements.",
        "The increasing use of machine learning in learning analytics (LA) has raised\nsignificant concerns around algorithmic fairness and privacy. Synthetic data\nhas emerged as a dual-purpose tool, enhancing privacy and improving fairness in\nLA models. However, prior research suggests an inverse relationship between\nfairness and privacy, making it challenging to optimize both. This study\ninvestigates which synthetic data generators can best balance privacy and\nfairness, and whether pre-processing fairness algorithms, typically applied to\nreal datasets, are effective on synthetic data. Our results highlight that the\nDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between\nprivacy and fairness. However, DECAF suffers in utility, as reflected in its\npredictive accuracy. Notably, we found that applying pre-processing fairness\nalgorithms to synthetic data improves fairness even more than when applied to\nreal data. These findings suggest that combining synthetic data generation with\nfairness pre-processing offers a promising approach to creating fairer LA\nmodels.",
        "Current machine learning paradigm relies on continuous representations like\nneural networks, which iteratively adjust parameters to approximate outcomes\nrather than directly learning the structure of problem. This spreads\ninformation across the network, causing issues like information loss and\nincomprehensibility Building on prior work in environment dynamics modeling, we\npropose a method that learns visual space in a structured, continual manner.\nOur approach refines networks to capture the core structure of objects while\nrepresenting significant subvariants in structure efficiently. We demonstrate\nthis with 2D shape detection, showing incremental learning on MNIST without\noverwriting knowledge and creating compact, comprehensible representations.\nThese results offer a promising step toward a transparent, continually learning\nalternative to traditional neural networks for visual processing.",
        "Data wrangling is a time-consuming and challenging task in a data science\npipeline. While many tools have been proposed to automate or facilitate data\nwrangling, they often misinterpret user intent, especially in complex tasks. We\npropose Dango, a mixed-initiative multi-agent system for data wrangling.\nCompared to existing tools, Dango enhances user communication of intent by\nallowing users to demonstrate on multiple tables and use natural language\nprompts in a conversation interface, enabling users to clarify their intent by\nanswering LLM-posed multiple-choice clarification questions, and providing\nmultiple forms of feedback such as step-by-step natural language explanations\nand data provenance to help users evaluate the data wrangling scripts. We\nconducted a within-subjects user study with 38 participants and demonstrated\nthat Dango's features can significantly improve intent clarification, accuracy,\nand efficiency in data wrangling. Furthermore, we demonstrated the\ngeneralizability of Dango by applying it to a broader set of data wrangling\ntasks.",
        "The emergence of the Non-Terrestrial Network (NTN) concept in the last years\nhas revolutionized the space industry. This novel network architecture composed\nof aircraft and spacecraft is currently being standardized by the 3GPP. This\nstandardization process follows dedicated phases in which experimentation of\nthe technology is needed. Although some missions have been conducted to\ndemonstrate specific and service-centric technologies, a open flexible in-orbit\ninfrastructure is demanded to support this standardization process. This work\npresents the 6GStarLab mission, which aims to address this gap. Specifically,\nthis mission envisions to provide a 6U CubeSat as the main in-orbit\ninfrastructure in which multiple technology validations can be uploaded. The\nconcept of this mission is depicted. Additionally, this work presents the\ndetails of the satellite platform and the payload. This last one is designed to\nenable the experimentation in multiple radio-frequency bands (i.e. UHF, S-, X-,\nand Ka-bands) and an optical terminal. The launch of the satellite is scheduled\nfor Q2 2025, and it will contribute to the standardization of future NTN\narchitectures.",
        "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
        "With the explosive growth of academic literature, effectively evaluating the\nknowledge value of literature has become quite essential. However, most of the\nexisting methods focus on modeling the entire citation network, which is\nstructurally complex and often suffers from long sequence dependencies when\ndealing with text embeddings. Thus, they might have low efficiency and poor\nrobustness in different fields. To address these issues, a novel knowledge\nevaluation method is proposed, called EMK-KEN. The model consists of two\nmodules. Specifically, the first module utilizes MetaFP and Mamba to capture\nsemantic features of node metadata and text embeddings to learn contextual\nrepresentations of each paper. The second module utilizes KAN to further\ncapture the structural information of citation networks in order to learn the\ndifferences in different fields of networks. Extensive experiments based on ten\nbenchmark datasets show that our method outperforms the state-of-the-art\ncompetitors in effectiveness and robustness.",
        "Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and\/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.",
        "Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms."
      ]
    }
  },
  {
    "id":"2411.00609",
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype\n  Identification Using a Novel AUROC Loss Function for Convolutional Neural\n  Networks",
    "start_abstract":"Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Pediatric low-grade glioma: State-of-the-art and ongoing challenges"
      ],
      "abstract":[
        "Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%\u201340% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "Inverse receptive field attention for naturalistic image reconstruction\n  from the brain",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced\n  Physical Demand",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "Anomalous bulk-edge correspondence of nonlinear Rice-Mele model",
        "Optical stabilization for laser communication satellite systems through\n  proportional-integral-derivative (PID) control and reinforcement learning\n  approach",
        "Mono-lepton Signature of a Neutrino-philic Dark Fermion at Hadron\n  Colliders",
        "Effect of thickness on the maximum potential drop of current collectors",
        "PPO-Q: Proximal Policy Optimization with Parametrized Quantum Policies\n  or Values",
        "Block structures of graphs and quantum isomorphism",
        "Comparison principles and asymptotic behavior of delayed age-structured\n  neuron models",
        "Regular Schwarzschild black holes and cosmological models",
        "A hidden Condorcet domain in Loday's realisation of the associahedron",
        "On the Wave Kinetic Equation in the presence of forcing and dissipation",
        "Influence of the growth temperature and annealing on the optical\n  properties of {CdO\/ZnO}30 superlattices",
        "Proceedings of the Erice Workshop: A new baseline for the hybrid,\n  asymmetric, linear Higgs factory HALHF",
        "Study of Null Geodesics and their Stability in Horndeski Black Holes",
        "Measuring decoherence due to quantum vacuum fluctuations",
        "Rank conditions for exactness of semidefinite relaxations in polynomial\n  optimization"
      ],
      "abstract":[
        "Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "Visual perception in the brain largely depends on the organization of\nneuronal receptive fields. Although extensive research has delineated the\ncoding principles of receptive fields, most studies have been constrained by\ntheir foundational assumptions. Moreover, while machine learning has\nsuccessfully been used to reconstruct images from brain data, this approach\nfaces significant challenges, including inherent feature biases in the model\nand the complexities of brain structure and function. In this study, we\nintroduce an inverse receptive field attention (IRFA) model, designed to\nreconstruct naturalistic images from neurophysiological data in an end-to-end\nfashion. This approach aims to elucidate the tuning properties and\nrepresentational transformations within the visual cortex. The IRFA model\nincorporates an attention mechanism that determines the inverse receptive field\nfor each pixel, weighting neuronal responses across the visual field and\nfeature spaces. This method allows for an examination of the dynamics of\nneuronal representations across stimuli in both spatial and feature dimensions.\nOur results show highly accurate reconstructions of naturalistic data,\nindependent of pre-trained models. Notably, IRF models trained on macaque V1,\nV4, and IT regions yield remarkably consistent spatial receptive fields across\ndifferent stimuli, while the features to which neuronal representations are\nselective exhibit significant variation. Additionally, we propose a data-driven\nmethod to explore representational clustering within various visual areas,\nfurther providing testable hypotheses.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "We present a hybrid brain-machine interface (BMI) that integrates\nsteady-state visually evoked potential (SSVEP)-based EEG and facial EMG to\nimprove multimodal control and mitigate fatigue in assistive applications.\nTraditional BMIs relying solely on EEG or EMG suffer from inherent limitations;\nEEG-based control requires sustained visual focus, leading to cognitive\nfatigue, while EMG-based control induces muscular fatigue over time. Our system\ndynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP\nsignals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize\ncontrol based on task demands. In a virtual turtle navigation task, the hybrid\nsystem achieved task completion times comparable to an EMG-only approach, while\n90% of users reported reduced or equal physical demand. These findings\ndemonstrate that multimodal BMI systems can enhance usability, reduce strain,\nand improve long-term adherence in assistive technologies.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "Bulk-edge correspondence (BEC) constitutes a fundamental concept within the\ndomain of topological physics, elucidating the profound interplay between the\ntopological invariants that characterize the bulk states and the emergent edge\nstates. A recent highlight along this research line consists of establishing\nBEC under the eigenvalue's nonlinearity in a linear Hamiltonian by introducing\nauxiliary eigenvalues [\\href{https:\/\/doi.org\/10.1103\/PhysRevLett.132.126601}{\nT. Isobe {\\it et al.,} Phys. Rev. Lett. 132, 126601 (2024)}]. The purpose of\nthis work aims to extend Isobe's analysis to uncover BEC of eigenvalue's\nnonlinearity in intrinsic nonlinear Hamiltonians. To achieve this, we\nnumerically solve the nonlinear Rice-Mele (RM) model and identify two distinct\ntypes of nonlinear eigenvalues: the intrinsically nonlinear eigenvalues and the\neigenvalue's nonlinearity introduced through the incorporation of auxiliary\neigenvalues. Furthermore, we establish a novel form of BEC based on these\nauxiliary nonlinear eigenvalues, which we term the anomalous BEC of a nonlinear\nphysical system. The concept of the anomalous BEC defined herein provides a\nnovel perspective on the intricate interplay between topology and nonlinearity\nin the context of BEC.",
        "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
        "Searching for dark matter at high-energy colliders and direct detection\nexperiments can effectively cover nearly the entire mass range from the MeV to\nthe TeV scale. In this paper, we focus on four-fermion contact interactions\nformulated within the framework of Effective Field Theory. Specifically, we\npresent a detailed analysis of mono-lepton production at the LHC. Our results\ndemonstrate that tensor operators exhibit superior sensitivity in the\nmono-lepton channel, constraining energy scales up to 3\\,TeV for a nearly\nmassless dark fermion using current LHC data. Moreover, these operators mediate\nboth spin-independent and spin-dependent absorption processes in nuclear\ntargets. A systematic comparison of constraints between direct detection\nexperiments and collider measurements reveals the LHC's distinct advantage in\nexploring sub-GeV dark matter candidates while maintaining competitive\nsensitivity at the TeV scale. Notably, direct detection experiments such as\nSuper-Kamiokande and Borexino achieve complementary constraints in the\n10-100\\,TeV mass range through their unique capabilities: utilization of light\nnuclei targets, large exposure volumes, and distinctive features of the recoil\nenergy spectra.",
        "The basic principle for achieving high-power capability on an electrochemical\nenergy storage cell is minimizing the overall resistance. The resistance due to\ncurrent collecting systems has not received sufficient attention in the past,\npresumably because it was not considered of significance for low-power\nbatteries and supercapacitors. However, the necessity of high-power cells has\nreduced other sources of the inner resistance, and the current collector\npotential drop has become more important. Moreover, the miniaturization of\nenergy storage devices could increase the ohmic loses in current collectors. In\nthis work, we have developed an electrical model to assess the effect of the\ncurrent collector thickness on the maximum potential drop. We have found that\nthe thickness of current collectors is a critical parameter that can increase\nthe maximum potential drop drastically. Indeed, the maximum potential drop of\ncurrent collectors remains almost constant for thicknesses greater than 500 lm,\nbut below this value, there is an inverse relationship between the maximum\npotential drop and the thickness. We have also analyzed the effect of the\nmaterial and tab position in the maximum potential drop.",
        "Quantum machine learning (QML), which combines quantum computing with machine\nlearning, is widely believed to hold the potential to outperform traditional\nmachine learning in the era of noisy intermediate-scale quantum (NISQ). As one\nof the most important types of QML, quantum reinforcement learning (QRL) with\nparameterized quantum circuits as agents has received extensive attention in\nthe past few years. Various algorithms and techniques have been introduced,\ndemonstrating the effectiveness of QRL in solving some popular benchmark\nenvironments such as CartPole, FrozenLake, and MountainCar. However, tackling\nmore complex environments with continuous action spaces and high-dimensional\nstate spaces remains challenging within the existing QRL framework. Here we\npresent PPO-Q, which, by integrating hybrid quantum-classical networks into the\nactor or critic part of the proximal policy optimization (PPO) algorithm,\nachieves state-of-the-art performance in a range of complex environments with\nsignificantly reduced training parameters. The hybrid quantum-classical\nnetworks in the PPO-Q incorporate two additional traditional neural networks to\naid the parameterized quantum circuits in managing high-dimensional state\nencoding and action selection. When evaluated on 8 diverse environments,\nincluding four with continuous action space, the PPO-Q achieved comparable\nperformance with the PPO algorithm but with significantly reduced training\nparameters. Especially, we accomplished the BipedalWalker environment, with a\nhigh-dimensional state and continuous action space simultaneously, which has\nnot previously been reported in the QRL. More importantly, the PPO-Q is very\nfriendly to the current NISQ hardware. We successfully trained two\nrepresentative environments on the real superconducting quantum devices via the\nQuafu quantum cloud service.",
        "We prove that for every pair of quantum isomorphic graphs, their block trees\nand their block graphs are isomorphic and that such an isomorphism can be\nchosen so that the corresponding blocks are quantum isomorphic. As a corollary\nof this result, we obtain that a minimal pair of quantum isomorphic graphs\nwhich are not isomorphic consists of 2-connected graphs.",
        "In the context of neuroscience the elapsed-time model is an age-structured\nequation that describes the behavior of interconnected spiking neurons through\nthe time since the last discharge, with many interesting dynamics depending on\nthe type of interactions between neurons. We investigate the asymptotic\nbehavior of this equation in the case of both discrete and distributed delays\nthat account for the time needed to transmit a nerve impulse from one neuron to\nthe rest the ensemble. To prove the convergence to the equilibrium, we follow\nan approach based on comparison principles for Volterra equations involving the\ntotal activity, which provides a simpler and more straightforward alternative\ntechnique than those in the existing literature on the elapsed-time model.",
        "We study regular Schwarzschild black holes in General Relativity as an\nalternative to the singular counterpart. We analyze two types of solutions\nwhich are completely parameterised by the ADM mass alone. We find that both\nfamilies of regular solutions contain a de Sitter condensate at the core and\nadmit (quasi) extremal black hole configurations in which the two horizons are\narbitrarily close. Cosmological models based on these regular configurations\nare also analyzed, finding that they describe non-trivial Kantowski-Sachs\nuniverses free of singularities.",
        "We prove that Loday's polytopal realisation of the nth Tamari lattice T_n,\ncalled associahedron, has 2^{n-1} common points with the permutohedron, which\nform a maximal never-middle (symmetric) Condorcet domain.",
        "The wave kinetic equation has become an important tool in different fields of\nphysics. In particular, for surface gravity waves, it is the backbone of wave\nforecasting models. Its derivation is based on the Hamiltonian dynamics of\nsurface gravity waves. Only at the end of the derivation are the\nnon-conservative effects, such as forcing and dissipation, included as\nadditional terms to the collision integral. In this paper, we present a first\nattempt to derive the wave kinetic equation when the dissipation\/forcing is\nincluded in the deterministic dynamics. If, in the dynamical equations, the\ndissipation\/forcing is one order of magnitude smaller than the nonlinear\neffect, then the classical wave action balance equation is obtained and the\nkinetic time scale corresponds to the dissipation\/forcing time scale. However,\nif we assume that the nonlinearity and the dissipation\/forcing act on the same\ndynamical time scale, we find that the dissipation\/forcing dominates the\ndynamics and the resulting collision integral appears in a modified form, at a\nhigher order.",
        "Optical properties of the short period {CdO\/ZnO} superlattices grown by\nplasma assisted MBE were analyzed. The superlattice (SLs) structures were\nsuccessfully obtained at different growth temperatures from 360 to 550 {\\deg}C.\nInterestingly, the growth temperature of the SLs influences quality of\nmultilayers and also optical properties of these structures. After annealing at\n900{\\deg}C by rapid thermal method various defect luminescence located at\ndifferent energetic positions , were detected, and intensity of luminescence\nstrongly depends on applied growth temperature.",
        "The HALHF collaboration has discussed a new baseline for the project, taking\ninto account comments from the accelerator community on various aspects of the\noriginal design. In particular, these concerned the practicality of the\ndual-purpose linac to accelerate both colliding positron bunches and the drive\nbeams required for the plasma linac. In addition, many other aspects of the\nproject were also considered; the discussion and conclusions are documented in\nthis paper. Finally, a new baseline is outlined that has been optimised and\naddresses several weaknesses in the original design, has higher luminosity,\nreduced centre-of-mass energy boost and additional features such as positron\npolarization as well as electron polarization. Although HALHF has become longer\nand more expensive, it remains significantly smaller and cheaper than other\nmature Higgs factory designs currently under discussion.",
        "We study the motion of particles in the background of a scalar-tensor theory\nof gravity in which the scalar field is kinetically coupled to the Einstein\ntensor and we present the null geodesic structure for asymptotically flat, AdS,\nand dS Horndeski black holes, studying the effect of the cosmological constant\non the orbits. Also, we consider three classical test of the gravity in the\nsolar system, such as, the bending of the light, the gravitational redshift,\nand the Shapiro time delay in order to constraint the coupling parameters of\nthe scalar field to gravity. Calculating the Lyapunov exponent we explore the\nstability of these geodesics for various values of the cosmological constant.",
        "The interaction of a particle with vacuum fluctuations -- which theoretically\nexist even in the complete absence of matter -- can lead to observable\nirreversible decoherence, if it were possible to switch on and off the particle\ncharge suddenly. We compute the leading order decoherence effect for such a\nscenario and propose an experimental setup for its detection. Such a\nmeasurement might provide further insights into the nature of vacuum\nfluctuations and a novel precision test for the decoherence theory.",
        "We consider the Moment-SOS hierarchy in polynomial optimization. We first\nprovide a sufficient condition to solve the truncated K-moment problem\nassociated with a given degree-$2n$ pseudo-moment sequence $\\phi$ n and a\nsemi-algebraic set $K \\subset \\mathbb{R}^d$. Namely, let $2v$ be the maximum\ndegree of the polynomials that describe $K$. If the rank $r$ of its associated\nmoment matrix is less than $nv + 1$, then $\\phi^n$ has an atomic representing\nmeasure supported on at most $r$ points of $K$. When used at step-$n$ of the\nMoment-SOS hierarchy, it provides a sufficient condition to guarantee its\nfinite convergence (i.e., the optimal value of the corresponding degree-n\nsemidefinite relaxation of the hierarchy is the global minimum). For Quadratic\nConstrained Quadratic Problems (QCQPs) one may also recover global minimizers\nfrom the optimal pseudo-moment sequence. Our condition is in the spirit of\nBlekherman's rank condition and while on the one-hand it is more restrictive,\non the other hand it applies to constrained POPs as it provides a localization\non $K$ for the representing measure."
      ]
    }
  },
  {
    "id":"2411.00726",
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema",
    "start_abstract":"This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D\n  Gaussian Modeling",
        "Hyper3D: Efficient 3D Representation via Hybrid Triplane and Octree\n  Feature for Enhanced 3D Shape Variational Auto-Encoders",
        "Task-Specific Knowledge Distillation from the Vision Foundation Model\n  for Enhanced Medical Image Segmentation",
        "AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware\n  Preference Alignment of Multi-modal Large Language Models",
        "Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior",
        "A Plug-and-Play Learning-based IMU Bias Factor for Robust\n  Visual-Inertial Odometry",
        "Advanced Object Detection and Pose Estimation with Hybrid Task Cascade\n  and High-Resolution Networks",
        "X-Field: A Physically Grounded Representation for 3D X-ray\n  Reconstruction",
        "VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity",
        "An anatomically-informed correspondence initialisation method to improve\n  learning-based registration for radiotherapy",
        "Robust Computer-Vision based Construction Site Detection for\n  Assistive-Technology Applications",
        "Data-Efficient Generalization for Zero-shot Composed Image Retrieval",
        "How to Probe: Simple Yet Effective Techniques for Improving Post-hoc\n  Explanations",
        "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity\n  Grouping",
        "Coded Deep Learning: Framework and Algorithm",
        "Universal programmable and self-configuring optical filter",
        "Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent",
        "Harnessing the Potential of Large Language Models in Modern Marketing\n  Management: Applications, Future Directions, and Strategic Recommendations",
        "Development of Application-Specific Large Language Models to Facilitate\n  Research Ethics Review",
        "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models\n  in Multi-Hop Tool Use",
        "Contracting low degree points on curves",
        "Domain-conditioned and Temporal-guided Diffusion Modeling for\n  Accelerated Dynamic MRI Reconstruction",
        "A study of the Antlion Random Walk",
        "Evolving Skeletons: Motion Dynamics in Action Recognition",
        "B-Call: Integrating Ideological Position and Political Cohesion in\n  Legislative Voting Models",
        "Foliated Plateau problems, geometric rigidity and equidistribution of\n  closed $k$-surfaces",
        "Evidence for J\/$\\psi$ suppression in incoherent photonuclear production",
        "A Machine Learning Approach for Design of Frequency Selective Surface\n  based Radar Absorbing Material via Image Prediction"
      ],
      "abstract":[
        "Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution\n(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors\nusing a single model, addressing the limitations of traditional SR methods\nconstrained to fixed-scale factors (\\textit{e.g.}, $\\times$ 2). Recent advances\nleveraging implicit neural representation (INR) have achieved great progress by\nmodeling coordinate-to-pixel mappings. However, the efficiency of these methods\nmay suffer from repeated upsampling and decoding, while their reconstruction\nfidelity and quality are constrained by the intrinsic representational\nlimitations of coordinate-based functions. To address these challenges, we\npropose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which\nexplicitly reconstructs 2D continuous HR signals from LR images using Gaussian\nSplatting. This approach eliminates the need for time-consuming upsampling and\ndecoding, enabling extremely fast arbitrary-scale super-resolution. Once the\nGaussian field is built in a single pass, ContinuousSR can perform\narbitrary-scale rendering in just 1ms per scale. Our method introduces several\nkey innovations. Through statistical ana",
        "Recent 3D content generation pipelines often leverage Variational\nAutoencoders (VAEs) to encode shapes into compact latent representations,\nfacilitating diffusion-based generation. Efficiently compressing 3D shapes\nwhile preserving intricate geometric details remains a key challenge. Existing\n3D shape VAEs often employ uniform point sampling and 1D\/2D latent\nrepresentations, such as vector sets or triplanes, leading to significant\ngeometric detail loss due to inadequate surface coverage and the absence of\nexplicit 3D representations in the latent space. Although recent work explores\n3D latent representations, their large scale hinders high-resolution encoding\nand efficient training. Given these challenges, we introduce Hyper3D, which\nenhances VAE reconstruction through efficient 3D representation that integrates\nhybrid triplane and octree features. First, we adopt an octree-based feature\nrepresentation to embed mesh information into the network, mitigating the\nlimitations of uniform point sampling in capturing geometric distributions\nalong the mesh surface. Furthermore, we propose a hybrid latent space\nrepresentation that integrates a high-resolution triplane with a low-resolution\n3D grid. This design not only compensates for the lack of explicit 3D\nrepresentations but also leverages a triplane to preserve high-resolution\ndetails. Experimental results demonstrate that Hyper3D outperforms traditional\nrepresentations by reconstructing 3D shapes with higher fidelity and finer\ndetails, making it well-suited for 3D generation pipelines.",
        "Large-scale pre-trained models, such as Vision Foundation Models (VFMs), have\ndemonstrated impressive performance across various downstream tasks by\ntransferring generalized knowledge, especially when target data is limited.\nHowever, their high computational cost and the domain gap between natural and\nmedical images limit their practical application in medical segmentation tasks.\nMotivated by this, we pose the following important question: \"How can we\neffectively utilize the knowledge of large pre-trained VFMs to train a small,\ntask-specific model for medical image segmentation when training data is\nlimited?\" To address this problem, we propose a novel and generalizable\ntask-specific knowledge distillation framework. Our method fine-tunes the VFM\non the target segmentation task to capture task-specific features before\ndistilling the knowledge to smaller models, leveraging Low-Rank Adaptation\n(LoRA) to reduce the computational cost of fine-tuning. Additionally, we\nincorporate synthetic data generated by diffusion models to augment the\ntransfer set, enhancing model performance in data-limited scenarios.\nExperimental results across five medical image datasets demonstrate that our\nmethod consistently outperforms task-agnostic knowledge distillation and\nself-supervised pretraining approaches like MoCo v3 and Masked Autoencoders\n(MAE). For example, on the KidneyUS dataset, our method achieved a 28% higher\nDice score than task-agnostic KD using 80 labeled samples for fine-tuning. On\nthe CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeled\nsamples. These results underscore the potential of task-specific knowledge\ndistillation to train accurate, efficient models for medical image segmentation\nin data-constrained settings.",
        "Visual layouts are essential in graphic design fields such as advertising,\nposters, and web interfaces. The application of generative models for\ncontent-aware layout generation has recently gained traction. However, these\nmodels fail to understand the contextual aesthetic requirements of layout\ndesign and do not align with human-like preferences, primarily treating it as a\nprediction task without considering the final rendered output. To overcome\nthese problems, we offer Aesthetic-Aware Preference Alignment(AAPA), a novel\ntechnique to train a Multi-modal Large Language Model (MLLM) for layout\nprediction that uses MLLM's aesthetic preferences for Direct Preference\nOptimization over graphic layouts. We propose a data filtering protocol\nutilizing our layout-quality heuristics for AAPA to ensure training happens on\nhigh-quality layouts. Additionally, we introduce a novel evaluation metric that\nuses another MLLM to compute the win rate of the generated layout against the\nground-truth layout based on aesthetics criteria. We also demonstrate the\napplicability of AAPA for MLLMs of varying scales (1B to 8B parameters) and LLM\nfamilies (Qwen, Phi, InternLM). By conducting thorough qualitative and\nquantitative analyses, we verify the efficacy of our approach on two\nchallenging benchmarks - Crello and Webui, showcasing 17%, and 16 improvement\nover current State-of-The-Art methods, thereby highlighting the potential of\nMLLMs in aesthetic-aware layout generation.",
        "Can our brain signals faithfully reflect the original visual stimuli, even\nincluding high-frequency details? Although human perceptual and cognitive\ncapacities enable us to process and remember visual information, these\nabilities are constrained by several factors, such as limited attentional\nresources and the finite capacity of visual memory. When visual stimuli are\nprocessed by human visual system into brain signals, some information is\ninevitably lost, leading to a discrepancy known as the \\textbf{System GAP}.\nAdditionally, perceptual and cognitive dynamics, along with technical noise in\nsignal acquisition, degrade the fidelity of brain signals relative to the\nvisual stimuli, known as the \\textbf{Random GAP}. When encoded brain\nrepresentations are directly aligned with the corresponding pretrained image\nfeatures, the System GAP and Random GAP between paired data challenge the\nmodel, requiring it to bridge these gaps. However, in the context of limited\npaired data, these gaps are difficult for the model to learn, leading to\noverfitting and poor generalization to new data. To address these GAPs, we\npropose a simple yet effective approach called the \\textbf{Uncertainty-aware\nBlur Prior (UBP)}. It estimates the uncertainty within the paired data,\nreflecting the mismatch between brain signals and visual stimuli. Based on this\nuncertainty, UBP dynamically blurs the high-frequency details of the original\nimages, reducing the impact of the mismatch and improving alignment. Our method\nachieves a top-1 accuracy of \\textbf{50.9\\%} and a top-5 accuracy of\n\\textbf{79.7\\%} on the zero-shot brain-to-image retrieval task, surpassing\nprevious state-of-the-art methods by margins of \\textbf{13.7\\%} and\n\\textbf{9.8\\%}, respectively. Code is available at\n\\href{https:\/\/github.com\/HaitaoWuTJU\/Uncertainty-aware-Blur-Prior}{GitHub}.",
        "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https:\/\/github.com\/yiyscut\/VIO-IPNet.git}.",
        "In the field of computer vision, 6D object detection and pose estimation are\ncritical for applications such as robotics, augmented reality, and autonomous\ndriving. Traditional methods often struggle with achieving high accuracy in\nboth object detection and precise pose estimation simultaneously. This study\nproposes an improved 6D object detection and pose estimation pipeline based on\nthe existing 6D-VNet framework, enhanced by integrating a Hybrid Task Cascade\n(HTC) and a High-Resolution Network (HRNet) backbone. By leveraging the\nstrengths of HTC's multi-stage refinement process and HRNet's ability to\nmaintain high-resolution representations, our approach significantly improves\ndetection accuracy and pose estimation precision. Furthermore, we introduce\nadvanced post-processing techniques and a novel model integration strategy that\ncollectively contribute to superior performance on public and private\nbenchmarks. Our method demonstrates substantial improvements over\nstate-of-the-art models, making it a valuable contribution to the domain of 6D\nobject detection and pose estimation.",
        "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly\nregulated due to potential health risks. To mitigate radiation exposure, recent\nresearch focuses on generating novel views from sparse inputs and\nreconstructing Computed Tomography (CT) volumes, borrowing representations from\nthe 3D reconstruction area. However, these representations originally target\nvisible light imaging that emphasizes reflection and scattering effects, while\nneglecting penetration and attenuation properties of X-ray imaging. In this\npaper, we introduce X-Field, the first 3D representation specifically designed\nfor X-ray imaging, rooted in the energy absorption rates across different\nmaterials. To accurately model diverse materials within internal structures, we\nemploy 3D ellipsoids with distinct attenuation coefficients. To estimate each\nmaterial's energy absorption of X-rays, we devise an efficient path\npartitioning algorithm accounting for complex ellipsoid intersections. We\nfurther propose hybrid progressive initialization to refine the geometric\naccuracy of X-Filed and incorporate material-based optimization to enhance\nmodel fitting along material boundaries. Experiments show that X-Field achieves\nsuperior visual fidelity on both real-world human organ and synthetic object\ndatasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis\nand CT Reconstruction.",
        "Visual reasoning is central to human cognition, enabling individuals to\ninterpret and abstractly understand their environment. Although recent\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance across language and vision-language tasks, existing benchmarks\nprimarily measure recognition-based skills and inadequately assess true visual\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\nreason primarily from visual information, providing minimal textual context to\nreduce reliance on domain-specific knowledge and linguistic biases. Each\nproblem is accompanied by a human-annotated reasoning path, making it the first\nto provide in-depth evaluation of model decision-making processes.\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\nbeyond mere accuracy, highlighting critical imbalances in current model\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\nsignificant limitations, underscoring the need for a balanced and holistic\napproach to both perception and reasoning. For more teaser and testing, visit\nour project page (https:\/\/verify-eqh.pages.dev\/).",
        "We propose an anatomically-informed initialisation method for interpatient CT\nnon-rigid registration (NRR), using a learning-based model to estimate\ncorrespondences between organ structures. A thin plate spline (TPS)\ndeformation, set up using the correspondence predictions, is used to initialise\nthe scans before a second NRR step. We compare two established NRR methods for\nthe second step: a B-spline iterative optimisation-based algorithm and a deep\nlearning-based approach. Registration performance is evaluated with and without\nthe initialisation by assessing the similarity of propagated structures. Our\nproposed initialisation improved the registration performance of the\nlearning-based method to more closely match the traditional iterative\nalgorithm, with the mean distance-to-agreement reduced by 1.8mm for structures\nincluded in the TPS and 0.6mm for structures not included, while maintaining a\nsubstantial speed advantage (5 vs. 72 seconds).",
        "Navigating urban environments poses significant challenges for people with\ndisabilities, particularly those with blindness and low vision. Environments\nwith dynamic and unpredictable elements like construction sites are especially\nchallenging. Construction sites introduce hazards like uneven surfaces,\nobstructive barriers, hazardous materials, and excessive noise, and they can\nalter routing, complicating safe mobility. Existing assistive technologies are\nlimited, as navigation apps do not account for construction sites during trip\nplanning, and detection tools that attempt hazard recognition struggle to\naddress the extreme variability of construction paraphernalia. This study\nintroduces a novel computer vision-based system that integrates open-vocabulary\nobject detection, a YOLO-based scaffolding-pole detection model, and an optical\ncharacter recognition (OCR) module to comprehensively identify and interpret\nconstruction site elements for assistive navigation. In static testing across\nseven construction sites, the system achieved an overall accuracy of 88.56\\%,\nreliably detecting objects from 2m to 10m within a 0$^\\circ$ -- 75$^\\circ$\nangular offset. At closer distances (2--4m), the detection rate was 100\\% at\nall tested angles. At",
        "Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image\nbased on a reference image and a text description without requiring\nin-distribution triplets for training. One prevalent approach follows the\nvision-language pretraining paradigm that employs a mapping network to transfer\nthe image embedding to a pseudo-word token in the text embedding space.\nHowever, this approach tends to impede network generalization due to modality\ndiscrepancy and distribution shift between training and inference. To this end,\nwe propose a Data-efficient Generalization (DeG) framework, including two novel\ndesigns, namely, Textual Supplement (TS) module and Semantic-Set (S-Set). The\nTS module exploits compositional textual semantics during training, enhancing\nthe pseudo-word token with more linguistic semantics and thus mitigating the\nmodality discrepancy effectively. The S-Set exploits the zero-shot capability\nof pretrained Vision-Language Models (VLMs), alleviating the distribution shift\nand mitigating the overfitting issue from the redundancy of the large-scale\nimage-text data. Extensive experiments over four ZS-CIR benchmarks show that\nDeG outperforms the state-of-the-art (SOTA) methods with much less training\ndata, and saves substantial training and inference time for practical usage.",
        "Post-hoc importance attribution methods are a popular tool for \"explaining\"\nDeep Neural Networks (DNNs) and are inherently based on the assumption that the\nexplanations can be applied independently of how the models were trained.\nContrarily, in this work we bring forward empirical evidence that challenges\nthis very notion. Surprisingly, we discover a strong dependency on and\ndemonstrate that the training details of a pre-trained model's classification\nlayer (less than 10 percent of model parameters) play a crucial role, much more\nthan the pre-training scheme itself. This is of high practical relevance: (1)\nas techniques for pre-training models are becoming increasingly diverse,\nunderstanding the interplay between these techniques and attribution methods is\ncritical; (2) it sheds light on an important yet overlooked assumption of\npost-hoc attribution methods which can drastically impact model explanations\nand how they are interpreted eventually. With this finding we also present\nsimple yet effective adjustments to the classification layers, that can\nsignificantly enhance the quality of model explanations. We validate our\nfindings across several visual pre-training frameworks (fully-supervised,\nself-supervised, contrastive vision-language training) and analyse how they\nimpact explanations for a wide range of attribution methods on a diverse set of\nevaluation metrics.",
        "Class Incremental Learning (CIL) aims to enable models to learn new classes\nsequentially while retaining knowledge of previous ones. Although current\nmethods have alleviated catastrophic forgetting (CF), recent studies highlight\nthat the performance of CIL models is highly sensitive to the order of class\narrival, particularly when sequentially introduced classes exhibit high\ninter-class similarity. To address this critical yet understudied challenge of\nclass order sensitivity, we first extend existing CIL frameworks through\ntheoretical analysis, proving that grouping classes with lower pairwise\nsimilarity during incremental phases significantly improves model robustness to\norder variations. Building on this insight, we propose Graph-Driven Dynamic\nSimilarity Grouping (GDDSG), a novel method that employs graph coloring\nalgorithms to dynamically partition classes into similarity-constrained groups.\nEach group trains an isolated CIL sub-model and constructs meta-features for\nclass group identification. Experimental results demonstrate that our method\neffectively addresses the issue of class order sensitivity while achieving\noptimal performance in both model accuracy and anti-forgetting capability. Our\ncode is available at https:\/\/github.com\/AIGNLAI\/GDDSG.",
        "The success of deep learning (DL) is often achieved with large models and\nhigh complexity during both training and post-training inferences, hindering\ntraining in resource-limited settings. To alleviate these issues, this paper\nintroduces a new framework dubbed ``coded deep learning'' (CDL), which\nintegrates information-theoretic coding concepts into the inner workings of DL,\nto significantly compress model weights and activations, reduce computational\ncomplexity at both training and post-training inference stages, and enable\nefficient model\/data parallelism. Specifically, within CDL, (i) we first\npropose a novel probabilistic method for quantizing both model weights and\nactivations, and its soft differentiable variant which offers an analytic\nformula for gradient calculation during training; (ii) both the forward and\nbackward passes during training are executed over quantized weights and\nactivations, eliminating most floating-point operations and reducing training\ncomplexity; (iii) during training, both weights and activations are entropy\nconstrained so that they are compressible in an information-theoretic sense\nthroughout training, thus reducing communication costs in model\/data\nparallelism; and (iv) the trained model in CDL is by default in a quantized\nformat with compressible quantized weights, reducing post-training inference\nand storage complexity. Additionally, a variant of CDL, namely relaxed CDL\n(R-CDL), is presented to further improve the trade-off between validation\naccuracy and compression though requiring full precision in training with other\nadvantageous features of CDL intact. Extensive empirical results show that CDL\nand R-CDL outperform the state-of-the-art algorithms in DNN compression in the\nliterature.",
        "We propose an approach to integrated optical spectral filtering that allows\narbitrary programmability, can compensate automatically for imperfections in\nfilter fabrication, allows multiple simultaneous and separately programmable\nfilter functions on the same input, and can configure itself automatically to\nthe problem of interest, for example to filter or reject multiple arbitrarily\nchosen frequencies. The approach exploits splitting the input light into an\narray of multiple waveguides of different lengths that then feed a programmable\ninterferometer array that can also self-configure. It can give spectral\nresponse similar to arrayed waveguide gratings but offers many other filtering\nfunctions, as well as supporting other structures based on non-redundant arrays\nfor precise spectral filtering. Simultaneous filtering also allows, for the\nfirst time to our knowledge, an automatic measurement of the temporal coherency\nmatrix and physical separation into the Karhunen-Lo\\`eve expansion of\ntemporally partially coherent light fields.",
        "In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1\/\\sqrt{n}$.",
        "Large Language Models (LLMs) have revolutionized the process of customer\nengagement, campaign optimization, and content generation, in marketing\nmanagement. In this paper, we explore the transformative potential of LLMs\nalong with the current applications, future directions, and strategic\nrecommendations for marketers. In particular, we focus on LLMs major business\ndrivers such as personalization, real-time-interactive customer insights, and\ncontent automation, and how they enable customers and business outcomes. For\ninstance, the ethical aspects of AI with respect to data privacy, transparency,\nand mitigation of bias are also covered, with the goal of promoting responsible\nuse of the technology through best practices and the use of new technologies\nbusinesses can tap into the LLM potential, which help growth and stay one step\nahead in the turmoil of digital marketing. This article is designed to give\nmarketers the necessary guidance by using best industry practices to integrate\nthese powerful LLMs into their marketing strategy and innovation without\ncompromising on the ethos of their brand.",
        "Institutional review boards (IRBs) play a crucial role in ensuring the\nethical conduct of human subjects research, but face challenges including\ninconsistency, delays, and inefficiencies. We propose the development and\nimplementation of application-specific large language models (LLMs) to\nfacilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on\nIRB-specific literature and institutional datasets, and equipped with retrieval\ncapabilities to access up-to-date, context-relevant information. We outline\npotential applications, including pre-review screening, preliminary analysis,\nconsistency checking, and decision support. While addressing concerns about\naccuracy, context sensitivity, and human oversight, we acknowledge remaining\nchallenges such as over-reliance on AI and the need for transparency. By\nenhancing the efficiency and quality of ethical review while maintaining human\njudgment in critical decisions, IRB-specific LLMs offer a promising tool to\nimprove research oversight. We call for pilot studies to evaluate the\nfeasibility and impact of this approach.",
        "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https:\/\/huggingface.co\/datasets\/bytedance-research\/ToolHop.",
        "The main result of this article is that all but finitely many points of small\nenough degree on a curve can be written as a pullback of a smaller degree\npoint. The main theorem has several corollaries that yield improvements on\nresults of Kadets and Vogt, Khawaja and Siksek, and Vojta under a slightly\nstronger assumption on the degree of the points.",
        "Purpose: To propose a domain-conditioned and temporal-guided diffusion\nmodeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated\ndynamic MRI reconstruction, enabling diffusion process to characterize\nspatiotemporal information for time-resolved multi-coil Cartesian and\nnon-Cartesian data. Methods: The dDiMo framework integrates temporal\ninformation from time-resolved dimensions, allowing for the concurrent capture\nof intra-frame spatial features and inter-frame temporal dynamics in diffusion\nmodeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent\nfrequency-temporal ($k$-$t$) priors to guide the diffusion process. This\napproach ensures precise temporal alignment and enhances the recovery of fine\nimage details. To facilitate a smooth diffusion process, the nonlinear\nconjugate gradient algorithm is utilized during the reverse diffusion steps.\nThe proposed model was tested on two types of MRI data: Cartesian-acquired\nmulti-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil\nfree-breathing lung MRI, across various undersampling rates. Results: dDiMo\nachieved high-quality reconstructions at various acceleration factors,\ndemonstrating improved temporal alignment and structural recovery compared to\nother competitive reconstruction methods, both qualitatively and\nquantitatively. This proposed diffusion framework exhibited robust performance\nin handling both Cartesian and non-Cartesian acquisitions, effectively\nreconstructing dynamic datasets in cardiac and lung MRI under different imaging\nconditions. Conclusion: This study introduces a novel diffusion modeling method\nfor dynamic MRI reconstruction.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "Skeleton-based action recognition has gained significant attention for its\nability to efficiently represent spatiotemporal information in a lightweight\nformat. Most existing approaches use graph-based models to process skeleton\nsequences, where each pose is represented as a skeletal graph structured around\nhuman physical connectivity. Among these, the Spatiotemporal Graph\nConvolutional Network (ST-GCN) has become a widely used framework.\nAlternatively, hypergraph-based models, such as the Hyperformer, capture\nhigher-order correlations, offering a more expressive representation of complex\njoint interactions. A recent advancement, termed Taylor Videos, introduces\nmotion-enhanced skeleton sequences by embedding motion concepts, providing a\nfresh perspective on interpreting human actions in skeleton-based action\nrecognition. In this paper, we conduct a comprehensive evaluation of both\ntraditional skeleton sequences and Taylor-transformed skeletons using ST-GCN\nand Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal\ngraph and hypergraph representations, analyzing static poses against\nmotion-injected poses. Our findings highlight the strengths and limitations of\nTaylor-transformed skeletons, demonstrating their potential to enhance motion\ndynamics while exposing current challenges in fully using their benefits. This\nstudy underscores the need for innovative skeletal modelling techniques to\neffectively handle motion-rich data and advance the field of action\nrecognition.",
        "This paper combines two significant areas of political science research:\nmeasuring individual ideological position and cohesion. Although both\napproaches help analyze legislative behaviors, no unified model currently\nintegrates these dimensions. To fill this gap, the paper proposes a methodology\ncalled B-Call that combines ideological positioning with voting cohesion,\ntreating votes as random variables. The model is empirically validated using\nroll-call data from the United States, Brazil, and Chile legislatures, which\nrepresent diverse legislative dynamics. The analysis aims to capture the\ncomplexities of voting and legislative behaviors, resulting in a\ntwo-dimensional indicator. This study addresses gaps in current legislative\nvoting models, particularly in contexts with limited party control.",
        "In this note, we survey recent advances in the study of dynamical properties\nof the space of surfaces with constant curvature in three-dimensional manifolds\nof negative sectional curvature. We interpret this space as a two-dimensional\nanalogue of the geodesic flow and explore the extent to which the thermodynamic\nproperties of the latter can be generalized to the surface setting.\nAdditionally, we apply this theory to derive geometric rigidity results,\nincluding the rigidity of the hyperbolic marked area spectrum.",
        "According to quantum chromodynamics, at sufficiently high energy, the\nstructure of hadrons reveals a dynamic equilibrium between gluon splitting and\ngluon recombination -- a phenomenon known as saturation. The process of\ndiffractive photonuclear production of a J\/$\\psi$ vector meson provides a\ndirect insight into the gluon composition of hadrons. The J\/$\\psi$ production\nas a function of momentum transferred in the interaction, quantified by the\nMandelstam-$t$ variable, serves as an excellent probe for studying the\nstructure of hadrons within the impact-parameter plane, because different\nranges in $t$ are sensitive to the dynamics of the gluon field at varying\nspatial size scales. The ALICE collaboration has measured the energy dependence\nof incoherent photonuclear production of J\/$\\psi$ mesons off lead ions, at\n$\\sqrt{s_{\\rm NN}} = 5.02$ TeV, for three Mandelstam-$t$ intervals. The energy\ndependence of the photonuclear cross section at the highest $|t|$ range\nmeasured, $(0.81< |t| <1.44)$ GeV$^2$, is sensitive to subnucleonic structures\nof the Pb target. The increase of the cross section with energy at large $|t|$\nshows evidence of suppression with respect to the increase seen at low $|t|$.\nThe observed pattern of the energy evolution in data is similar to that of\ngluon saturation models.",
        "The paper presents an innovative methodology for designing frequency\nselective surface (FSS) based radar absorbing materials using machine learning\n(ML) technique. In conventional electromagnetic design, unit cell dimensions of\nFSS are used as input and absorption coefficient is then predicted for a given\ndesign. In this paper, absorption coefficient is considered as input to ML\nmodel and image of FSS unit cell is predicted. Later, this image is used for\ngenerating the FSS unit cell parameters. Eleven different ML models are studied\nover a wide frequency band of 1GHz to 30GHz. Out of which six ML models (i.e.\n(a) Random Forest classification, (b) K- Neighbors Classification, (c) Grid\nsearch regression, (d) Random Forest regression, (e) Decision tree\nclassification, and (f) Decision tree regression) show training accuracy more\nthan 90%. The absorption coefficients with varying frequencies of these\npredicted images are subsequently evaluated using commercial electromagnetic\nsolver. The performance of these ML models is encouraging, and it can be used\nfor accelerating design and optimization of high performance FSS based radar\nabsorbing material for advanced electromagnetic applications in future."
      ]
    }
  },
  {
    "id":"2411.00726",
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema"
      ],
      "abstract":[
        "This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Social dynamics can delay or prevent climate tipping points by speeding\n  the adoption of climate change mitigation",
        "Greedy Stein Variational Gradient Descent: An algorithmic approach for\n  wave prospection problems",
        "Uncertainty-permitting machine learning reveals sources of dynamic sea\n  level predictability across daily-to-seasonal timescales",
        "Leveraging statistical models to improve pre-season forecasting and\n  in-season management of a recreational fishery",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Accurate myocardial T1 mapping at 5T using an improved MOLLI method: A\n  validation study",
        "Sparse Identification for bifurcating phenomena in Computational Fluid\n  Dynamics",
        "The $p$-adic limits of iterated $p$-power cyclic resultants of\n  multivariable polynomials",
        "An exactly solvable tight-binding billiard in graphene",
        "What Kind of Morphisms Induces Covering Maps over a Real Closed Field?",
        "Learning Robot Safety from Sparse Human Feedback using Conformal\n  Prediction",
        "Semiclassical resolvent estimates for the magnetic Schr \\\"odinger\n  operator",
        "On Construction, Properties and Simulation of Haar-Based Multifractional\n  Processes",
        "TRANSIT your events into a new mass: Fast background interpolation for\n  weakly-supervised anomaly searches"
      ],
      "abstract":[
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Social behaviour models are increasingly integrated into climate change\nstudies, and the significance of climate tipping points for `runaway' climate\nchange is well recognised. However, there has been insufficient focus on\ntipping points in social-climate dynamics. We developed a coupled\nsocial-climate model consisting of an Earth system model and a social behaviour\nmodel, both with tipping elements. The social model explores opinion formation\nby analysing social learning rates, the net cost of mitigation, and the\nstrength of social norms. Our results indicate that the net cost of mitigation\nand social norms have minimal impact on tipping points when social norms are\nweak. As social norms strengthen, the climate tipping point can trigger a\ntipping element in the social model. However, faster social learning can delay\nor prevent the climate tipping point: sufficiently fast social learning means\ngrowing climate change mitigation can outpace the oncoming climate tipping\npoint, despite social-climate feedback. By comparing high- and low-risk\nscenarios, we demonstrated high-risk scenarios increase the likelihood of\ntipping points. We also illustrate the role of a critical temperature anomaly\nin triggering tipping points. In conclusion, understanding social behaviour\ndynamics is vital for predicting climate tipping points and mitigating their\nimpacts.",
        "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
        "Reliable dynamic sea level forecasts are hindered by numerous sources of\nuncertainty on daily-to-seasonal timescales (1-180 days) due to atmospheric\nboundary conditions and internal ocean variability. Studies have demonstrated\nthat certain initial states can extend predictability horizons; thus,\nidentifying these initial conditions may help improve forecast skill. Here, we\nidentify sources of dynamic sea level predictability on daily-to-seasonal\ntimescales using neural networks trained on CESM2 large ensemble data to\nforecast dynamic sea level. The forecasts yield not only a point estimate for\nsea level but also a standard deviation to quantify forecast uncertainty based\non the initial conditions. Forecasted uncertainties can be leveraged to\nidentify state-dependent sources of predictability at most locations and\nforecast leads. Network forecasts, particularly in the low-latitude\nIndo-Pacific, exhibit skillful deterministic predictions and skillfully\nforecast exceedance probabilities relative to local linear baselines. For\nnetworks trained at Guam and in the western Indian Ocean, the transfer of\nsources of predictability from local sources to remote sources is presented by\nthe deteriorating utility of initial condition information for predicting\nexceedance events. Propagating Rossby waves are identified as a potential\nsource of predictability for dynamic sea level at Guam. In the Indian Ocean,\npersistence of thermosteric sea level anomalies from the Indian Ocean Dipole\nmay be a source of predictability on subseasonal timescales, but El Ni\\~no\ndrives predictability on seasonal timescales. This work shows how\nuncertainty-quantifying machine learning can help identify changes in sources\nof state-dependent predictability over a range of forecast leads.",
        "Effective management of recreational fisheries requires accurate forecasting\nof future harvests and real-time monitoring of ongoing harvests. Traditional\nmethods that rely on historical catch data to predict short-term harvests can\nbe unreliable, particularly if changes in management regulations alter angler\nbehavior. In contrast, statistical modeling approaches can provide faster, more\nflexible, and potentially more accurate predictions, enhancing management\noutcomes. In this study, we developed and tested models to improve predictions\nof Gulf of Mexico gag harvests for both pre-season planning and in-season\nmonitoring. Our best-fitting model outperformed traditional methods (i.e.,\nestimates derived from historical average harvest) for both cumulative\npre-season projections and in-season monitoring. Notably, our modeling\nframework appeared to be more accurate in more recent, shorter seasons due to\nits ability to account for effort compression. A key advantage of our framework\nis its ability to explicitly quantify the probability of exceeding harvest\nquotas for any given season duration. This feature enables managers to evaluate\ntrade-offs between season duration and conservation goals. This is especially\ncritical for vulnerable, highly targeted stocks. Our findings also underscore\nthe value of statistical models to complement and advance traditional fisheries\nmanagement approaches.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "Purpose: To develop 5T-SRIS, an improved 5T myocardial T1 mapping method\nbased on MOLLI, which addresses limitations in inversion efficiency, readout\nperturbations, and imperfect magnetization recovery. Methods: The proposed\n5T-SRIS method is based on a modified 5-(3)-3 MOLLI sequence with ECG gating\nand gradient echo readout. To improve inversion efficiency at 5T, the inversion\npulse was redesigned using adiabatic hyperbolic secant (HSn) and\ntangent\/hyperbolic tangent (Tan\/Tanh) pulses. Signal evolution was modeled\nrecursively with inversion efficiency and a correction factor (C) to correct\ninversion imperfections, and T1 values were estimated via nonlinear\noptimization. The method was validated in phantom studies, as well as in 21\nhealthy volunteers and 9 patients at 5T. Results: The optimized IR pulse based\non the tangent\/hyperbolic tangent pulse was found to outperform the\nconventional hyperbolic secant IR pulse at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014within a B0 range of 250Hz\nand a B1 range of -50% to 20%. Phantom studies show that the 5T-SRIS achieved\nhigh accuracy with errors within 5%. In vivo studies with 21 healthy\nvolunteers, the native myocardial T1 values were 1468 ms (apex), 1514 ms\n(middle), and 1545 ms (base). In vivo studies with 9 heart patients, the native\nmyocardial T1 values were 1484 ms (apex), 1532 ms (middle), and 1581 ms (base).\nAnd the post myocardial T1 values were 669 ms (apex), 698 ms (middle), and 675\nms (base). Conclusion: The 5T-SRIS technique is robust and suitable for\nclinical cardiac imaging. This study demonstrates its feasibility for accurate\nmyocardial T1 mapping at 5T, despite challenges related to magnetic field\ninhomogeneity. Keywords: Myocardial T1 mapping, 5T, improved MOLLI, 5T-SRIS",
        "This work investigates model reduction techniques for nonlinear parameterized\nand time-dependent PDEs, specifically focusing on bifurcating phenomena in\nComputational Fluid Dynamics (CFD). We develop interpretable and non-intrusive\nReduced Order Models (ROMs) capable of capturing dynamics associated with\nbifurcations by identifying a minimal set of coordinates. Our methodology\ncombines the Sparse Identification of Nonlinear Dynamics (SINDy) method with a\ndeep learning framework based on Autoencoder (AE) architectures. To enhance\ndimensionality reduction, we integrate a nested Proper Orthogonal Decomposition\n(POD) with the SINDy-AE architecture. This novel combination enables a sparse\ndiscovery of system dynamics while maintaining efficiency of the reduced model.\nWe demonstrate our approach via two challenging test cases defined on\nsudden-expansion channel geometries: a symmetry-breaking bifurcation and a Hopf\nbifurcation. Starting from a comprehensive analysis of their high-fidelity\nbehavior, i.e. symmetry-breaking phenomena and the rise of unsteady periodic\nsolutions, we validate the accuracy and computational efficiency of our ROMs.\nThe results show successful reconstruction of the bifurcations, accurate\nprediction of system evolution for unseen parameter values, and significant\nspeed-up compared to full-order methods.",
        "Let $p$ be a prime number. The $p$-power cyclic resultant of a polynomial is\nthe determinant of the Sylvester matrix of $t^{p^n}-1$ and the polynomial. It\nis known that the sequence of $p$-power cyclic resultants and its non-$p$-parts\nconverge in $\\mathbb{Z}_p$. This article shows the $p$-adic convergence of the\niterated $p$-power cyclic resultants of multivariable polynomials. As an\napplication, we show the $p$-adic convergence of the torsion numbers of\n$\\mathbb{Z}_p^d$-coverings of links. We also explicitly calculate the $p$-adic\nlimits for the twisted Whitehead links as concrete examples. Moreover, in a\nspecific case, we show that our $p$-adic limit of torsion numbers coincides\nwith the $p$-adic torsion, which is a homotopy invariant of a CW-complex\nintroduced by S. Kionke.",
        "A triangular graphenic billiard is defined as a planar carbon polymer in the\nH\\\"uckeloid approximation of $\\pi-$band electrons. It is shown that the\nequilateral triangle of arbitrary size and zig-zag edges allows for exact\nsolutions of the associated spectral problem. This is done by a construction of\nwave superpositions similar to the Lam\\'e solution of the Helmholtz equation in\na triangular cavity, revisited by Pinsky. Exact wave functions, eigenvalues,\ndegeneracies, and edge states are provided. The edge states are also obtained\nby a non-periodic construction of waves with vanishing energy. A comment on its\nconnection with recent molecular models, such as triangulene, is given.",
        "In this article, we show that a flat morphism of $k$-varieties\n($\\mathop{\\mathrm{char}} k=0$) with locally constant geometric fibers becomes\nfinite \\'etale after reduction. When $k$ is a real closed field, we prove that\nsuch a morphism induces a covering map on the rational points. We further give\na triviality result different from Hardt's and a new interpretation of the\nconstruction of cylindrical algebraic decomposition as applications.",
        "Ensuring robot safety can be challenging; user-defined constraints can miss\nedge cases, policies can become unsafe even when trained from safe data, and\nsafety can be subjective. Thus, we learn about robot safety by showing policy\ntrajectories to a human who flags unsafe behavior. From this binary feedback,\nwe use the statistical method of conformal prediction to identify a region of\nstates, potentially in learned latent space, guaranteed to contain a\nuser-specified fraction of future policy errors. Our method is\nsample-efficient, as it builds on nearest neighbor classification and avoids\nwithholding data as is common with conformal prediction. By alerting if the\nrobot reaches the suspected unsafe region, we obtain a warning system that\nmimics the human's safety preferences with guaranteed miss rate. From video\nlabeling, our system can detect when a quadcopter visuomotor policy will fail\nto steer through a designated gate. We present an approach for policy\nimprovement by avoiding the suspected unsafe region. With it we improve a model\npredictive controller's safety, as shown in experimental testing with 30\nquadcopter flights across 6 navigation tasks. Code and videos are provided.",
        "We obtain semiclassical resolvent estimates for the Schr{\\\"o}dinger operator\n(ih$\\nabla$ + b)^2 + V in R^d , d $\\ge$ 3, where h is a semiclassical\nparameter, V and b are real-valued electric and magnetic potentials independent\nof h.Under quite general assumptions, we prove that the norm of the weighted\nresolvent is bounded by exp(Ch^{-2} log(h^{ -1} )) . We get better resolvent\nbounds for electric potentials which are H{\\\"o}lder with respect to the radial\nvariable and magnetic potentials which are H{\\\"o}lder with respect to the space\nvariable. For long-range electric potentials which are Lipschitz with respect\nto the radial variable and long-range magnetic potentials which are Lipschitz\nwith respect to the space variable we obtain a resolvent bound of the form\nexp(Ch^{-1}) .",
        "Multifractional processes extend the concept of fractional Brownian motion by\nreplacing the constant Hurst parameter with a time-varying Hurst function. This\nextension allows for modulation of the roughness of sample paths over time. The\npaper introduces a new class of multifractional processes, the Gaussian\nHaar-based multifractional processes (GHBMP), which is based on the Haar\nwavelet series representations. The resulting processes cover a significantly\nbroader set of Hurst functions compared to the existing literature, enhancing\ntheir suitability for both practical applications and theoretical studies. The\ntheoretical properties of these processes are investigated. Simulation studies\nconducted for various Hurst functions validate the proposed model and\ndemonstrate its applicability, even for Hurst functions exhibiting\ndiscontinuous behaviour.",
        "We introduce a new model for conditional and continuous data morphing called\nTRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it\nto create a background data template for weakly-supervised searches at the LHC.\nThe method smoothly transforms sideband events to match signal region mass\ndistributions. We demonstrate the performance of TRANSIT using the LHC Olympics\nR\\&D dataset. The model captures non-linear mass correlations of features and\nproduces a template that offers a competitive anomaly sensitivity compared to\nstate-of-the-art transport-based template generators. Moreover, the\ncomputational training time required for TRANSIT is an order of magnitude lower\nthan that of competing deep learning methods. This makes it ideal for analyses\nthat iterate over many signal regions and signal models. Unlike generative\nmodels, which must learn a full probability density distribution, i.e., the\ncorrelations between all the variables, the proposed transport model only has\nto learn a smooth conditional shift of the distribution. This allows for a\nsimpler, more efficient residual architecture, enabling mass uncorrelated\nfeatures to pass the network unchanged while the mass correlated features are\nadjusted accordingly. Furthermore, we show that the latent space of the model\nprovides a set of mass decorrelated features useful for anomaly detection\nwithout background sculpting."
      ]
    }
  },
  {
    "id":"2411.05236",
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel",
    "start_abstract":"Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2",
        "b0"
      ],
      "title":[
        "Shannon capacity of signal transduction for multiple independent receptors",
        "DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT"
      ],
      "abstract":[
        "Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
        "Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver."
      ],
      "categories":[
        "eess.SP",
        "cs.SY"
      ]
    },
    "list":{
      "title":[
        "Diffusion Model for Multiple Antenna Communications",
        "Research on the Offshore Marine Communication Environment Based on\n  Satellite Remote Sensing Data",
        "A Hybrid Dynamic Subarray Architecture for Efficient DOA Estimation in\n  THz Ultra-Massive Hybrid MIMO Systems",
        "Diffeomorphic ICP Registration for Single and Multiple Point Sets",
        "SDM Optical Systems with MMSE Equalizers: Information Rates and\n  Performance Monitoring",
        "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous\n  Normalizing Flow with Self-Attention",
        "Noise Modulation over Wireless Energy Transfer: JEIH-Noise Mod",
        "Robust Phantom-Assisted Framework for Multi-Person Localization and\n  Vital Signs Monitoring Using MIMO FMCW Radar",
        "Multi-Cell Coordinated Beamforming for Integrate Communication and\n  Multi-TMT Localization",
        "Modern Base Station Architecture: Enabling Passive Beamforming with\n  Beyond Diagonal RISs",
        "An Efficient Pre-Processing Method for 6G Dynamic Ray-Tracing Channel\n  Modeling",
        "Kise-Manitow's Hand in Space: Securing Communication and Connections in\n  Space",
        "Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based\n  Tiling Generation",
        "A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning",
        "Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf",
        "Enhancing Retrieval Systems with Inference-Time Logical Reasoning",
        "Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization",
        "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "Parallelizing Multi-objective A* Search",
        "The Role of Artificial Intelligence in Enhancing Insulin Recommendations\n  and Therapy Outcomes",
        "Algorithmical Aspects of Some Bio Inspired Operations",
        "Proximal Flow Inspired Multi-Step Methods",
        "E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization",
        "Fast-COS: A Fast One-Stage Object Detector Based on Reparameterized\n  Attention Vision Transformer for Autonomous Driving",
        "One-Loop QCD Corrections to $\\bar{u}d \\rightarrow t\\bar{t}W$ at\n  $\\mathcal{O}(\\varepsilon^2)$",
        "Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches",
        "Realization of Two-dimensional Discrete Time Crystals with Anisotropic\n  Heisenberg Coupling"
      ],
      "abstract":[
        "The potential of applying diffusion models (DMs) for multiple antenna\ncommunications is discussed. A unified framework of applying DM for multiple\nantenna tasks is first proposed. Then, the tasks are innovatively divided into\ntwo categories, i.e., decision-making tasks and generation tasks, depending on\nwhether an optimization of system parameters is involved. For each category, it\nis conceived 1) how the framework can be used for each task and 2) why the DM\nis superior to traditional artificial intelligence (TAI) and conventional\noptimization tasks. It is highlighted that the DMs are well-suited for\nscenarios with strong interference and noise, excelling in modeling complex\ndata distribution and exploring better actions. A case study of learning\nbeamforming with a DM is then provided, to demonstrate the superiority of the\nDMs with simulation results. Finally, the applications of DM for emerging\nmultiple antenna technologies and promising research directions are discussed.",
        "Air-sea interface fluxes significantly impact the reliability and efficiency\nof maritime communication. Compared to sparse in-situ ocean observations,\nsatellite remote sensing data offers broader coverage and extended temporal\nspan. This study utilizes COARE V3.5 algorithm to calculate momentum flux,\nsensible heat flux, and latent heat flux at the air-sea interface, based on\nsatellite synthetic aperture radar (SAR) wind speed data, reanalysis data, and\nbuoy measurements, combined with neural network methods. Findings indicate that\nSAR wind speed data corrected via neural networks show improved consistency\nwith buoy-measured wind speeds in flux calculations. Specifically, the bias in\nfriction velocity decreased from -0.03 m\/s to 0.01 m\/s, wind stress bias from\n-0.03 N\/m^2 to 0.00 N\/m^2, drag coefficient bias from -0.29 to -0.21, latent\nheat flux bias from -8.32 W\/m^2 to 5.41 W\/m^2, and sensible heat flux bias from\n0.67 W\/m^2 to 0.06 W\/m^2. Results suggest that the neural network-corrected SAR\nwind speed data can provide more reliable environmental data for maritime\ncommunication.",
        "Terahertz (THz) communication combined with ultra-massive multiple-input\nmultiple-output (UM-MIMO) technology is promising for 6G wireless systems,\nwhere fast and precise direction-of-arrival (DOA) estimation is crucial for\neffective beamforming. However, finding DOAs in THz UM-MIMO systems faces\nsignificant challenges: while reducing hardware complexity, the hybrid\nanalog-digital (HAD) architecture introduces inherent difficulties in spatial\ninformation acquisition the large-scale antenna array causes significant\ndeviations in eigenvalue decomposition results; and conventional\ntwo-dimensional DOA estimation methods incur prohibitively high computational\noverhead, hindering fast and accurate realization. To address these challenges,\nwe propose a hybrid dynamic subarray (HDS) architecture that strategically\ndivides antenna elements into subarrays, ensuring phase differences between\nsubarrays correlate exclusively with single-dimensional DOAs. Leveraging this\narchitectural innovation, we develop two efficient algorithms for DOA\nestimation: a reduced-dimension MUSIC (RD-MUSIC) algorithm that enables fast\nprocessing by correcting large-scale array estimation bias, and an improved\nversion that further accelerates estimation by exploiting THz channel sparsity\nto obtain initial closed-form solutions through specialized two-RF-chain\nconfiguration. Furthermore, we develop a theoretical framework through\nCram\\'{e}r-Rao lower bound analysis, providing fundamental insights for\ndifferent HDS configurations. Extensive simulations demonstrate that our\nsolution achieves both superior estimation accuracy and computational\nefficiency, making it particularly suitable for practical THz UM-MIMO systems.",
        "We propose a generalization of the iterative closest point (ICP) algorithm\nfor point set registration, in which the registration functions are non-rigid\nand follow the large deformation diffeomorphic metric mapping (LDDMM)\nframework. The algorithm is formulated as a well-posed probabilistic inference,\nand requires to solve a novel variation of LDDMM landmark registration with an\nadditional term involving the Jacobian of the mapping. The algorithm can easily\nbe generalized to construct a diffeomorphic, statistical atlas of multiple\npoint sets. The method is successfully validated on a first set of synthetic\ndata.",
        "The information rate of coupled space-division multiplexing (SDM)\ntransmission systems is impaired by the stochastic effects of mode-dependent\ngain (MDG) and mode-dependent loss (MDL), turning it into a random variable and\nreducing its average value. In systems operating with minimum mean squared\nerror (MMSE) equalizers and no channel-state information (CSI), co-channel\ninterference further reduces the instantaneous and average information rates.\nAnalytical solutions for the average information rate in MDG- and MDL-impaired\nsystems under strong coupling have been presented in early studies assuming\nideal maximum-likelihood (ML) equalization. However, to the best of our\nknowledge, a solution encompassing co-channel interference under MMSE\nequalization has not been presented yet. In this work, we derive statistical\nmodels for the MMSE equalizer coefficients and develop analytical solutions for\nthe post-filtering information rate. We also use these statistical models and\nanalytical solutions to carry out MDG and signal-to-noise ratio (SNR)\nmonitoring in coupled SDM systems. The derived analytical solutions and\nmonitoring techniques are validated by Monte-Carlo simulations, exhibiting a\nsuitable accuracy within practical operational values.",
        "As 3D point clouds become the prevailing shape representation in computer\nvision, how to generate high-resolution point clouds has become a pressing\nissue. Flow-based generative models can effectively perform point cloud\ngeneration tasks. However, traditional CNN-based flow architectures rely only\non local information to extract features, making it difficult to capture global\ncontextual information. Inspired by the wide adoption of Transformers, we\nexplored the complementary roles of self-attention mechanisms in Transformers,\nCNN, and continuous normalizing flows. To this end, we propose a probabilistic\nmodel via adaptive normalizing flows and self-attention. Our idea leverages\nself-attention mechanisms to capture global contextual information. We also\npropose adaptive continuous normalizing flows by introducing adaptive bias\ncorrection mechanism. Combined with normalization, the mechanism dynamically\nhandles different input contexts and mitigates potential bias-shift issues from\nstandard initialization. Experimental results demonstrate that EAGLE achieves\ncompetitive performance in point cloud generation.",
        "This letter presents an innovative energy harvesting (EH) and communication\nscheme for Internet of Things (IoT) devices by utilizing the emerging noise\nmodulation (Noise-Mod) technique. Our proposed approach embeds information into\nthe mean value of real Gaussian noise samples, enabling simultaneous power\ntransfer and data transmission. We analyze our system under the Rician fading\nchannels with path loss and derive the bit error probability (BEP) expression.\nOur simulation results demonstrate that the proposed scheme outperforms\nconventional modulation schemes in terms of energy harvesting capability across\nvarious channel conditions. This scheme offers a novel solution by directly\nembedding data into the noise-modulated signal to enable information decoding\nthrough mean-based detection. Furthermore, it increases energy harvesting\ncapability thanks to the utilized Gaussian waveform.",
        "With the rising prevalence of cardiovascular and respiratory disorders and an\naging global population, healthcare systems face increasing pressure to adopt\nefficient, non-contact vital sign monitoring (NCVSM) solutions. This study\nintroduces a robust framework for multi-person localization and vital signs\nmonitoring, using multiple-input-multiple-output frequency-modulated continuous\nwave radar, addressing challenges in real-world, cluttered environments. Two\nkey contributions are presented. First, a custom hardware phantom was developed\nto simulate multi-person NCVSM scenarios, utilizing recorded thoracic impedance\nsignals to replicate realistic cardiopulmonary dynamics. The phantom's design\nfacilitates repeatable and rapid validation of radar systems and algorithms\nunder diverse conditions to accelerate deployment in human monitoring. Second,\naided by the phantom, we designed a robust algorithm for multi-person\nlocalization utilizing joint sparsity and cardiopulmonary properties, alongside\nharmonics-resilient dictionary-based vital signs estimation, to mitigate\ninterfering respiration harmonics. Additionally, an adaptive signal refinement\nprocedure is introduced to enhance the accuracy of continuous NCVSM by\nleveraging the continuity of the estimates. Performance was validated and\ncompared to existing techniques through 12 phantom trials and 12 human trials,\nincluding both single- and multi-person scenarios, demonstrating superior\nlocalization and NCVSM performance. For example, in multi-person human trials,\nour method achieved average respiration rate estimation accuracies of 94.14%,\n98.12%, and 98.69% within error thresholds of 2, 3, and 4 breaths per minute,\nrespectively, and heart rate accuracies of 87.10%, 94.12%, and 95.54% within\nthe same thresholds. These results highlight the potential of this framework\nfor reliable multi-person NCVSM in healthcare and IoT applications.",
        "This paper investigates integrated localization and communication in a\nmulti-cell system and proposes a coordinated beamforming algorithm to enhance\ntarget localization accuracy while preserving communication performance. Within\nthis integrated sensing and communication (ISAC) system, the Cramer-Rao lower\nbound (CRLB) is adopted to quantify the accuracy of target localization, with\nits closed-form expression derived for the first time. It is shown that the\nnuisance parameters can be disregarded without impacting the CRLB of time of\narrival (TOA)-based target localization. Capitalizing on the derived CRLB, we\nformulate a nonconvex coordinated beamforming problem to minimize the CRLB\nwhile satisfying signal-to-interference-plus-noise ratio (SINR) constraints in\ncommunication. To facilitate the development of solution, we reformulate the\noriginal problem into a more tractable form and solve it through semi-definite\nprogramming (SDP). Notably, we show that the proposed algorithm can always\nobtain rank-one global optimal solutions under mild conditions. Finally,\nnumerical results demonstrate the superiority of the proposed algorithm over\nbenchmark algorithms and reveal the performance trade-off between localization\naccuracy and communication SINR.",
        "Beamforming plays a crucial role in millimeter wave (mmWave) communication\nsystems to mitigate the severe attenuation inherent to this spectrum. However,\nthe use of large active antenna arrays in conventional architectures often\nresults in high implementation costs and excessive power consumption, limiting\ntheir practicality. As an alternative, deploying large arrays at transceivers\nusing passive devices, such as reconfigurable intelligent surfaces (RISs),\noffers a more cost-effective and energy-efficient solution. In this paper, we\ninvestigate a promising base station (BS) architecture that integrates a beyond\ndiagonal RIS (BD-RIS) within the BS to enable passive beamforming. By utilizing\nTakagi's decomposition and leveraging the effective beamforming vector, the RIS\nprofile can be designed to enable passive beamforming directed toward the\ntarget. Through the beamforming analysis, we reveal that BD-RIS provides robust\nbeamforming performance across various system configurations, whereas the\ntraditional diagonal RIS (D-RIS) exhibits instability with increasing RIS size\nand decreasing BS-RIS separation-two critical factors in optimizing\nRIS-assisted systems. Comprehensive computer simulation results across various\naspects validate the superiority of the proposed BS-integrated BD-RIS over\nconventional D-RIS architectures, showcasing performance comparable to active\nanalog beamforming antenna arrays.",
        "The ray-tracing is often employed in urban areas for channel modeling with\nhigh accuracy but encounters a substantial computational complexity for high\nmobility scenarios. In this paper, we propose a novel pre-processing method for\ndynamic ray-tracing to reduce the computational burden in high-mobility\nscenarios by prepending the intersection judgment to the pre-processing stage.\nThe proposed method generates an inter-visibility matrix that establishes\nvisibility relationships among static objects in the environment considering\nthe intersection judgment. Moreover, the inter-visibility matrix can be\nemployed to create the inter-visibility table for mobile transmitters and\nreceivers, which can improve the efficiency of constructing an image tree for\nthe three-dimensional (3D) dynamic ray-tracing method. The results show that\nthe proposed pre-processing method in dynamic ray-tracing has considerable\ntime-saving compared with the traditional method while maintaining the same\naccuracy. The channel characteristics computed by the proposed method can well\nmatch to the channel measurements.",
        "The increasing complexity of space systems, coupled with their critical\noperational roles, demands a robust, scalable, and sustainable security\nframework. This paper presents a novel system-of-systems approach for the\nupcoming Lunar Gateway. We demonstrate the application of the\nsecure-by-component approach to the two earliest deployed systems in the\nGateway, emphasizing critical security controls both internally and for\nexternal communication and connections. Additionally, we present a phased\napproach for the integration of Canadarm3, addressing the unique security\nchallenges that arise from both inter-system interactions and the arm's\nautonomous capabilities.",
        "As an artistic aid in tiled level design, Constraint Based Tiling Generation\n(CBTG) algorithms can help to automatically create level realizations from a\nset of tiles and placement constraints. Merrell's Modify in Blocks Model\nSynthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as\nConstraint Based Tiling Generation (CBTG) algorithms that work well for many\nscenarios but have limitations in problem size, problem setup and solution\nbiasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling\nGeneration algorithm, that can handle large problem sizes, requires minimal\nassumptions for setup and can help mitigate solution biasing. POMS attempts to\nresolve indeterminate grid regions by trying to progressively realize\nsub-blocks, performing a stochastic boundary erosion on previously resolved\nregions should sub-block resolution fail. We highlight the results of running a\nreference implementation on different tile sets and discuss a tile correlation\nlength, implied by the tile constraints, and its role in choosing an\nappropriate block size to aid POMS in successfully finding grid realizations.",
        "Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.",
        "In this paper, we give an explicit chain map, which induces the algebra\nisomorphism between the Hochschild cohomology ${\\bf HH}^{\\bullet}(B)$ and the\n$H$-invariant subalgebra ${\\bf H}^{\\bullet}(A, B)^{H}$ under two mild\nhypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$\nis an $H$-Galois extension of $A$. In particular, the smash product $B=A\\#H$\nalways satisfies the mild hypotheses. The isomorphism between ${\\bf\nHH}^{\\bullet}(A\\#H)$ and ${\\bf H}^{\\bullet}(A, A\\#H)^{H}$ generalizes the\nclassical result of group actions. As an application, Hochschild cohomology and\ncup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin\nHopf algebra are computed.",
        "Traditional retrieval methods rely on transforming user queries into vector\nrepresentations and retrieving documents based on cosine similarity within an\nembedding space. While efficient and scalable, this approach often fails to\nhandle complex queries involving logical constructs such as negations,\nconjunctions, and disjunctions. In this paper, we propose a novel\ninference-time logical reasoning framework that explicitly incorporates logical\nreasoning into the retrieval process. Our method extracts logical reasoning\nstructures from natural language queries and then composes the individual\ncosine similarity scores to formulate the final document scores. This approach\nenables the retrieval process to handle complex logical reasoning without\ncompromising computational efficiency. Our results on both synthetic and\nreal-world benchmarks demonstrate that the proposed method consistently\noutperforms traditional retrieval methods across different models and datasets,\nsignificantly improving retrieval performance for complex queries.",
        "The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.",
        "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities.",
        "The Multi-objective Shortest Path (MOSP) problem is a classic network\noptimization problem that aims to find all Pareto-optimal paths between two\npoints in a graph with multiple edge costs. Recent studies on multi-objective\nsearch with A* (MOA*) have demonstrated superior performance in solving\ndifficult MOSP instances. This paper presents a novel search framework that\nallows efficient parallelization of MOA* with different objective orders. The\nframework incorporates a unique upper bounding strategy that helps the search\nreduce the problem's dimensionality to one in certain cases. Experimental\nresults demonstrate that the proposed framework can enhance the performance of\nrecent A*-based solutions, with the speed-up proportional to the problem\ndimension.",
        "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
        "This thesis investigates three biologically inspired operations:\nprefix-suffix duplication, bounded prefix-suffix duplication, and\nprefix-suffix-square completion. Duplication, a common genetic mutation,\ninvolves repeating DNA sequences and is modeled here as formal operations on\nwords. The prefix-suffix duplication generates non-context-free languages, even\nfrom simple initial words. To better reflect biological processes, we propose a\nbounded variant that limits duplication length, resolving unsolved problems and\naligning with biochemical realities.\n  We also introduce the prefix-suffix-square completion operation, which\ngenerates squares at sequence ends. This operation enables the generation of\ninfinite words such as Fibonacci, Period-doubling, and Thue-Morse, which\ncontain squares but avoid higher exponent repetitions, highlighting unique\nstructural properties. In contrast, prefix-suffix duplication cannot generate\ncertain infinite words, such as Thue-Morse, but can produce cube-free words.\n  Additionally, we address the detection of gapped repeats and\npalindromes-structures important in DNA and RNA analysis. These involve\nrepeating or reversed factors flanking a central gap. Previous studies imposed\nconstraints on gap length or arm-gap relationships; we extend this by solving\nthe problem in three novel settings. This work advances theoretical insights\ninto biologically inspired operations and their computational applications in\ngenetic modeling.",
        "We investigate a family of approximate multi-step proximal point methods,\nframed as implicit linear discretizations of gradient flow. The resulting\nmethods are multi-step proximal point methods, with similar computational cost\nin each update as the proximal point method. We explore several optimization\nmethods where applying an approximate multistep proximal points method results\nin improved convergence behavior. We also include convergence analysis for the\nproposed method in several problem settings: quadratic problems, general\nproblems that are strongly or weakly (non)convex, and accelerated results for\nalternating projections.",
        "We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.",
        "The perception system is a a critical role of an autonomous driving system\nfor ensuring safety. The driving scene perception system fundamentally\nrepresents an object detection task that requires achieving a balance between\naccuracy and processing speed. Many contemporary methods focus on improving\ndetection accuracy but often overlook the importance of real-time detection\ncapabilities when computational resources are limited. Thus, it is vital to\ninvestigate efficient object detection strategies for driving scenes. This\npaper introduces Fast-COS, a novel single-stage object detection framework\ncrafted specifically for driving scene applications. The research initiates\nwith an analysis of the backbone, considering both macro and micro\narchitectural designs, yielding the Reparameterized Attention Vision\nTransformer (RAViT). RAViT utilizes Reparameterized Multi-Scale Depth-Wise\nConvolution (RepMSDW) and Reparameterized Self-Attention (RepSA) to enhance\ncomputational efficiency and feature extraction. In extensive tests across GPU,\nedge, and mobile platforms, RAViT achieves 81.4% Top-1 accuracy on the\nImageNet-1K dataset, demonstrating significant throughput improvements over\ncomparable backbone models such as ResNet, FastViT, RepViT, and\nEfficientFormer. Additionally, integrating RepMSDW into a feature pyramid\nnetwork forms RepFPN, enabling fast and multi-scale feature fusion. Fast-COS\nenhances object detection in driving scenes, attaining an AP50 score of 57.2%\non the BDD100K dataset and 80.0% on the TJU-DHD Traffic dataset. It surpasses\nleading models in efficiency, delivering up to 75.9% faster GPU inference and\n1.38 higher throughput on edge devices compared to FCOS, YOLOF, and RetinaNet.\nThese findings establish Fast-COS as a highly scalable and reliable solution\nsuitable for real-time applications, especially in resource-limited\nenvironments like autonomous driving systems",
        "We present a computation of the one-loop QCD corrections to top-quark pair\nproduction in association with a $W$ boson, including terms up to order\n$\\varepsilon^2$ in dimensional regularization. Providing a first glimpse into\nthe complexity of the corresponding two-loop amplitude, this result is a first\nstep towards a description of this process at next-to-next-to-leading order\n(NNLO) in QCD. We perform a tensor decomposition and express the corresponding\nform factors in terms of a basis of independent special functions with compact\nrational coefficients, providing a structured framework for future\ndevelopments. In addition, we derive an explicit analytic representation of the\nform factors, valid up to order $\\varepsilon^0$, expressed in terms of\nlogarithms and dilogarithms. For the complete set of special functions\nrequired, we obtain a semi-numerical solution based on generalized power series\nexpansion.",
        "This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.",
        "A discrete time crystal (DTC) is the paradigmatic example of a phase of\nmatter that occurs exclusively in systems out of equilibrium. This phenomenon\nis characterized by the spontaneous symmetry breaking of discrete\ntime-translation and provides a rich playground to study a fundamental question\nin statistical physics: what mechanism allows for driven quantum systems to\nexhibit emergent behavior that deviates from their counterparts with\ntime-independent evolution? Unlike equilibrium phases, DTCs exhibit macroscopic\nmanifestations of coherent quantum dynamics, challenging the conventional\nnarrative that thermodynamic behavior universally erases quantum signatures.\nHowever, due to the difficulty of simulating these systems with either\nclassical or quantum computers, previous studies have been limited to a set of\nmodels with Ising-like couplings -- and mostly only in one dimension -- thus\nprecluding our understanding of the existence (or not) of DTCs in models with\ninteractions that closely align with what occurs in nature. In this work, by\ncombining the latest generation of IBM quantum processors with state-of-the-art\ntensor network methods, we are able to demonstrate the existence of a DTC in a\ntwo-dimensional system governed by anisotropic Heisenberg interactions. Our\ncomprehensive analysis reveals a rich phase diagram encompassing spin-glass,\nergodic, and time-crystalline phases, highlighting the tunability of these\nphases through multiple control parameters. Crucially, our results emphasize\nthe interplay of initialization, interaction anisotropy, and driving protocols\nin stabilizing the DTC phase. By extending the study of Floquet matter beyond\nsimplified models, we lay the groundwork for exploring how driven systems\nbridge the gap between quantum coherence and emergent non-equilibrium\nthermodynamics."
      ]
    }
  },
  {
    "id":"2411.05236",
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Shannon capacity of signal transduction for multiple independent receptors",
    "start_abstract":"Cyclic adenosine monophosphate (cAMP) is considered a model system for signal transduction, the mechanism by which cells exchange chemical messages. Our previous work calculated Shannon capacity of single cAMP receptor; however, typical cell may have thousands receptors operating in parallel. In this paper, we calculate transduction with an arbitrary number independent, indistinguishable receptors. By leveraging prior results on feedback receptor, show (somewhat unexpectedly) that achieved IID input distribution, and n times receptor.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Silicon is the next frontier in plant synthetic biology",
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "A Comprehensive Review of Protein Language Models",
        "Inverse problems with experiment-guided AlphaFold",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Vacuum Polarization, Geodesic Equation and Sachs-Wolfe Effect",
        "Production of doubly heavy baryon at the Muon-Ion Collider",
        "Mangnon-Phonon-Photon Interactions in Cubic Ferromagnets in the Vicinity\n  of Orientational Phase Transition: Retrospective and Phenomenological Study",
        "Arithmetic properties of Cantor sets involving non-diagonal forms",
        "Fully comprehensive diagnostic of galaxy activity using principal\n  components of visible spectra: implementation on nearby S0s",
        "A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational\n  Inequalities",
        "Ca II K Polar Network Index of the Sun: A Proxy for Historical Polar\n  Magnetic Field",
        "Strong law of large numbers for random walks in weakly dependent random\n  scenery",
        "Using Co-Located Range and Doppler Radars for Initial Orbit\n  Determination",
        "Polymer-based solid-state electrolytes for lithium sulfur batteries",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Euclid preparation: Extracting physical parameters from galaxies with\n  machine learning",
        "Information Consistent Pruning: How to Efficiently Search for Sparse\n  Networks?",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "The SARAO MeerKAT Galactic Plane Survey extended source catalogue"
      ],
      "abstract":[
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "We show that the null geodesic equation for photons is modified in the\npresence of a charged scalar field, with quantum fluctuations acting as an\neffective mass term that changes the null paths to timelike curves. This effect\ncan be interpreted as a vacuum polarization phenomenon in curved spacetime. The\nresulting contribution to the Sachs-Wolfe effect varies with photon frequency,\nleading to frequency-dependent corrections to the cosmic microwave background\n(CMB) blackbody spectrum in the form of a $\\mu$-distortion, as well as\nmodifications to the CMB power spectrum. We estimate these within a standard\ninflationary scenario and find that while the correction to the CMB power\nspectrum is significant when the scalar field is light, the magnitude of the\n$\\mu$-distortion depends strongly on the regularization prescription.",
        "This study forecasts the production of doubly heavy baryons, $\\Xi_{cc}$,\n$\\Xi_{bc}$, and $\\Xi_{bb}$, within the nonrelativistic QCD framework at the\nMuon-Ion Collider (MuIC). It examines two production mechanisms: photon-gluon\nfusion ($\\gamma + g \\to (QQ')[n] +\\bar{Q} +\\bar{Q'}$) and extrinsic heavy quark\nchannels ($\\gamma + Q \\to (QQ')[n] + \\bar{Q'}$), where $Q$ and $Q'$ denote\nheavy quarks ($c$ or $b$) and $(QQ')[n]$ represents a diquark in specific\nspin-color configurations. The diquark fragments into $\\Xi_{QQ'}$ baryons with\nhigh probability. For $\\Xi_{cc}$ and $\\Xi_{bb}$, the relevant configurations\nare $[^1S_0]_{\\textbf{6}}$ (spin-singlet and color-sextuplet) and\n$[^3S_1]_{\\bar{\\textbf{3}}}$ (spin-triplet and color-antitriplet). For\n$\\Xi_{bc}$, the configurations are $[^1S_0]_{\\bar{\\textbf{3}}}$,\n$[^1S_0]_{\\textbf{6}}$, $[^3S_1]_{\\bar{\\textbf{3}}}$, and\n$[^3S_1]_{\\textbf{6}}$. The study compares total and differential\ncross-sections for these channels, highlighting their uncertainties. The\nresults indicate that the extrinsic heavy quark channel, particularly the\n$[^3S_1]_{\\bar{\\textbf{3}}}$ configuration, dominates $\\Xi_{QQ'}$ production,\nthough other diquark states also contribute significantly. Using quark masses\n$m_c = 1.80 \\pm 0.10$ GeV and $m_b = 5.1 \\pm 0.20$ GeV, the study estimates\nannual event yields at MuIC ($\\sqrt{s} = 1$ TeV, luminosity ${\\mathcal L}\\simeq\n40$ ${\\rm fb}^{-1}$) of $(3.67^{+1.29}_{-0.91}) \\times 10^9$ for $\\Xi_{cc}$,\n$(2.24^{+0.28}_{-0.20}) \\times 10^8$ for $\\Xi_{bc}$, and\n$(3.00^{+0.64}_{-0.56}) \\times 10^6$ for $\\Xi_{bb}$. These findings suggest\nthat MuIC will significantly enhance our understanding of doubly heavy baryons.",
        "This work presents a comprehensive theoretical investigation of\nmagnon-phonon-photon interactions in cubic ferromagnets near orientational\nphase transitions (OPT). The study focuses on the interplay of magnetoelastic\n(ME), electromagnetic-spin (EMS), and acousticelectromagnetic (AEM)\ninteractions in ferromagnetic dielectrics and metals. By deriving dispersion\nequations for coupled waves, the research reveals how the dynamic properties of\nmagnetically ordered crystals evolve in response to these interactions under\nvarying external magnetic fields and near OPT points. The ME interaction\nprominently influences spin and elastic wave dynamics, giving rise to coupled\nmodes such as quasimagnon and quasi-acoustic waves. Near the OPT, these\ninteractions become dominant, leading to phenomena like the magnetoelastic gap\nand softening of vibrational modes. The EMS interaction significantly alters\nthe activation energy and dispersion of quasi-spin and quasi-electromagnetic\nwaves. In ferromagnetic metals, helicons (weakly damped electromagnetic waves)\nexhibit strong coupling with spin and elastic waves, particularly in the OPT\nregion. The study identifies conditions for triple resonance among spin,\nelastic, and electromagnetic waves. Additionally, it explores the\nfrequency-dependent rotation of polarization planes in electromagnetic and\nmagnetoelastic waves, which sharpens near OPT points. These results provide a\ndeeper understanding of the coupled dynamics in ferromagnetic materials, paving\nthe way for new technological applications in spintronics, signal processing,\nand advanced magneto-optical devices. The theoretical framework developed here\nemphasizes the critical role of ME, EMS, and AEM interactions in tailoring wave\nproperties for specific applications, particularly in designing next-generation\nmagnetic and electronic systems.",
        "We show conditions on $k$ such that any number $x$ in the interval $[0, k\/2]$\ncan be represented in the form $x_1^{a_1} x_2^{a_2} + x_3^{a_3} x_4^{a_4} +\n\\cdots + x_{k-1}^{a_{k-1}} x_k^{a_k}$, where the exponents $a_{2i-1}$ and\n$a_{2i}$ are positive integers satisfying $a_{2i-1} + a_{2i} = s$ for $i = 1,\n2, \\dots, k\/2$, and each $x_i$ belongs to the generalized Cantor set. Moreover,\nwe discuss different types of non-diagonal polynomials and clarify the optimal\nresults in low-dimensional cases.",
        "We introduce a novel galaxy classification methodology based on the visible\nspectra of a sample of over 68,000 nearby ($z\\leq 0.1$) Sloan Digital Sky\nSurvey lenticular (S0) galaxies. Unlike traditional diagnostic diagrams, which\nrely on a limited set of emission lines and class dividers to identify ionizing\nsources, our approach provides a comprehensive framework for characterizing\ngalaxies regardless of their activity level. By projecting galaxies into the 2D\nlatent space defined by the first three principal components (PCs) of their\nentire visible spectra, our method remains robust even when data from\nindividual emission lines are missing. We employ Gaussian kernel density\nestimates of the classical Baldwin-Phillips-Terlevich (BPT) activity classes in\nthe new classification subspace, adjusted according to their relative abundance\nin our S0 sample, to generate probability maps for star-forming, Seyfert,\ncomposite, and LINER galaxies. These maps closely mirror the canonical\ndistribution of BPT classes shown by the entire galaxy population,\ndemonstrating that our PC-based taxonomy effectively predicts the dominant\nionizing mechanisms through a probabilistic approach that provides a realistic\nreflection of galaxy activity and allows for refined class membership. Our\nanalysis further reveals that flux-limited BPT-like diagrams are inherently\nbiased against composite and star-forming galaxies due to their weaker [OIII]\nemission. Besides, it suggests that although most low-activity galaxies\nexcluded from these diagnostics exhibit visual spectra with LINER-like\ncharacteristics, their remaining activity is likely driven by mechanisms\nunrelated to either star formation or supermassive black hole accretion. A\nmachine-readable catalogue listing BPT-class probabilities for the galaxies\nanalysed is available online at the CDS website.",
        "In this paper, we develop a Discontinuous Galerkin (DG) method for solving\nH(curl)-elliptic hemivariational inequalities. By selecting an appropriate\nnumerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG)\nscheme. A comprehensive numerical analysis of the IPDG method is conducted,\naddressing key aspects such as consistency, boundedness, stability, and the\nexistence, uniqueness, uniform boundedness of the numerical solutions. Building\non these properties, we establish a priori error estimates, demonstrating the\noptimal convergence order of the numerical solutions under suitable solution\nregularity assumptions. Finally, a numerical example is presented to illustrate\nthe theoretically predicted convergence order and to show the effectiveness of\nthe proposed method.",
        "The Sun's polar magnetic field is pivotal in understanding solar dynamo\nprocesses and forecasting future solar cycles. However, direct measurements of\nthe polar field is only available since the 1970s. The chromospheric Ca II K\npolar network index (PNI; the fractional area of the chromospheric network\nregions above a certain latitude) has recently emerged as a reliable proxy for\npolar magnetic fields. In this study, we derive PNI estimates from newly\ncalibrated, rotation-corrected Ca II K observations from the Kodaikanal Solar\nObservatory (1904-2007) and modern data from the Rome Precision Solar\nPhotometric Telescope (2000-2022). We use both of those Ca II K archives to\nidentify polar network regions with an automatic adaptive threshold\nsegmentation technique and calculate the PNI. The PNI obtained from both the\narchives shows a significant correlation with the measured polar field from WSO\n(Pearson correlation coefficient r > 0.93) and the derived polar field based on\nan Advective Flux Transport Model (r > 0.91). The PNI series also shows a\nsignificant correlation with faculae counts derived from Mount Wilson\nObservatory observations (r > 0.87) for both KoSO and Rome-PSPT data. Finally,\nwe use the PNI series from both archives to reconstruct the polar magnetic\nfield over a 119-year-long period, which includes last 11 solar cycles (Cycle\n14-24). We also obtain a relationship between the amplitude of solar cycles (in\n13-month smoothed sunspot number) and the strength of the reconstructed polar\nfield at the preceding solar cycle minimum to validate the prediction of the\nongoing solar cycle, Cycle 25.",
        "In this brief note, we study the strong law of large numbers for random walks\nin random scenery. Under the assumptions that the random scenery is\nnon-stationary and satisfies weakly dependent condition with an appropriate\nrate, we establish strong law of large numbers for random walks in random\nscenery. Our results extend the known results in the literature.",
        "With debris larger than 1 cm in size estimated to be over one million,\nprecise cataloging efforts are essential to ensure space operations' safety.\nCompounding this challenge is the oversubscribed problem, where the sheer\nvolume of space objects surpasses ground-based observatories' observational\ncapacity. This results in sparse, brief observations and extended intervals\nbefore image acquisition. LeoLabs' network of phased-array radars addresses\nthis need by reliably tracking 10 cm objects and larger in low Earth orbit with\n10 independent radars across six sites. While LeoLabs tracklets are extremely\nshort, they hold much more information than typical radar observations.\nFurthermore, two tracklets are generally available, separated by a couple of\nminutes. Thus, this paper develops a tailored approach to initialize state and\nuncertainty from a single or pair of tracklets. Through differential algebra,\nthe initial orbit determination provides the state space compatible with the\navailable measurements, namely an orbit set. This practice, widely used in\nprevious research, allows for efficient data association of different\ntracklets, thus enabling the addition of accurate tracks to the catalog\nfollowing their independent initialization. The algorithm's efficacy is tested\nusing real measurements, evaluating the IOD solution's accuracy and ability to\npredict the next passage from a single or a pair of tracklets.",
        "Lithium-sulfur (Li-S) batteries offer substantial theoretical energy density\ngains over Li-ion bat-teries, a crucial factor for transportation\nelectrification. In addition, sulfur is an earth-abundant, inexpensive material\nobtainable from multiple resources; thus, Li-S batteries are envisioned to\nprovide environmentally sustainable solutions to the growing demand for energy\nstorage. A crit-ical roadblock to the realization of commercial Li-S batteries\nis the formation of polysulfides and their secondary reactions with liquid\norganic electrolytes, resulting in low coulombic efficiency for charging and\nfast self-discharge rates. The realization of solid-state electrolytes for Li-S\nbat-teries provides potential pathways to address the safety concerns of liquid\nelectrolytes and inhib-it the formation of polysulfides and\/or prevent their\ndiffusion into the anode electrode. However, current solid-state electrolytes\nare limited by low ionic conductivity, inadequate electrode inter-facial\ncompatibility, and restricted electrochemical windows. This review discusses\nthe status of polymer-based electrolytes for Li-S batteries, and outlines\ncurrent methods for their fabrication, their transport characteristics and\nongoing research aimed at overcoming material properties hindering the\ndevelopment of all-solid-state Li-S batteries.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "The Euclid mission is generating a vast amount of imaging data in four\nbroadband filters at high angular resolution. This will allow the detailed\nstudy of mass, metallicity, and stellar populations across galaxies, which will\nconstrain their formation and evolutionary pathways. Transforming the Euclid\nimaging for large samples of galaxies into maps of physical parameters in an\nefficient and reliable manner is an outstanding challenge. We investigate the\npower and reliability of machine learning techniques to extract the\ndistribution of physical parameters within well-resolved galaxies. We focus on\nestimating stellar mass surface density, mass-averaged stellar metallicity and\nage. We generate noise-free, synthetic high-resolution imaging data in the\nEuclid photometric bands for a set of 1154 galaxies from the TNG50 cosmological\nsimulation. The images are generated with the SKIRT radiative transfer code,\ntaking into account the complex 3D distribution of stellar populations and\ninterstellar dust attenuation. We use a machine learning framework to map the\nidealised mock observational data to the physical parameters on a\npixel-by-pixel basis. We find that stellar mass surface density can be\naccurately recovered with a $\\leq 0.130 {\\rm \\,dex}$ scatter. Conversely,\nstellar metallicity and age estimates are, as expected, less robust, but still\ncontain significant information which originates from underlying correlations\nat a sub-kpc scale between stellar mass surface density and stellar population\nproperties.",
        "Iterative magnitude pruning methods (IMPs), proven to be successful in\nreducing the number of insignificant nodes in over-parameterized deep neural\nnetworks (DNNs), have been getting an enormous amount of attention with the\nrapid deployment of DNNs into cutting-edge technologies with computation and\nmemory constraints. Despite IMPs popularity in pruning networks, a fundamental\nlimitation of existing IMP algorithms is the significant training time required\nfor each pruning iteration. Our paper introduces a novel \\textit{stopping\ncriterion} for IMPs that monitors information and gradient flows between\nnetworks layers and minimizes the training time. Information Consistent Pruning\n(\\ourmethod{}) eliminates the need to retrain the network to its original\nperformance during intermediate steps while maintaining overall performance at\nthe end of the pruning process. Through our experiments, we demonstrate that\nour algorithm is more efficient than current IMPs across multiple dataset-DNN\ncombinations. We also provide theoretical insights into the core idea of our\nalgorithm alongside mathematical explanations of flow-based IMP. Our code is\navailable at \\url{https:\/\/github.com\/Sekeh-Lab\/InfCoP}.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "We present a catalogue of extended radio sources from the SARAO MeerKAT\nGalactic Plane Survey (SMGPS). Compiled from 56 survey tiles and covering\napproximately 500 deg$^2$ across the first, third, and fourth Galactic\nquadrants, the catalogue includes 16534 extended and diffuse sources with areas\nlarger than 5 synthesised beams. Of them, 3891 (24\\% of the total) are\nconfidently associated with known Galactic radio-emitting objects in the\nliterature, such as HII regions, supernova remnants, planetary nebulae,\nluminous blue variables, and Wolf-Rayet stars. A significant fraction of the\nremaining sources, 5462 (33\\%), are candidate extragalactic sources, while 7181\n(43\\%) remain unclassified. Isolated radio filaments are excluded from the\ncatalogue. The diversity of extended sources underscores MeerKAT's contribution\nto the completeness of censuses of Galactic radio emitters, and its potential\nfor new scientific discoveries. For the catalogued sources, we derived basic\npositional and morphological parameters, as well as flux density estimates,\nusing standard aperture photometry. This paper describes the methods followed\nto generate the catalogue from the original SMGPS tiles, detailing the source\nextraction, characterisation, and crossmatching procedures. Additionally, we\nanalyse the statistical properties of the catalogued populations"
      ]
    }
  },
  {
    "id":"2411.05236",
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT",
    "start_abstract":"Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",
    "start_categories":[
      "eess.SP",
      "cs.SY"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Channelrhodopsin-2, a directly light-gated cation-selective membrane channel"
      ],
      "abstract":[
        "Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane \u03b1 helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Silicon is the next frontier in plant synthetic biology",
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "A Comprehensive Review of Protein Language Models",
        "Inverse problems with experiment-guided AlphaFold",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein\n  Structure Representation",
        "Vacuum Polarization, Geodesic Equation and Sachs-Wolfe Effect",
        "Production of doubly heavy baryon at the Muon-Ion Collider",
        "Mangnon-Phonon-Photon Interactions in Cubic Ferromagnets in the Vicinity\n  of Orientational Phase Transition: Retrospective and Phenomenological Study",
        "Arithmetic properties of Cantor sets involving non-diagonal forms",
        "Fully comprehensive diagnostic of galaxy activity using principal\n  components of visible spectra: implementation on nearby S0s",
        "A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational\n  Inequalities",
        "Ca II K Polar Network Index of the Sun: A Proxy for Historical Polar\n  Magnetic Field",
        "Strong law of large numbers for random walks in weakly dependent random\n  scenery",
        "Using Co-Located Range and Doppler Radars for Initial Orbit\n  Determination",
        "Polymer-based solid-state electrolytes for lithium sulfur batteries",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Euclid preparation: Extracting physical parameters from galaxies with\n  machine learning",
        "Information Consistent Pruning: How to Efficiently Search for Sparse\n  Networks?",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "The SARAO MeerKAT Galactic Plane Survey extended source catalogue"
      ],
      "abstract":[
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Structure-informed protein representation learning is essential for effective\nprotein function annotation and \\textit{de novo} design. However, the presence\nof inherent noise in both crystal and AlphaFold-predicted structures poses\nsignificant challenges for existing methods in learning robust protein\nrepresentations. To address these issues, we propose a novel equivariant\nTransformer-State Space Model(SSM) hybrid framework, termed $E^3$former,\ndesigned for efficient protein representation. Our approach uses energy\nfunction-based receptive fields to construct proximity graphs and incorporates\nan equivariant high-tensor-elastic selective SSM within the transformer\narchitecture. These components enable the model to adapt to complex atom\ninteractions and extract geometric features with higher signal-to-noise ratios.\nEmpirical results demonstrate that our model outperforms existing methods in\nstructure-intensive tasks, such as inverse folding and binding site prediction,\nparticularly when using predicted structures, owing to its enhanced tolerance\nto data deviation and noise. Our approach offers a novel perspective for\nconducting biological function research and drug discovery using noisy protein\nstructure data.",
        "We show that the null geodesic equation for photons is modified in the\npresence of a charged scalar field, with quantum fluctuations acting as an\neffective mass term that changes the null paths to timelike curves. This effect\ncan be interpreted as a vacuum polarization phenomenon in curved spacetime. The\nresulting contribution to the Sachs-Wolfe effect varies with photon frequency,\nleading to frequency-dependent corrections to the cosmic microwave background\n(CMB) blackbody spectrum in the form of a $\\mu$-distortion, as well as\nmodifications to the CMB power spectrum. We estimate these within a standard\ninflationary scenario and find that while the correction to the CMB power\nspectrum is significant when the scalar field is light, the magnitude of the\n$\\mu$-distortion depends strongly on the regularization prescription.",
        "This study forecasts the production of doubly heavy baryons, $\\Xi_{cc}$,\n$\\Xi_{bc}$, and $\\Xi_{bb}$, within the nonrelativistic QCD framework at the\nMuon-Ion Collider (MuIC). It examines two production mechanisms: photon-gluon\nfusion ($\\gamma + g \\to (QQ')[n] +\\bar{Q} +\\bar{Q'}$) and extrinsic heavy quark\nchannels ($\\gamma + Q \\to (QQ')[n] + \\bar{Q'}$), where $Q$ and $Q'$ denote\nheavy quarks ($c$ or $b$) and $(QQ')[n]$ represents a diquark in specific\nspin-color configurations. The diquark fragments into $\\Xi_{QQ'}$ baryons with\nhigh probability. For $\\Xi_{cc}$ and $\\Xi_{bb}$, the relevant configurations\nare $[^1S_0]_{\\textbf{6}}$ (spin-singlet and color-sextuplet) and\n$[^3S_1]_{\\bar{\\textbf{3}}}$ (spin-triplet and color-antitriplet). For\n$\\Xi_{bc}$, the configurations are $[^1S_0]_{\\bar{\\textbf{3}}}$,\n$[^1S_0]_{\\textbf{6}}$, $[^3S_1]_{\\bar{\\textbf{3}}}$, and\n$[^3S_1]_{\\textbf{6}}$. The study compares total and differential\ncross-sections for these channels, highlighting their uncertainties. The\nresults indicate that the extrinsic heavy quark channel, particularly the\n$[^3S_1]_{\\bar{\\textbf{3}}}$ configuration, dominates $\\Xi_{QQ'}$ production,\nthough other diquark states also contribute significantly. Using quark masses\n$m_c = 1.80 \\pm 0.10$ GeV and $m_b = 5.1 \\pm 0.20$ GeV, the study estimates\nannual event yields at MuIC ($\\sqrt{s} = 1$ TeV, luminosity ${\\mathcal L}\\simeq\n40$ ${\\rm fb}^{-1}$) of $(3.67^{+1.29}_{-0.91}) \\times 10^9$ for $\\Xi_{cc}$,\n$(2.24^{+0.28}_{-0.20}) \\times 10^8$ for $\\Xi_{bc}$, and\n$(3.00^{+0.64}_{-0.56}) \\times 10^6$ for $\\Xi_{bb}$. These findings suggest\nthat MuIC will significantly enhance our understanding of doubly heavy baryons.",
        "This work presents a comprehensive theoretical investigation of\nmagnon-phonon-photon interactions in cubic ferromagnets near orientational\nphase transitions (OPT). The study focuses on the interplay of magnetoelastic\n(ME), electromagnetic-spin (EMS), and acousticelectromagnetic (AEM)\ninteractions in ferromagnetic dielectrics and metals. By deriving dispersion\nequations for coupled waves, the research reveals how the dynamic properties of\nmagnetically ordered crystals evolve in response to these interactions under\nvarying external magnetic fields and near OPT points. The ME interaction\nprominently influences spin and elastic wave dynamics, giving rise to coupled\nmodes such as quasimagnon and quasi-acoustic waves. Near the OPT, these\ninteractions become dominant, leading to phenomena like the magnetoelastic gap\nand softening of vibrational modes. The EMS interaction significantly alters\nthe activation energy and dispersion of quasi-spin and quasi-electromagnetic\nwaves. In ferromagnetic metals, helicons (weakly damped electromagnetic waves)\nexhibit strong coupling with spin and elastic waves, particularly in the OPT\nregion. The study identifies conditions for triple resonance among spin,\nelastic, and electromagnetic waves. Additionally, it explores the\nfrequency-dependent rotation of polarization planes in electromagnetic and\nmagnetoelastic waves, which sharpens near OPT points. These results provide a\ndeeper understanding of the coupled dynamics in ferromagnetic materials, paving\nthe way for new technological applications in spintronics, signal processing,\nand advanced magneto-optical devices. The theoretical framework developed here\nemphasizes the critical role of ME, EMS, and AEM interactions in tailoring wave\nproperties for specific applications, particularly in designing next-generation\nmagnetic and electronic systems.",
        "We show conditions on $k$ such that any number $x$ in the interval $[0, k\/2]$\ncan be represented in the form $x_1^{a_1} x_2^{a_2} + x_3^{a_3} x_4^{a_4} +\n\\cdots + x_{k-1}^{a_{k-1}} x_k^{a_k}$, where the exponents $a_{2i-1}$ and\n$a_{2i}$ are positive integers satisfying $a_{2i-1} + a_{2i} = s$ for $i = 1,\n2, \\dots, k\/2$, and each $x_i$ belongs to the generalized Cantor set. Moreover,\nwe discuss different types of non-diagonal polynomials and clarify the optimal\nresults in low-dimensional cases.",
        "We introduce a novel galaxy classification methodology based on the visible\nspectra of a sample of over 68,000 nearby ($z\\leq 0.1$) Sloan Digital Sky\nSurvey lenticular (S0) galaxies. Unlike traditional diagnostic diagrams, which\nrely on a limited set of emission lines and class dividers to identify ionizing\nsources, our approach provides a comprehensive framework for characterizing\ngalaxies regardless of their activity level. By projecting galaxies into the 2D\nlatent space defined by the first three principal components (PCs) of their\nentire visible spectra, our method remains robust even when data from\nindividual emission lines are missing. We employ Gaussian kernel density\nestimates of the classical Baldwin-Phillips-Terlevich (BPT) activity classes in\nthe new classification subspace, adjusted according to their relative abundance\nin our S0 sample, to generate probability maps for star-forming, Seyfert,\ncomposite, and LINER galaxies. These maps closely mirror the canonical\ndistribution of BPT classes shown by the entire galaxy population,\ndemonstrating that our PC-based taxonomy effectively predicts the dominant\nionizing mechanisms through a probabilistic approach that provides a realistic\nreflection of galaxy activity and allows for refined class membership. Our\nanalysis further reveals that flux-limited BPT-like diagrams are inherently\nbiased against composite and star-forming galaxies due to their weaker [OIII]\nemission. Besides, it suggests that although most low-activity galaxies\nexcluded from these diagnostics exhibit visual spectra with LINER-like\ncharacteristics, their remaining activity is likely driven by mechanisms\nunrelated to either star formation or supermassive black hole accretion. A\nmachine-readable catalogue listing BPT-class probabilities for the galaxies\nanalysed is available online at the CDS website.",
        "In this paper, we develop a Discontinuous Galerkin (DG) method for solving\nH(curl)-elliptic hemivariational inequalities. By selecting an appropriate\nnumerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG)\nscheme. A comprehensive numerical analysis of the IPDG method is conducted,\naddressing key aspects such as consistency, boundedness, stability, and the\nexistence, uniqueness, uniform boundedness of the numerical solutions. Building\non these properties, we establish a priori error estimates, demonstrating the\noptimal convergence order of the numerical solutions under suitable solution\nregularity assumptions. Finally, a numerical example is presented to illustrate\nthe theoretically predicted convergence order and to show the effectiveness of\nthe proposed method.",
        "The Sun's polar magnetic field is pivotal in understanding solar dynamo\nprocesses and forecasting future solar cycles. However, direct measurements of\nthe polar field is only available since the 1970s. The chromospheric Ca II K\npolar network index (PNI; the fractional area of the chromospheric network\nregions above a certain latitude) has recently emerged as a reliable proxy for\npolar magnetic fields. In this study, we derive PNI estimates from newly\ncalibrated, rotation-corrected Ca II K observations from the Kodaikanal Solar\nObservatory (1904-2007) and modern data from the Rome Precision Solar\nPhotometric Telescope (2000-2022). We use both of those Ca II K archives to\nidentify polar network regions with an automatic adaptive threshold\nsegmentation technique and calculate the PNI. The PNI obtained from both the\narchives shows a significant correlation with the measured polar field from WSO\n(Pearson correlation coefficient r > 0.93) and the derived polar field based on\nan Advective Flux Transport Model (r > 0.91). The PNI series also shows a\nsignificant correlation with faculae counts derived from Mount Wilson\nObservatory observations (r > 0.87) for both KoSO and Rome-PSPT data. Finally,\nwe use the PNI series from both archives to reconstruct the polar magnetic\nfield over a 119-year-long period, which includes last 11 solar cycles (Cycle\n14-24). We also obtain a relationship between the amplitude of solar cycles (in\n13-month smoothed sunspot number) and the strength of the reconstructed polar\nfield at the preceding solar cycle minimum to validate the prediction of the\nongoing solar cycle, Cycle 25.",
        "In this brief note, we study the strong law of large numbers for random walks\nin random scenery. Under the assumptions that the random scenery is\nnon-stationary and satisfies weakly dependent condition with an appropriate\nrate, we establish strong law of large numbers for random walks in random\nscenery. Our results extend the known results in the literature.",
        "With debris larger than 1 cm in size estimated to be over one million,\nprecise cataloging efforts are essential to ensure space operations' safety.\nCompounding this challenge is the oversubscribed problem, where the sheer\nvolume of space objects surpasses ground-based observatories' observational\ncapacity. This results in sparse, brief observations and extended intervals\nbefore image acquisition. LeoLabs' network of phased-array radars addresses\nthis need by reliably tracking 10 cm objects and larger in low Earth orbit with\n10 independent radars across six sites. While LeoLabs tracklets are extremely\nshort, they hold much more information than typical radar observations.\nFurthermore, two tracklets are generally available, separated by a couple of\nminutes. Thus, this paper develops a tailored approach to initialize state and\nuncertainty from a single or pair of tracklets. Through differential algebra,\nthe initial orbit determination provides the state space compatible with the\navailable measurements, namely an orbit set. This practice, widely used in\nprevious research, allows for efficient data association of different\ntracklets, thus enabling the addition of accurate tracks to the catalog\nfollowing their independent initialization. The algorithm's efficacy is tested\nusing real measurements, evaluating the IOD solution's accuracy and ability to\npredict the next passage from a single or a pair of tracklets.",
        "Lithium-sulfur (Li-S) batteries offer substantial theoretical energy density\ngains over Li-ion bat-teries, a crucial factor for transportation\nelectrification. In addition, sulfur is an earth-abundant, inexpensive material\nobtainable from multiple resources; thus, Li-S batteries are envisioned to\nprovide environmentally sustainable solutions to the growing demand for energy\nstorage. A crit-ical roadblock to the realization of commercial Li-S batteries\nis the formation of polysulfides and their secondary reactions with liquid\norganic electrolytes, resulting in low coulombic efficiency for charging and\nfast self-discharge rates. The realization of solid-state electrolytes for Li-S\nbat-teries provides potential pathways to address the safety concerns of liquid\nelectrolytes and inhib-it the formation of polysulfides and\/or prevent their\ndiffusion into the anode electrode. However, current solid-state electrolytes\nare limited by low ionic conductivity, inadequate electrode inter-facial\ncompatibility, and restricted electrochemical windows. This review discusses\nthe status of polymer-based electrolytes for Li-S batteries, and outlines\ncurrent methods for their fabrication, their transport characteristics and\nongoing research aimed at overcoming material properties hindering the\ndevelopment of all-solid-state Li-S batteries.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "The Euclid mission is generating a vast amount of imaging data in four\nbroadband filters at high angular resolution. This will allow the detailed\nstudy of mass, metallicity, and stellar populations across galaxies, which will\nconstrain their formation and evolutionary pathways. Transforming the Euclid\nimaging for large samples of galaxies into maps of physical parameters in an\nefficient and reliable manner is an outstanding challenge. We investigate the\npower and reliability of machine learning techniques to extract the\ndistribution of physical parameters within well-resolved galaxies. We focus on\nestimating stellar mass surface density, mass-averaged stellar metallicity and\nage. We generate noise-free, synthetic high-resolution imaging data in the\nEuclid photometric bands for a set of 1154 galaxies from the TNG50 cosmological\nsimulation. The images are generated with the SKIRT radiative transfer code,\ntaking into account the complex 3D distribution of stellar populations and\ninterstellar dust attenuation. We use a machine learning framework to map the\nidealised mock observational data to the physical parameters on a\npixel-by-pixel basis. We find that stellar mass surface density can be\naccurately recovered with a $\\leq 0.130 {\\rm \\,dex}$ scatter. Conversely,\nstellar metallicity and age estimates are, as expected, less robust, but still\ncontain significant information which originates from underlying correlations\nat a sub-kpc scale between stellar mass surface density and stellar population\nproperties.",
        "Iterative magnitude pruning methods (IMPs), proven to be successful in\nreducing the number of insignificant nodes in over-parameterized deep neural\nnetworks (DNNs), have been getting an enormous amount of attention with the\nrapid deployment of DNNs into cutting-edge technologies with computation and\nmemory constraints. Despite IMPs popularity in pruning networks, a fundamental\nlimitation of existing IMP algorithms is the significant training time required\nfor each pruning iteration. Our paper introduces a novel \\textit{stopping\ncriterion} for IMPs that monitors information and gradient flows between\nnetworks layers and minimizes the training time. Information Consistent Pruning\n(\\ourmethod{}) eliminates the need to retrain the network to its original\nperformance during intermediate steps while maintaining overall performance at\nthe end of the pruning process. Through our experiments, we demonstrate that\nour algorithm is more efficient than current IMPs across multiple dataset-DNN\ncombinations. We also provide theoretical insights into the core idea of our\nalgorithm alongside mathematical explanations of flow-based IMP. Our code is\navailable at \\url{https:\/\/github.com\/Sekeh-Lab\/InfCoP}.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "We present a catalogue of extended radio sources from the SARAO MeerKAT\nGalactic Plane Survey (SMGPS). Compiled from 56 survey tiles and covering\napproximately 500 deg$^2$ across the first, third, and fourth Galactic\nquadrants, the catalogue includes 16534 extended and diffuse sources with areas\nlarger than 5 synthesised beams. Of them, 3891 (24\\% of the total) are\nconfidently associated with known Galactic radio-emitting objects in the\nliterature, such as HII regions, supernova remnants, planetary nebulae,\nluminous blue variables, and Wolf-Rayet stars. A significant fraction of the\nremaining sources, 5462 (33\\%), are candidate extragalactic sources, while 7181\n(43\\%) remain unclassified. Isolated radio filaments are excluded from the\ncatalogue. The diversity of extended sources underscores MeerKAT's contribution\nto the completeness of censuses of Galactic radio emitters, and its potential\nfor new scientific discoveries. For the catalogued sources, we derived basic\npositional and morphological parameters, as well as flux density estimates,\nusing standard aperture photometry. This paper describes the methods followed\nto generate the catalogue from the original SMGPS tiles, detailing the source\nextraction, characterisation, and crossmatching procedures. Additionally, we\nanalyse the statistical properties of the catalogued populations"
      ]
    }
  },
  {
    "id":"2411.02815",
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.AI",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Infectious diseases, imposing density-dependent mortality on MHC\/HLA\n  variation, can account for balancing selection and MHC\/HLA polymorphism",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Multimodal Prescriptive Deep Learning",
        "Formation of Frustrated Charge Density Waves in Kagome Metal\n  LuNb$_6$Sn$_6$",
        "Tautological characteristic classes III: the Witt class for PSL(2)",
        "Exploring the Growth-Index ($\\gamma$) Tension with $\\Lambda_{\\rm s}$CDM",
        "Extremal Betti Numbers and Persistence in Flag Complexes",
        "Disk reflection and energetics from the accreting millisecond pulsar\n  SRGA J144459.2-604207",
        "A faithful action of Gal($\\overline{\\mathbb{Q}}\/\\mathbb{Q}$) on Zariski\n  multiplets",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Curves on Endo-Pajitnov manifolds",
        "Integral gains for non-autonomous Wazewski systems",
        "Removal of excess iron by annealing processes and emergence of bulk\n  superconductivity in sulfur-substituted FeTe",
        "A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on\n  Estimating Conditional Relationships",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Equidistribution of saddle periodic points for H\\'enon-like maps",
        "Determination and evaluation of the critical liquid nitrogen for\n  superconducting levitator based on a novel temperature-weight coupling\n  measurement device"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are\nthe most polymorphic in the human genome. It is generally accepted this\npolymorphism reflects a role in presenting pathogen-derived peptide to the\nadaptive immune system. Proposed mechanisms for the polymorphism such as\nnegative frequency-dependent selection (NFDS) and heterozygote advantage (HA)\nfocus on HLA alleles, not haplotypes. Here, we propose a model for the\npolymorphism in which infectious diseases impose independent density-dependent\nregulation on HLA haplotypes. More specifically, a complex pathogen environment\ndrives extensive host polymorphism through a guild of HLA haplotypes that are\nspecialised and show incomplete peptide recognition. Separation of haplotype\nguilds is maintained by limiting similarity. The outcome is a wide and stable\nrange of haplotype densities at steady-state in which effective Fisher\nfitnesses are zero. Densities, and therefore frequencies, emerge theoretically\nas alternative measures of fitness. A catalogue of ranked frequencies is\ntherefore one of ranked fitnesses. The model is supported by data from a range\nof sources including a Caucasian HLA dataset compiled by the US National Marrow\nDonor Program (NMDP). These provide evidence of positive selection on the top\n350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5.\nHigh-fitness haplotypes drive the selection of 137 high-frequency alleles\nspread across the 5 HLA loci under consideration. These alleles demonstrate\npositive epistasis and pleiotropy in the formation of haplotypes. Allelic\npleiotropy creates a network of highly inter-related HLA haplotypes that\naccount for 97% of the census sample. We suggest this network has properties of\na quasi-species and is itself under selection. We also suggest this is the\norigin of balancing selection in the HLA system.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "We introduce a multimodal deep learning framework, Prescriptive Neural\nNetworks (PNNs), that combines ideas from optimization and machine learning,\nand is, to the best of our knowledge, the first prescriptive method to handle\nmultimodal data. The PNN is a feedforward neural network trained on embeddings\nto output an outcome-optimizing prescription. In two real-world multimodal\ndatasets, we demonstrate that PNNs prescribe treatments that are able to\nsignificantly improve estimated outcomes in transcatheter aortic valve\nreplacement (TAVR) procedures by reducing estimated postoperative complication\nrates by 32% and in liver trauma injuries by reducing estimated mortality rates\nby over 40%. In four real-world, unimodal tabular datasets, we demonstrate that\nPNNs outperform or perform comparably to other well-known, state-of-the-art\nprescriptive models; importantly, on tabular datasets, we also recover\ninterpretability through knowledge distillation, fitting interpretable Optimal\nClassification Tree models onto the PNN prescriptions as classification\ntargets, which is critical for many real-world applications. Finally, we\ndemonstrate that our multimodal PNN models achieve stability across randomized\ndata splits comparable to other prescriptive methods and produce realistic\nprescriptions across the different datasets.",
        "The charge density wave (CDW), a translational symmetry breaking electronic\nliquid, plays a pivotal role in correlated quantum materials, such as\nhigh-T$_c$ superconductors and topological semimetals. Recently, CDWs that\npossibly intertwine with superconductivity and magnetism are observed in\nvarious kagome metals. However, the nature of CDWs and the role of the Fermi\nsurface (FS) topology in these materials remain an unresolved challenge. In\nthis letter, we reveal the formation of CDWs in the newly discovered kagome\nmetal LuNb$_6$Sn$_6$. We observe a precursor CDW correlation that features a\n\"yield sign\"-like hollow triangle diffuse scattering pattern and nearly\ncomplete softening of a flat optical phonon band near Q$_H$=(1\/3, 1\/3, 1\/2).\nThe scattering intensity of the precursor CDW displays divergent behavior as\ndecreasing temperature until T$_{CDW}$=70 K, where a competing CDW at\nQ$_{CDW}$=(1\/3, 1\/3, 1\/3) emerges. Using scanning tunneling\nmicroscopy\/spectroscopy, we image the frustrated CDW patterns that show a short\nphase coherence length about ~20 nm in real space. Combined with first\nprinciples calculations, our observations support frustrated lattice\ninteractions that are modulated by the FS topology. These results shed light on\nthe interplay between FS and CDW in quantum materials with nearly degenerate\nstructural deformation patterns.",
        "We explain the relation between the Witt class and the universal\nequicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood\ninequality.",
        "Recent observational analyses have revealed a significant tension in the\ngrowth index $\\gamma$, which characterizes the growth rate of cosmic\nstructures. Specifically, when treating $\\gamma$ as a free parameter within\n$\\Lambda$CDM framework, a combination of Planck and $ f\\sigma_8 $ data yields\n$\\gamma \\approx 0.64$, in $\\sim4\\sigma$ tension with the theoretically expected\nvalue $\\gamma \\approx 0.55$ (assuming general relativity). This discrepancy,\nclosely related to the $ S_8 $ tension, poses a new challenge to the standard\ncosmological model by suggesting that it predicts an excessive growth of\nstructure. In this work, we demonstrate that the $\\Lambda_{\\rm s}$CDM framework\n(featuring a rapid sign-switching cosmological constant (mirror AdS-to-dS\ntransition) in the late universe at redshift $ z_\\dagger \\sim 2 $) can\nsimultaneously alleviate the $ \\gamma $, $ H_0 $, and $ S_8 $ tensions. We also\nexamined a scenario with fixed $ z_\\dagger = 1.7 $, previously identified as a\nsweet spot for alleviating multiple major cosmological tensions (including\nthose in $ H_0 $, $ M_B $, and $ S_8 $) finding that it completely eliminates\nboth the $ \\gamma $ and $ H_0 $ tensions, although it is statistically\ndisfavored by our dataset combinations. Our findings suggest that $\\Lambda_{\\rm\ns}$CDM is a promising model, providing a potential unified resolution to\nmultiple major cosmological tensions.",
        "We investigate several problems concerning extremal Betti numbers and\npersistence in filtrations of flag complexes. For graphs on $n$ vertices, we\nshow that $\\beta_k(X(G))$ is maximal when $G=\\mathcal{T}_{n,k+1}$, the Tur\\'an\ngraph on $k+1$ partition classes, where $X(G)$ denotes the flag complex of $G$.\nBuilding on this, we construct an edgewise (one edge at a time) filtration\n$\\mathcal{G}=G_1\\subseteq \\cdots \\subseteq \\mathcal{T}_{n,k+1}$ for which\n$\\beta_k(X(G_i))$ is maximal for all graphs on $n$ vertices and $i$ edges.\nMoreover, the persistence barcode $\\mathcal{B}_k(X(G))$ achieves a maximal\nnumber of intervals, and total persistence, among all edgewise filtrations with\n$|E(\\mathcal{T}_{n,k+1})|$ edges.\n  For $k=1$, we consider edgewise filtrations of the complete graph $K_n$. We\nshow that the maximal number of intervals in the persistence barcode is\nobtained precisely when $G_{\\lceil n\/2\\rceil \\cdot \\lfloor n\/2\n\\rfloor}=\\mathcal{T}_{n,2}$. Among such filtrations, we characterize those\nachieving maximal total persistence. We further show that no filtration can\noptimize $\\beta_1(X(G_i))$ for all $i$, and conjecture that our filtrations\nmaximize the total persistence over all edgewise filtrations of $K_n$.",
        "Accreting millisecond pulsars (AMSPs) are excellent laboratories to study\nreflection spectra and their features from an accretion disk truncated by a\nrapidly rotating magnetosphere near the neutron star surface. These systems\nalso exhibit thermonuclear (type-I) bursts that can provide insights on the\naccretion physics and fuel composition. We explore spectral properties of the\nAMSP SRGA J144459.2-0604207 observed during the outburst that recently led to\nits discovery in February 2024. We aim to characterize the spectral shape of\nthe persistent emission, both its continuum and discrete features, and to\nanalyze type-I bursts properties. We employ XMM and NuSTAR overlapping\nobservations taken during the most recent outburst from SRGA J1444. We perform\nspectral analysis of the persistent (i.e., non-bursting) emission employing a\nsemi-phenomenological continuum model composed of a dominant thermal\nComptonization plus two thermal contributions, and a physical reflection model.\nWe also perform time-resolved spectral analysis of a type-I burst employing a\nblackbody model. We observe a broadened iron emission line, thus suggesting\nrelativistic effects, supported by the physical model accounting for\nrelativistically blurred reflection. The resulting accretion disk extends down\nto 6 gravitational radii, inclined at ~$53^{\\circ}$, and only moderately\nionized (log$\\xi\\simeq2.3$). We observe an absorption edge at ~9.7 keV that can\nbe interpreted as an Fe XXVI edge blueshifted by an ultrafast ($\\simeq0.04$c)\noutflow. Our broadband observations of type-I bursts do not find evidence of\nphotospheric radius expansion. The burst recurrence time shows a dependence on\nthe count rate with the steepest slope ever observed in these systems. We also\nobserve a discrepancy of ~3 between the observed and expected burst recurrence\ntime, which we discuss in the framework of fuel composition and high NS mass\nscenarios.",
        "In this work, we establish two main results in the context of arithmetic and\ngeometric properties of plane curves. First, we construct numerous new examples\nof arithmetic Zariski pairs and multiplets, where only a few ones were\npreviously available. Second, we describe a faithful action of the absolute\nGalois group on the equisingular strata of plane curves, providing insights\ninto the interplay between Galois representations and the geometry of singular\nplane curves. We conclude the paper with very concretes examples of the general\nresults obtained.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "Endo--Pajitnov manifolds are generalizations to higher dimensions of the\nInoue surfaces $S^M$. We study the existence of complex submanifolds in\nEndo--Pajitnov manifolds. We identify a class of these manifolds that do\ncontain compact complex submanifolds and establish an algebraic condition under\nwhich an Endo--Pajitnov manifold contains no compact complex curves.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "There are several strategies to discover new superconductors. Growing new\nmaterials and applying high pressures can be the classic ways since\nsuperconductivity was found. Also, chemical processing, such as annealing, is\nanother way to induce superconductivity in a non-superconducting material.\nHere, we show chemical processing effects in the non-superconducting material,\nsulfur-substituted FeTe. It has been known that superconductivity in\nS-substituted FeTe is induced by O$_2$ annealing. We revealed that hydrochloric\nacid etching and vacuum annealing for O$_2$-annealed samples made the quality\nof superconductivity higher by several physical property measurements.\nFurthermore, we visualized the superconducting regions by a magneto-optical\nimaging technique, indicating that the superconductivity in the processed\nsample was bulk. In this sample, we confirmed that the concentration of excess\niron was reduced compared to that in the as-grown state. These results provide\nan important route to bulk superconductivity in S-substituted FeTe and its\nrelated iron-based compounds.",
        "Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu\n(2019, HMX), arguing that failing to model nonlinear relationships between the\ntreatment and moderator leads to biased marginal effect estimates and\nuncontrolled Type-I error rates. While these critiques highlight the issue of\nunder-modeling nonlinearity in applied research, they are fundamentally flawed\nin several key ways. First, the causal estimand for interaction effects and the\nnecessary identifying assumptions are not clearly defined in these critiques.\nOnce properly stated, the critiques no longer hold. Second, the kernel\nestimator HMX proposes recovers the true causal effects in the scenarios\npresented in these recent critiques, which compared effects to the wrong\nbenchmark, producing misleading conclusions. Third, while Generalized Additive\nModels (GAM) can be a useful exploratory tool (as acknowledged in HMX), they\nare not designed to estimate marginal effects, and better alternatives exist,\nparticularly in the presence of additional covariates. Our response aims to\nclarify these misconceptions and provide updated recommendations for\nresearchers studying interaction effects through the estimation of conditional\nmarginal effects.",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "We prove that under the natural assumption over the dynamical degrees, the\nsaddle periodic points of a H\\'enon-like map in any dimension equidistribute\nwith respect to the equilibrium measure. Our work is a generalization of\nresults of Bedford-Lyubich-Smillie, Dujardin and Dinh-Sibony along with\nimprovements of their techniques. We also investigate some fine properties of\nGreen currents associated with the map. On the pluripotential-theory side, in\nour non-compact setting, the wedge product of two positive closed currents of\ncomplementary bi-degrees can be defined using super-potentials and the density\ntheory. We prove that these two definitions are coherent.",
        "Liquid nitrogen (LN2) is the only cooling medium for the high-temperature\nsuperconducting (HTS) bulks in the superconducting levitator, which is the\nheart of the maglev train, to reach working state. The detection and\ndetermination of the critical LN2 content are crucial for reliable operation of\nthe HTS maglev train. However, the related intelligent detection model and\ntechnology is lack in the combination filed of the cryogenic environment and\nmaglev application, and there is no existing method to detect the LN2 content\nin superconducting levitator. This paper proposes to employ multisensor fusion\nframework to fuse and enhance the accuracy of critical LN2 content testing.\nFour temperature sensors were deployed inside superconducting levitator to\nmeasure the temperature change during the LN2 content changing from 100 % to 0.\nIt was first obtained that the critical LN2 content in the superconducting\nlevitator is 4%. To accurately monitor the critical LN2 content in the\nsuperconducting levitator, a matrix-weighted information fusion Kalman filter\nalgorithm was used. Compared with the previous single sensor method, the\ntesting accuracy of the multisensor fusion method can be improved by 5.6%. The\nwork can provide a preliminary research foundation for the online monitoring\nand fault diagnosis of HTS maglev train."
      ]
    }
  },
  {
    "id":"2411.02815",
    "research_type":"applied",
    "start_id":"b33",
    "start_title":"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network",
    "start_abstract":"We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.",
    "start_categories":[
      "cs.AI",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation"
      ],
      "abstract":[
        "In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Infectious diseases, imposing density-dependent mortality on MHC\/HLA\n  variation, can account for balancing selection and MHC\/HLA polymorphism",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Multimodal Prescriptive Deep Learning",
        "Formation of Frustrated Charge Density Waves in Kagome Metal\n  LuNb$_6$Sn$_6$",
        "Tautological characteristic classes III: the Witt class for PSL(2)",
        "Exploring the Growth-Index ($\\gamma$) Tension with $\\Lambda_{\\rm s}$CDM",
        "Extremal Betti Numbers and Persistence in Flag Complexes",
        "Disk reflection and energetics from the accreting millisecond pulsar\n  SRGA J144459.2-604207",
        "A faithful action of Gal($\\overline{\\mathbb{Q}}\/\\mathbb{Q}$) on Zariski\n  multiplets",
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Curves on Endo-Pajitnov manifolds",
        "Integral gains for non-autonomous Wazewski systems",
        "Removal of excess iron by annealing processes and emergence of bulk\n  superconductivity in sulfur-substituted FeTe",
        "A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on\n  Estimating Conditional Relationships",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Equidistribution of saddle periodic points for H\\'enon-like maps",
        "Determination and evaluation of the critical liquid nitrogen for\n  superconducting levitator based on a novel temperature-weight coupling\n  measurement device"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are\nthe most polymorphic in the human genome. It is generally accepted this\npolymorphism reflects a role in presenting pathogen-derived peptide to the\nadaptive immune system. Proposed mechanisms for the polymorphism such as\nnegative frequency-dependent selection (NFDS) and heterozygote advantage (HA)\nfocus on HLA alleles, not haplotypes. Here, we propose a model for the\npolymorphism in which infectious diseases impose independent density-dependent\nregulation on HLA haplotypes. More specifically, a complex pathogen environment\ndrives extensive host polymorphism through a guild of HLA haplotypes that are\nspecialised and show incomplete peptide recognition. Separation of haplotype\nguilds is maintained by limiting similarity. The outcome is a wide and stable\nrange of haplotype densities at steady-state in which effective Fisher\nfitnesses are zero. Densities, and therefore frequencies, emerge theoretically\nas alternative measures of fitness. A catalogue of ranked frequencies is\ntherefore one of ranked fitnesses. The model is supported by data from a range\nof sources including a Caucasian HLA dataset compiled by the US National Marrow\nDonor Program (NMDP). These provide evidence of positive selection on the top\n350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5.\nHigh-fitness haplotypes drive the selection of 137 high-frequency alleles\nspread across the 5 HLA loci under consideration. These alleles demonstrate\npositive epistasis and pleiotropy in the formation of haplotypes. Allelic\npleiotropy creates a network of highly inter-related HLA haplotypes that\naccount for 97% of the census sample. We suggest this network has properties of\na quasi-species and is itself under selection. We also suggest this is the\norigin of balancing selection in the HLA system.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "We introduce a multimodal deep learning framework, Prescriptive Neural\nNetworks (PNNs), that combines ideas from optimization and machine learning,\nand is, to the best of our knowledge, the first prescriptive method to handle\nmultimodal data. The PNN is a feedforward neural network trained on embeddings\nto output an outcome-optimizing prescription. In two real-world multimodal\ndatasets, we demonstrate that PNNs prescribe treatments that are able to\nsignificantly improve estimated outcomes in transcatheter aortic valve\nreplacement (TAVR) procedures by reducing estimated postoperative complication\nrates by 32% and in liver trauma injuries by reducing estimated mortality rates\nby over 40%. In four real-world, unimodal tabular datasets, we demonstrate that\nPNNs outperform or perform comparably to other well-known, state-of-the-art\nprescriptive models; importantly, on tabular datasets, we also recover\ninterpretability through knowledge distillation, fitting interpretable Optimal\nClassification Tree models onto the PNN prescriptions as classification\ntargets, which is critical for many real-world applications. Finally, we\ndemonstrate that our multimodal PNN models achieve stability across randomized\ndata splits comparable to other prescriptive methods and produce realistic\nprescriptions across the different datasets.",
        "The charge density wave (CDW), a translational symmetry breaking electronic\nliquid, plays a pivotal role in correlated quantum materials, such as\nhigh-T$_c$ superconductors and topological semimetals. Recently, CDWs that\npossibly intertwine with superconductivity and magnetism are observed in\nvarious kagome metals. However, the nature of CDWs and the role of the Fermi\nsurface (FS) topology in these materials remain an unresolved challenge. In\nthis letter, we reveal the formation of CDWs in the newly discovered kagome\nmetal LuNb$_6$Sn$_6$. We observe a precursor CDW correlation that features a\n\"yield sign\"-like hollow triangle diffuse scattering pattern and nearly\ncomplete softening of a flat optical phonon band near Q$_H$=(1\/3, 1\/3, 1\/2).\nThe scattering intensity of the precursor CDW displays divergent behavior as\ndecreasing temperature until T$_{CDW}$=70 K, where a competing CDW at\nQ$_{CDW}$=(1\/3, 1\/3, 1\/3) emerges. Using scanning tunneling\nmicroscopy\/spectroscopy, we image the frustrated CDW patterns that show a short\nphase coherence length about ~20 nm in real space. Combined with first\nprinciples calculations, our observations support frustrated lattice\ninteractions that are modulated by the FS topology. These results shed light on\nthe interplay between FS and CDW in quantum materials with nearly degenerate\nstructural deformation patterns.",
        "We explain the relation between the Witt class and the universal\nequicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood\ninequality.",
        "Recent observational analyses have revealed a significant tension in the\ngrowth index $\\gamma$, which characterizes the growth rate of cosmic\nstructures. Specifically, when treating $\\gamma$ as a free parameter within\n$\\Lambda$CDM framework, a combination of Planck and $ f\\sigma_8 $ data yields\n$\\gamma \\approx 0.64$, in $\\sim4\\sigma$ tension with the theoretically expected\nvalue $\\gamma \\approx 0.55$ (assuming general relativity). This discrepancy,\nclosely related to the $ S_8 $ tension, poses a new challenge to the standard\ncosmological model by suggesting that it predicts an excessive growth of\nstructure. In this work, we demonstrate that the $\\Lambda_{\\rm s}$CDM framework\n(featuring a rapid sign-switching cosmological constant (mirror AdS-to-dS\ntransition) in the late universe at redshift $ z_\\dagger \\sim 2 $) can\nsimultaneously alleviate the $ \\gamma $, $ H_0 $, and $ S_8 $ tensions. We also\nexamined a scenario with fixed $ z_\\dagger = 1.7 $, previously identified as a\nsweet spot for alleviating multiple major cosmological tensions (including\nthose in $ H_0 $, $ M_B $, and $ S_8 $) finding that it completely eliminates\nboth the $ \\gamma $ and $ H_0 $ tensions, although it is statistically\ndisfavored by our dataset combinations. Our findings suggest that $\\Lambda_{\\rm\ns}$CDM is a promising model, providing a potential unified resolution to\nmultiple major cosmological tensions.",
        "We investigate several problems concerning extremal Betti numbers and\npersistence in filtrations of flag complexes. For graphs on $n$ vertices, we\nshow that $\\beta_k(X(G))$ is maximal when $G=\\mathcal{T}_{n,k+1}$, the Tur\\'an\ngraph on $k+1$ partition classes, where $X(G)$ denotes the flag complex of $G$.\nBuilding on this, we construct an edgewise (one edge at a time) filtration\n$\\mathcal{G}=G_1\\subseteq \\cdots \\subseteq \\mathcal{T}_{n,k+1}$ for which\n$\\beta_k(X(G_i))$ is maximal for all graphs on $n$ vertices and $i$ edges.\nMoreover, the persistence barcode $\\mathcal{B}_k(X(G))$ achieves a maximal\nnumber of intervals, and total persistence, among all edgewise filtrations with\n$|E(\\mathcal{T}_{n,k+1})|$ edges.\n  For $k=1$, we consider edgewise filtrations of the complete graph $K_n$. We\nshow that the maximal number of intervals in the persistence barcode is\nobtained precisely when $G_{\\lceil n\/2\\rceil \\cdot \\lfloor n\/2\n\\rfloor}=\\mathcal{T}_{n,2}$. Among such filtrations, we characterize those\nachieving maximal total persistence. We further show that no filtration can\noptimize $\\beta_1(X(G_i))$ for all $i$, and conjecture that our filtrations\nmaximize the total persistence over all edgewise filtrations of $K_n$.",
        "Accreting millisecond pulsars (AMSPs) are excellent laboratories to study\nreflection spectra and their features from an accretion disk truncated by a\nrapidly rotating magnetosphere near the neutron star surface. These systems\nalso exhibit thermonuclear (type-I) bursts that can provide insights on the\naccretion physics and fuel composition. We explore spectral properties of the\nAMSP SRGA J144459.2-0604207 observed during the outburst that recently led to\nits discovery in February 2024. We aim to characterize the spectral shape of\nthe persistent emission, both its continuum and discrete features, and to\nanalyze type-I bursts properties. We employ XMM and NuSTAR overlapping\nobservations taken during the most recent outburst from SRGA J1444. We perform\nspectral analysis of the persistent (i.e., non-bursting) emission employing a\nsemi-phenomenological continuum model composed of a dominant thermal\nComptonization plus two thermal contributions, and a physical reflection model.\nWe also perform time-resolved spectral analysis of a type-I burst employing a\nblackbody model. We observe a broadened iron emission line, thus suggesting\nrelativistic effects, supported by the physical model accounting for\nrelativistically blurred reflection. The resulting accretion disk extends down\nto 6 gravitational radii, inclined at ~$53^{\\circ}$, and only moderately\nionized (log$\\xi\\simeq2.3$). We observe an absorption edge at ~9.7 keV that can\nbe interpreted as an Fe XXVI edge blueshifted by an ultrafast ($\\simeq0.04$c)\noutflow. Our broadband observations of type-I bursts do not find evidence of\nphotospheric radius expansion. The burst recurrence time shows a dependence on\nthe count rate with the steepest slope ever observed in these systems. We also\nobserve a discrepancy of ~3 between the observed and expected burst recurrence\ntime, which we discuss in the framework of fuel composition and high NS mass\nscenarios.",
        "In this work, we establish two main results in the context of arithmetic and\ngeometric properties of plane curves. First, we construct numerous new examples\nof arithmetic Zariski pairs and multiplets, where only a few ones were\npreviously available. Second, we describe a faithful action of the absolute\nGalois group on the equisingular strata of plane curves, providing insights\ninto the interplay between Galois representations and the geometry of singular\nplane curves. We conclude the paper with very concretes examples of the general\nresults obtained.",
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "Endo--Pajitnov manifolds are generalizations to higher dimensions of the\nInoue surfaces $S^M$. We study the existence of complex submanifolds in\nEndo--Pajitnov manifolds. We identify a class of these manifolds that do\ncontain compact complex submanifolds and establish an algebraic condition under\nwhich an Endo--Pajitnov manifold contains no compact complex curves.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "There are several strategies to discover new superconductors. Growing new\nmaterials and applying high pressures can be the classic ways since\nsuperconductivity was found. Also, chemical processing, such as annealing, is\nanother way to induce superconductivity in a non-superconducting material.\nHere, we show chemical processing effects in the non-superconducting material,\nsulfur-substituted FeTe. It has been known that superconductivity in\nS-substituted FeTe is induced by O$_2$ annealing. We revealed that hydrochloric\nacid etching and vacuum annealing for O$_2$-annealed samples made the quality\nof superconductivity higher by several physical property measurements.\nFurthermore, we visualized the superconducting regions by a magneto-optical\nimaging technique, indicating that the superconductivity in the processed\nsample was bulk. In this sample, we confirmed that the concentration of excess\niron was reduced compared to that in the as-grown state. These results provide\nan important route to bulk superconductivity in S-substituted FeTe and its\nrelated iron-based compounds.",
        "Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu\n(2019, HMX), arguing that failing to model nonlinear relationships between the\ntreatment and moderator leads to biased marginal effect estimates and\nuncontrolled Type-I error rates. While these critiques highlight the issue of\nunder-modeling nonlinearity in applied research, they are fundamentally flawed\nin several key ways. First, the causal estimand for interaction effects and the\nnecessary identifying assumptions are not clearly defined in these critiques.\nOnce properly stated, the critiques no longer hold. Second, the kernel\nestimator HMX proposes recovers the true causal effects in the scenarios\npresented in these recent critiques, which compared effects to the wrong\nbenchmark, producing misleading conclusions. Third, while Generalized Additive\nModels (GAM) can be a useful exploratory tool (as acknowledged in HMX), they\nare not designed to estimate marginal effects, and better alternatives exist,\nparticularly in the presence of additional covariates. Our response aims to\nclarify these misconceptions and provide updated recommendations for\nresearchers studying interaction effects through the estimation of conditional\nmarginal effects.",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "We prove that under the natural assumption over the dynamical degrees, the\nsaddle periodic points of a H\\'enon-like map in any dimension equidistribute\nwith respect to the equilibrium measure. Our work is a generalization of\nresults of Bedford-Lyubich-Smillie, Dujardin and Dinh-Sibony along with\nimprovements of their techniques. We also investigate some fine properties of\nGreen currents associated with the map. On the pluripotential-theory side, in\nour non-compact setting, the wedge product of two positive closed currents of\ncomplementary bi-degrees can be defined using super-potentials and the density\ntheory. We prove that these two definitions are coherent.",
        "Liquid nitrogen (LN2) is the only cooling medium for the high-temperature\nsuperconducting (HTS) bulks in the superconducting levitator, which is the\nheart of the maglev train, to reach working state. The detection and\ndetermination of the critical LN2 content are crucial for reliable operation of\nthe HTS maglev train. However, the related intelligent detection model and\ntechnology is lack in the combination filed of the cryogenic environment and\nmaglev application, and there is no existing method to detect the LN2 content\nin superconducting levitator. This paper proposes to employ multisensor fusion\nframework to fuse and enhance the accuracy of critical LN2 content testing.\nFour temperature sensors were deployed inside superconducting levitator to\nmeasure the temperature change during the LN2 content changing from 100 % to 0.\nIt was first obtained that the critical LN2 content in the superconducting\nlevitator is 4%. To accurately monitor the critical LN2 content in the\nsuperconducting levitator, a matrix-weighted information fusion Kalman filter\nalgorithm was used. Compared with the previous single sensor method, the\ntesting accuracy of the multisensor fusion method can be improved by 5.6%. The\nwork can provide a preliminary research foundation for the online monitoring\nand fault diagnosis of HTS maglev train."
      ]
    }
  },
  {
    "id":"2411.02815",
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation",
    "start_abstract":"In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b36",
        "b33"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
        "We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99\/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images."
      ],
      "categories":[
        "cs.AI",
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "k-fold Subsampling based Sequential Backward Feature Elimination",
        "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning",
        "HarmonySet: A Comprehensive Dataset for Understanding Video-Music\n  Semantic Alignment and Temporal Synchronization",
        "Classifier-guided CLIP Distillation for Unsupervised Multi-label\n  Classification",
        "Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion",
        "MM-OR: A Large Multimodal Operating Room Dataset for Semantic\n  Understanding of High-Intensity Surgical Environments",
        "Se\\~norita-2M: A High-Quality Instruction-based Dataset for General\n  Video Editing by Video Specialists",
        "VidChain: Chain-of-Tasks with Metric-based Direct Preference\n  Optimization for Dense Video Captioning",
        "Medical Image Classification with KAN-Integrated Transformers and\n  Dilated Neighborhood Attention",
        "PixelPonder: Dynamic Patch Adaptation for Enhanced Multi-Conditional\n  Text-to-Image Generation",
        "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
        "Free-Lunch Color-Texture Disentanglement for Stylized Image Generation",
        "Irreducibility results for equivariant $\\mathcal{D}$-modules on rigid\n  analytic spaces",
        "Benign Overfitting and the Geometry of the Ridge Regression Solution in\n  Binary Classification",
        "A Chain-Driven, Sandwich-Legged Quadruped Robot: Design and Experimental\n  Analysis",
        "TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning\n  of Sentence Embeddings",
        "Bridging Contrastive Learning and Domain Adaptation: Theoretical\n  Perspective and Practical Application",
        "Is excess smoothing of Planck CMB ansiotropy data partially responsible\n  for evidence for dark energy dynamics in other $w(z)$CDM parametrizations?",
        "Generic uniqueness and conjugate points for optimal control problems",
        "Explicit Formulas for the Alexander Polynomial of Pretzel Knots",
        "MLLM4PUE: Toward Universal Embeddings in Digital Pathology through\n  Multimodal LLMs",
        "Dispider: Enabling Video LLMs with Active Real-Time Interaction via\n  Disentangled Perception, Decision, and Reaction",
        "Large Language Models for Video Surveillance Applications",
        "Now you see me! A framework for obtaining class-relevant saliency maps",
        "The two extremal rays of some Hyper-K\\\"ahler fourfolds",
        "Orthogonal projection-based regularization for efficient model\n  augmentation",
        "Can Bayesian Neural Networks Make Confident Predictions?"
      ],
      "abstract":[
        "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy.",
        "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps:\/\/github.com\/mrazhou\/BRSIC.",
        "This paper introduces HarmonySet, a comprehensive dataset designed to advance\nvideo-music understanding. HarmonySet consists of 48,328 diverse video-music\npairs, annotated with detailed information on rhythmic synchronization,\nemotional alignment, thematic coherence, and cultural relevance. We propose a\nmulti-step human-machine collaborative framework for efficient annotation,\ncombining human insights with machine-generated descriptions to identify key\ntransitions and assess alignment across multiple dimensions. Additionally, we\nintroduce a novel evaluation framework with tasks and metrics to assess the\nmulti-dimensional alignment of video and music, including rhythm, emotion,\ntheme, and cultural context. Our extensive experiments demonstrate that\nHarmonySet, along with the proposed evaluation framework, significantly\nimproves the ability of multimodal models to capture and analyze the intricate\nrelationships between video and music.",
        "Multi-label classification is crucial for comprehensive image understanding,\nyet acquiring accurate annotations is challenging and costly. To address this,\na recent study suggests exploiting unsupervised multi-label classification\nleveraging CLIP, a powerful vision-language model. Despite CLIP's proficiency,\nit suffers from view-dependent predictions and inherent bias, limiting its\neffectiveness. We propose a novel method that addresses these issues by\nleveraging multiple views near target objects, guided by Class Activation\nMapping (CAM) of the classifier, and debiasing pseudo-labels derived from CLIP\npredictions. Our Classifier-guided CLIP Distillation (CCD) enables selecting\nmultiple local views without extra labels and debiasing predictions to enhance\nclassification performance. Experimental results validate our method's\nsuperiority over existing techniques across diverse datasets. The code is\navailable at https:\/\/github.com\/k0u-id\/CCD.",
        "The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https:\/\/github.com\/LemuelPuglisi\/BrLP.",
        "Operating rooms (ORs) are complex, high-stakes environments requiring precise\nunderstanding of interactions among medical staff, tools, and equipment for\nenhancing surgical assistance, situational awareness, and patient safety.\nCurrent datasets fall short in scale, realism and do not capture the multimodal\nnature of OR scenes, limiting progress in OR modeling. To this end, we\nintroduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR\ndataset, and the first dataset to enable multimodal scene graph generation.\nMM-OR captures comprehensive OR scenes containing RGB-D data, detail views,\naudio, speech transcripts, robotic logs, and tracking data and is annotated\nwith panoptic segmentations, semantic scene graphs, and downstream task labels.\nFurther, we propose MM2SG, the first multimodal large vision-language model for\nscene graph generation, and through extensive experiments, demonstrate its\nability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG\nestablish a new benchmark for holistic OR understanding, and open the path\ntowards multimodal scene analysis in complex, high-stakes environments. Our\ncode, and data is available at https:\/\/github.com\/egeozsoy\/MM-OR.",
        "Recent advancements in video generation have spurred the development of video\nediting techniques, which can be divided into inversion-based and end-to-end\nmethods. However, current video editing methods still suffer from several\nchallenges. Inversion-based methods, though training-free and flexible, are\ntime-consuming during inference, struggle with fine-grained editing\ninstructions, and produce artifacts and jitter. On the other hand, end-to-end\nmethods, which rely on edited video pairs for training, offer faster inference\nspeeds but often produce poor editing results due to a lack of high-quality\ntraining video pairs. In this paper, to close the gap in end-to-end methods, we\nintroduce Se\\~norita-2M, a high-quality video editing dataset. Se\\~norita-2M\nconsists of approximately 2 millions of video editing pairs. It is built by\ncrafting four high-quality, specialized video editing models, each crafted and\ntrained by our team to achieve state-of-the-art editing results. We also\npropose a filtering pipeline to eliminate poorly edited video pairs.\nFurthermore, we explore common video editing architectures to identify the most\neffective structure based on current pre-trained generative model. Extensive\nexperiments show that our dataset can help to yield remarkably high-quality\nvideo editing results. More details are available at\nhttps:\/\/senorita-2m-dataset.github.io.",
        "Despite the advancements of Video Large Language Models (VideoLLMs) in\nvarious tasks, they struggle with fine-grained temporal understanding, such as\nDense Video Captioning (DVC). DVC is a complicated task of describing all\nevents within a video while also temporally localizing them, which integrates\nmultiple fine-grained tasks, including video segmentation, video captioning,\nand temporal video grounding. Previous VideoLLMs attempt to solve DVC in a\nsingle step, failing to utilize their reasoning capability. Moreover, previous\ntraining objectives for VideoLLMs do not fully reflect the evaluation metrics,\ntherefore not providing supervision directly aligned to target tasks. To\naddress such a problem, we propose a novel framework named VidChain comprised\nof Chain-of-Tasks (CoTasks) and Metric-based Direct Preference Optimization\n(M-DPO). CoTasks decompose a complex task into a sequence of sub-tasks,\nallowing VideoLLMs to leverage their reasoning capabilities more effectively.\nM-DPO aligns a VideoLLM with evaluation metrics, providing fine-grained\nsupervision to each task that is well-aligned with metrics. Applied to two\ndifferent VideoLLMs, VidChain consistently improves their fine-grained video\nunderstanding, thereby outperforming previous VideoLLMs on two different DVC\nbenchmarks and also on the temporal video grounding task. Code is available at\n\\url{https:\/\/github.com\/mlvlab\/VidChain}.",
        "Convolutional networks, transformers, hybrid models, and Mamba-based\narchitectures have demonstrated strong performance across various medical image\nclassification tasks. However, these methods were primarily designed to\nclassify clean images using labeled data. In contrast, real-world clinical data\noften involve image corruptions that are unique to multi-center studies and\nstem from variations in imaging equipment across manufacturers. In this paper,\nwe introduce the Medical Vision Transformer (MedViTV2), a novel architecture\nincorporating Kolmogorov-Arnold Network (KAN) layers into the transformer\narchitecture for the first time, aiming for generalized medical image\nclassification. We have developed an efficient KAN block to reduce\ncomputational load while enhancing the accuracy of the original MedViT.\nAdditionally, to counteract the fragility of our MedViT when scaled up, we\npropose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the\nefficient fused dot-product attention kernel capable of capturing global\ncontext and expanding receptive fields to scale the model effectively and\naddressing feature collapse issues. Moreover, a hierarchical hybrid strategy is\nintroduced to stack our Local Feature Perception and Global Feature Perception\nblocks in an efficient manner, which balances local and global feature\nperceptions to boost performance. Extensive experiments on 17 medical image\nclassification datasets and 12 corrupted medical image datasets demonstrate\nthat MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments\nwith reduced computational complexity. MedViTV2 is 44\\% more computationally\nefficient than the previous version and significantly enhances accuracy,\nachieving improvements of 4.6\\% on MedMNIST, 5.8\\% on NonMNIST, and 13.4\\% on\nthe MedMNIST-C benchmark.",
        "Recent advances in diffusion-based text-to-image generation have demonstrated\npromising results through visual condition control. However, existing\nControlNet-like methods struggle with compositional visual conditioning -\nsimultaneously preserving semantic fidelity across multiple heterogeneous\ncontrol signals while maintaining high visual quality, where they employ\nseparate control branches that often introduce conflicting guidance during the\ndenoising process, leading to structural distortions and artifacts in generated\nimages. To address this issue, we present PixelPonder, a novel unified control\nframework, which allows for effective control of multiple visual conditions\nunder a single control structure. Specifically, we design a patch-level\nadaptive condition selection mechanism that dynamically prioritizes spatially\nrelevant control signals at the sub-region level, enabling precise local\nguidance without global interference. Additionally, a time-aware control\ninjection scheme is deployed to modulate condition influence according to\ndenoising timesteps, progressively transitioning from structural preservation\nto texture refinement and fully utilizing the control information from\ndifferent categories to promote more harmonious image generation. Extensive\nexperiments demonstrate that PixelPonder surpasses previous methods across\ndifferent benchmark datasets, showing superior improvement in spatial alignment\naccuracy while maintaining high textual semantic consistency.",
        "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps:\/\/github.com\/duyuwen-duen\/RoCo-Sim",
        "Recent advances in Text-to-Image (T2I) diffusion models have transformed\nimage generation, enabling significant progress in stylized generation using\nonly a few style reference images. However, current diffusion-based methods\nstruggle with fine-grained style customization due to challenges in controlling\nmultiple style attributes, such as color and texture. This paper introduces the\nfirst tuning-free approach to achieve free-lunch color-texture disentanglement\nin stylized T2I generation, addressing the need for independently controlled\nstyle elements for the Disentangled Stylized Image Generation (DisIG) problem.\nOur approach leverages the Image-Prompt Additivity property in the CLIP image\nembedding space to develop techniques for separating and extracting\nColor-Texture Embeddings (CTE) from individual color and texture reference\nimages. To ensure that the color palette of the generated image aligns closely\nwith the color reference, we apply a whitening and coloring transformation to\nenhance color consistency. Additionally, to prevent texture loss due to the\nsignal-leak bias inherent in diffusion training, we introduce a noise term that\npreserves textural fidelity during the Regularized Whitening and Coloring\nTransformation (RegWCT). Through these methods, our Style Attributes\nDisentanglement approach (SADis) delivers a more precise and customizable\nsolution for stylized image generation. Experiments on images from the WikiArt\nand StyleDrop datasets demonstrate that, both qualitatively and quantitatively,\nSADis surpasses state-of-the-art stylization methods in the DisIG task.Code\nwill be released at https:\/\/deepffff.github.io\/sadis.github.io\/.",
        "We prove a general irreducibility result for geometrically induced\ncoadmissible equivariant $\\mathcal{D}$-modules on rigid analytic spaces. As an\napplication, we geometrically reprove the irreducibility of certain locally\nanalytic representations previously constructed by Orlik-Strauch.",
        "In this work, we investigate the behavior of ridge regression in an\noverparameterized binary classification task. We assume examples are drawn from\n(anisotropic) class-conditional cluster distributions with opposing means and\nwe allow for the training labels to have a constant level of label-flipping\nnoise. We characterize the classification error achieved by ridge regression\nunder the assumption that the covariance matrix of the cluster distribution has\na high effective rank in the tail. We show that ridge regression has\nqualitatively different behavior depending on the scale of the cluster mean\nvector and its interaction with the covariance matrix of the cluster\ndistributions. In regimes where the scale is very large, the conditions that\nallow for benign overfitting turn out to be the same as those for the\nregression task. We additionally provide insights into how the introduction of\nlabel noise affects the behavior of the minimum norm interpolator (MNI). The\noptimal classifier in this setting is a linear transformation of the cluster\nmean vector and in the noiseless setting the MNI approximately learns this\ntransformation. On the other hand, the introduction of label noise can\nsignificantly change the geometry of the solution while preserving the same\nqualitative behavior.",
        "This paper introduces a chain-driven, sandwich-legged, mid-size quadruped\nrobot designed as an accessible research platform. The design prioritizes\nenhanced locomotion capabilities, improved reliability and safety of the\nactuation system, and simplified, cost-effective manufacturing processes.\nLocomotion performance is optimized through a sandwiched leg design and a\ndual-motor configuration, reducing leg inertia for agile movements. Reliability\nand safety are achieved by integrating robust cable strain reliefs, efficient\nheat sinks for motor thermal management, and mechanical limits to restrict leg\nmotion. Simplified design considerations include a quasi-direct drive (QDD)\nactuator and the adoption of low-cost fabrication techniques, such as laser\ncutting and 3D printing, to minimize cost and ensure rapid prototyping. The\nrobot weighs approximately 25 kg and is developed at a cost under \\$8000,\nmaking it a scalable and affordable solution for robotics research.\nExperimental validations demonstrate the platform's capability to execute trot\nand crawl gaits on flat terrain and slopes, highlighting its potential as a\nversatile and reliable quadruped research platform.",
        "Unsupervised sentence embedding representation has become a hot research\ntopic in natural language processing. As a tensor, sentence embedding has two\ncritical properties: direction and norm. Existing works have been limited to\nconstraining only the orientation of the samples' representations while\nignoring the features of their module lengths. To address this issue, we\npropose a new training objective that optimizes the training of unsupervised\ncontrastive learning by constraining the module length features between\npositive samples. We combine the training objective of Tensor's Norm\nConstraints with ensemble learning to propose a new Sentence Embedding\nrepresentation framework, TNCSE. We evaluate seven semantic text similarity\ntasks, and the results show that TNCSE and derived models are the current\nstate-of-the-art approach; in addition, we conduct extensive zero-shot\nevaluations, and the results show that TNCSE outperforms other baselines.",
        "This work studies the relationship between Contrastive Learning and Domain\nAdaptation from a theoretical perspective. The two standard contrastive losses,\nNT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to\nthe Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely\nused for Domain Adaptation. Our work shows that minimizing the contrastive\nlosses decreases the CMMD and simultaneously improves class-separability,\nlaying the theoretical groundwork for the use of Contrastive Learning in the\ncontext of Domain Adaptation. Due to the relevance of Domain Adaptation in\nmedical imaging, we focused the experiments on mammography images. Extensive\nexperiments on three mammography datasets - synthetic patches, clinical (real)\npatches, and clinical (real) images - show improved Domain Adaptation,\nclass-separability, and classification performance, when minimizing the\nSupervised Contrastive loss.",
        "We study spatially-flat dynamical dark energy parametrizations, $w(z)$CDM,\nwith redshift-dependent dark energy equation of state parameter $w(z)$\nexpressed using three different quadratic and other polynomial forms (as\nfunctions of $1-a$, where $a$ is the scale factor), without and with a varying\ncosmic microwave background (CMB) lensing consistency parameter $A_L$. We use\nPlanck CMB anisotropy data (P18 and lensing) and a large, mutually-consistent\nnon-CMB data compilation that includes Pantheon+ type Ia supernova, baryon\nacoustic oscillation (BAO), Hubble parameter ($H(z)$), and growth factor\n($f\\sigma_8$) measurements, but not recent DESI BAO data. The six $w(z)$CDM\n($+A_L$) parametrizations show higher consistency between the CMB and non-CMB\ndata constraints compared to the XCDM ($+A_L$) and $w_0 w_a$CDM ($+A_L$) cases.\nConstraints from the most-restrictive P18+lensing+non-CMB data compilation on\nthe six $w(z)$CDM ($+A_L$) parametrizations indicate that dark energy dynamics\nis favored over a cosmological constant by $\\gtrsim 2\\sigma$ when $A_L = 1$,\nbut only by $\\gtrsim 1\\sigma$ when $A_L$ is allowed to vary (and $A_L>1$ at\n$\\sim2\\sigma$ significance). Non-CMB data dominate the P18+lensing+non-CMB\ncompilation at low $z$ and favor quintessence-like dark energy. At high $z$\nP18+lensing data dominate, favoring phantom-like dark energy with significance\nfrom $1.5\\sigma$ to $2.9 \\sigma$ when $A_L = 1$, and from $1.1\\sigma$ to\n$1.8\\sigma$ when $A_L$ varies. These results suggest that the observed excess\nweak lensing smoothing of some of the Planck CMB anistropy multipoles is\npartially responsible for the $A_L = 1$ cases $\\gtrsim 2\\sigma$ evidence for\ndark energy dynamics over a cosmological constant.",
        "The paper is concerned with an optimal control problem on $\\mathbb{R}^n$,\nwhere the dynamics is linear w.r.t.~the control functions. For a terminal cost\n$\\psi$ in a $mathcal{G}_\\delta$ set of $\\mathcal{C}^4(\\mathbb{R}^n)$ (i.e., in\na countable intersection of open dense subsets), two main results are\nproved.Namely: the set $\\Gamma_\\psi\\subset\\mathbb{R}^n$ of conjugate points is\nclosed, with locally bounded $(n-2)$-dimensional Hausdorff measure. Moreover,\nthe set of initial points $y\\in \\mathbb{R}^n\\setminus\\Gamma_\\psi$, which admit\ntwo or more globally optimal trajectories, is contained in the union of a\nlocally finite family of embedded manifolds. In particular, the value function\nis continuously differentiable on an open, dense subset of $\\mathbb{R}^n$.",
        "We provide explicit formulas for the Alexander polynomial of Pretzel knots\nand establish several immediate corollaries, including the characterization of\nPretzel knots with a trivial Alexander polynomial.",
        "Pathology plays a critical role in diagnosing a wide range of diseases, yet\nexisting approaches often rely heavily on task-specific models trained on\nextensive, well-labeled datasets. These methods face sustainability challenges\ndue to the diversity of pathologies and the labor-intensive nature of data\ncollection. To address these limitations, we highlight the need for universal\nmultimodal embeddings that can support multiple downstream tasks. Previous\napproaches involve fine-tuning CLIP-based models, which handle images and texts\nseparately, limiting their ability to capture complex multimodal relationships.\nAdditionally, these models are evaluated across diverse datasets without a\nunified benchmark. In this paper, we explore the possibility of applying\nMultimodal Large Language Models (MLLMs) to generate pathology universal\nembeddings to address these challenges. Our contributions can be summarized in\nthe following aspects: 1) We propose MLLM4PUE, a novel framework that leverages\nMLLMs to generate embeddings for various pathology downstream tasks. 2) We\nfurther introduce the Pathology Multimodal Embedding Benchmark (PMEB), a\ncomprehensive benchmark designed to assess the quality of pathology multimodal\nembeddings, which comprises 16 original tasks drawn from 15 datasets. 3)\nExtensive experimental results demonstrate the superiority of MLLM4PUE,\nillustrating MLLM-based models can effectively support a wide range of\ndownstream tasks and unify the research direction for foundation models in\npathology.",
        "Active Real-time interaction with video LLMs introduces a new paradigm for\nhuman-computer interaction, where the model not only understands user intent\nbut also responds while continuously processing streaming video on the fly.\nUnlike offline video LLMs, which analyze the entire video before answering\nquestions, active real-time interaction requires three capabilities: 1)\nPerception: real-time video monitoring and interaction capturing. 2) Decision:\nraising proactive interaction in proper situations, 3) Reaction: continuous\ninteraction with users. However, inherent conflicts exist among the desired\ncapabilities. The Decision and Reaction require a contrary Perception scale and\ngrain, and the autoregressive decoding blocks the real-time Perception and\nDecision during the Reaction. To unify the conflicted capabilities within a\nharmonious system, we present Dispider, a system that disentangles Perception,\nDecision, and Reaction. Dispider features a lightweight proactive streaming\nvideo processing module that tracks the video stream and identifies optimal\nmoments for interaction. Once the interaction is triggered, an asynchronous\ninteraction module provides detailed responses, while the processing module\ncontinues to monitor the video in the meantime. Our disentangled and\nasynchronous design ensures timely, contextually accurate, and computationally\nefficient responses, making Dispider ideal for active real-time interaction for\nlong-duration video streams. Experiments show that Dispider not only maintains\nstrong performance in conventional video QA tasks, but also significantly\nsurpasses previous online models in streaming scenario responses, thereby\nvalidating the effectiveness of our architecture. The code and model are\nreleased at \\url{https:\/\/github.com\/Mark12Ding\/Dispider}.",
        "The rapid increase in video content production has resulted in enormous data\nvolumes, creating significant challenges for efficient analysis and resource\nmanagement. To address this, robust video analysis tools are essential. This\npaper presents an innovative proof of concept using Generative Artificial\nIntelligence (GenAI) in the form of Vision Language Models to enhance the\ndownstream video analysis process. Our tool generates customized textual\nsummaries based on user-defined queries, providing focused insights within\nextensive video datasets. Unlike traditional methods that offer generic\nsummaries or limited action recognition, our approach utilizes Vision Language\nModels to extract relevant information, improving analysis precision and\nefficiency. The proposed method produces textual summaries from extensive CCTV\nfootage, which can then be stored for an indefinite time in a very small\nstorage space compared to videos, allowing users to quickly navigate and verify\nsignificant events without exhaustive manual review. Qualitative evaluations\nresult in 80% and 70% accuracy in temporal and spatial quality and consistency\nof the pipeline respectively.",
        "Neural networks are part of daily-life decision-making, including in\nhigh-stakes settings where understanding and transparency are key. Saliency\nmaps have been developed to gain understanding into which input features neural\nnetworks use for a specific prediction. Although widely employed, these methods\noften result in overly general saliency maps that fail to identify the specific\ninformation that triggered the classification. In this work, we suggest a\nframework that allows to incorporate attributions across classes to arrive at\nsaliency maps that actually capture the class-relevant information. On\nestablished benchmarks for attribution methods, including the grid-pointing\ngame and randomization-based sanity checks, we show that our framework heavily\nboosts the performance of standard saliency map approaches. It is, by design,\nagnostic to model architectures and attribution methods and now allows to\nidentify the distinguishing and shared features used for a model prediction.",
        "We consider projective Hyper-K\\\"ahler manifolds of dimension four that are\ndeformation equivalent to Hilbert squares of K3 surfaces. In case such a\nmanifold admits a divisorial contraction, the exceptional divisor is a conic\nbundle over a K3 surface. There are five types of such conic bundles. In case\nthe manifold has Picard rank two and has two (birational) divisorial\ncontractions we determine the types of these conic bundles. There are exactly\nseven cases. For the Fano varieties of cubic fourfolds there are only four\ncases and we provide examples of these.",
        "Deep-learning-based nonlinear system identification has shown the ability to\nproduce reliable and highly accurate models in practice. However, these\nblack-box models lack physical interpretability, and often a considerable part\nof the learning effort is spent on capturing already expected\/known behavior\ndue to first-principles-based understanding of some aspects of the system. A\npotential solution is to integrate prior physical knowledge directly into the\nmodel structure, combining the strengths of physics-based modeling and\ndeep-learning-based identification. The most common approach is to use an\nadditive model augmentation structure, where the physics-based and the\nmachine-learning (ML) components are connected in parallel. However, such\nmodels are overparametrized, training them is challenging, potentially causing\nthe physics-based part to lose interpretability. To overcome this challenge,\nthis paper proposes an orthogonal projection-based regularization technique to\nenhance parameter learning, convergence, and even model accuracy in\nlearning-based augmentation of nonlinear baseline models.",
        "Bayesian inference promises a framework for principled uncertainty\nquantification of neural network predictions. Barriers to adoption include the\ndifficulty of fully characterizing posterior distributions on network\nparameters and the interpretability of posterior predictive distributions. We\ndemonstrate that under a discretized prior for the inner layer weights, we can\nexactly characterize the posterior predictive distribution as a Gaussian\nmixture. This setting allows us to define equivalence classes of network\nparameter values which produce the same likelihood (training error) and to\nrelate the elements of these classes to the network's scaling regime -- defined\nvia ratios of the training sample size, the size of each layer, and the number\nof final layer parameters. Of particular interest are distinct parameter\nrealizations that map to low training error and yet correspond to distinct\nmodes in the posterior predictive distribution. We identify settings that\nexhibit such predictive multimodality, and thus provide insight into the\naccuracy of unimodal posterior approximations. We also characterize the\ncapacity of a model to \"learn from data\" by evaluating contraction of the\nposterior predictive in different scaling regimes."
      ]
    }
  },
  {
    "id":"2411.00561",
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study",
    "start_abstract":"In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "What is a cell type, really? The quest to categorize life's myriad forms."
      ],
      "abstract":[
        "The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Leveraging Sequence Purification for Accurate Prediction of Multiple\n  Conformational States with AlphaFold2",
        "An Evaluation on the Role of Non-Coding RNA in HIV Transcription and\n  Latency: A Review",
        "Mechanism of Electricacupuncture Treating Detrusor Bladder Neck\n  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics",
        "The standard coil or globule phases cannot describe the denatured state\n  of structured proteins and intrinsically disordered proteins",
        "Pushing the boundaries of Structure-Based Drug Design through\n  Collaboration with Large Language Models",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL",
        "ProtComposer: Compositional Protein Structure Generation with 3D\n  Ellipsoids",
        "Silicon is the next frontier in plant synthetic biology",
        "Fluorescence Phasor Analysis: Basic Principles and Biophysical\n  Applications",
        "Inverse problems with experiment-guided AlphaFold",
        "A Comprehensive Review of Protein Language Models",
        "A Multiple Transferable Neural Network Method with Domain Decomposition\n  for Elliptic Interface Problems",
        "Transparent Graphene-Superconductor Interfaces: Quantum Hall and Zero\n  Field Regimes",
        "Nuclear level density of ${}^{128}$Te from\n  $(\\mathrm{p},\\mathrm{p}'\\gamma)$ scattering and complementary photonuclear\n  data",
        "Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates\n  and Fundamental Limits",
        "Bounds for within-household encouragement designs with interference",
        "The irreducibility of Hurwitz spaces and Severi varieties on toric\n  surfaces",
        "Electric fields-tuning plasmon and coupled plasmon-phonon modes in\n  monolayer transition metal dichalcogenides",
        "A Global Existence Theorem for a Fourth-Order Crystal Surface Model with\n  Gradient Dependent Mobility",
        "Effect of a polymeric compound layer on jetting dynamics produced by\n  bursting bubbles",
        "Symmetric observations without symmetric causal explanations",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "Engineering of electronic and magnetic modulations in gradient\n  functional oxide heterostructures",
        "Dual spectroscopy of quantum simulated Fermi-Hubbard systems",
        "On weight and variance uncertainty in neural networks for regression\n  tasks",
        "Steady vortex patches on flat torus with a constant background vorticity"
      ],
      "abstract":[
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "AlphaFold2 (AF2) has transformed protein structure prediction by harnessing\nco-evolutionary constraints embedded in multiple sequence alignments (MSAs).\nMSAs not only encode static structural information, but also hold critical\ndetails about protein dynamics, which underpin biological functions. However,\nthese subtle co-evolutionary signatures, which dictate conformational state\npreferences, are often obscured by noise within MSA data and thus remain\nchallenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework\nthat isolates these co-evolutionary signals through sequence purification and\niterative enrichment. By extracting sequence subsets that preferentially encode\ndistinct structural states, AF-ClaSeq enables high-confidence predictions of\nalternative conformations. Our findings reveal that the successful sampling of\nalternative states depends not on MSA depth but on sequence purity.\nIntriguingly, purified sequences encoding specific structural states are\ndistributed across phylogenetic clades and superfamilies, rather than confined\nto specific lineages. Expanding upon AF2's transformative capabilities,\nAF-ClaSeq provides a powerful approach for uncovering hidden structural\nplasticity, advancing allosteric protein and drug design, and facilitating\ndynamics-based protein function annotation.",
        "The existence of latent cellular reservoirs is recognized as the major\nbarrier to an HIV cure. Reactivating and eliminating \"shock and kill\" or\npermanently silencing \"block and lock\" the latent HIV reservoir, as well as\ngene editing, remain promising approaches, but so far have proven to be only\npartially successful. Moreover, using latency reversing agents or \"block and\nlock\" drugs pose additional considerations, including the ability to cause\ncellular toxicity, a potential lack of specificity for HIV, or low potency when\neach agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long\nnon-coding RNAs (lncRNAs) are becoming increasingly recognized as important\nregulators of gene expression. RNA-based approaches for combatting HIV latency\nrepresent a promising strategy since both miRNAs and lncRNAs are more cell-type\nand tissue specific than protein coding genes. Thus, a higher specificity of\ntargeting the latent HIV reservoir with less overall cellular toxicity can\nlikely be achieved. In this review, we summarize current knowledge about HIV\ngene expression regulation by miRNAs and lncRNAs encoded in the human genome,\nas well as regulatory molecules encoded in the HIV genome. We discuss both the\ntranscriptional and post-transcriptional regulation of HIV gene expression to\nalign with the current definition of latency, and describe RNA molecules that\neither promote HIV latency or have anti-latency properties. Finally, we provide\nperspectives on using each class of RNAs as potential targets for combatting\nHIV latency, and describe the complexity of the interactions between different\nRNA molecules, their protein targets, and HIV.",
        "Objectives This study aimed to elucidate the potential mechanisms of\nelectroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)\nfollowing suprasacral spinal cord injury.\n  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned\nto either a sham group (n=12) or a spinal cord injury model group (n=40). In\nthe model group, DBND was induced in 40 rats through Hassan Shaker spinal cord\ntransection, with 24 rats surviving spinal shock and subsequently randomized\ninto two groups: a model-only group (DBND, n=12) and an EA intervention group\n(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and\nSanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10\nconsecutive days. On day 29 post-injury, all rats underwent urodynamic\nassessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag\n(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder\nneck tissues.\n  Results Urodynamic evaluation demonstrated that EA intervention enhanced\nbladder function in DBND rats. HE staining indicated reduced fibroplasia in the\ndetrusor muscle and alleviated inflammation in the bladder neck following EA.\nTMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in\nthe detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results\ncorroborated these TMT findings.\n  Conclusion EA effectively promotes synergy between the detrusor muscle and\nbladder neck in DBND, likely by enhancing detrusor contractility and\nfacilitating bladder neck relaxation during urination. This study provides\nmechanistic insights into the therapeutic role of EA in managing DBND.",
        "The concepts of globule and random coil were developed to describe the phases\nof homopolymers and then used to characterize the denatured state of structured\ncytosolic proteins and intrinsically disordered proteins. Using multi-scale\nmolecular dynamics simulations, we were able to explore the conformational\nspace of the disordered conformations of both types of protein under biological\nconditions in an affordable amount of computational time. By studying the size\nof the protein and the density correlations in space, we conclude that the\nstandard phases of homopolymers and the tools to detect them cannot be applied\nstraightforwardly to proteins.",
        "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "PyMOLfold is a flexible and open-source plugin designed to seamlessly\nintegrate AI-based protein structure prediction and visualization within the\nwidely used PyMOL molecular graphics system. By leveraging state-of-the-art\nprotein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows\nresearchers to directly predict protein tertiary structures from amino acid\nsequences without requiring external tools or complex workflows. Furthermore,\nwith certain models, users can provide a SMILES string of a ligand and have the\nsmall molecule placed in the protein structure. This unique capability bridges\nthe gap between computational folding and structural visualization, enabling\nusers to input a primary sequence, perform a folding prediction, and\nimmediately explore the resulting 3D structure within the same intuitive\nplatform.",
        "We develop ProtComposer to generate protein structures conditioned on spatial\nprotein layouts that are specified via a set of 3D ellipsoids capturing\nsubstructure shapes and semantics. At inference time, we condition on\nellipsoids that are hand-constructed, extracted from existing proteins, or from\na statistical model, with each option unlocking new capabilities.\nHand-specifying ellipsoids enables users to control the location, size,\norientation, secondary structure, and approximate shape of protein\nsubstructures. Conditioning on ellipsoids of existing proteins enables\nredesigning their substructure's connectivity or editing substructure\nproperties. By conditioning on novel and diverse ellipsoid layouts from a\nsimple statistical model, we improve protein generation with expanded Pareto\nfrontiers between designability, novelty, and diversity. Further, this enables\nsampling designable proteins with a helix-fraction that matches PDB proteins,\nunlike existing generative models that commonly oversample conceptually simple\nhelix bundles. Code is available at https:\/\/github.com\/NVlabs\/protcomposer.",
        "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
        "Fluorescence is one of the most widely used techniques in biological\nsciences. Its exceptional sensitivity and versatility make it a tool of first\nchoice for quantitative studies in biophysics. The concept of phasors,\noriginally introduced by Charles Steinmetz in the late 19th century for\nanalyzing alternating current circuits, has since found applications across\ndiverse disciplines, including fluorescence spectroscopy. The main idea behind\nfluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the\ncomplementary nature of pulse and phase fluorometry data, he shows that two\nmagnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain\nfluorescence measurements correspond to the real and imaginary part of the\nFourier transform of the fluorescence intensity in the time domain. This review\nprovides a historical perspective on how the concept of phasors originates and\nhow it integrates into fluorescence spectroscopy. We discuss their fundamental\nalgebraic properties, which enable intuitive model-free analysis of\nfluorescence data despite the complexity of the underlying phenomena. Some\napplications in biophysics illustrate the power of this approach in studying\ndiverse phenomena, including protein folding, protein interactions, phase\ntransitions in lipid mixtures and formation of high-order structures in nucleic\nacids.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "At the intersection of the rapidly growing biological data landscape and\nadvancements in Natural Language Processing (NLP), protein language models\n(PLMs) have emerged as a transformative force in modern research. These models\nhave achieved remarkable progress, highlighting the need for timely and\ncomprehensive overviews. However, much of the existing literature focuses\nnarrowly on specific domains, often missing a broader analysis of PLMs. This\nstudy provides a systematic review of PLMs from a macro perspective, covering\nkey historical milestones and current mainstream trends. We focus on the models\nthemselves and their evaluation metrics, exploring aspects such as model\narchitectures, positional encoding, scaling laws, and datasets. In the\nevaluation section, we discuss benchmarks and downstream applications. To\nfurther support ongoing research, we introduce relevant mainstream tools.\nLastly, we critically examine the key challenges and limitations in this\nrapidly evolving field.",
        "The transferable neural network (TransNet) is a two-layer shallow neural\nnetwork with pre-determined and uniformly distributed neurons in the hidden\nlayer, and the least-squares solvers can be particularly used to compute the\nparameters of its output layer when applied to the solution of partial\ndifferential equations. In this paper, we integrate the TransNet technique with\nthe nonoverlapping domain decomposition and the interface conditions to develop\na novel multiple transferable neural network (Multi-TransNet) method for\nsolving elliptic interface problems, which typically contain discontinuities in\nboth solutions and their derivatives across interfaces. We first propose an\nempirical formula for the TransNet to characterize the relationship between the\nradius of the domain-covering ball, the number of hidden-layer neurons, and the\noptimal neuron shape. In the Multi-TransNet method, we assign each subdomain\none distinct TransNet with an adaptively determined number of hidden-layer\nneurons to maintain the globally uniform neuron distribution across the entire\ncomputational domain, and then unite all the subdomain TransNets together by\nincorporating the interface condition terms into the loss function. The\nempirical formula is also extended to the Multi-TransNet and further employed\nto estimate appropriate neuron shapes for the subdomain TransNets, greatly\nreducing the parameter tuning cost. Additionally, we propose a normalization\napproach to adaptively select the weighting parameters for the terms in the\nloss function. Ablation studies and extensive experiments with comparison tests\non different types of elliptic interface problems with low to high contrast\ndiffusion coefficients in two and three dimensions are carried out to\nnumerically demonstrate the superior accuracy, efficiency, and robustness of\nthe proposed Multi-TransNet method.",
        "We study clean, edge-contacted graphene\/superconductor interfaces in both the\nquantum Hall (QH) and zero field regimes. We find that Andreev reflection is\nsubstantially stronger than at an interface with a semiconductor\ntwo-dimensional electron gas: the large velocity at graphene's conical Dirac\npoints makes the requirement of current continuity to a metal much less\nrestrictive. In both our tight-binding and continuum models, we find a wide\nrange of parameters for which Andreev reflection is strong. For a transparent\ninterface, we demonstrate the following for graphene in the lowest Landau level\nQH state: (i) Excellent electron-hole hybridization occurs: the electron and\nhole components in graphene are simply related by an exchange of sublattice.\nThe spatial profile for the electron component is predominantly gaussian on one\nsublattice and peaked at the interface, and so very different from the QH edge\nstate of a terminated lattice. (ii) The degree of hybridization is independent\nof the graphene filling: no fine-tuning is needed. (iii) The spectrum is valley\ndegenerate: the dispersion of each chiral Andreev edge mode (CAEM) self-aligns\nto be antisymmetric about the center of each valley, independent of filling.\nAchieving a transparent interface requires the absence of any barrier as well\nas a superconductor that is suitably matched to graphene; we argue that the\nlatter condition is not very stringent. We further consider the effect of\nreduced transparency and Zeeman splitting on both the wavefunctions of the CAEM\nand their dispersion.",
        "We have extracted the nuclear level density of ${}^{128}$Te from a\n$(\\mathrm{p},\\mathrm{p} '\\gamma)$ scattering experiment using the large-volume\n\\labr\\ and \\cebr\\ detectors from ELI-NP at the 9~MV Tandem facilities at\nIFIN-HH. The decay data were normalised using photonuclear data, resulting in\nnuclear level densities without intrinsic model dependencies from the constant\ntemperature or Fermi gas models. The measured nuclear level density follows\nclosely between the expectations from these two models, but we observe a clear\ndivergence from a microscopic model based on the Skyrme force.",
        "One of the most basic problems in reinforcement learning (RL) is policy\nevaluation: estimating the long-term return, i.e., value function,\ncorresponding to a given fixed policy. The celebrated Temporal Difference (TD)\nlearning algorithm addresses this problem, and recent work has investigated\nfinite-time convergence guarantees for this algorithm and variants thereof.\nHowever, these guarantees hinge on the reward observations being always\ngenerated from a well-behaved (e.g., sub-Gaussian) true reward distribution.\nMotivated by harsh, real-world environments where such an idealistic assumption\nmay no longer hold, we revisit the policy evaluation problem from the\nperspective of adversarial robustness. In particular, we consider a\nHuber-contaminated reward model where an adversary can arbitrarily corrupt each\nreward sample with a small probability $\\epsilon$. Under this observation\nmodel, we first show that the adversary can cause the vanilla TD algorithm to\nconverge to any arbitrary value function. We then develop a novel algorithm\ncalled Robust-TD and prove that its finite-time guarantees match that of\nvanilla TD with linear function approximation up to a small $O(\\epsilon)$ term\nthat captures the effect of corruption. We complement this result with a\nminimax lower bound, revealing that such an additive corruption-induced term is\nunavoidable. To our knowledge, these results are the first of their kind in the\ncontext of adversarial robustness of stochastic approximation schemes driven by\nMarkov noise. The key new technical tool that enables our results is an\nanalysis of the Median-of-Means estimator with corrupted, time-correlated data\nthat might be of independent interest to the literature on robust statistics.",
        "We obtain partial identification of direct and spillover effects in settings\nwith strategic interaction and discrete treatments, outcome and independent\ninstruments. We consider a framework with two decision-makers who play\npure-strategy Nash equilibria in treatment take-up, whose outcomes are\ndetermined by their joint take-up decisions. We obtain a latent-type\nrepresentation at the pair level. We enumerate all types that are consistent\nwith pure-strategy Nash equilibria and exclusion restrictions, and then impose\nconditions such as symmetry, strategic complementarity\/substitution, several\nnotions of monotonicity, and homogeneity. Under any combination of the above\nrestrictions, we provide sharp bounds for our parameters of interest via a\nsimple Python optimization routine. Our framework allows the empirical\nresearcher to tailor the above menu of assumptions to their empirical\napplication and to assess their individual and joint identifying power.",
        "In 1969, Fulton introduced classical Hurwitz spaces parametrizing simple\nd-sheeted coverings of the projective line in the algebro-geometric setting. He\nestablished the irreducibility of these spaces under the assumption that the\ncharacteristic of the ground field is greater than d, but the irreducibility\nproblem in smaller characteristics remained open. We resolve this problem in\nthe current paper and prove that the classical Hurwitz spaces are irreducible\nover any algebraically closed field. On the way, we establish the\nirreducibility of Severi varieties in arbitrary characteristic for a rich class\nof toric surfaces, including all classical toric surfaces. Our approach to the\nirreducibility problems comes from tropical geometry, and the paper contains\ntwo more results of independent interest - a lifting result for parametrized\ntropical curves and a strong connectedness property of the moduli spaces of\nparametrized tropical curves.",
        "We theoretically investigate the electric field-tuning plasmons and\nplasmon-phonon couplings of two-dimensional (2D) transition metal\ndichalcogenides (TMDs), such as monolayer MoS2, under the consideration of\nspin-orbit coupling. It is revealed that the frequencies of plasmons and\ncoupled plasmon-phonon modes originating from electron-electron and\nelectron-phonon interactions can be effectively changed by using applied\ndriving electric fields. Notably, these frequencies exhibit a decreasing trend\nwith an increasing electric field. Moreover, the weak angular dependence of\nthese modes suggests that the driving electric field does not induce\nsignificant anisotropy in the plasmon modes. The outcomes of this work\ndemonstrate that the plasmon and coupled plasmon-phonon modes can be tuned not\nonly by manipulating the electron density via the application of a gate voltage\nbut also by tuning the applied driving electric field. These findings hold\nrelevance for facilitating the application of 2D TMDs in optoelectronic\ndevices.",
        "In this article we study the existence of solutions to a fourth-order\nnonlinear PDE related to crystal surface growth. The key difficulty in the\nequations comes from the mobility matrix, which depends on the gradient of the\nsolution. When the mobility matrix is the identity matrix there are now many\nexistence results, however when it is allowed to depend on the solution we lose\ncrucial estimates in the time direction. In this work we are able to prove the\nglobal existence of weak solutions despite this lack of estimates in the time\ndirection.",
        "Jetting dynamics from bursting bubbles play a key role in mediating mass and\nmomentum transport across the air-liquid interface. In marine environments,\nthis phenomenon has drawn considerable attention due to its role in releasing\nbiochemical contaminants, such as extracellular polymeric substances, into the\natmosphere through aerosol production. These biocontaminants often exhibit\nnon-Newtonian characteristics, yet the physics of bubble bursting with a\nrheologically complex layer at bubble-liquid interfaces remains largely\nunexplored. In this study, we experimentally investigate the jetting dynamics\nof bubble bursting events in the presence of such polymeric compound layers.\nUsing bubbles coated by a polyethylene oxide solution, we document the cavity\ncollapse and jetting dynamics produced by bubble bursting. At a fixed polymer\nconcentration, the jet velocity increases while the jet radius decreases with\nan increasing compound layer fraction, as a result of stronger capillary wave\ndamping due to capillary wave separation at the compound interface as well as\nthe formation of smaller cavity cone angles during bubble cavity collapse.\nThese dynamics produce smaller and more numerous jet drops. Meanwhile, as the\npolymer concentration increases, the jet velocity decreases while the jet\nradius increases for the same compound layer fraction due to the increasing\nviscoelastic stresses. In addition, fewer jet drops are ejected as the jets\nbecome slower and broader with increasing polymer concentration, as\nviscoelastic stresses persist throughout the jet formation and thinning\nprocess. We further obtain a regime map delineating the conditions for jet drop\nejection versus no jet drop ejection in bursting bubbles coated with a\npolymeric compound layer. Our results may provide new insights into the\nmechanisms of mass transport of organic materials in bubble-mediated\naerosolization processes.",
        "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "Advanced interface engineering provides a way to control the ground state of\ncorrelated oxide heterostructures, which enables the shaping of future\nelectronic and magnetic nanodevices with enhanced performance. An especially\npromising and rather new avenue is to find and explore low-dimensional phases\nof structural, ferroic and superconducting origin. In this multimodal study, we\npresent a novel dynamic growth control method that enables synthesizing\ncompositionally graded superlattices (SLs) of (LaMnO_3)_10\/(SrMnO_3)_10\n(LMO\/SMO), in which the layers gradually change their composition between LMO\nand SMO with gradient G values ranging from 0 to 100 %. This leads to strong\nmodulations in the material's electronic properties and of the two-phase\nferromagnetic (FM) behavior. In particular, we observe that G has almost no\nimpact on the emergent high-temperature FM phase; in contrast, the\nlow-temperature volume-like FM phase increases drastically with higher\nG-factors and thus can serve as a precise marker for chemical composition on a\nnanoscale. Focusing on the interfacial charge transfer found at sharp SMO\/LMO\ninterfaces (G=0), we observe that for higher G-factors a long-range charge\nmodulation develops, which is accompanied by an insulator-to-metal transition.\nThese findings showcase G as a crucial control parameter that can shape the\nsuperlattice's intrinsic properties and provide a perspective for designing\nfunctional oxide heterostructures with artificially disordered interfaces.",
        "Quantum gas microscopy with atoms in optical lattices provides remarkable\ninsights into the real space properties of many-body systems, but does not\ndirectly reveal the nature of their fundamental excitation spectrum. Here, we\ndemonstrate that radio-frequency spectroscopy can reveal the quasi-particle\nnature of doped quantum many-body systems, crucial for our understanding of,\ne.g., high-temperature superconductors. In particular, we showcase how the\nexistence and energy of magnetic polaron quasi-particles in doped Fermi-Hubbard\nsystems may be probed, revealed by hallmark peaks in the spectroscopic\nspectrum. In combination with fundamental dualities of the Fermi-Hubbard model,\nwe describe how these findings may be tested using several experimental\nplatforms.",
        "We consider the problem of weight uncertainty proposed by [Blundell et al.\n(2015). Weight uncertainty in neural network. In International conference on\nmachine learning, 1613-1622, PMLR.] in neural networks {(NNs)} specialized for\nregression tasks. {We further} investigate the effect of variance uncertainty\nin {their model}. We show that including the variance uncertainty can improve\nthe prediction performance of the Bayesian {NN}. Variance uncertainty enhances\nthe generalization of the model {by} considering the posterior distribution\nover the variance parameter. { We examine the generalization ability of the\nproposed model using a function approximation} example and {further illustrate\nit with} the riboflavin genetic data set. {We explore fully connected dense\nnetworks and dropout NNs with} Gaussian and spike-and-slab priors,\nrespectively, for the network weights.",
        "We construct a series of vortex patch solutions in a doubly-periodic\nrectangular domain (flat torus), which is accomplished by studying the contour\ndynamic equation for patch boundaries. We will illustrate our key idea by\ndiscussing the single-layered patches as the most fundamental configuration,\nand then investigate the general construction for $N$ patches near a point\nvortex equilibrium. Different with the case of bounded domains in $\\mathbb\nR^2$, a constant background vorticity will arise from the compact nature of\nflat torus, and the $2$-dimensional translational invariance will bring\ntroubles on determining patch locations. To overcome these two difficulties, we\nwill add additional terms for background vorticity and introduce a centralized\ncondition for location vector. By utilizing the regularity difference of terms\nin contour dynamic equations, we also obtain the $C^\\infty$ regularity and\nconvexity of boundary curves."
      ]
    }
  },
  {
    "id":"2411.00561",
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"What is a cell type, really? The quest to categorize life's myriad forms.",
    "start_abstract":"The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. \u201cI realized that there were probably hundreds of flavours,\u201d recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study"
      ],
      "abstract":[
        "In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "On Scaling Neurosymbolic Programming through Guided Logical Inference",
        "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents",
        "AI Generations: From AI 1.0 to AI 4.0",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Infrastructure for AI Agents",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "A Study on Neuro-Symbolic Artificial Intelligence: Healthcare\n  Perspectives",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "Discovering Directly-Follows Graph Model for Acyclic Processes",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
        "Applications of Large Models in Medicine",
        "HI-MaNGA: Results from (21cm-HI) single-dish observations of MaNGA\n  Survey Galaxies",
        "\\'El\\'ements de comptage sur les g\\'en\\'erateurs du groupe modulaire et\n  les $\\lambda$-quiddit\\'es",
        "Dark Energy Survey Year 6 Results: Synthetic-source Injection Across the\n  Full Survey Using Balrog",
        "On the Commuting Problem of Toeplitz Operators on the Harmonic Bergman\n  Space",
        "Generalization Performance of Hypergraph Neural Networks",
        "COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation",
        "Properties of the one-component Coulomb gas on a sphere with two\n  macroscopic external charges",
        "Norm-one points in convex combinations of relatively weakly open subsets\n  of the unit ball in the spaces $L_1(\\mu,X)$",
        "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "Deriving motivic coactions and single-valued maps at genus zero from\n  zeta generators",
        "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to\n  Counteract Alpha Decay",
        "Relative knot probabilities in confined lattice polygons",
        "Global branching of solutions to ODEs and integrability",
        "Kink dynamics for the Yang-Mills field in an extremal\n  Reissner-Nordstr\\\"om black hole",
        "Multimodal Emotion Recognition and Sentiment Analysis in Multi-Party\n  Conversation Contexts"
      ],
      "abstract":[
        "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
        "Probabilistic neurosymbolic learning seeks to integrate neural networks with\nsymbolic programming. Many state-of-the-art systems rely on a reduction to the\nProbabilistic Weighted Model Counting Problem (PWMC), which requires computing\na Boolean formula called the logical provenance.However, PWMC is \\\\#P-hard, and\nthe number of clauses in the logical provenance formula can grow exponentially,\ncreating a major bottleneck that significantly limits the applicability of PNL\nsolutions in practice.We propose a new approach centered around an exact\nalgorithm DPNL, that enables bypassing the computation of the logical\nprovenance.The DPNL approach relies on the principles of an oracle and a\nrecursive DPLL-like decomposition in order to guide and speed up logical\ninference.Furthermore, we show that this approach can be adapted for\napproximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees,\ncalled ApproxDPNL.Experiments show significant performance gains.DPNL enables\nscaling exact inference further, resulting in more accurate models.Further,\nApproxDPNL shows potential for advancing the scalability of neurosymbolic\nprogramming by incorporating approximations even further, while simultaneously\nensuring guarantees for the reasoning process.",
        "While Large Language Models (LLMs) can exhibit impressive proficiency in\nisolated, short-term tasks, they often fail to maintain coherent performance\nover longer time horizons. In this paper, we present Vending-Bench, a simulated\nenvironment designed to specifically test an LLM-based agent's ability to\nmanage a straightforward, long-running business scenario: operating a vending\nmachine. Agents must balance inventories, place orders, set prices, and handle\ndaily fees - tasks that are each simple but collectively, over long horizons\n(>20M tokens per run) stress an LLM's capacity for sustained, coherent\ndecision-making. Our experiments reveal high variance in performance across\nmultiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most\nruns and turn a profit, but all models have runs that derail, either through\nmisinterpreting delivery schedules, forgetting orders, or descending into\ntangential \"meltdown\" loops from which they rarely recover. We find no clear\ncorrelation between failures and the point at which the model's context window\nbecomes full, suggesting that these breakdowns do not stem from memory limits.\nApart from highlighting the high variance in performance over long time\nhorizons, Vending-Bench also tests models' ability to acquire capital, a\nnecessity in many hypothetical dangerous AI scenarios. We hope the benchmark\ncan help in preparing for the advent of stronger AI systems.",
        "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "Increasingly many AI systems can plan and execute interactions in open-ended\nenvironments, such as making phone calls or buying online goods. As developers\ngrow the space of tasks that such AI agents can accomplish, we will need tools\nboth to unlock their benefits and manage their risks. Current tools are largely\ninsufficient because they are not designed to shape how agents interact with\nexisting institutions (e.g., legal and economic systems) or actors (e.g.,\ndigital service providers, humans, other AI agents). For example, alignment\ntechniques by nature do not assure counterparties that some human will be held\naccountable when a user instructs an agent to perform an illegal action. To\nfill this gap, we propose the concept of agent infrastructure: technical\nsystems and shared protocols external to agents that are designed to mediate\nand influence their interactions with and impacts on their environments. Agent\ninfrastructure comprises both new tools and reconfigurations or extensions of\nexisting tools. For example, to facilitate accountability, protocols that tie\nusers to agents could build upon existing systems for user authentication, such\nas OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue\nthat agent infrastructure will be similarly indispensable to ecosystems of\nagents. We identify three functions for agent infrastructure: 1) attributing\nactions, properties, and other information to specific agents, their users, or\nother actors; 2) shaping agents' interactions; and 3) detecting and remedying\nharmful actions from agents. We propose infrastructure that could help achieve\neach function, explaining use cases, adoption, limitations, and open questions.\nMaking progress on agent infrastructure can prepare society for the adoption of\nmore advanced agents.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "Over the last few decades, Artificial Intelligence (AI) scientists have been\nconducting investigations to attain human-level performance by a machine in\naccomplishing a cognitive task. Within machine learning, the ultimate\naspiration is to attain Artificial General Intelligence (AGI) through a\nmachine. This pursuit has led to the exploration of two distinct AI paradigms.\nSymbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and\nConnectionist (Sub-symbolic) AI, represented by Neural Systems, are two\nmutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,\nand knowledge representation but faces challenges in processing complex\nreal-world data with noise. Conversely, deep learning (Black-Box systems)\nresearch breakthroughs in neural networks are notable, yet they lack reasoning\nand interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI\nresearch, attempts to bridge this gap by integrating logical reasoning into\nneural networks, enabling them to learn and reason with symbolic\nrepresentations. While a long path, this strategy has made significant progress\ntowards achieving common sense reasoning by systems. This article conducts an\nextensive review of over 977 studies from prominent scientific databases (DBLP,\nACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the\nmultifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its\nhealthcare applications, particularly in drug discovery, and Protein\nengineering research. The survey addresses vital themes, including reasoning,\nexplainability, integration strategies, 41 healthcare-related use cases,\nbenchmarking, datasets, current approach limitations from both healthcare and\nbroader perspectives, and proposed novel approaches for future experiments.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "Process mining is the common name for a range of methods and approaches aimed\nat analysing and improving processes. Specifically, methods that aim to derive\nprocess models from event logs fall under the category of process discovery.\nWithin the range of processes, acyclic processes form a distinct category. In\nsuch processes, previously performed actions are not repeated, forming chains\nof unique actions. However, due to differences in the order of actions,\nexisting process discovery methods can provide models containing cycles even if\na process is acyclic. This paper presents a new process discovery algorithm\nthat allows to discover acyclic DFG models for acyclic processes. A model is\ndiscovered by partitioning an event log into parts that provide acyclic DFG\nmodels and merging them while avoiding the formation of cycles. The resulting\nalgorithm was tested both on real-life and artificial event logs. Absence of\ncycles improves model visual clarity and precision, also allowing to apply\ncycle-sensitive methods or visualisations to the model.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
        "This paper explores the advancements and applications of large-scale models\nin the medical field, with a particular focus on Medical Large Models (MedLMs).\nThese models, encompassing Large Language Models (LLMs), Vision Models, 3D\nLarge Models, and Multimodal Models, are revolutionizing healthcare by\nenhancing disease prediction, diagnostic assistance, personalized treatment\nplanning, and drug discovery. The integration of graph neural networks in\nmedical knowledge graphs and drug discovery highlights the potential of Large\nGraph Models (LGMs) in understanding complex biomedical relationships. The\nstudy also emphasizes the transformative role of Vision-Language Models (VLMs)\nand 3D Large Models in medical image analysis, anatomical modeling, and\nprosthetic design. Despite the challenges, these technologies are setting new\nbenchmarks in medical innovation, improving diagnostic accuracy, and paving the\nway for personalized healthcare solutions. This paper aims to provide a\ncomprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health.",
        "In a poster presentation for IAU Symposium 392: \"Neutral hydrogen in and\naround galaxies in the SKA era\", we gave an overview of the HI-MaNGA project\nwhich is working to obtain complementary information about the cold gas\n(neutral hydrogen traced by the radio 21cm line) content of Mapping Nearby\nGalaxies at Apache Point Observatory (MaNGA) sample galaxies. MaNGA, part of\nthe fourth incarnation of the Sloan Digital Sky Surveys (SDSS-IV), obtained\nspatially resolved spectral maps for 10,000 nearby galaxies selected to create\na representative sample out of the SDSS Main Galaxy Sample. MaNGA data have\nprovided a census of the stellar and ionized gas content of these galaxies, as\nwell as kinematics of both stars and gas. Adding HI information via the\nHI-MaNGA program, which has observed or collected 21cm line data for 70% of the\nfull MaNGA sample, has been crucial for a number of applications, but\nespecially understanding the physical mechanisms that regulate gas accretion,\nand through that star formation and quenching of star formation. This\nconference proceedings article accompanies the release of the DR3 version of\nHI-MaNGA data.",
        "The aim of this article is to count the $n$-tuples of positive integers\n$(a_{1},\\ldots,a_{n})$ solutions of the equation $\\begin{pmatrix} a_{n} & -1\n\\\\[4pt] 1 & 0 \\end{pmatrix} \\begin{pmatrix} a_{n-1} & -1 \\\\[4pt] 1 & 0\n\\end{pmatrix} \\cdots \\begin{pmatrix} a_{1} & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}=\\pm\nM$ when $M$ is equal to the generators of the modular group $S=\\begin{pmatrix}\n0 & -1 \\\\[4pt] 1 & 0 \\end{pmatrix}$ and $T=\\begin{pmatrix} 1 & 1 \\\\[4pt] 0 & 1\n\\end{pmatrix}$. To count these elements, we will study the\n$\\lambda$-quiddities, which are the solutions of the equation in the case\n$M=Id$ (related to Coxeter's friezes), whose last component is fixed.",
        "Synthetic source injection (SSI), the insertion of sources into pixel-level\non-sky images, is a powerful method for characterizing object detection and\nmeasurement in wide-field, astronomical imaging surveys. Within the Dark Energy\nSurvey (DES), SSI plays a critical role in characterizing all necessary\nalgorithms used in converting images to catalogs, and in deriving quantities\nneeded for the cosmology analysis, such as object detection rates, galaxy\nredshift estimation, galaxy magnification, star-galaxy classification, and\nphotometric performance. We present here a source injection catalog of $146$\nmillion injections spanning the entire 5000 deg$^2$ DES footprint, generated\nusing the Balrog SSI pipeline. Through this sample, we demonstrate that the DES\nYear 6 (Y6) image processing pipeline provides accurate estimates of the object\nproperties, for both galaxies and stars, at the percent-level, and we highlight\nspecific regimes where the accuracy is reduced. We then show the consistency\nbetween SSI and data catalogs, for all galaxy samples developed within the weak\nlensing and galaxy clustering analyses of DES Y6. The consistency between the\ntwo catalogs also extends to their correlations with survey observing\nproperties (seeing, airmass, depth, extinction, etc.). Finally, we highlight a\nnumber of applications of this catalog to the DES Y6 cosmology analysis. This\ndataset is the largest SSI catalog produced at this fidelity and will serve as\na key testing ground for exploring the utility of SSI catalogs in upcoming\nsurveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time.",
        "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
        "Hypergraph neural networks have been promising tools for handling learning\ntasks involving higher-order data, with notable applications in web graphs,\nsuch as modeling multi-way hyperlink structures and complex user interactions.\nYet, their generalization abilities in theory are less clear to us. In this\npaper, we seek to develop margin-based generalization bounds for four\nrepresentative classes of hypergraph neural networks, including\nconvolutional-based methods (UniGCN), set-based aggregation (AllDeepSets),\ninvariant and equivariant transformations (M-IGN), and tensor-based approaches\n(T-MPHN). Through the PAC-Bayes framework, our results reveal the manner in\nwhich hypergraph structure and spectral norms of the learned weights can affect\nthe generalization bounds, where the key technical challenge lies in developing\nnew perturbation analysis for hypergraph neural networks, which offers a\nrigorous understanding of how variations in the model's weights and hypergraph\nstructure impact its generalization behavior. Our empirical study examines the\nrelationship between the practical performance and theoretical bounds of the\nmodels over synthetic and real-world datasets. One of our primary observations\nis the strong correlation between the theoretical bounds and empirical loss,\nwith statistically significant consistency in most cases.",
        "Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.",
        "The one-component Coulomb gas on the sphere, consisting on $N$ unit charges\ninteracting via a logarithmic potential, and in the presence of two external\ncharges each of strength proportional to $N$, is considered. There are two\nspherical caps naturally associated with the external charges, giving rise to\ntwo distinct phases depending on them not overlapping (post-critical) or\noverlapping (pre-critical). The equilibrium measure in the post-critical phase\nis known from earlier work. We determine the equilibrium measure in the\npre-critical phase using a particular conformal map, with the parameters\ntherein specified in terms of a root of a certain fourth order polynomial. This\nis used to determine the exact form of the electrostatic energy for the\npre-critical phase. Using a duality relation from random matrix theory, the\npartition function for the Coulomb gas at the inverse temperature $\\beta = 2$\ncan be expanded for large $N$ in the post-critical phase, and in a scaling\nregion of the post and pre-critical boundary. For the pre-critical phase, the\nduality identity implies a relation between two electrostatic energies, one for\nthe present sphere system, and the other for a certain constrained log-gas\nrelating to the Jacobi unitary ensemble.",
        "In a paper published in 2020 in Studia Mathematica, Abrahamsen et al. proved\nthat in the real space $L_1(\\mu)$, where $\\mu$ is a non-zero $\\sigma$-finite\n(countably additive non-negative) measure, norm-one elements in finite convex\ncombinations of relatively weakly open subsets of the unit ball are interior\npoints of these convex combinations in the relative weak topology. In this\npaper that result is generalised by proving that the same is true in the (real\nor complex) Lebesgue--Bochner spaces $L_1(\\mu,X)$ where $X$ is a weakly\nuniformly rotund Banach space.",
        "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
        "Multiple polylogarithms are equipped with rich algebraic structures including\nthe motivic coaction and the single-valued map which both found fruitful\napplications in high-energy physics. In recent work arXiv:2312.00697, the\ncurrent authors presented a conjectural reformulation of the motivic coaction\nand the single-valued map via zeta generators, certain operations on\nnon-commuting variables in suitable generating series of multiple\npolylogarithms. In this work, the conjectures of the reference will be proven\nfor multiple polylogarithms that depend on any number of variables on the\nRiemann sphere.",
        "Alpha mining, a critical component in quantitative investment, focuses on\ndiscovering predictive signals for future asset returns in increasingly complex\nfinancial markets. However, the pervasive issue of alpha decay, where factors\nlose their predictive power over time, poses a significant challenge for alpha\nmining. Traditional methods like genetic programming face rapid alpha decay\nfrom overfitting and complexity, while approaches driven by Large Language\nModels (LLMs), despite their promise, often rely too heavily on existing\nknowledge, creating homogeneous factors that worsen crowding and accelerate\ndecay. To address this challenge, we propose AlphaAgent, an autonomous\nframework that effectively integrates LLM agents with ad hoc regularizations\nfor mining decay-resistant alpha factors. AlphaAgent employs three key\nmechanisms: (i) originality enforcement through a similarity measure based on\nabstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor\nalignment via LLM-evaluated semantic consistency between market hypotheses and\ngenerated factors, and (iii) complexity control via AST-based structural\nconstraints, preventing over-engineered constructions that are prone to\noverfitting. These mechanisms collectively guide the alpha generation process\nto balance originality, financial rationale, and adaptability to evolving\nmarket conditions, mitigating the risk of alpha decay. Extensive evaluations\nshow that AlphaAgent outperforms traditional and LLM-based methods in\nmitigating alpha decay across bull and bear markets, consistently delivering\nsignificant alpha in Chinese CSI 500 and US S&P 500 markets over the past four\nyears. Notably, AlphaAgent showcases remarkable resistance to alpha decay,\nelevating the potential for yielding powerful factors.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "We consider a natural generalisation of the Painlev\\'e property and use it to\nidentify the known integrable cases of the Lane-Emden equation with a real\npositive index. We classify certain first-order ordinary differential equations\nwith this property and find necessary conditions for a large family of\nsecond-order equations. We consider ODEs such that, given any simply connected\ndomain $\\Omega$ not containing fixed singularities of the equation, the Riemann\nsurface of any solution obtained by analytic continuation along curves in\n$\\Omega$ has a finite number of sheets over $\\Omega$.",
        "Considered in this work is the Yang-Mills field in an extremal\nReissner-Nordstr\\\"om black hole, a physically motivated mathematical model\nintroduced by Bizo\\'n and Kahl. The kink is a fundamental, strongly unstable\nstationary solution in this non-perturbative, variable coefficients model, with\na polynomial tail and no explicit form. In this paper, we introduce and extend\nseveral virial techniques, adapt them to the inhomogeneous medium setting, and\nconstruct a finite codimensional manifold of the energy space where the kink is\nasymptotically stable. In particular, we handle, using virial techniques, the\nemergence of a weak threshold resonance in the description of the stable\nmanifold.",
        "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities\/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis."
      ]
    }
  },
  {
    "id":"2411.00922",
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images",
    "start_abstract":"Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Enhancing SAR Object Detection with Self-Supervised Pre-training on\n  Masked Auto-Encoders",
        "ERANet: Edge Replacement Augmentation for Semi-Supervised Meniscus\n  Segmentation with Prototype Consistency Alignment and Conditional\n  Self-Training",
        "Sonata: Self-Supervised Learning of Reliable Point Representations",
        "Multimodal Large Models Are Effective Action Anticipators",
        "Improved Partial Differential Equation and Fast Approximation Algorithm\n  for Hazy\/Underwater\/Dust Storm Image Enhancement",
        "Boosting Salient Object Detection with Knowledge Distillated from Large\n  Foundation Models",
        "Learning semantical dynamics and spatiotemporal collaboration for human\n  pose estimation in video",
        "Boosting the Generalization and Reasoning of Vision Language Models with\n  Curriculum Reinforcement Learning",
        "A-IDE : Agent-Integrated Denoising Experts",
        "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion\n  Transformer",
        "SVIA: A Street View Image Anonymization Framework for Self-Driving\n  Applications",
        "Diversity Covariance-Aware Prompt Learning for Vision-Language Models",
        "Unsupervised Class Generation to Expand Semantic Segmentation Datasets",
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei"
      ],
      "abstract":[
        "Supervised fine-tuning methods (SFT) perform great efficiency on artificial\nintelligence interpretation in SAR images, leveraging the powerful\nrepresentation knowledge from pre-training models. Due to the lack of\ndomain-specific pre-trained backbones in SAR images, the traditional strategies\nare loading the foundation pre-train models of natural scenes such as ImageNet,\nwhose characteristics of images are extremely different from SAR images. This\nmay hinder the model performance on downstream tasks when adopting SFT on\nsmall-scale annotated SAR data. In this paper, an self-supervised learning\n(SSL) method of masked image modeling based on Masked Auto-Encoders (MAE) is\nproposed to learn feature representations of SAR images during the pre-training\nprocess and benefit the object detection task in SAR images of SFT. The\nevaluation experiments on the large-scale SAR object detection benchmark named\nSARDet-100k verify that the proposed method captures proper latent\nrepresentations of SAR images and improves the model generalization in\ndownstream tasks by converting the pre-trained domain from natural scenes to\nSAR images through SSL. The proposed method achieves an improvement of 1.3 mAP\non the SARDet-100k benchmark compared to only the SFT strategies.",
        "Manual segmentation is labor-intensive, and automatic segmentation remains\nchallenging due to the inherent variability in meniscal morphology, partial\nvolume effects, and low contrast between the meniscus and surrounding tissues.\nTo address these challenges, we propose ERANet, an innovative semi-supervised\nframework for meniscus segmentation that effectively leverages both labeled and\nunlabeled images through advanced augmentation and learning strategies. ERANet\nintegrates three key components: edge replacement augmentation (ERA), prototype\nconsistency alignment (PCA), and a conditional self-training (CST) strategy\nwithin a mean teacher architecture. ERA introduces anatomically relevant\nperturbations by simulating meniscal variations, ensuring that augmentations\nalign with the structural context. PCA enhances segmentation performance by\naligning intra-class features and promoting compact, discriminative feature\nrepresentations, particularly in scenarios with limited labeled data. CST\nimproves segmentation robustness by iteratively refining pseudo-labels and\nmitigating the impact of label noise during training. Together, these\ninnovations establish ERANet as a robust and scalable solution for meniscus\nsegmentation, effectively addressing key barriers to practical implementation.\nWe validated ERANet comprehensively on 3D Double Echo Steady State (DESS) and\n3D Fast\/Turbo Spin Echo (FSE\/TSE) MRI sequences. The results demonstrate the\nsuperior performance of ERANet compared to state-of-the-art methods. The\nproposed framework achieves reliable and accurate segmentation of meniscus\nstructures, even when trained on minimal labeled data. Extensive ablation\nstudies further highlight the synergistic contributions of ERA, PCA, and CST,\nsolidifying ERANet as a transformative solution for semi-supervised meniscus\nsegmentation in medical imaging.",
        "In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.",
        "The task of long-term action anticipation demands solutions that can\neffectively model temporal dynamics over extended periods while deeply\nunderstanding the inherent semantics of actions. Traditional approaches, which\nprimarily rely on recurrent units or Transformer layers to capture long-term\ndependencies, often fall short in addressing these challenges. Large Language\nModels (LLMs), with their robust sequential modeling capabilities and extensive\ncommonsense knowledge, present new opportunities for long-term action\nanticipation. In this work, we introduce the ActionLLM framework, a novel\napproach that treats video sequences as successive tokens, leveraging LLMs to\nanticipate future actions. Our baseline model simplifies the LLM architecture\nby setting future tokens, incorporating an action tuning module, and reducing\nthe textual decoder layer to a linear layer, enabling straightforward action\nprediction without the need for complex instructions or redundant descriptions.\nTo further harness the commonsense reasoning of LLMs, we predict action\ncategories for observed frames and use sequential textual clues to guide\nsemantic understanding. In addition, we introduce a Cross-Modality Interaction\nBlock, designed to explore the specificity within each modality and capture\ninteractions between vision and textual modalities, thereby enhancing\nmultimodal tuning. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of the proposed ActionLLM framework, encouraging a promising\ndirection to explore LLMs in the context of action anticipation. Code is\navailable at https:\/\/github.com\/2tianyao1\/ActionLLM.git.",
        "This paper presents an improved and modified partial differential equation\n(PDE)-based de-hazing algorithm. The proposed method combines logarithmic image\nprocessing models in a PDE formulation refined with linear filter-based\noperators in either spatial or frequency domain. Additionally, a fast,\nsimplified de-hazing function approximation of the hazy image formation model\nis developed in combination with fuzzy homomorphic refinement. The proposed\nalgorithm solves the problem of image darkening and over-enhancement of edges\nin addition to enhancement of dark image regions encountered in previous\nformulations. This is in addition to avoiding enhancement of sky regions in\nde-hazed images while avoiding halo effect. Furthermore, the proposed algorithm\nis utilized for underwater and dust storm image enhancement with the\nincorporation of a modified global contrast enhancement algorithm. Experimental\ncomparisons indicate that the proposed approach surpasses a majority of the\nalgorithms from the literature based on quantitative image quality metrics.",
        "Salient Object Detection (SOD) aims to identify and segment prominent regions\nwithin a scene. Traditional models rely on manually annotated pseudo labels\nwith precise pixel-level accuracy, which is time-consuming. We developed a\nlow-cost, high-precision annotation method by leveraging large foundation\nmodels to address the challenges. Specifically, we use a weakly supervised\napproach to guide large models in generating pseudo-labels through textual\nprompts. Since large models do not effectively focus on the salient regions of\nimages, we manually annotate a subset of text to fine-tune the model. Based on\nthis approach, which enables precise and rapid generation of pseudo-labels, we\nintroduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset,\nBDS-TR is more prominent in scale and encompasses a wider variety of categories\nand scenes. This expansion will enhance our model's applicability across a\nbroader range of scenarios and provide a more comprehensive foundational\ndataset for future SOD research. Additionally, we present an edge decoder based\non dynamic upsampling, which focuses on object edges while gradually recovering\nimage feature resolution. Comprehensive experiments on five benchmark datasets\ndemonstrate that our method significantly outperforms state-of-the-art\napproaches and also surpasses several existing fully-supervised SOD methods.\nThe code and results will be made available.",
        "Temporal modeling and spatio-temporal collaboration are pivotal techniques\nfor video-based human pose estimation. Most state-of-the-art methods adopt\noptical flow or temporal difference, learning local visual content\ncorrespondence across frames at the pixel level, to capture motion dynamics.\nHowever, such a paradigm essentially relies on localized pixel-to-pixel\nsimilarity, which neglects the semantical correlations among frames and is\nvulnerable to image quality degradations (e.g. occlusions or blur). Moreover,\nexisting approaches often combine motion and spatial (appearance) features via\nsimple concatenation or summation, leading to practical challenges in fully\nleveraging these distinct modalities. In this paper, we present a novel\nframework that learns multi-level semantical dynamics and dense spatio-temporal\ncollaboration for multi-frame human pose estimation. Specifically, we first\ndesign a Multi-Level Semantic Motion Encoder using a multi-masked context and\npose reconstruction strategy. This strategy stimulates the model to explore\nmulti-granularity spatiotemporal semantic relationships among frames by\nprogressively masking the features of (patch) cubes and frames. We further\nintroduce a Spatial-Motion Mutual Learning module which densely propagates and\nconsolidates context information from spatial and motion features to enhance\nthe capability of the model. Extensive experiments demonstrate that our\napproach sets new state-of-the-art results on three benchmark datasets,\nPoseTrack2017, PoseTrack2018, and PoseTrack21.",
        "While state-of-the-art vision-language models (VLMs) have demonstrated\nremarkable capabilities in complex visual-text tasks, their success heavily\nrelies on massive model scaling, limiting their practical deployment.\nSmall-scale VLMs offer a more practical alternative but face significant\nchallenges when trained with traditional supervised fine-tuning (SFT),\nparticularly in two aspects: out-of-domain (OOD) generalization and reasoning\nabilities, which significantly lags behind the contemporary Large language\nmodels (LLMs). To address these challenges, we propose Curriculum Reinforcement\nFinetuning (Curr-ReFT), a novel post-training paradigm specifically designed\nfor small-scale VLMs. Inspired by the success of reinforcement learning in\nLLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement\nLearning, which ensures steady progression of model capabilities through\ndifficulty-aware reward design, transitioning from basic visual perception to\ncomplex reasoning tasks; and (2) Rejected Sampling-based Self-improvement,\nwhich maintains the fundamental capabilities of VLMs through selective learning\nfrom high-quality multimodal and language examples. Extensive experiments\ndemonstrate that models trained with Curr-ReFT paradigm achieve\nstate-of-the-art performance across various visual tasks in both in-domain and\nout-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the\nperformance of 32B-parameter models, demonstrating that efficient training\nparadigms can effectively bridge the gap between small and large models.",
        "Recent advances in deep-learning based denoising methods have improved\nLow-Dose CT image quality. However, due to distinct HU distributions and\ndiverse anatomical characteristics, a single model often struggles to\ngeneralize across multiple anatomies. To address this limitation, we introduce\n\\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates\nthree anatomical region-specialized RED-CNN models under the management of\ndecision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to\ndynamically route incoming LDCT scans to the most appropriate expert model. We\nhighlight three major advantages of our approach. A-IDE excels in\nheterogeneous, data-scarce environments. The framework automatically prevents\noverfitting by distributing tasks among multiple experts. Finally, our\nLLM-driven agentic pipeline eliminates the need for manual interventions.\nExperimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves\nsuperior performance in RMSE, PSNR, and SSIM compared to a single unified\ndenoiser.",
        "Generating cognitive-aligned layered SVGs remains challenging due to existing\nmethods' tendencies toward either oversimplified single-layer outputs or\noptimization-induced shape redundancies. We propose LayerTracer, a diffusion\ntransformer based framework that bridges this gap by learning designers'\nlayered SVG creation processes from a novel dataset of sequential design\noperations. Our approach operates in two phases: First, a text-conditioned DiT\ngenerates multi-phase rasterized construction blueprints that simulate human\ndesign workflows. Second, layer-wise vectorization with path deduplication\nproduces clean, editable SVGs. For image vectorization, we introduce a\nconditional diffusion mechanism that encodes reference images into latent\ntokens, guiding hierarchical reconstruction while preserving structural\nintegrity. Extensive experiments demonstrate LayerTracer's superior performance\nagainst optimization-based and neural baselines in both generation quality and\neditability, effectively aligning AI-generated vectors with professional design\ncognition.",
        "In recent years, there has been an increasing interest in image\nanonymization, particularly focusing on the de-identification of faces and\nindividuals. However, for self-driving applications, merely de-identifying\nfaces and individuals might not provide sufficient privacy protection since\nstreet views like vehicles and buildings can still disclose locations,\ntrajectories, and other sensitive information. Therefore, it remains crucial to\nextend anonymization techniques to street view images to fully preserve the\nprivacy of users, pedestrians, and vehicles. In this paper, we propose a Street\nView Image Anonymization (SVIA) framework for self-driving applications. The\nSVIA framework consists of three integral components: a semantic segmenter to\nsegment an input image into functional regions, an inpainter to generate\nalternatives to privacy-sensitive regions, and a harmonizer to seamlessly\nstitch modified regions to guarantee visual coherence. Compared to existing\nmethods, SVIA achieves a much better trade-off between image generation quality\nand privacy protection, as evidenced by experimental results for five common\nmetrics on two widely used public datasets.",
        "Prompt tuning can further enhance the performance of visual-language models\nacross various downstream tasks (e.g., few-shot learning), enabling them to\nbetter adapt to specific applications and needs. In this paper, we present a\nDiversity Covariance-Aware framework that learns distributional information\nfrom the data to enhance the few-shot ability of the prompt model. First, we\npropose a covariance-aware method that models the covariance relationships\nbetween visual features and uses anisotropic Mahalanobis distance, instead of\nthe suboptimal cosine distance, to measure the similarity between two\nmodalities. We rigorously derive and prove the validity of this modeling\nprocess. Then, we propose the diversity-aware method, which learns multiple\ndiverse soft prompts to capture different attributes of categories and aligns\nthem independently with visual modalities. This method achieves multi-centered\ncovariance modeling, leading to more diverse decision boundaries. Extensive\nexperiments on 11 datasets in various tasks demonstrate the effectiveness of\nour method.",
        "Semantic segmentation is a computer vision task where classification is\nperformed at a pixel level. Due to this, the process of labeling images for\nsemantic segmentation is time-consuming and expensive. To mitigate this cost\nthere has been a surge in the use of synthetically generated data -- usually\ncreated using simulators or videogames -- which, in combination with domain\nadaptation methods, can effectively learn how to segment real data. Still,\nthese datasets have a particular limitation: due to their closed-set nature, it\nis not possible to include novel classes without modifying the tool used to\ngenerate them, which is often not public. Concurrently, generative models have\nmade remarkable progress, particularly with the introduction of diffusion\nmodels, enabling the creation of high-quality images from text prompts without\nadditional supervision.\n  In this work, we propose an unsupervised pipeline that leverages Stable\nDiffusion and Segment Anything Module to generate class examples with an\nassociated segmentation mask, and a method to integrate generated cutouts for\nnovel classes in semantic segmentation datasets, all with minimal user input.\nOur approach aims to improve the performance of unsupervised domain adaptation\nmethods by introducing novel samples into the training data without\nmodifications to the underlying algorithms. With our methods, we show how\nmodels can not only effectively learn how to segment novel classes, with an\naverage performance of 51% IoU, but also reduce errors for other, already\nexisting classes, reaching a higher performance level overall.",
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode."
      ]
    }
  },
  {
    "id":"2411.00922",
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "SPM 25: open source neuroimaging analysis software",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Spontaneous helix formation in polar smectic phase",
        "Which Sensor to Observe? Timely Tracking of a Joint Markov Source with\n  Model Predictive Control",
        "Properties of the weakly-bound (1,1)-states and rotationally excited\n  (2,0)-states in the muonic molecular $d d \\mu, d t \\mu$ and $t t \\mu$ ions",
        "The Roberge-Weiss transition for QCD in a magnetic background",
        "Constraints on optical and near-infrared variability in the localisation\n  of the long-period radio transient GLEAM-X J1627-52",
        "Pisarenko's Formula for the Thermopower",
        "Joint Antenna Selection and Beamforming Design for Active RIS-aided ISAC\n  Systems",
        "Boundary stratifications of Hurwitz spaces",
        "Algebraic and optical properties of generalized Kerr-Schild spacetimes\n  in arbitrary dimensions",
        "Cosmic distance duality after DESI 2024 data release and dark energy\n  evolution",
        "Transfer learning of many-body electronic correlation entropy from local\n  measurements",
        "Nonnegative Biquadratic Tensors",
        "Extreme vulnerability to intruder attacks destabilizes network dynamics",
        "A parabolic Hardy-H\\'enon equation with quasilinear degenerate diffusion",
        "Measuring the dynamical evolution of the United States lobbying network"
      ],
      "abstract":[
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "Statistical Parametric Mapping (SPM) is an integrated set of methods for\ntesting hypotheses about the brain's structure and function, using data from\nimaging devices. These methods are implemented in an open source software\npackage, SPM, which has been in continuous development for more than 30 years\nby an international community of developers. This paper reports the release of\nSPM 25.01, a major new version of the software that incorporates novel analysis\nmethods, optimisations of existing methods, as well as improved practices for\nopen science and software development.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In soft ferroelectric crystals, the depolarization field can be reduced by\nperiodic distortion of the polarization direction. In the polar nematic and\ntilted smectic phases, this process is energetically favorured , as it only\nrequires changes in the director orientation. We demonstrate the spontaneous\nformation of a helical structure in the proper ferroelectric tilted smectic\n(SmCTBF) phase, the phase is formed below the heliconical polar nematic (NTBF)\nphase. The helical pitch in the smectic phase is approximately 600 nm and\nremains nearly constant across the entire temperature range of the phase. Under\nweak electric fields, the helix reorients while its structure remains largely\nintact; however, in stronger fields, the helix is destroyed as the electric\npolarization aligns along the electric field.",
        "In this paper, we investigate the problem of remote estimation of a\ndiscrete-time joint Markov process using multiple sensors. Each sensor observes\na different component of the joint Markov process, and in each time slot, the\nmonitor obtains a partial state value by sending a pull request to one of the\nsensors. The monitor chooses the sequence of sensors to observe with the goal\nof minimizing the mean of age of incorrect information (MAoII) by using the\npartial state observations obtained, which have different freshness levels. For\ninstance, a monitor may be interested in tracking the location of an object by\nobtaining observations from two sensors, which observe the $x$ and $y$\ncoordinates of the object separately, in different time slots. The monitor,\nthen, needs to decide which coordinate to observe in the next time slot given\nthe history. In addition to this partial observability of the state of Markov\nprocess, there is an erasure channel with a fixed one-slot delay between each\nsensor and the monitor. First, we obtain a sufficient statistic, namely the\n\\emph{belief}, representing the joint distribution of the age of incorrect\ninformation (AoII) and the current state of the observed process by using the\nhistory of all pull requests and observations. Then, we formulate the problem\nwith a continuous state-space Markov decision problem (MDP), namely belief MDP.\nTo solve the problem, we propose two model predictive control (MPC) methods,\nnamely MPC without terminal costs (MPC-WTC) and reinforcement learning MPC\n(RL-MPC), that have different advantages in implementation.",
        "Total energies and other bound state properties of the weakly-bound\n(1,1)-states and rotationally excited (2,0)-states in the three-body muonic\nmolecular $d d \\mu, d t \\mu$ and $t t \\mu$ ions are determined to high\nnumerical accuracy and investigated. Our current numerical accuracy achieved\nfor the total and binding energies of the weakly-bound (1,1)-states in the both\n$d d \\mu$ and $d t \\mu$ ions significantly exceeds similar accuracy obtained in\nearlier computations of these weakly-bound states in these two ions. The bound\nstate properties of the weakly-bound (1,1)-states and (2,0)-states in the $d d\n\\mu, d t \\mu$ and $t t \\mu$ muonic ions have never been determined to high\naccuracy in earlier studies. We also briefly discuss the current status of\nmuon-catalyzed nuclear fusion and develop the new universal variational\nexpansion which can be used for extremely accurate bound state calculations of\narbitrary three-body systems, including the truly adiabatic\n${}^{\\infty}$H$^{+}_{2}$ ion and close systems.",
        "We investigate how a magnetic background field influences the location and\nthe nature of the Roberge-Weiss (RW) finite temperature transition for $N_f =\n2+1$ QCD with physical quark masses. To that purpose, we perform numerical\nsimulations of the finite temperature theory, discretized through stout\nstaggered quarks and the tree-level improved Symanzik pure gauge action,\nconsidering two different values of the Euclidean temporal extent in lattice\nunits, $N_t = 6, 8$. The RW transition temperature $T_{RW}$ decreases with\n$eB$, in particular it follows closely the behavior of the pseudo-critical QCD\ncrossover temperature $T_{pc}$, so that $T_{RW} (eB) - T_{pc}(eB)$ is\npractically constant, within errors, for magnetic fields up to $eB \\sim 1$\nGeV$^2$; consistent results are found from the drop of the chiral condensate,\nwhich signals chiral symmetry restoration, leading also to the phenomenon of\ninverse magnetic catalysis above the transition. Moreover, we find that the\nmagnetic field turns the RW transition from second order to first order, with a\ntri-critical magnetic field in-between 1 and 2.4 GeV$^2$, i.e. for magnetic\nfields substantially lower than those for which the standard QCD transition\nturns to first order.",
        "GLEAM-X J1627-52 was discovered as a periodic (~18 min) radio signal over a\nduration of three months in 2018. It is an enigmatic example of a growing\npopulation of 'long-period radio transients' consistent with Galactic origins.\nTheir nature is uncertain, and leading models invoke magnetic neutron stars or\nwhite dwarfs, potentially in close binary systems, to power them. GLEAM-X\nJ1627-52 resides in the Galactic plane with a comparatively coarse localisation\n(~2 arcsecond). Here we study the localisation region to search for\nspectrophotometric signatures of a counterpart using time-domain searches in\noptical and near-infrared imaging, and MUSE integral field spectroscopy. No\nsources in the localisation display clear white dwarf spectral signatures,\nalthough at the expected distance we can only provide modest limits on their\npresence directly. We rule out the presence of hot sub-dwarfs in the vicinity.\nWe found no candidate within our search for variability or periodic behaviour\nin the light curves. Radial velocity curves additionally show only weak\nevidence of variation, requiring any realistic underlying system to have very\nlow orbital inclination (i < 5 deg). Two Balmer emission line sources are\nreminiscent of white dwarf pulsar systems, but their characteristics fall\nwithin expected M-dwarf chromospheric activity with no signs of being in a\nclose binary. Currently the white dwarf pulsar scenario is not supported,\nalthough longer baseline data and data contemporaneous with a radio active\nepoch are required before stronger statements. Isolated magnetars, or compact\nbinaries remain viable. Our limits highlight the difficulty of these searches\nin dense environments at the limits of ground-based data.",
        "The thermopower $\\alpha$ (also known as the Seebeck coefficient) is one of\nthe most fundamental material characteristics for understanding charge carrier\ntransport in thermoelectric materials. Here, we revisit the Pisarenko formula\nfor the thermopower, which was traditionally considered valid only for\nnon-degenerate semiconductors. We demonstrate that regardless of the dominating\nscattering mechanism, the Pisarenko formula describes accurately enough the\nrelationship between thermopower $\\alpha$ and charge carrier concentration $n$\nbeyond the non-degenerate limit. Moreover, the Pisarenko formula provides a\nsimple thermopower-conductivity relation, $\\alpha = \\pm\n\\frac{k_{\\mathrm{B}}}{e} (b - \\ln \\sigma)$, valid for materials with $\\alpha >\n90$ $\\mu$V K$^{-1}$ when acoustic phonon scattering is predominant. This offers\nan alternative way to analyze electron transport when Hall measurements are\ndifficult or inaccessible. Additionally, we show how the Pisarenko formula can\nbe used to estimate the maximum power factor of a thermoelectric material from\nthe weighted mobility of a single, not necessarily optimized, sample at any\ngiven temperature.",
        "Active reconfigurable intelligent surface (A-RIS) aided integrated sensing\nand communications (ISAC) system has been considered as a promising paradigm to\nimprove spectrum efficiency. However, massive energy-hungry radio frequency\n(RF) chains hinder its large-scale deployment. To address this issue, an\nA-RIS-aided ISAC system with antenna selection (AS) is proposed in this work,\nwhere a target is sensed while multiple communication users are served with\nspecifically selected antennas. Specifically, a cuckoo search-based scheme is\nfirst utilized to select the antennas associated with high-gain channels.\nSubsequently, with the properly selected antennas, the weighted sum-rate (WSR)\nof the system is optimized under the condition of radar probing power level,\npower budget for the A-RIS and transmitter. To solve the highly non-convex\noptimization problem, we develop an efficient algorithm based on weighted\nminimum mean square error (WMMSE) and fractional programming (FP). Simulation\nresults show that the proposed AS scheme and the algorithm are effective, which\nreduce the number of RF chains without significant performance degradation.",
        "Let $\\mathcal{H}$ be a Hurwitz space that parametrises holomorphic maps to\n$\\mathbb{P}^1$. Abramovich, Corti and Vistoli, building on work of Harris and\nMumford, describe a compactification $\\overline{\\mathcal{H}}$ with a natural\nboundary stratification. We show that the irreducible strata of\n$\\overline{\\mathcal{H}}$ are in bijection with combinatorial objects called\ndecorated trees (up to a suitable equivalence), and that containment of\nirreducible strata is given by edge contraction of decorated trees. This\ncombinatorial description allows us to define a tropical Hurwitz space,\nrefining a definition given by Cavalieri, Markwig and Ranganathan. We also\ndiscuss applications to complex dynamics.",
        "We study the class of generalized Kerr-Schild (GKS) spacetimes in dimensions\n$n\\geq 3$ and analyze their geometric and algebraic properties in a completely\ntheory-independent setting. First, considering the case of a general null\nvector $\\mathbf{k}$ defined by the GKS metric, we obtain the conditions under\nwhich it is geodesic. Assuming $\\mathbf{k}$ to be geodesic for the remainder of\nthe paper, we examine the alignment properties of the curvature tensors, namely\nthe Ricci and Weyl tensors. We show that the algebraic types of the curvatures\nof the full (GKS) geometry are constrained by those of the respective\nbackground curvatures, thereby listing all kinematically allowed combinations\nof the algebraic types for the background and the full geometry. A notable\naspect of these results is that, unlike the case of Kerr-Schild (KS)\nspacetimes, the Weyl types of the GKS spacetimes need not be type $II$ or more\nspecial. Then, focusing on the case of an expanding $\\mathbf k$, we derive the\nconditions for it to satisfy the optical constraint, extending the previous\nresults of KS spacetimes. We illustrate the general results using the example\nof (A)dS-Taub-NUT spacetimes in $n=4$, where we also comment on their KS double\ncopy from a GKS perspective. Finally, as an application of our general results,\nwe obtain the full family of GKS spacetimes with a geodesic, expanding,\ntwistfree, and shearfree $\\mathbf k$, satisfying the vacuum Einstein equations,\nand identify it with a subset of the higher-dimensional vacuum\nRobinson-Trautman solutions. In passing, we also determine the subcase of these\nsolutions that manifests the KS double copy.",
        "The cosmic distance duality relates the angular-diameter and luminosity\ndistances and its possible violation may puzzle the standard cosmological\nmodel. This appears particularly interesting in view of the recent results\nfound by the DESI Collaboration, suggesting that a dynamical dark energy\nscenario seems to be favored than a genuine cosmological constant. Accordingly,\nwe take into account possible violations by considering four different\nparameterizations, namely: a Taylor expansion around $z\\simeq 0$, a\nslightly-departing logarithmic correction, a (1;2) Pad\\'e rational series to\nheal the convergence problem and a Chebyshev polynomial expansion, reducing\n\\emph{de facto} the systematic errors associated with the analysis. We test\neach of them in a model-independent (-dependent) way, by working out\nMonte-Carlo Markov chain analyses, employing the B\\'ezier interpolation of the\nHubble rate $H(z)$ for the model-independent approach while assuming the flat\n(non-flat) $\\Lambda$CDM and $\\omega_0\\omega_1$CDM models, motivating the latter\nparadigm in view of the DESI findings. Subsequently, we explore two analyses,\nemploying observational Hubble data, galaxy clusters from the Sunyaev-Zeldovich\neffect and type Ia supernovae, investigating the impact of the DESI data\ncatalog, first including then excluding the entire data set. Afterwards, we\nadopt statistical model selection criteria to assess the statistically favored\ncosmological model. Our results suggest \\emph{no violation} of the cosmic\ndistance duality. While a slight spatial curvature cannot be entirely excluded,\nthe preferred cosmological model remains the flat $\\Lambda$CDM background, even\nwhen incorporating DESI data. Finally, concerning the Hubble tension, our\nfindings match the Riess estimates, as BAO data points are excluded.",
        "The characterization of quantum correlations in many-body systems is\ninstrumental to understanding the nature of emergent phenomena in quantum\nmaterials. The correlation entropy serves as a key metric for assessing the\ncomplexity of a quantum many-body state in interacting electronic systems.\nHowever, its determination requires the measurement of all single-particle\ncorrelators across a macroscopic sample, which can be impractical. Machine\nlearning methods have been shown to allow learning the correlation entropy from\na reduced set of measurements, yet these methods assume that the targeted\nsystem is contained in the set of training Hamiltonians. Here we show that a\ntransfer learning strategy enables correlation entropy learning from a reduced\nset of measurements in families of Hamiltonians never considered in the\ntraining set. We demonstrate this transfer learning methodology in a wide\nvariety of interacting models including local and non-local attractive and\nrepulsive many-body interactions, long-range hopping, doping, magnetic field,\nand spin-orbit coupling. Furthermore, we show how this transfer learning\nmethodology allows detecting quantum many-body phases never observed during\ntheir training set without prior knowledge about them. Our results demonstrate\nthat correlation entropy learning can be potentially performed experimentally\nwithout requiring training in the experimentally realized Hamiltonian.",
        "An M-eigenvalue of a nonnegative biquadratic tensor is referred to as an\nM$^+$-eigenvalue if it has a pair of nonnegative M-eigenvectors. If furthermore\nthat pair of M-eigenvectors is positive, then that M$^+$-eigenvalue is called\nan M$^{++}$-eigenvalue. A nonnegative biquadratic tensor always has at least\none M$^+$ eigenvalue, and the largest M$^+$-eigenvalue is also the largest\nM-eigenvalue and the M-spectral radius. In the case of an irreducible\nnonnegative biquadratic tensor, all the M$^+$-eigenvalues are\nM$^{++}$-eigenvalues. Although the M$^+$-eigenvalues of irreducible nonnegative\nbiquadratic tensors are not unique in general, we establish a sufficient\ncondition to ensure their uniqueness. For an irreducible nonnegative\nbiquadratic tensor, the largest M$^+$-eigenvalue has a max-min\ncharacterization, while the smallest M$^+$-eigenvalue has a min-max\ncharacterization. A Collatz algorithm for computing the largest\nM$^+$-eigenvalues is proposed. Numerical results are reported.",
        "Consensus, synchronization, formation control, and power grid balance are all\nexamples of virtuous dynamical states that may arise in networks. Here we focus\non how such states can be destabilized from a fundamental perspective; namely,\nwe address the question of how one or a few intruder agents within an otherwise\nfunctioning network may compromise its dynamics. We show that a single\nadversarial node coupled via adversarial couplings to one or more other nodes\nis sufficient to destabilize the entire network, which we prove to be more\nefficient than targeting multiple nodes. Then, we show that concentrating the\nattack on a single low-indegree node induces the greatest instability,\nchallenging the common assumption that hubs are the most critical nodes. This\nleads to a new characterization of the vulnerability of a node, which contrasts\nwith previous work, and identifies low-indegree nodes (as opposed to the hubs)\nas the most vulnerable components of a network. Our results are derived for\nlinear systems but hold true for nonlinear networks, including those described\nby the Kuramoto model. Finally, we derive scaling laws showing that larger\nnetworks are less susceptible, on average, to single-node attacks. Overall,\nthese findings highlight an intrinsic vulnerability of technological systems\nsuch as autonomous networks, sensor networks, power grids, and the internet of\nthings, with implications also to the realm of complex social and biological\nnetworks.",
        "Local and global well-posedness, along with finite time blow-up, are\ninvestigated for the following Hardy-H\\'enon equation involving a quasilinear\ndegenerate diffusion and a space-dependent superlinear source featuring a\nsingular potential $$\\partial_t u=\\Delta u^m+|x|^{\\sigma}u^p,\\quad t>0,\\\nx\\in\\mathbb{R}^N,$$ when $m>1$, $p>1$ and $\\sigma\\in \\big(\\max\\{-2,-N\\},0\n\\big)$. While the superlinear source induces finite time blow-up when\n$\\sigma=0$, whatever the value of $p>1$, at least for sufficiently large\ninitial conditions, a striking effect of the singular potential $|x|^\\sigma$ is\nthe prevention of finite time blow-up for suitably small values of $p$, namely,\n$1<p\\le p_G := [2-\\sigma(m-1)]\/2$. Such a result, as well as the local\nexistence of solutions for $p>p_G$, is obtained by employing the\nCaffarelli-Kohn-Nirenberg inequalities. Another interesting feature is that\nuniqueness and comparison principle hold true for generic non-negative initial\nconditions when $p>p_G$, but their validity is restricted to initial conditions\nwhich are positive in a neighborhood of $x=0$ when $p\\in (1,p_G)$, a range in\nwhich non-uniqueness holds true without this positivity condition. Finite time\nblow-up of any non-trivial, non-negative solution is established when\n$p_G<p\\leq p_F:=m+(\\sigma+2)\/N$, while global existence for small initial data\nin some critical Lebesgue spaces and blow-up in finite time for initial data\nwith a negative energy are proved for $p>p_F$. Optimal temporal growth rates\nare also derived for global solutions when $p\\in (1,p_G]$. All the results are\nsharp with respect to the exponents $(m,p,\\sigma)$ and conditions on $u_0$.",
        "Lobbying systems are complex political networks that influence governmental\ndecisions, with often profound socio-economic consequences on national and\nglobal scales. For most political systems, a comprehensive understanding of\nlobbying strategies and dynamics is likely to remain elusive as time-resolved\nsystem-spanning data and analysis are lacking. A notable exception is the\nUnited States (U.S.), where the Lobbying Disclosure Act (LDA) of 1995 mandates\nthat all federal lobbying activities be disclosed in detailed quarterly\nreports. Here, we introduce our recently completed relational LobbyView\ndatabase that accounts for every reported lobbying instance since the\nimplementation of the LDA. We demonstrate how LobbyView can be used as a\nresource to quantify the salient aspects of the U.S.~lobbying, such as\ndynamical evolution, economic impacts, or political polarization. By analyzing\nthe dynamic evolution of the lobbying network, we identify fundamental\nself-organization principles, such as the self-accelerating accumulation of\ninfluence within a small group of powerful lobbying firms. We further show how\nLobbyView data can be used to accurately measure the synchronization of\nlobbying activities with election cycles. Finally, as a guide to future\nresearch, we illustrate how this data resource can enable quantitative\ntime-resolved analysis to investigate a wide range of critical issues,\nincluding concentration of resources, demographic representation, and\npolarization dynamics in the U.S.~lobbying system. We envisage our database to\nbe not only a potent resource for political and social scientists but also a\nstarting point for quantitative interdisciplinary research, by leveraging\ninsights and methods from statistical physics, systems biology, and machine\nlearning."
      ]
    }
  },
  {
    "id":"2411.03522",
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Fungal Genetic Variants in Oceanic Environments",
        "Multicellular self-organization in Escherichia coli",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Reproductive system and interaction with fauna in a Mediterranean\n  Pyrophite shrub",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "Stochastic Time to Extinction of an SIQS Epidemic Model with Quiescence",
        "Algebraic approaches for the decomposition of reaction networks and the\n  determination of existence and number of steady states",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Explicit polynomial bounds on Dehn functions of subgroups of hyperbolic\n  groups",
        "Spatiotemporal steering of non-diffracting wavepackets",
        "Characterising the Surface Resistance of Laser-Treated LHC Beam Screens\n  with the Shielded Pair Method",
        "Hodge theory and o-minimality at CIRM",
        "On uniqueness of free boundary minimal annuli in geodesic balls of\n  $\\mathbb{S}^3_+$ and $\\mathbb{H}^3$",
        "Averaging method for quasi-periodic response solutions",
        "Anisotropic Schottky-barrier-height in high-symmetry 2D WSe$_2$:\n  Momentum-space anisotropy",
        "Constraints on the Scale Parameter of Regular Black Hole in\n  Asymptotically Safe Gravity from Extreme Mass Ratio Inspirals",
        "The localization problem: an antinomy between measurability and causal\n  dynamics",
        "Non-Abelian interlayer coherent fractional quantum Hall states",
        "How the CME on 21 April 2023 Triggered the First Severe Geomagnetic\n  Storm of Solar Cycle 25",
        "SEW: A full-spectrum linear fitting with stellar population synthesis\n  method Based on \"Equivalent Widths spectrum\"",
        "Wafer-scale Integration of Single-Crystalline MoS$_2$ for Flexible\n  Electronics Enabled by Oxide Dry-transfer",
        "High-Quality Pulse Compression Using a Hybrid All-Bulk Multipass Cell\n  Scheme",
        "Constructing PDFs of spatially dependent fields using finite elements"
      ],
      "abstract":[
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Comparing specific types of organisms as they are found across environmental\nconditions has helped inform how genes and gene products of these organisms\nrelate to phenotypes and adaptation. In this study, we examine\nmetatranscriptomic data as found for oceanic fungi across different oceanic\nsampling sites. A specific set of three genes was chosen for evaluation based\non conserved orthology, known association with core physiological processes in\nfungi, and level of abundance within oceanic metatranscriptomic data. We report\nupon a potential association of genetic variance with environmental conditions\nof iron, salt and phosphate in oceanic waters based on heatmap visualization\nand PERMANOVA analysis.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "The ULEX model, in its present state, involves the study of the biomass and\nthe population of the shrub Ulex parviflorus Pourret, but while being a dynamic\nmodel, it is static in the sense that it does not imply the appearance of new\nspecimens of this plant. As a complement to the ULEX model in its two dynamic\nand spatial aspects, and with the idea of extending the model, the authors have\nintroduced from a biological and statistical point of view four characteristics\nof this species, flowering, pollination, fructification, taking special\ninterest in the role played by the pollinators (bees) and dispersion of seeds.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
        "Chemical reaction network theory provides powerful tools for rigorously\nunderstanding chemical reactions and the dynamical systems and differential\nequations that represent them. A frequent issue with mathematical analyses of\nthese networks is the reliance on explicit parameter values which in many cases\ncannot be determined experimentally. This can make analyzing a dynamical system\ninfeasible, particularly when the size of the system is large. One approach is\nto analyze subnetworks of the full network and use the results for a full\nanalysis.\n  Our focus is on the equilibria of reaction networks. Gr\\\"obner basis\ncomputation is a useful approach for solving the polynomial equations which\ncorrespond to equilibria of a dynamical system. We identify a class of networks\nfor which Gr\\\"obner basis computations of subnetworks can be used to\nreconstruct the more expensive Gr\\\"obner basis computation of the whole\nnetwork. We compliment this result with tools to determine if a steady state\ncan exist, and if so, how many.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In 1999 Brady constructed the first example of a non-hyperbolic finitely\npresented subgroup of a hyperbolic group by fibring a non-positively curved\ncube complex over the circle. We show that his example has Dehn function\nbounded above by $n^{96}$. This provides the first explicit polynomial upper\nbound on the Dehn function of a finitely presented non-hyperbolic subgroup of a\nhyperbolic group. We also determine the precise hyperbolicity constant for the\n$1$-skeleton of the universal cover of the cube complex in Brady's construction\nwith respect to the $4$-point condition for hyperbolicity.",
        "We study the dynamics of space-time non-diffracting wavepackets, commonly\nknown as light bullets, in a spatiotemporally varying medium. We show that by\nspatiotemporal refraction, a monochromatic focused beam can be converted to a\nlight bullet that propagates at a given velocity. By further designing the\nindex profile of the spatiotemporal boundary, the group velocity and the\npropagation direction of the light bullet can be engineered in a programmable\nway. All effects mentioned above cannot be achieved by spatial or temporal\nboundaries, and are only possible with spatiotemporal boundaries. These\nfindings provide unique ways to engineer the dynamics of electromagnetic\nwavepackets in space-time. Such wavepackets with engineered spacetime\ntrajectories may find potential applications in the spatiotemporal control of\nmaterial properties or particles, or for use as a way to emulate relativistic\nphysics in the laboratory.",
        "The presence of strong electron clouds in the quadrupole magnetic field\nregions of the Large Hadron Collider (LHC) leads to considerable heating that\nposes challenges for the cryogenic cooling system, and under certain conditions\nto proton beam quality deterioration. Research is being conducted on\nlaser-treated inner beam screen surfaces for the upgraded High-Luminosity LHC\nto mitigate this issue. Laser-induced surface structuring, a technique that\neffectively roughens surfaces, has been shown to reduce secondary electron\nemission; an essential factor in controlling electron cloud formation.\nConversely, the resulting surface roughening also alters the material's surface\nimpedance, potentially impacting beam stability and increasing beam-induced\nresistive wall heating. Different laser treatment patterns have been applied to\nLHC beam screens to estimate this potential impact and assessed for their\nmicrowave responses.",
        "We discuss the relationship between o-minimality and the so called\nZilber-Pink conjecture. Since the work of Pila and Zannier, algebraization\ntheorems in o-minimal geometry had profound impacts in Diophantine geometry\n(most notably on the study of special points in abelian and Shimura varieties).\nWe will first focus on functional transcendence, discussing various recent and\nspectacular Ax-Schanuel theorems, and the related geometric part of\nZilber-Pink. Armed with these tools, we will study the distribution of the\nHodge locus of an arbitrary variation of Hodge structures (the typical\/atypical\ndichotomy) and present some recent applications. We will conclude by describing\nthe algebraicity and quasiprojectivity of images of period maps.",
        "We consider $\\Sigma$ an embedded free boundary minimal annulus in a geodesic\nball in the round hemisphere $\\mathbb{S}^3_+$ or in the hyperbolic space\n$\\mathbb{H}^3$. Under the hypothesis of invariance due to an antipodal map on\nthe geodesic ball and using the fact that this surface satisfies the Steklov\nproblem with frequency, we prove that $\\Sigma$ is congruent to a critical\nrotational annulus.",
        "In this paper, we present an averaging method for obtaining quasi-periodic\nresponse solutions in perturbed real analytic quasi-periodic systems with\nDiophantine frequency vectors. Assuming that the averaged system possesses a\nnon-degenerate equilibrium and the eigenvalues of the linearized matrix are\npairwise distinct, we show that the original system admits a quasi-periodic\nresponse solution for the parameter belonging to a Cantorian set. The proof is\nbased on the KAM techniques, and this averaging method can be extended to the\nsecond-order systems. It is worth mentioning that our results do not require\nthe equilibrium point to be hyperbolic, which means that the eigenvalues of the\nlinearized matrix of the averaging system may be purely imaginary.",
        "It is usually supposed that only low-symmetry two-dimensional (2D) materials\nexhibit anisotropy, here we show that high-symmetry 2D semiconductors can show\nsignificant anisotropy in momentum space due to the band structure anisotropy\nin k-space. The basic reason is that different k-points in the Brillouin zone\nhave different symmetry. Using 2D semiconductor WSe$_2$ as the example, we\nconstruct lateral heterostructures with zigzag and armchair connections to 2D\nmetal NbSe$_2$, and the electronic structure and contact characteristics of\nthese two connections are analyzed. It is found that both connections exhibit\np-type Schottky barrier height (SBH) but the sizes of SBH are very different\n(of 0.03 eV and 0.50 eV), mainly because the band-edge energies of WSe$_2$ are\ndifferent along the two mutually perpendicular directions in momentum space.\nThere are two factors contributing to the SBH anisotropy: one is the different\ninterface structure and the other is the band edge anisotropy of the 2D\nsemiconductor WSe$_2$. Since the two interface structures give only a\ndifference in interface potential change by less than 0.1 eV, the SBH variation\nof ~0.47 eV is mainly from the band structure anisotropy in momentum-space. So,\nhigh-symmetry 2D materials may exhibit highly anisotropic electronic states in\nmomentum space and this affects the transport properties. Our current work\nextends the research field of 2D material anisotropy to 2D materials with high\nreal-space symmetry, thus greatly expands the candidate materials for\nanisotropic studies and provides new guidance for optimizing the performance of\n2D material devices via controlling transport directions.",
        "This paper evaluates the potential for constraining the quantum scale\nparameter $\\xi$ of regular black hole within the asymptotically safe gravity\nframework using gravitational waves from extreme mass ratio inspirals (EMRI).\nSince $\\xi$ cannot be precisely determined from first principles, observational\nconstraints become crucial. We employ the Augmented Analytical Kludge (AAK)\nmethod to calculate gravitational waveforms in the equatorial plane and\nsystematically analyze the influence of different $\\xi$ values on phase\nevolution. Comparison with the Schwarzschild case demonstrates that the\ncorrective effects of $\\xi$ accumulate in the phase over observation time,\nthereby providing distinguishable observational signatures. Through waveform\nmismatch analysis, our results indicate that the LISA detector can effectively\ndetect the presence of $\\xi$ at the $\\sim10^{-4}$ level for systems with a mass\nof $10^6M_\\odot$. Further assessment using the Fisher information matrix (FIM)\nconfirms a measurement precision of $\\Delta\\xi\\approx3.225\\times10^{-4}$, which\nsignificantly surpasses existing observational methods, providing quantitative\nobservational evidence for asymptotically safe quantum gravity theory in the\nstrong-field regime.",
        "The localization problem in relativistic quantum theory has persisted for\nmore than seven decades, yet it is largely unknown and continues to perplex\neven those well-versed in the subject. At the heart of this problem lies a\nfundamental conflict between localizability and relativistic causality, which\ncan also be construed as part of the broader dichotomy between measurement and\nunitary dynamics. This article provides a historical review of the localization\nproblem in one-particle relativistic quantum mechanics, clarifying some\npersistent misconceptions in the literature, and underscoring the antinomy\nbetween causal dynamics and localized observables.",
        "We study non-Abelian fractional quantum Hall state in double layer systems at\ntotal filling factor $1\/2$. Recent progresses in two-dimensional van der Waals\nmaterials made it possible to explore the regime with very small interlayer\ndistance. Numerical calculations suggests interlayer phase coherence can\ndevelop between the layers such that the electrons may redistribute between\nthem without changing the Hall response. It corresponds to spontaneous breaking\nof the U(1) symmetry associated with the particle number difference in the\nlayers. This state manifests itself as superfluid in counterflow measurement\nand has characteristic Hall response when current is passed through one layer\nand voltages in both layers are measured. As the interlayer distance increases,\na phase transition to the Halperin 331 state occurs. We also discuss similar\nphysics for bosonic systems with specially designed interactions.",
        "The first severe (G4) geomagnetic storm of Solar Cycle 25 occurred on 23-24\nApril 2023, following the arrival of a Coronal Mass Ejection (CME) on 23 April.\nThe characteristics of this CME, measured from coronagraphs (speed and mass),\ndid not indicate that it would trigger such an intense geomagnetic storm. In\nthis work, our aim is to understand why this CME led to such a geoeffective\noutcome. Our analysis spans from the source active region to the corona and\ninner heliosphere through 1 au using multiwavelength, multi-viewpoint remote\nsensing observations and in situ data. We find that rotation and possibly\ndeflection of the CME resulted in an axial magnetic field nearly parallel to\nthe ecliptic plane during the Earth encounter, which might explain the storm's\nseverity. Additionally, we find that imaging away from the Sun-Earth line is\ncrucial in hindcasting the CME Time-of-Arrival at Earth. The position (0.39 au)\nand detailed images from the SoloHI telescope onboard the Solar Orbiter\nmission, in combination with SOHO and STEREO images, helped decisively with the\nthree-dimensional (3D) reconstruction of the CME.",
        "We present a full-spectrum linear fitting method, SEW, for stellar population\nsynthesis based on equivalent widths (EWs) to extract galaxy properties from\nobserved spectra. This approach eliminates the need for prior assumptions about\ndust attenuation curves, which are instead derived as outputs of the fitting\nprocess. By leveraging the invariance of EWs and employing the Discrete\nPenalised Least Squares (DPLS) method to extract EWs, we address the nonlinear\naspects of the fitting process by linearising the matrix equations. This\nenables accurate recovery of key parameters, stellar age, metallicity and dust\nattenuation, even under systematic calibration biases and varying attenuation\nconditions. Rigorous testing with mock spectra across signal-to-noise ratios\n(S\/N = 5-30) and calibration biases demonstrates the robustness of method. The\nderived attenuation curves align closely with input models, and stellar\npopulation parameters are recovered with minimal bias. To facilitate adoption,\nwe implement this method as a Python extension package for \\texttt{pPXF}\n(\\texttt{pPXF-SEW}). Our work addresses critical degeneracies in traditional\nspectral fitting and enhances the reliability of extragalactic studies.",
        "Atomically thin, single-crystalline transition metal dichalcogenides (TMDCs)\ngrown via chemical vapor deposition (CVD) on sapphire substrates exhibit\nexceptional mechanical and electrical properties, positioning them as excellent\nchannel materials for flexible electronics. However, conventional wet-transfer\nprocesses for integrating these materials onto flexible substrates often\nintroduce surface contamination, significantly degrading device performance.\nHere, we present a wafer-scale dry-transfer technique using a high-dielectric\noxide as the transfer medium, enabling the integration of 4-inch\nsingle-crystalline MoS$_2$ onto flexible substrates. This method eliminates\ncontact with polymers or solvents, thus preserving the intrinsic electronic\nproperties of MoS$_2$. As a result, the fabricated flexible field-effect\ntransistor (FET) arrays exhibit remarkable performance, with a mobility of 117\ncm$^2$\/Vs, a subthreshold swing of 68.8 mV dec$^{-1}$, and an ultra-high\ncurrent on\/off ratio of $10^{12}$-values comparable to those achieved on rigid\nsubstrates. Leveraging the outstanding electrical characteristics, we\ndemonstrated MoS$_2$-based flexible inverters operating in the subthreshold\nregime, achieving both a high gain of 218 and ultra-low power consumption of\n1.4 pW\/$\\mu$m. Additionally, we integrated a flexible tactile sensing system\ndriven by active-matrix MoS$_2$ FET arrays onto a robotic gripper, enabling\nreal-time object identification. These findings demonstrate the simultaneous\nachievement of high electrical performance and flexibility, highlighting the\nimmense potential of single-crystalline TMDC-based flexible electronics for\nreal-world applications.",
        "We present a detailed numerical study of ultrashort pulse compression using a\nthree-stage hybrid all-bulk multipass cell scheme. By operating in the enhanced\nfrequency chirp regime, we achieve the compression of pulses from around 180 fs\nto 4 fs pulse duration (a total compression factor above 45), with side lobes\ncontributing with intensity values lower than 0.2 % of the peak intensity.\nOptimal conditions for the enhanced frequency chirp regime propagation have\nbeen identified, enabling smooth spectral broadening and high-quality temporal\nprofiles. The first two stages are based on bulk multipass cells to achieve a\ncontrolled spectral broadening, while the third stage consists of a thin plate\nto reach the spectral broadening needed for few cycle pulses without leaving\nthe enhanced frequency chirp regime.",
        "A probability density function (PDF) of a spatially dependent field provides\na means of calculating moments of the field or, equivalently, the proportion of\na spatial domain that is mapped to a given set of values. This paper describes\na finite element approach to estimating the PDF of a spatially dependent field\nand its numerical implementation in the Python package NumDF."
      ]
    }
  },
  {
    "id":"2411.03522",
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect\n  Controllable Text Generation with Large Language Models",
        "BottleHumor: Self-Informed Humor Explanation using the Information\n  Bottleneck Principle",
        "Enhancing RAG with Active Learning on Conversation Records: Reject\n  Incapables and Answer Capables",
        "TablePilot: Recommending Human-Preferred Tabular Data Analysis with\n  Large Language Models",
        "LabelCoRank: Revolutionizing Long Tail Multi-Label Classification with\n  Co-Occurrence Reranking",
        "LongRoPE2: Near-Lossless LLM Context Window Scaling",
        "RACCOON: A Retrieval-Augmented Generation Approach for Location\n  Coordinate Capture from News Articles",
        "ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation",
        "Synthetic Function Demonstrations Improve Generation in Low-Resource\n  Programming Languages",
        "Multi-stage Training of Bilingual Islamic LLM for Neural Passage\n  Retrieval",
        "Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning",
        "Exploring Robustness of Multilingual LLMs on Real-World Noisy Data",
        "The Multilingual Mind : A Survey of Multilingual Reasoning in Language\n  Models",
        "Inducing Diversity in Differentiable Search Indexing",
        "Identification of Genetic Factors Associated with Corpus Callosum\n  Morphology: Conditional Strong Independence Screening for Non-Euclidean\n  Responses",
        "Memory Efficient Transformer Adapter for Dense Predictions",
        "Probing growth precursor diffusion lengths by inter-surface diffusion",
        "The characterizations of hyperspaces and free topological groups with an\n  $\\omega^\\omega$-base",
        "AI Alignment at Your Discretion",
        "Unevolved Li-rich stars at low metallicity: a possible formation pathway\n  through novae",
        "AI Enabled User-Specific Cyberbullying Severity Detection with\n  Explainability",
        "Chip-to-chip photonic connectivity in multi-accelerator servers for ML",
        "Static spherically symmetric solutions of the f(R) gravity",
        "Large class of many-to-one mappings over quadratic extension of finite\n  fields",
        "The dynamical and thermodynamic effects of turbulence for the cosmic\n  baryonic fluid",
        "EmoXpt: Analyzing Emotional Variances in Human Comments and\n  LLM-Generated Responses",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "Quantum geometry in the dynamics of band-projected operators"
      ],
      "abstract":[
        "Multi-aspect controllable text generation aims to control text generation in\nattributes from multiple aspects, making it a complex but powerful task in\nnatural language processing. Supervised fine-tuning methods are often employed\nfor this task due to their simplicity and effectiveness. However, they still\nhave some limitations: low rank adaptation (LoRA) only fine-tunes a few\nparameters and has suboptimal control effects, while full fine-tuning (FFT)\nrequires significant computational resources and is susceptible to overfitting,\nparticularly when data is limited. Moreover, existing works typically train\nmulti-aspect controllable text generation models using only single-aspect\nannotated data, which results in discrepancies in data distribution; at the\nsame time, accurately generating text with specific attributes is a challenge\nthat requires strong attribute-aware capabilities. To address these\nlimitations, we propose a lightweight, adaptive and attribute-aware framework\nfor multi-aspect controllable text generation. Our framework can dynamically\nadjust model parameters according to different aspects of data to achieve\ncontrollable text generation, aiming to optimize performance across multiple\naspects. Experimental results show that our framework outperforms other strong\nbaselines, achieves state-of-the-art performance, adapts well to data\ndiscrepancies, and is more accurate in attribute perception.",
        "Humor is prevalent in online communications and it often relies on more than\none modality (e.g., cartoons and memes). Interpreting humor in multimodal\nsettings requires drawing on diverse types of knowledge, including\nmetaphorical, sociocultural, and commonsense knowledge. However, identifying\nthe most useful knowledge remains an open question. We introduce \\method{}, a\nmethod inspired by the information bottleneck principle that elicits relevant\nworld knowledge from vision and language models which is iteratively refined\nfor generating an explanation of the humor in an unsupervised manner. Our\nexperiments on three datasets confirm the advantage of our method over a range\nof baselines. Our method can further be adapted in the future for additional\ntasks that can benefit from eliciting and conditioning on relevant world\nknowledge and open new research avenues in this direction.",
        "Retrieval-augmented generation (RAG) is a key technique for leveraging\nexternal knowledge and reducing hallucinations in large language models (LLMs).\nHowever, RAG still struggles to fully prevent hallucinated responses. To\naddress this, it is essential to identify samples prone to hallucination or\nguide LLMs toward correct responses, which experts then annotate to develop\nhigh-quality datasets for refining LLMs. However, the growing scarcity of such\ndatasets makes their creation challenging. This paper proposes using the vast\namount of conversations from widespread LLM usage to build these datasets,\ntraining LLMs to avoid hallucination-prone questions while accurately\nresponding to manageable ones. Given the impracticality of expert-annotating\nall conversation records, the paper introduces AL4RAG, which uses active\nlearning to select the most suitable conversation samples for annotation,\noptimizing performance within an annotation budget. Additionally, recognizing\nthat traditional active learning methods are not fully compatible with RAG due\nto unsuitable distance metrics, we develop a novel sample distance measurement\nfor RAG active learning. Extensive experiments show that our method\nconsistently outperforms baselines across multiple metrics.",
        "Tabular data analysis is crucial in many scenarios, yet efficiently\nidentifying the most relevant data analysis queries and results for a new table\nremains a significant challenge. The complexity of tabular data, diverse\nanalytical operations, and the demand for high-quality analysis make the\nprocess tedious. To address these challenges, we aim to recommend\nquery-code-result triplets tailored for new tables in tabular data analysis\nworkflows. In this paper, we present TablePilot, a pioneering tabular data\nanalysis framework leveraging large language models to autonomously generate\ncomprehensive and superior analytical results without relying on user profiles\nor prior interactions. The framework incorporates key designs in analysis\npreparation and analysis optimization to enhance accuracy. Additionally, we\npropose Rec-Align, a novel method to further improve recommendation quality and\nbetter align with human preferences. Experiments on DART, a dataset\nspecifically designed for comprehensive tabular data analysis recommendation,\ndemonstrate the effectiveness of our framework. Based on GPT-4o, the tuned\nTablePilot achieves 77.0% top-5 recommendation recall. Human evaluations\nfurther highlight its effectiveness in optimizing tabular data analysis\nworkflows.",
        "Motivation: Despite recent advancements in semantic representation driven by\npre-trained and large-scale language models, addressing long tail challenges in\nmulti-label text classification remains a significant issue. Long tail\nchallenges have persistently posed difficulties in accurately classifying less\nfrequent labels. Current approaches often focus on improving text semantics\nwhile neglecting the crucial role of label relationships. Results: This paper\nintroduces LabelCoRank, a novel approach inspired by ranking principles.\nLabelCoRank leverages label co-occurrence relationships to refine initial label\nclassifications through a dual-stage reranking process. The first stage uses\ninitial classification results to form a preliminary ranking. In the second\nstage, a label co-occurrence matrix is utilized to rerank the preliminary\nresults, enhancing the accuracy and relevance of the final classifications. By\nintegrating the reranked label representations as additional text features,\nLabelCoRank effectively mitigates long tail issues in multi-labeltext\nclassification. Experimental evaluations on popular datasets including MAG-CS,\nPubMed, and AAPD demonstrate the effectiveness and robustness of LabelCoRank.",
        "LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps:\/\/github.com\/microsoft\/LongRoPE.",
        "Geocoding involves automatic extraction of location coordinates of incidents\nreported in news articles, and can be used for epidemic intelligence or\ndisaster management. This paper introduces Retrieval-Augmented Coordinate\nCapture Of Online News articles (RACCOON), an open-source geocoding approach\nthat extracts geolocations from news articles. RACCOON uses a\nretrieval-augmented generation (RAG) approach where candidate locations and\nassociated information are retrieved in the form of context from a location\ndatabase, and a prompt containing the retrieved context, location mentions and\nnews articles is fed to an LLM to generate the location coordinates. Our\nevaluation on three datasets, two underlying LLMs, three baselines and several\nablation tests based on the components of RACCOON demonstrate the utility of\nRACCOON. To the best of our knowledge, RACCOON is the first RAG-based approach\nfor geocoding using pre-trained LLMs.",
        "While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers.",
        "A key consideration when training an LLM is whether the target language is\nmore or less resourced, whether this is English compared to Welsh, or Python\ncompared to Excel. Typical training data for programming languages consist of\nreal program demonstrations coupled with human-written comments. Here we\npresent novel approaches to the creation of such data for low resource\nprogramming languages. We generate fully-synthetic, textbook-quality\ndemonstrations of common library functions in an example domain of Excel\nformulas, using a teacher model. We then finetune an underperforming student\nmodel, and show improvement on 2 question-answering datasets recast into the\nExcel domain. We show advantages of finetuning over standard, off-the-shelf RAG\napproaches, which can offer only modest improvement due to the unfamiliar\ntarget domain.",
        "This study examines the use of Natural Language Processing (NLP) technology\nwithin the Islamic domain, focusing on developing an Islamic neural retrieval\nmodel. By leveraging the robust XLM-R model, the research employs a language\nreduction technique to create a lightweight bilingual large language model\n(LLM). Our approach for domain adaptation addresses the unique challenges faced\nin the Islamic domain, where substantial in-domain corpora exist only in Arabic\nwhile limited in other languages, including English.\n  The work utilizes a multi-stage training process for retrieval models,\nincorporating large retrieval datasets, such as MS MARCO, and smaller,\nin-domain datasets to improve retrieval performance. Additionally, we have\ncurated an in-domain retrieval dataset in English by employing data\naugmentation techniques and involving a reliable Islamic source. This approach\nenhances the domain-specific dataset for retrieval, leading to further\nperformance gains.\n  The findings suggest that combining domain adaptation and a multi-stage\ntraining method for the bilingual Islamic neural retrieval model enables it to\noutperform monolingual models on downstream retrieval tasks.",
        "Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.",
        "Large Language Models (LLMs) are trained on Web data that might contain\nspelling errors made by humans. But do they become robust to similar real-world\nnoise? In this paper, we investigate the effect of real-world spelling mistakes\non the performance of 9 language models, with parameters ranging from 0.2B to\n13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name\nEntity Recognition (NER), and Intent Classification (IC). We perform our\nexperiments on 6 different languages and build a dictionary of real-world noise\nfor them using the Wikipedia edit history. We show that the performance gap of\nthe studied models on the clean and noisy test data averaged across all the\ndatasets and languages ranges from 2.3 to 4.3 absolute percentage points. In\naddition, mT5 models, in general, show more robustness compared to BLOOM,\nFalcon, and BERT-like models. In particular, mT5 (13B), was the most robust on\naverage overall, across the 3 tasks, and in 4 of the 6 languages.",
        "While reasoning and multilingual capabilities in Language Models (LMs) have\nachieved remarkable progress in recent years, their integration into a unified\nparadigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning\nrequires language models to handle logical reasoning across languages while\naddressing misalignment, biases, and challenges in low-resource settings. This\nsurvey provides the first in-depth review of multilingual reasoning in LMs. In\nthis survey, we provide a systematic overview of existing methods that leverage\nLMs for multilingual reasoning, specifically outlining the challenges,\nmotivations, and foundational aspects of applying language models to reason\nacross diverse languages. We provide an overview of the standard data resources\nused for training multilingual reasoning in LMs and the evaluation benchmarks\nemployed to assess their multilingual capabilities. Next, we analyze various\nstate-of-the-art methods and their performance on these benchmarks. Finally, we\nexplore future research opportunities to improve multilingual reasoning in LMs,\nfocusing on enhancing their ability to handle diverse languages and complex\nreasoning tasks.",
        "Differentiable Search Indexing (DSI) is a recent paradigm for information\nretrieval which uses a transformer-based neural network architecture as the\ndocument index to simplify the retrieval process. A differentiable index has\nmany advantages enabling modifications, updates or extensions to the index. In\nthis work, we explore balancing relevance and novel information content\n(diversity) for training DSI systems inspired by Maximal Marginal Relevance\n(MMR), and show the benefits of our approach over the naive DSI training. We\npresent quantitative and qualitative evaluations of relevance and diversity\nmeasures obtained using our method on NQ320K and MSMARCO datasets in comparison\nto naive DSI. With our approach, it is possible to achieve diversity without\nany significant impact to relevance. Since we induce diversity while training\nDSI, the trained model has learned to diversify while being relevant. This\nobviates the need for a post-processing step to induce diversity in the recall\nset as typically performed using MMR. Our approach will be useful for\nInformation Retrieval problems where both relevance and diversity are important\nsuch as in sub-topic retrieval. Our work can also be easily be extended to the\nincremental DSI settings which would enable fast updates to the index while\nretrieving a diverse recall set.",
        "The corpus callosum, the largest white matter structure in the brain, plays a\ncritical role in interhemispheric communication. Variations in its morphology\nare associated with various neurological and psychological conditions, making\nit a key focus in neurogenetics. Age is known to influence the structure and\nmorphology of the corpus callosum significantly, complicating the\nidentification of specific genetic factors that contribute to its shape and\nsize. We propose a conditional strong independence screening method to address\nthese challenges for ultrahigh-dimensional predictors and non-Euclidean\nresponses. Our approach incorporates prior knowledge, such as age. It\nintroduces a novel concept of conditional metric dependence, quantifying\nnon-linear conditional dependencies among random objects in metric spaces\nwithout relying on predefined models. We apply this framework to identify\ngenetic factors associated with the morphology of the corpus callosum.\nSimulation results demonstrate the efficacy of this method across various\nnon-Euclidean data types, highlighting its potential to drive genetic discovery\nin neuroscience.",
        "While current Vision Transformer (ViT) adapter methods have shown promising\naccuracy, their inference speed is implicitly hindered by inefficient memory\naccess operations, e.g., standard normalization and frequent reshaping. In this\nwork, we propose META, a simple and fast ViT adapter that can improve the\nmodel's memory efficiency and decrease memory time consumption by reducing the\ninefficient memory access operations. Our method features a memory-efficient\nadapter block that enables the common sharing of layer normalization between\nthe self-attention and feed-forward network layers, thereby reducing the\nmodel's reliance on normalization operations. Within the proposed block, the\ncross-shaped self-attention is employed to reduce the model's frequent\nreshaping operations. Moreover, we augment the adapter block with a lightweight\nconvolutional branch that can enhance local inductive biases, particularly\nbeneficial for the dense prediction tasks, e.g., object detection, instance\nsegmentation, and semantic segmentation. The adapter block is finally\nformulated in a cascaded manner to compute diverse head features, thereby\nenriching the variety of feature representations. Empirically, extensive\nevaluations on multiple representative datasets validate that META\nsubstantially enhances the predicted quality, while achieving a new\nstate-of-the-art accuracy-efficiency trade-off. Theoretically, we demonstrate\nthat META exhibits superior generalization capability and stronger\nadaptability.",
        "Understanding and optimizing thin-film synthesis requires measuring the\ndiffusion length $d_\\alpha$ of adsorbed growth precursors. Despite\ntechnological advances, in-situ measurements of $d_\\alpha$ are often\nunachievable due to harsh deposition conditions, such as high temperatures or\nreactive environments. In this paper, we propose a fitting approach to\ndetermine $d_\\alpha$ from experimental data by leveraging inter-surface\ndiffusion between a substrate and a strip obtained by, for example, processing\na film. The substrate serves as a source or sink of precursors, influencing the\ngrowth dynamics and shaping the profile of the strip. By fitting simulated\nprofiles to given profiles, we demonstrate that $d_\\alpha$ can be determined.\nTo achieve this, we develop a theoretical growth model, a simulation strategy,\nand a fitting procedure. The growth model incorporates inter-surface diffusion,\nadsorption, and desorption of growth precursors, with growth being proportional\nto the concentration of adsorbed precursors. In our simulations, a chain of\nnodes represents a profile, and growth is captured by the displacement of those\nnodes, while keeping the node density approximately constant. For strips\nsignificantly wider than $d_\\alpha$, a scaled precursor concentration and\n$d_\\alpha$ are the fitting parameters that are determined by minimizing a\nsuitably defined measure of the distance between simulated and given profiles.\nWe evaluate the robustness of our procedure by analyzing the effect of profile\nresolution and noise on the fitted parameters. Our approach can offer valuable\ninsights into thin-film growth processes, such as those occurring during\nplasma-enhanced chemical vapor deposition.",
        "A topological space $(X, \\tau)$ is said to be have an {\\it\n$\\omega^\\omega$-base} if for each point $x\\in X$ there exists a neighborhood\nbase $\\{U_{\\alpha}[x]: \\alpha\\in\\omega^\\omega\\}$ such that $U_{\\beta}[x]\\subset\nU_{\\alpha}[x]$ for all $\\alpha\\leq\\beta$ in $\\omega^\\omega$. In this paper, the\ncharacterization of a space $X$ is given such that the free Abelian topological\ngroup $A(X)$, the hyperspace $CL(X)$ with the Vietoris topology and the\nhyperspace $CL(X)$ with the Fell topology have $\\omega^\\omega$-bases\nrespectively. The main results are listed as follows:\n  (1) For a Tychonoff space $X$, the free Abelian topological group $A(X)$ is a\n$k$-space with an $\\omega^\\omega$-base if and only if $X$ is a topological sum\nof a discrete space and a submetrizable $k_\\omega$-space.\n  (2) If $X$ is a metrizable space, then $(CL(X), \\tau_V)$ has an\n$\\omega^\\omega$-base if and only if $X$ is separable and the boundary of each\nclosed subset of $X$ is $\\sigma$-compact.\n  (3) If $X$ is a metrizable space, then $(CL(X), \\tau_F)$ has an\n$\\omega^\\omega$-base consisting of basic neighborhoods if and only if $X$ is a\nPolish space.\n  (4) If $X$ is a metrizable space, then $(CL(X), \\tau_F)$ is a\nFr\\'echet-Urysohn space with an $\\omega^\\omega$-base, if and only if $(CL(X),\n\\tau_F)$ is first-countable, if and only if $X$ is a locally compact and second\ncountable space.",
        "In AI alignment, extensive latitude must be granted to annotators, either\nhuman or algorithmic, to judge which model outputs are `better' or `safer.' We\nrefer to this latitude as alignment discretion. Such discretion remains largely\nunexamined, posing two risks: (i) annotators may use their power of discretion\narbitrarily, and (ii) models may fail to mimic this discretion. To study this\nphenomenon, we draw on legal concepts of discretion that structure how\ndecision-making authority is conferred and exercised, particularly in cases\nwhere principles conflict or their application is unclear or irrelevant.\nExtended to AI alignment, discretion is required when alignment principles and\nrules are (inevitably) conflicting or indecisive. We present a set of metrics\nto systematically analyze when and how discretion in AI alignment is exercised,\nsuch that both risks (i) and (ii) can be observed. Moreover, we distinguish\nbetween human and algorithmic discretion and analyze the discrepancy between\nthem. By measuring both human and algorithmic discretion over safety alignment\ndatasets, we reveal layers of discretion in the alignment process that were\npreviously unaccounted for. Furthermore, we demonstrate how algorithms trained\non these datasets develop their own forms of discretion in interpreting and\napplying these principles, which challenges the purpose of having any\nprinciples at all. Our paper presents the first step towards formalizing this\ncore gap in current alignment processes, and we call on the community to\nfurther scrutinize and control alignment discretion.",
        "A small fraction of low-mass stars have been found to have anomalously high\nLi abundances. Although it has been suggested that mixing during the red giant\nbranch phase can lead to Li production, this method of intrinsic Li production\ncannot explain Li-rich stars that have not yet undergone the first dredge-up.\nTo obtain clues about the origin of such stars, we present a detailed chemical\nabundance analysis of four unevolved Li-rich stars with $-2.1 < [\\mathrm{Fe\/H}]\n< -1.3$ and $2.9<A({\\rm Li})<3.6$, $0.7-1.4$ dex higher Li abundance than\ntypical unevolved metal-poor stars. One of the stars, Gaia DR3\n6334970766103389824 (D25_6334), was serendipitously found in the stellar stream\nED-3, and the other three stars have been reported to have massive ($M\\gtrsim\n1.3\\,\\mathrm{M_\\odot}$) non-luminous companions. We show that three of the four\nstars exhibit abundance patterns similar to those of known unevolved Li-rich\nstars, namely normal abundances in most elements except for Li and Na. These\nabundance similarities suggest a common origin for the unevolved Li-rich stars\nand low-mass metal-poor stars with massive compact companions. We also made the\nfirst detection of N abundance to unevolved Li-rich stars in D25_6334, and\nfound that it is significantly enhanced ($[\\mathrm{N\/Fe}]=1.3$). The observed\nabundance pattern of D25_6334, spanning from C to Si, indicates that its\nsurface has been polluted by an intermediate-mass former companion star or a\nnova system that involves a massive ONe white dwarf. Using a population\nsynthesis model, we show that the nova scenario can lead to the observed level\nof Li enhancement and also provide an explanation for Li-rich stars without\ncompanions and those with massive compact companions.",
        "The rise of social media has significantly increased the prevalence of\ncyberbullying (CB), posing serious risks to both mental and physical\nwell-being. Effective detection systems are essential for mitigating its\nimpact. While several machine learning (ML) models have been developed, few\nincorporate victims' psychological, demographic, and behavioral factors\nalongside bullying comments to assess severity. In this study, we propose an AI\nmodel intregrating user-specific attributes, including psychological factors\n(self-esteem, anxiety, depression), online behavior (internet usage,\ndisciplinary history), and demographic attributes (race, gender, ethnicity),\nalong with social media comments. Additionally, we introduce a re-labeling\ntechnique that categorizes social media comments into three severity levels:\nNot Bullying, Mild Bullying, and Severe Bullying, considering user-specific\nfactors.Our LSTM model is trained using 146 features, incorporating emotional,\ntopical, and word2vec representations of social media comments as well as\nuser-level attributes and it outperforms existing baseline models, achieving\nthe highest accuracy of 98\\% and an F1-score of 0.97. To identify key factors\ninfluencing the severity of cyberbullying, we employ explainable AI techniques\n(SHAP and LIME) to interpret the model's decision-making process. Our findings\nreveal that, beyond hate comments, victims belonging to specific racial and\ngender groups are more frequently targeted and exhibit higher incidences of\ndepression, disciplinary issues, and low self-esteem. Additionally, individuals\nwith a prior history of bullying are at a greater risk of becoming victims of\ncyberbullying.",
        "We present a rack-scale compute architecture for ML using multi-accelerator\nservers connected via chip-to-chip silicon photonic components. Our\narchitecture achieves (1) multi-tenanted resource slicing without\nfragmentation, (2) 74% faster rack-scale collective communication, and (3) 1.7X\nspeedup in end-to-end ML training throughput.",
        "Static spherically symmetric (SSS) solutions of f(R) gravity are studied in\nthe Einstein frame. The solutions involve SSS configuration mass M and scalaron\nmass $\\mu$ (in geometrized units); for typical astrophysical masses, the\ndimensionless parameter $M\\mu$ has very large value. We found analytic\nsolutions on a finite interval for $M\\mu\\to \\infty$ in case of a family of\nscalaron potentials. The asymptotically flat solutions on $(0,\\infty)$ have\nbeen studied numerically for $M\\mu$ up to $10^{20}$ in case of the quadratic\nf(R) model.",
        "Many-to-one mappings and permutation polynomials over finite fields have\nimportant applications in cryptography and coding theory. In this paper, we\nstudy the many-to-one property of a large class of polynomials such as $f(x) =\nh(a x^q + b x + c) + u x^q + v x$, where $h(x) \\in \\mathbb{F}_{q^2}[x]$ and\n$a$, $b$, $c$, $u$, $v \\in \\mathbb{F}_{q^2}$. Using a commutative diagram\nsatisfied by $f(x)$ and trace functions over finite fields, we reduce the\nproblem whether $f(x)$ is a many-to-one mapping on $\\mathbb{F}_{q^2}$ to\nanother problem whether an associated polynomial $g(x)$ is a many-to-one\nmapping on the subfield $\\mathbb{F}_{q}$. In particular, when $h(x) = x^{r}$\nand $r$ satisfies certain conditions, we reduce $g(x)$ to polynomials of small\ndegree or linearized polynomials. Then by employing the many-to-one properties\nof these low degree or linearized polynomials on $\\mathbb{F}_{q}$, we derive a\nseries of explicit characterization for $f(x)$ to be many-to-one on\n$\\mathbb{F}_{q^2}$. On the other hand, for all $1$-to-$1$ mappings obtained in\nthis paper, we determine the inverses of these permutation polynomials.\nMoreover, we also explicitly construct involutions from $2$-to-$1$ mappings of\nthis form. Our findings generalize and unify many results in the literature.",
        "Both simulations and observations indicate that the so-called missing baryons\nreside in the intergalactic medium (IGM) known as the warm-hot intergalactic\nmedium (WHIM). In this article, we demonstrate that turbulence in the cosmic\nbaryonic fluid is crucial for correctly understanding both the spatial\ndistribution and the physical origins of the missing baryons in the universe.\nFirst, we find that dynamical effects cause the gas to be detained in\nlow-density and intermediate-density regions, resulting in high baryon\nfractions, while prevent the inflow of the gas in high-density regions, leading\nto low baryon fractions. Second, turbulent energy is converted into thermal\nenergy, and the injection and dissipation of turbulent energy have essentially\nreached a balance from $z=1$ to $0$. This indicates that the cosmic fluid is in\na state of fully-developed turbulence within this redshift range. Due to\nturbulent heating, as redshift decreases, an increasing amount of warm gas is\nheated and transitions into the WHIM, and some even into hot gas.",
        "The widespread adoption of generative AI has generated diverse opinions, with\nindividuals expressing both support and criticism of its applications. This\nstudy investigates the emotional dynamics surrounding generative AI by\nanalyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and\nLLMs. To further understand the emotional intelligence of ChatGPT, we examine\nits responses to selected tweets, highlighting differences in sentiment between\nhuman comments and LLM-generated responses. We introduce EmoXpt, a sentiment\nanalysis framework designed to assess both human perspectives on generative AI\nand the sentiment embedded in ChatGPT's responses. Unlike prior studies that\nfocus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional\nexpression of ChatGPT. Experimental results demonstrate that LLM-generated\nresponses are notably more efficient, cohesive, and consistently positive than\nhuman responses.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "We study the dynamics of electrons in crystalline solids in the presence of\ninhomogeneous external electric and magnetic fields. We present a manifestly\ngauge-invariant operator-based approach without relying on a semiclassical\nwavepacket construction, and derive the field-induced corrections to the\nequations of motion at the operator level. This includes the Berry curvature\ninduced anomalous velocity and contributions arising from the quantum geometry\nof the Bloch bands. We show explicitly how these multi-band effects are\nmanifested in an effective single band approximation. We present a formalism\nthat allows for a systematic expansion to an arbitrary order in the\ninhomogeneity of the applied fields, as well as a way to compute the matrix\nelements in Bloch basis."
      ]
    }
  },
  {
    "id":"2411.06785",
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Loss Landscape Analysis for Reliable Quantized ML Models for Scientific\n  Sensing",
        "RIGNO: A Graph-based framework for robust and accurate operator learning\n  for PDEs on arbitrary domains",
        "Transformed Low-rank Adaptation via Tensor Decomposition and Its\n  Applications to Text-to-image Models",
        "InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse\n  Problems in Physical Sciences",
        "Error Slice Discovery via Manifold Compactness",
        "Uncertainty-Aware Graph Structure Learning",
        "A Generative Model Enhanced Multi-Agent Reinforcement Learning Method\n  for Electric Vehicle Charging Navigation",
        "TabularARGN: A Flexible and Efficient Auto-Regressive Framework for\n  Generating High-Fidelity Synthetic Data",
        "Low-Rank Adapting Models for Sparse Autoencoders",
        "Adaptive UAV-Assisted Hierarchical Federated Learning: Optimizing\n  Energy, Latency, and Resilience for Dynamic Smart IoT",
        "No Task Left Behind: Isotropic Model Merging with Common and\n  Task-Specific Subspaces",
        "AlphaNet: Scaling Up Local Frame-based Atomistic Interatomic Potential",
        "Interpretable Rules for Online Failure Prediction: A Case Study on the\n  Metro do Porto dataset",
        "Condensation of lighter-than-physical pions in QCD",
        "The Cygnus Allscale Survey of Chemistry and Dynamical Environments:\n  CASCADE. IV. Unveiling the hidden structures in DR18",
        "Gravitational Vacuum Condensate Stars in the Effective Theory of Gravity",
        "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
        "Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning",
        "Polynomial algebra from the Lie algebra reduction chain\n  $\\mathfrak{su}(4) \\supset \\mathfrak{su}(2) \\times \\mathfrak{su}(2)$: The\n  supermultiplet model",
        "Asteroseismology of the red giant companions to Gaia BH2 and BH3",
        "Unification of stochastic matrices and quantum operations for N-level\n  systems",
        "Asymptotics for EBLUPs within crossed mixed effect models",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Transport Characteristics and Modelling of ST40 Hot Ion Plasmas",
        "Robust Body Composition Analysis by Generating 3D CT Volumes from\n  Limited 2D Slices",
        "A Doubly-Dispersive MIMO Channel Model Parametrized with Stacked\n  Intelligent Metasurfaces",
        "GraphRAG under Fire",
        "3D\/2D Registration of Angiograms using Silhouette-based Differentiable\n  Rendering"
      ],
      "abstract":[
        "In this paper, we propose a method to perform empirical analysis of the loss\nlandscape of machine learning (ML) models. The method is applied to two ML\nmodels for scientific sensing, which necessitates quantization to be deployed\nand are subject to noise and perturbations due to experimental conditions. Our\nmethod allows assessing the robustness of ML models to such effects as a\nfunction of quantization precision and under different regularization\ntechniques -- two crucial concerns that remained underexplored so far. By\ninvestigating the interplay between performance, efficiency, and robustness by\nmeans of loss landscape analysis, we both established a strong correlation\nbetween gently-shaped landscapes and robustness to input and weight\nperturbations and observed other intriguing and non-obvious phenomena. Our\nmethod allows a systematic exploration of such trade-offs a priori, i.e.,\nwithout training and testing multiple models, leading to more efficient\ndevelopment workflows. This work also highlights the importance of\nincorporating robustness into the Pareto optimization of ML models, enabling\nmore reliable and adaptive scientific sensing systems.",
        "Learning the solution operators of PDEs on arbitrary domains is challenging\ndue to the diversity of possible domain shapes, in addition to the often\nintricate underlying physics. We propose an end-to-end graph neural network\n(GNN) based neural operator to learn PDE solution operators from data on point\nclouds in arbitrary domains. Our multi-scale model maps data between\ninput\/output point clouds by passing it through a downsampled regional mesh.\nMany novel elements are also incorporated to ensure resolution invariance and\ntemporal continuity. Our model, termed RIGNO, is tested on a challenging suite\nof benchmarks, composed of various time-dependent and steady PDEs defined on a\ndiverse set of domains. We demonstrate that RIGNO is significantly more\naccurate than neural operator baselines and robustly generalizes to unseen\nspatial resolutions and time instances.",
        "Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an\nincreasingly popular technique with many applications. Among the various PEFT\nmethods, Low-Rank Adaptation (LoRA) and its variants have gained significant\nattention due to their effectiveness, enabling users to fine-tune models with\nlimited computational resources. However, the approximation gap between the\nlow-rank assumption and desired fine-tuning weights prevents the simultaneous\nacquisition of ultra-parameter-efficiency and better performance. To reduce\nthis gap and further improve the power of LoRA, we propose a new PEFT method\nthat combines two classes of adaptations, namely, transform and residual\nadaptations. In specific, we first apply a full-rank and dense transform to the\npre-trained weight. This learnable transform is expected to align the\npre-trained weight as closely as possible to the desired weight, thereby\nreducing the rank of the residual weight. Then, the residual part can be\neffectively approximated by more compact and parameter-efficient structures,\nwith a smaller approximation error. To achieve ultra-parameter-efficiency in\npractice, we design highly flexible and effective tensor decompositions for\nboth the transform and residual adaptations. Additionally, popular PEFT methods\nsuch as DoRA can be summarized under this transform plus residual adaptation\nscheme. Experiments are conducted on fine-tuning Stable Diffusion models in\nsubject-driven and controllable generation. The results manifest that our\nmethod can achieve better performances and parameter efficiency compared to\nLoRA and several baselines.",
        "Plug-and-play diffusion priors (PnPDP) have emerged as a promising research\ndirection for solving inverse problems.\n  However, current studies primarily focus on natural image restoration,\nleaving the performance of these algorithms in scientific inverse problems\nlargely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a\nframework that evaluates diffusion models across five distinct scientific\ninverse problems. These problems present unique structural challenges that\ndiffer from existing benchmarks, arising from critical scientific applications\nsuch as optical tomography, medical imaging, black hole imaging, seismology,\nand fluid dynamics. With \\textsc{InverseBench}, we benchmark 14 inverse problem\nalgorithms that use plug-and-play diffusion priors against strong,\ndomain-specific baselines, offering valuable new insights into the strengths\nand weaknesses of existing algorithms. To facilitate further research and\ndevelopment, we open-source the codebase, along with datasets and pre-trained\nmodels, at https:\/\/devzhk.github.io\/InverseBench\/.",
        "Despite the great performance of deep learning models in many areas, they\nstill make mistakes and underperform on certain subsets of data, i.e. error\nslices. Given a trained model, it is important to identify its semantically\ncoherent error slices that are easy to interpret, which is referred to as the\nerror slice discovery problem. However, there is no proper metric of slice\ncoherence without relying on extra information like predefined slice labels.\nCurrent evaluation of slice coherence requires access to predefined slices\nformulated by metadata like attributes or subclasses. Its validity heavily\nrelies on the quality and abundance of metadata, where some possible patterns\ncould be ignored. Besides, current algorithms cannot directly incorporate the\nconstraint of coherence into their optimization objective due to the absence of\nan explicit coherence metric, which could potentially hinder their\neffectiveness. In this paper, we propose manifold compactness, a coherence\nmetric without reliance on extra information by incorporating the data geometry\nproperty into its design, and experiments on typical datasets empirically\nvalidate the rationality of the metric. Then we develop Manifold Compactness\nbased error Slice Discovery (MCSD), a novel algorithm that directly treats risk\nand coherence as the optimization objective, and is flexible to be applied to\nmodels of various tasks. Extensive experiments on the benchmark and case\nstudies on other typical datasets demonstrate the superiority of MCSD.",
        "Graph Neural Networks (GNNs) have become a prominent approach for learning\nfrom graph-structured data. However, their effectiveness can be significantly\ncompromised when the graph structure is suboptimal. To address this issue,\nGraph Structure Learning (GSL) has emerged as a promising technique that\nrefines node connections adaptively. Nevertheless, we identify two key\nlimitations in existing GSL methods: 1) Most methods primarily focus on node\nsimilarity to construct relationships, while overlooking the quality of node\ninformation. Blindly connecting low-quality nodes and aggregating their\nambiguous information can degrade the performance of other nodes. 2) The\nconstructed graph structures are often constrained to be symmetric, which may\nlimit the model's flexibility and effectiveness. To overcome these limitations,\nwe propose an Uncertainty-aware Graph Structure Learning (UnGSL) strategy.\nUnGSL estimates the uncertainty of node information and utilizes it to adjust\nthe strength of directional connections, where the influence of nodes with high\nuncertainty is adaptively reduced. Importantly, UnGSL serves as a plug-in\nmodule that can be seamlessly integrated into existing GSL methods with minimal\nadditional computational cost. In our experiments, we implement UnGSL into six\nrepresentative GSL methods, demonstrating consistent performance improvements.",
        "With the widespread adoption of electric vehicles (EVs), navigating for EV\ndrivers to select a cost-effective charging station has become an important yet\nchallenging issue due to dynamic traffic conditions, fluctuating electricity\nprices, and potential competition from other EVs. The state-of-the-art deep\nreinforcement learning (DRL) algorithms for solving this task still require\nglobal information about all EVs at the execution stage, which not only\nincreases communication costs but also raises privacy issues among EV drivers.\nTo overcome these drawbacks, we introduce a novel generative model-enhanced\nmulti-agent DRL algorithm that utilizes only the EV's local information while\nachieving performance comparable to these state-of-the-art algorithms.\nSpecifically, the policy network is implemented on the EV side, and a\nConditional Variational Autoencoder-Long Short Term Memory (CVAE-LSTM)-based\nrecommendation model is developed to provide recommendation information.\nFurthermore, a novel future charging competition encoder is designed to\neffectively compress global information, enhancing training performance. The\nmulti-gradient descent algorithm (MGDA) is also utilized to adaptively balance\nthe weight between the two parts of the training objective, resulting in a more\nstable training process. Simulations are conducted based on a practical area in\nXi\\'an, China. Experimental results show that our proposed algorithm, which\nrelies on local information, outperforms existing local information-based\nmethods and achieves less than 8\\% performance loss compared to global\ninformation-based methods.",
        "Synthetic data generation for tabular datasets must balance fidelity,\nefficiency, and versatility to meet the demands of real-world applications. We\nintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), a\nflexible framework designed to handle mixed-type, multivariate, and sequential\ndatasets. By training on all possible conditional probabilities, TabularARGN\nsupports advanced features such as fairness-aware generation, imputation, and\nconditional generation on any subset of columns. The framework achieves\nstate-of-the-art synthetic data quality while significantly reducing training\nand inference times, making it ideal for large-scale datasets with diverse\nstructures. Evaluated across established benchmarks, including realistic\ndatasets with complex relationships, TabularARGN demonstrates its capability to\nsynthesize high-quality data efficiently. By unifying flexibility and\nperformance, this framework paves the way for practical synthetic data\ngeneration across industries.",
        "Sparse autoencoders (SAEs) decompose language model representations into a\nsparse set of linear latent vectors. Recent works have improved SAEs using\nlanguage model gradients, but these techniques require many expensive backward\npasses during training and still cause a significant increase in cross entropy\nloss when SAE reconstructions are inserted into the model. In this work, we\nimprove on these limitations by taking a fundamentally different approach: we\nuse low-rank adaptation (LoRA) to finetune the language model itself around a\npreviously trained SAE. We analyze our method across SAE sparsity, SAE width,\nlanguage model size, LoRA rank, and model layer on the Gemma Scope family of\nSAEs. In these settings, our method reduces the cross entropy loss gap by 30%\nto 55% when SAEs are inserted during the forward pass. We also find that\ncompared to end-to-end (e2e) SAEs, our approach achieves the same downstream\ncross entropy loss 3$\\times$ to 20$\\times$ faster on Gemma-2-2B and 2$\\times$\nto 10$\\times$ faster on Llama-3.2-1B. We further show that our technique\nimproves downstream metrics and can adapt multiple SAEs at once. Our results\ndemonstrate that improving model interpretability is not limited to post-hoc\nSAE training; Pareto improvements can also be achieved by directly optimizing\nthe model itself.",
        "A key application of HFL lies in smart Internet of Things (IoT) systems,\nincluding remote monitoring and battlefield operations, where cellular\nconnectivity is often unavailable. In such scenarios, UAVs can act as mobile\naggregators, dynamically providing connectivity to terrestrial IoT devices.\nSubsequently, this paper investigates an HFL architecture enabled by\nenergy-constrained, dynamically deployed UAVs that are susceptible to\ncommunication disruptions. We propose a novel approach to minimize global\ntraining costs in such environments by formulating a joint optimization problem\nthat integrates learning configuration, bandwidth allocation, and IoT\ndevice-to-UAV association, ensuring timely global aggregation before UAV\ndisconnections and redeployments. The problem explicitly captures the dynamic\nnature of IoT devices and their intermittent connectivity to UAVs and is shown\nto be NP-hard. To address its complexity, we decompose the problem into three\ninterrelated subproblems. First, we optimize learning configuration and\nbandwidth allocation using an augmented Lagrangian function to reduce training\ncosts. Second, we introduce a device fitness score that accounts for data\nheterogeneity (via Kullback-Leibler divergence), device-to-UAV proximity, and\ncomputational resources, leveraging a Twin Delayed Deep Deterministic Policy\nGradient (TD3)-based algorithm for adaptive device-to-UAV assignment. Third, we\ndevelop a low-complexity two-stage greedy strategy for UAV redeployment and\nglobal aggregator selection, ensuring efficient model aggregation despite UAV\ndisconnections.",
        "Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance across multiple scenarios, including\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps:\/\/github.com\/danielm1405\/iso-merging .",
        "Molecular dynamics simulations demand unprecedented accuracy and scalability\nto tackle grand challenges in energy materials, catalytic processes, and\nbiomolecular design. To bridge this gap, we present AlphaNet, a local\nframe-based equivariant model that simultaneously advances computational\nefficiency and predictive precision for atomistic systems. By constructing\nequivariant local frames with learnable geometric transitions, AlphaNet encodes\natomic environments with enhanced representational capacity, achieving state of\nthe art accuracy in energy and force predictions. Extensive benchmarks spanning\ndefected graphene, formate decomposition, inorganic bulks, and large-scale\ndatasets (OC2M and Matbench Discovery) demonstrate its superior performance\nover existing neural network interatomic potentials while ensuring scalability\nacross diverse system sizes. The synergy of accuracy, efficiency, and\ntransferability positions AlphaNet as a transformative tool for simulating\nmultiscale phenomena, from catalyst dynamics to energy storage interfaces, with\ndirect implications for accelerating the discovery of functional materials and\ncomplex molecular systems.",
        "Due to their high predictive performance, predictive maintenance applications\nhave increasingly been approached with Deep Learning techniques in recent\nyears. However, as in other real-world application scenarios, the need for\nexplainability is often stated but not sufficiently addressed. This study will\nfocus on predicting failures on Metro trains in Porto, Portugal. While recent\nworks have found high-performing deep neural network architectures that feature\na parallel explainability pipeline, the generated explanations are fairly\ncomplicated and need help explaining why the failures are happening. This work\nproposes a simple online rule-based explainability approach with interpretable\nfeatures that leads to straightforward, interpretable rules. We showcase our\napproach on MetroPT2 and find that three specific sensors on the Metro do Porto\ntrains suffice to predict the failures present in the dataset with simple\nrules.",
        "We report on the results of the 2+1 flavour QCD simulations at nonzero\nisospin chemical potential performed at half the physical light quark mass. At\nlow temperatures and large isospin chemical potential Bose-Einstein\nCondensation (BEC) occurs, creating a pion condensed phase, separated from the\nhadronic and quark-gluon plasma phases by the BEC transition line. For physical\nquark masses, the section of this line between the hadronic and BEC phases was\nfound to be almost perfectly vertical, i.e. aligned with the temperature axis.\nWe show that for lighter than physical pions, this section remains vertical,\nand approaches the axis of vanishing chemical potential linearly with the pion\nmass, giving a prediction of the phase diagram in the chiral limit.",
        "The Cygnus-X complex is a massive, nearby (1.4 kpc) star-forming region with\nseveral OB associations. As part of the Cygnus Allscale Survey of Chemistry and\nDynamical Environments (CASCADE) program, we carried out 3.6 millimeter (mm)\ncontinuum and spectral line high-resolution observations ($\\sim$ 3 - 4$''$)\ntoward DR18, covering several molecular species with the Northern Extended\nMillimeter Array (NOEMA) and the Institut de Radioastronomie Millim\\'etrique\n(IRAM) 30m telescope. In addition, multi-wavelength archival datasets were used\nto provide a comprehensive analysis of the region. A comparison of the 3.6mm\nand 6 cm continuum emission confirms that a B2 star (DR18-05) shapes the\ncometary HII region in the DR18 cavity, with ionized gas escaping toward the\nOB2 association. On the other hand, the extended 3.6mm and 6 cm continuum\nemission are likely to trace photoevaporating ionized gas from ultraviolet\nradiation from the Cyg OB2 association, not from DR18-05. The shell structure\naround DR18-05 indicates photodissociation regions (PDRs) formed by the\nexpanding HII region and photo-erosion from DR18-05 and OB2 stars. We also\nidentified 18 compact cores with N$_2$H$^+$ emission, half of which are\ngravitationally bound and mostly located in colder regions behind the PDRs. The\nSiO emission is found only in PDRs, with narrow-line widths ( 0.8 - 2.0 km\ns$^{-1}$) and lower abundances (X(SiO) $\\sim$ 5$\\times$10$^{-11}$ -\n1$\\times$10$^{-10}$). Comparing with the UV irradiated shock models, we suggest\nthat the SiO emission partially encompassing the HII region arises from the\nmolecular gas region, marginally compressed by low-velocity shocks with $\\sim$\n5 km s$^{-1}$, irradiated by external UV radiation (G$_{\\rm 0} \\sim 10^{2} -\n10^{3}$), as they traverse through a medium with $n_{\\rm H} \\sim 10^{4}$ to\n10$^5$ cm$^{-3}$.",
        "The low energy effective theory of gravity comprises two elements of quantum\ntheory joined to classical general relativity. The first is the quantum\nconformal anomaly, which is responsible for macroscopic correlations on light\ncones and a stress tensor that can strongly modify the classical geometry at\nblack hole horizons. The second is the formulation of vacuum energy as\n$\\Lambda_{\\rm eff}\\!\\propto\\! F^2$ in terms of an exact $4$-form abelian gauge\nfield strength $F\\!=\\!dA$. When $A$ is identified with the Chern-Simons\n$3$-form of the Euler class, defined in terms of the spin connection, a $J\\cdot\nA$ interaction is generated by the conformal anomaly of massless fermions. Due\nto the extreme blueshifting of local frequencies in the near-horizon region of\na `black hole,' the lightest fermions of the Standard Model can be treated as\nmassless there, contributing to the anomaly and providing a $3$-current source\n$J$ for the `Maxwell' equation $d\\ast F = \\ast J$. In this phase boundary\nregion, torsion is activated, and $F$ can change rapidly. The Schwarzschild\nblack hole horizon is thereby replaced by a surface, with a positive surface\ntension and $\\mathbb{R}\\otimes \\mathbb{S}^2$ worldtube topology, separating\nregions of differing vacuum energy. The result is a gravitational vacuum\ncondensate star, a cold, compact, horizonless object with a $p_{_V}\\!=\\! -\n\\rho_{_V}$ zero entropy, non-singular de Sitter interior and thin quantum phase\nboundary layer at the Schwarzschild radius $2GM\/c^2$.",
        "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}\/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
        "Ultrahigh field (UHF) Magnetic Resonance Imaging (MRI) provides a high\nsignal-to-noise ratio (SNR), enabling exceptional spatial resolution for\nclinical diagnostics and research. However, higher fields introduce challenges\nsuch as transmit radiofrequency (RF) field inhomogeneities, which result in\nuneven flip angles and image intensity artifacts. These artifacts degrade image\nquality and limit clinical adoption. Traditional RF shimming methods, including\nMagnitude Least Squares (MLS) optimization, mitigate RF field inhomogeneity but\nare time-intensive and often require the presence of the patient. Recent\nmachine learning methods, such as RF Shim Prediction by Iteratively Projected\nRidge Regression and other deep learning architectures, offer alternative\napproaches but face challenges such as extensive training requirements, limited\ncomplexity, and practical data constraints. This paper introduces a holistic\nlearning-based framework called Fast RF Shimming, which achieves a 5000-fold\nspeedup compared to MLS methods. First, random-initialized Adaptive Moment\nEstimation (Adam) derives reference shimming weights from multichannel RF\nfields. Next, a Residual Network (ResNet) maps RF fields to shimming outputs\nwhile incorporating a confidence parameter into the loss function. Finally, a\nNon-uniformity Field Detector (NFD) identifies extreme non-uniform outcomes.\nComparative evaluations demonstrate significant improvements in both speed and\npredictive accuracy. The proposed pipeline also supports potential extensions,\nsuch as the integration of anatomical priors or multi-echo data, to enhance the\nrobustness of RF field correction. This approach offers a faster and more\nefficient solution to RF shimming challenges in UHF MRI.",
        "The supermultiplet model, based on the reduction chain $\\mathfrak{su}(4)\n\\supset \\mathfrak{su}(2) \\times \\mathfrak{su}(2)$, is revisited through the\nlens of commutants within universal enveloping algebras of Lie algebras. From\nthis analysis, a collection of twenty polynomials up to degree nine emerges\nfrom the commutant associated with the $\\mathfrak{su}(2) \\times\n\\mathfrak{su}(2)$ subalgebra. This study is conducted in the Poisson\n(commutative) framework using the Lie-Poisson bracket associated with the dual\nof the Lie algebra under consideration. As the main result, we obtain the\npolynomial Poisson algebra generated by these twenty linearly independent and\nindecomposable polynomials, with five elements being central. This incorporates\npolynomial expansions up to degree seventeen in the Lie algebra generators. We\nfurther discuss additional algebraic relations among these polynomials,\nexplicitly detailing some of the lower-order ones. As a byproduct of these\nresults, we also show that the recently introduced 'grading method' turns out\nto be essential for deriving the Poisson bracket relations when the degree of\nthe expansions becomes so high that standard approaches are no longer\napplicable, due to computational limitations. These findings represent a\nfurther step toward the systematic exploration of polynomial algebras relevant\nto nuclear models.",
        "The stellar companions in the binary black hole systems Gaia BH2 and BH3,\nboth of which are $\\alpha$-enhanced red giant branch stars, are expected to\nshow normal modes with the characteristic signature of convectively-driven\nsolar-like oscillations. We investigate this using photometry from the TESS\nmission and find such a signal for Gaia BH2. For Gaia BH2, we measure a power\nexcess frequency of $\\nu_{\\rm max}=60.15\\pm0.57$ $\\mu$Hz and a large separation\nof $\\Delta\\nu=5.99\\pm0.03$ $\\mu$Hz, yielding a mass of $1.19^{+0.08}_{-0.08}$\nM$_\\odot$, which is in agreement with spectroscopically derived parameters.\nSeismic modeling favors an age for the red giant of $5.03^{+2.58}_{-3.05}$ Gyr,\nstrongly suggesting that it is a young, $\\alpha$-enriched giant star, which are\nthought to arise from a binary accretion or merger scenario. Ground-based\nphotometry of Gaia BH2 spanning 8 years indicates a photometric period of\n$398\\pm5$ d, which we tentatively attribute to rotation. If this rotation is\nphysical, it can not be explained solely by evolutionary spin-down or magnetic\nbraking, and implies that the red giant underwent some tidal forcing mechanism.\nSuggestively, this period is close to the pseudo-synchronous spin period of\nP$_\\text{spin}=428\\pm1$ days derived from the binary orbit. For Gaia BH3, we\nare unable to identify an asteroseismic signal in the TESS data despite\npredicting that the amplitude of the signal should lie well above the measured\nnoise level. We discuss a number of scenarios for why this signal may not be\nvisible.",
        "The time evolution of the one-point probability distribution of stochastic\nprocesses and quantum processes for $N$-level systems has been unified. Hence,\nquantum states and quantum operations can be regarded as generalizations of the\none-point probability vectors and stochastic matrices, respectively. It has\nalso been proven that completely positive divisibility (CP-divisibility) for\nquantum operations is the natural extension of the Chapman-Kolmogorov equation.\nIt is thus shown that CP-divisibility is a necessary but insufficient condition\nfor a quantum process to be specified as Markovian. The main results have been\nillustrated through a dichotomic Markov process.",
        "In this article, we derive the joint asymptotic distribution of empirical\nbest linear unbiased predictors (EBLUPs) for individual and cell-level random\neffects in a crossed mixed effect model. Under mild conditions (which include\nmoment conditions instead of normality for the random effects and model\nerrors), we demonstrate that as the sizes of rows, columns, and, when we\ninclude interactions, cells simultaneously increase to infinity, the\ndistribution of the differences between the EBLUPs and the random effects\nsatisfy central limit theorems. These central limit theorems mean the EBLUPs\nasymptotically follow the convolution of the true random effect distribution\nand a normal distribution. Moreover, our results enable simple asymptotic\napproximations and estimators for the mean squared error (MSE) of the EBLUPs,\nwhich in turn facilitates the construction of asymptotic prediction intervals\nfor the unobserved random effects. We show in simulations that our simple\nestimator of the MSE of the EBLUPs works very well in finite samples. Finally,\nwe illustrate the use of the asymptotic prediction intervals with an analysis\nof movie rating data.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "In this paper, the turbulent transport properties of ST40 hot ion plasmas are\nexamined and fully predictive time evolving modelling of a hot ion plasma pulse\nwas performed. Understanding turbulent transport on spherical tokamaks (STs) is\nchallenging due to their unique geometry characteristics. ST40 hot ion plasmas\nare typically unstable to ion scale Trapped Electron Modes (TEMs) and\nUbiquitous Modes (UMs), driven from the kinetic response of trapped particles\nand passing ions, and electron scale Electron Temperature Gradient Modes (ETGs)\nat the edge of the plasma. A comparison between the linear unstable modes of\nthe gyro-kinetic code GS2 and the gyro-fluid code TGLF showed that both models\nagree to a satisfactory level. However, some discrepancy was observed at the\ncore of the plasma where a large fraction of beams ions exists, and\nelectromagnetic effects are potentially important. Turbulent fluxes were also\nobserved to be somewhat overpredicted with TGLF. The core heat ion transport is\nobserved to be close to neoclassical levels due to turbulence suppression from\nhigh rotation and fast ion stabilisation, while the edge region is dominated by\nanomalous transport in both ions and electrons. As a result, enhanced energy\nconfinement is observed in those plasmas driven by the reduced turbulent core\nregion and the confined beam ions. Fully predictive simulations using the ASTRA\ntransport solver coupled with SPIDER, NUBEAM, NCLASS and TGLF together with a\nnovel reduced scrape of layer (SOL) model for the simulation of the last closed\nflux surface (LCFS) boundary conditions was attempted. Agreement in global\nquantities but also kinetic profiles between the predictive and interpretative\nmodelling as well as experimental measurements was observed.",
        "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%.",
        "Introduced with the advent of statistical wireless channel models for high\nmobility communications and having a profound role in communication-centric\n(CC) integrated sensing and communications (ISAC), the doubly-dispersive (DD)\nchannel structure has long been heralded as a useful tool enabling the capture\nof the most important fading effects undergone by an arbitrary time-domain\ntransmit signal propagating through some medium. However, the incorporation of\nthis model into multiple-input multiple-output (MIMO) system setups, relying on\nthe recent paradigm-shifting transceiver architecture based on stacked\nintelligent metasurfaces (SIM), in an environment with reconfigurable\nintelligent surfaces (RISs) remains an open problem due to the many intricate\ndetails that have to be accounted for. In this paper, we fill this gap by\nintroducing a novel DD MIMO channel model that incorporates an arbitrary number\nof RISs in the ambient, as well as SIMs equipping both the transmitter and\nreceiver. We then discuss how the proposed metasurfaces-parametrized DD (MPDD)\nchannel model can be seamlessly applied to waveforms that are known to perform\nwell in DD environments, namely, orthogonal frequency division multiplexing\n(OFDM), orthogonal time frequency space (OTFS), and affine frequency division\nmultiplexing (AFDM), with each having their own inherent advantages and\ndisadvantages. An illustrative application of the programmable functionality of\nthe proposed model is finally presented to showcase its potential for boosting\nthe performance of the aforementioned waveforms. Our numerical results indicate\nthat the design of waveforms suitable to mitigating the effects of DD channels\nis significantly impacted by the emerging SIM technology.",
        "GraphRAG advances retrieval-augmented generation (RAG) by structuring\nexternal knowledge as multi-scale knowledge graphs, enabling language models to\nintegrate both broad context and granular details in their reasoning. While\nGraphRAG has demonstrated success across domains, its security implications\nremain largely unexplored. To bridge this gap, this work examines GraphRAG's\nvulnerability to poisoning attacks, uncovering an intriguing security paradox:\ncompared to conventional RAG, GraphRAG's graph-based indexing and retrieval\nenhance resilience against simple poisoning attacks; meanwhile, the same\nfeatures also create new attack surfaces. We present GRAGPoison, a novel attack\nthat exploits shared relations in the knowledge graph to craft poisoning text\ncapable of compromising multiple queries simultaneously. GRAGPoison employs\nthree key strategies: i) relation injection to introduce false knowledge, ii)\nrelation enhancement to amplify poisoning influence, and iii) narrative\ngeneration to embed malicious content within coherent text. Empirical\nevaluation across diverse datasets and models shows that GRAGPoison\nsubstantially outperforms existing attacks in terms of effectiveness (up to 98%\nsuccess rate) and scalability (using less than 68% poisoning text). We also\nexplore potential defensive measures and their limitations, identifying\npromising directions for future research.",
        "We present a method for 3D\/2D registration of Digital Subtraction Angiography\n(DSA) images to provide valuable insight into brain hemodynamics and\nangioarchitecture. Our approach formulates the registration as a pose\nestimation problem, leveraging both anteroposterior and lateral DSA views and\nemploying differentiable rendering. Preliminary experiments on real and\nsynthetic datasets demonstrate the effectiveness of our method, with both\nqualitative and quantitative evaluations highlighting its potential for\nclinical applications. The code is available at\nhttps:\/\/github.com\/taewoonglee17\/TwoViewsDSAReg."
      ]
    }
  },
  {
    "id":"2411.06785",
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT. Code at \\url{https:\/\/github.com\/Ma-Lab-Berkeley\/CRATE}.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "CoverM: Read alignment statistics for metagenomics",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Causes of evolutionary divergence in prostate cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow\n  in Shallow Linear Networks",
        "Selecting Optimal Sampling Rate for Stable Super-Resolution",
        "A Time-Resolved High-Resolution Spectroscopic Analysis of Ionized\n  Calcium and Dynamical Processes in the Ultra-Hot Jupiter HAT-P-70 b",
        "Determining the Density of the Sun with Neutrinos",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Covering half-grids with lines and planes",
        "Non-collapsing volume estimate for local K\\\"ahler metrics in big\n  cohomology classes",
        "Multi-view biclustering via non-negative matrix tri-factorisation",
        "Robust high-order low-rank BUG integrators based on explicit Runge-Kutta\n  methods",
        "Effective range expansion with the left-hand cut and its application to\n  the $T_{cc}(3875)$",
        "Efficient LP warmstarting for linear modifications of the constraint\n  matrix",
        "Quantum Characterization, Verification, and Validation",
        "Incorporating Backreaction in One-Loop Corrections in Ultra-Slow-Roll\n  Inflation",
        "Precise constraint on properties of neutron stars through new universal\n  relations and astronomical observations",
        "Uniform mean estimation via generic chaining"
      ],
      "abstract":[
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "We study the gradient descent (GD) dynamics of a depth-2 linear neural\nnetwork with a single input and output. We show that GD converges at an\nexplicit linear rate to a global minimum of the training loss, even with a\nlarge stepsize -- about $2\/\\textrm{sharpness}$. It still converges for even\nlarger stepsizes, but may do so very slowly. We also characterize the solution\nto which GD converges, which has lower norm and sharpness than the gradient\nflow solution. Our analysis reveals a trade off between the speed of\nconvergence and the magnitude of implicit regularization. This sheds light on\nthe benefits of training at the ``Edge of Stability'', which induces additional\nregularization by delaying convergence and may have implications for training\nmore complex models.",
        "We investigate the recovery of nodes and amplitudes from noisy frequency\nsamples in spike train signals, also known as the super-resolution (SR)\nproblem. When the node separation falls below the Rayleigh limit, the problem\nbecomes ill-conditioned. Admissible sampling rates, or decimation parameters,\nimprove the conditioning of the SR problem, enabling more accurate recovery. We\npropose an efficient preprocessing method to identify the optimal sampling\nrate, significantly enhancing the performance of SR techniques.",
        "We present the first transmission spectroscopy study of an exoplanet\natmosphere with the high-resolution mode of the new Gemini High-resolution\nOptical SpecTrograph (GHOST) instrument at the Gemini South Observatory. We\nobserved one transit of HAT-P-70 b - an ultra-hot Jupiter with an inflated\nradius - and made a new detection of the infrared Ca II triplet in its\ntransmission spectrum. The depth of the strongest line implies that a\nsubstantial amount of Ca II extends to at least 47% above the bulk planetary\nradius. The triplet lines are blueshifted between ~ 3 to 5 km\/s, indicative of\nstrong dayside-to-nightside winds common on highly irradiated gas giants.\nComparing the transmission spectrum with atmospheric models that incorporate\nnon-local thermodynamic equilibrium effects suggests that the planetary mass is\nlikely between 1 to 2 $M_{\\rm J}$, much lighter than the upper limit previously\nderived from radial velocity measurements. Importantly, thanks to the the high\nsignal-to-noise ratio achieved by GHOST\/Gemini South, we are able to measure\nthe temporal variation of these signals. Absorption depths and velocity offsets\nof the individual Ca II lines remain mostly consistent across the transit,\nexcept for the egress phases, where weaker absorption and stronger blueshifts\nare observed, highlighting the atmospheric processes within the trailing limb\nalone. Our study demonstrates the ability of GHOST to make time-resolved\ndetections of individual spectral lines, providing valuable insights into the\n3D nature of exoplanet atmospheres by probing different planetary longitudes as\nthe tidally locked planet rotates during the transit.",
        "The discovery of solar neutrinos confirmed that the inner workings of the Sun\ngenerally match our theoretical understanding of the fusion process. Solar\nneutrinos have also played a role in discovering that neutrinos have mass and\nthat they oscillate. We combine the latest solar neutrino data along with other\noscillation data from reactors to determine the Sun's density profile. We\nderive constraints given the current data and show the anticipated improvements\nwith more reactor neutrino data from JUNO constraining the true oscillation\nparameters and more solar neutrino data from DUNE which should provide a\ncrucial measurement of $hep$ neutrinos.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "We study hyperplane covering problems for finite grid-like structures in\n$\\mathbb{R}^d$. We call a set $\\mathcal{C}$ of points in $\\mathbb{R}^2$ a\nconical grid if the line $y = a_i$ intersects $\\mathcal{C}$ in exactly $i$\npoints, for some $a_1 > \\cdots > a_n \\in \\mathbb{R}$. We prove that the number\nof lines required to cover every point of such a grid at least $k$ times is at\nleast $nk\\left(1-\\frac{1}{e}-O(\\frac{1}{n}) \\right)$. If the grid $\\mathcal{C}$\nis obtained by cutting an $m \\times n$ grid of points into a half along one of\nthe diagonals, then we prove the lower bound of\n$mk\\left(1-e^{-\\frac{n}{m}}-O(\\frac{n}{m^2})\\right)$.\n  Motivated by the Alon-F\\\"uredi theorem on hyperplane coverings of grids that\nmiss a point and its multiplicity variations, we study the problem of finding\nthe minimum number of hyperplanes required to cover every point of an $n \\times\n\\cdots \\times n$ half-grid in $\\mathbb{R}^d$ at least $k$ times while missing a\npoint $P$. For almost all such half-grids, with $P$ being the corner point, we\nprove asymptotically sharp upper and lower bounds for the covering number in\ndimensions $2$ and $3$. For $k = 1$, $d = 2$, and an arbitrary $P$, we\ndetermine this number exactly by using the polynomial method bound for grids.",
        "We prove a uniform local non-collapsing volume estimate for a large family of\nK\\\"ahler metrics in the big cohomology classes. The key ingredient is a\ngeneralization of a mixed energy estimate for functions in the complex Sobolev\nspace to the setting of big cohomology classes.",
        "Multi-view data is ever more apparent as methods for production, collection\nand storage of data become more feasible both practically and fiscally.\nHowever, not all features are relevant to describe the patterns for all\nindividuals. Multi-view biclustering aims to simultaneously cluster both rows\nand columns, discovering clusters of rows as well as their view-specific\nidentifying features. A novel multi-view biclustering approach based on\nnon-negative matrix factorisation is proposed (ResNMTF). Demonstrated through\nextensive experiments on both synthetic and real datasets, ResNMTF successfully\nidentifies both overlapping and non-exhaustive biclusters, without pre-existing\nknowledge of the number of biclusters present, and is able to incorporate any\ncombination of shared dimensions across views. Further, to address the lack of\na suitable bicluster-specific intrinsic measure, the popular silhouette score\nis extended to the bisilhouette score. The bisilhouette score is demonstrated\nto align well with known extrinsic measures, and proves useful as a tool for\nhyperparameter tuning as well as visualisation.",
        "In this work, we propose high-order basis-update & Galerkin (BUG) integrators\nbased on explicit Runge-Kutta methods for large-scale matrix differential\nequations. These dynamical low-rank integrators are high-order extensions of\nthe BUG integrator and are constructed by performing one time-step of the\nfirst-order BUG integrator at each stage of the Runge-Kutta method. In this\nway, the resulting Runge-Kutta BUG integrator is robust to the presence of\nsmall singular values and does not involve backward time-integration steps. We\nprovide an error bound, which shows that the Runge-Kutta BUG integrator retains\nthe order of convergence of the associated Runge-Kutta method until the error\nreaches a plateau corresponding to the low-rank truncation and which vanishes\nas the rank increases. This error bound is finally validated numerically on\nthree different test cases. The results demonstrate the high-order convergence\nof the Runge-Kutta BUG integrator and its superior accuracy compared to other\nlow-rank integrators proposed in the literature.",
        "The validity range of the widely used traditional effective range expansion\ncan be severely limited by the presence of a left-hand cut near the\ntwo-particle threshold. Such a left-hand cut emerges in two-particle scattering\nprocesses involving either a light particle exchange in the $t$-channel or a\nparticle exchange with a mass slightly heavier than the mass difference of the\ntwo particles in the $u$-channel, which occurs in a wide range of physical\nsystems. We propose a new parameterization for the low-energy scattering\namplitude that incorporates these left-hand cuts arising from particle exchange\ndiagrams. This parameterization extends the convergence radius of the effective\nrange expansion beyond the branch point of the left-hand cut and is applicable\nto a broad range of systems. The parameterization enables the extraction of\ncoupling strengths between the exchange particle and the scattering particles,\nand reveals amplitude zeros resulting from the interplay between short- and\nlong-range interactions. We demonstrate the effectiveness of this new\nparameterization through its application to $DD^*$ scattering with meson masses\nobtained in a lattice QCD calculation.",
        "We consider the problem of computing the optimal solution and objective of a\nlinear program under linearly changing linear constraints. More specifically,\nwe want to compute the optimal solution of a linear optimization where the\nconstraint matrix linearly depends on a paramater that can take p different\nvalues. Based on the information given by a precomputed basis, we present three\nefficient LP warm-starting algorithms. Each algorithm is either based on the\neigenvalue decomposition, the Schur decomposition, or a tweaked eigenvalue\ndecomposition to evaluate the optimal solution and optimal objective of these\nproblems. The three algorithms have an overall complexity O(m^3 + pm^2) where m\nis the number of constraints of the original problem and p the number of values\nof the parameter that we want to evaluate. We also provide theorems related to\nthe optimality conditions to verify when a basis is still optimal and a local\nbound on the objective.",
        "Quantum characterization, verification, and validation (QCVV) is a set of\ntechniques to probe, describe, and assess the behavior of quantum bits\n(qubits), quantum information-processing registers, and quantum computers. QCVV\nprotocols probe and describe the effects of unwanted decoherence so that it can\nbe eliminated or mitigated. They can be usefully divided into characterization\ntechniques that estimate predictive models for a device's behavior from data,\nand benchmarking techniques that assess overall performance of a device. In\nthis introductory article, we briefly summarize the history of QCVV, introduce\nthe mathematical models and metrics upon which it relies, and then summarize\nthe foundational fields of tomography, randomized benchmarking, and holistic\nbenchmarks. We conclude with brief descriptions of (and references to) advanced\ntopics including gate set tomography, phase estimation, Pauli noise learning,\ncharacterization of mid-circuit measurements and non-Markovianity, classical\nshadows, verification and certification, and logical qubit assessment.",
        "We investigate the one-loop quantum correction to the power spectrum of\nprimordial curvature perturbations in the ultra-slow-roll (USR) inflationary\nscenario, incorporating the backreaction effect from curvature perturbations.\nIn the spatially-flat gauge, we expand the background inflaton field up to\nsecond order and identify the one-loop level backreaction term in the action.\nUtilizing a gauge transformation, we derive the comoving curvature interaction\nHamiltonian in the presence of the backreaction term and calculate the one-loop\ncorrection using the in-in formalism. Our results reveal that the one-loop\nsuper-horizon corrections previously reported in the literature are canceled by\nthe backreaction contributions. This finding underscores the importance of\naccounting for the backreaction effects in the analysis of quantum corrections\nduring USR inflation.",
        "In view of the great uncertainty of the equation of state (EOS) of\nhigh-density nuclear matter, establishing EOS-independent universal relations\nbetween global properties of neutron stars provides a practical way to\nconstrain the unobservable or difficult-to-observe properties through\nastronomical observations. It is common to construct universal relations\nbetween EOS-dependent properties (e.g., moment of inertia, tidal deformation,\netc.) or combined properties (e.g., compactness). Improving the precision of\nthe universal relations may provide stricter constraint on the properties of\nneutron star. We find that in 3-dimensional space with mass and radius as the\nbase coordinates, the points corresponding to a certain property of neutron\nstar described by different EOSs are almost located in the same surface. Thus\nthe universal relation between the property and the stellar mass-radius can be\nexpressed through describing the surface.\n  It is shown that the resulting universal relations have higher precisions. As\nan example, we construct high-precision universal relations for the moment of\ninertia, the $f$-mode frequency, and the dimensionless tidal deformation\nrespect to the mass-radius. As the observational data of neutron star mass and\nradius from NICER grows in data and accuracy, these universal relations allow\nfor more precise constraints on the unobservable or difficult-to-observe\nproperties.",
        "We introduce an empirical functional $\\Psi$ that is an optimal uniform mean\nestimator: Let $F\\subset L_2(\\mu)$ be a class of mean zero functions, $u$ is a\nreal valued function, and $X_1,\\dots,X_N$ are independent, distributed\naccording to $\\mu$. We show that under minimal assumptions, with $\\mu^{\\otimes\nN}$ exponentially high probability, \\[ \\sup_{f\\in F} |\\Psi(X_1,\\dots,X_N,f) -\n\\mathbb{E} u(f(X))| \\leq c R(F) \\frac{ \\mathbb{E} \\sup_{f\\in F } |G_f| }{\\sqrt\nN}, \\] where $(G_f)_{f\\in F}$ is the gaussian processes indexed by $F$ and\n$R(F)$ is an appropriate notion of `diameter' of the class $\\{u(f(X)) : f\\in\nF\\}$.\n  The fact that such a bound is possible is surprising, and it leads to the\nsolution of various key problems in high dimensional probability and high\ndimensional statistics. The construction is based on combining Talagrand's\ngeneric chaining mechanism with optimal mean estimation procedures for a single\nreal-valued random variable."
      ]
    }
  }
]