id,date,a_abstract,b_id,b_abstract,b_categories,c_id,c_abstract,c_categories,y_true,research_type,a_title,a_categories,b_title,c_title
neg-d2-0,2025-03-06,,2503.04166," We study the composition of bivariate L\'evy process with bivariate inverse
subordinator. The explicit expressions for its dispersion and auto correlation
matrices are obtained. Also, the time-changed two parameter L\'evy processes
with rectangular increments are studied. We introduce some time-changed
variants of the Poisson random field in plane with and without drift, and
derive the associated fractional differential equations for their
distributions. Later, we consider some time-changed L\'evy processes where the
time-changing components are two parameter Poisson random fields with drifts.
Moreover, two parameter coordinatewise semigroup operators associated with some
of the introduced processes are discussed.",['math.PR'],2502.13448," In this paper, we establish three criteria for the asymptotic behavior of
Markov-Feller semigroups. First, we present a criterion for convergence in
total variation to a unique invariant measure, requiring only $TV$-eventual
continuity of the semigroup at a single point. Second, we propose two new
criteria for asymptotic stability that require eventual continuity at a single
point. This localized condition is more practical and easier to check. To
illustrate the advantages of our framework, we provide an explicit example
where verifying eventual continuity at a single point is straightforward,
whereas establishing the corresponding global property is challenging.",['math.PR'],False,,,,On Two Parameter Time-Changed Poisson Random Fields with Drifts,"Criteria for asymptotic stability of eventually continuous Markov-Feller
  semigroups"
neg-d2-1,2025-02-26,,2502.18931," We investigate axion emission from singlet proton Cooper pairs in neutron
stars, a process that dominates axion emission in young neutron stars in the
KSVZ model. By re-deriving its emissivity, we confirm consistency with most
existing literature, except for a recent study that exhibits a different
dependence on the effective mass. This discrepancy results in more than an
order-of-magnitude deviation in emissivity, significantly impacting constraints
on the KSVZ axion from the cooling observations of the Cassiopeia A neutron
star. Furthermore, we examine uncertainties arising from neutron-star equations
of state and their role in the discrepancy, finding that the large deviation
persists regardless of the choice of equations of state.",['hep-ph'],2503.1081," We investigate the factorization properties of the massive fermion form
factor in QED, to next-to-leading power in the fermion mass, and up to two-loop
order. For this purpose we define new jet functions that have multiple
connections to the hard part as operator matrix elements, and compute them to
second order in the coupling. We test our factorization formula using these new
jet functions in a region-based analysis and find that factorization indeed
holds. We address a number of subtle aspects such as rapidity regulators and
external line corrections, and we find an interesting sequence of relations
among the jet functions.",['hep-ph'],False,,,,Axion Emission from Proton Cooper Pairs in Neutron Stars,Next-to-leading power jet functions in the small-mass limit in QED
neg-d2-2,2025-03-03,,2503.01833," Lateral heterostructures built of monolayers of transition metal
dichalcogenides (TMDs) are characterized by a thin 1D interface exhibiting a
large energy offset. Recently, the formation of spatially separated
charge-transfer (CT) excitons at the interface has been demonstrated. The
technologically important exciton propagation across the interface and the
impact of CT excitons has remained in the dark so far. In this work, we
microscopically investigate the spatiotemporal exciton dynamics in the
exemplary hBN-encapsulated WSe$_2$-MoSe$_2$ lateral heterostructure and reveal
a highly interesting interplay of energy offset-driven unidirectional exciton
drift across the interface and efficient capture into energetically lower CT
excitons at the interface. This interplay triggers a counterintuitive thermal
control of exciton transport with a less efficient propagation at lower
temperatures - opposite to the behavior in conventional semiconductors. We
predict clear signatures of this intriguing exciton propagation both in far-
and near-field photoluminescence experiments. Our results present an important
step toward a microscopic understanding of the technologically relevant
unidirectional exciton transport in lateral heterostructures.",['cond-mat.mes-hall'],2502.12931," We continue our explorations of the transport characteristics in
junction-configurations comprising semimetals with quadratic band-crossings,
observed in the bandstructures of both two- and three-dimensional materials.
Here, we consider short potential barriers/wells modelled by delta-function
potentials. We also generalize our analysis by incorporating tilts in the
dispersion. Due to the parabolic nature of the spectra, caused by
quadratic-in-momentum dependence, there exist evanescent waves, which decay
exponentially as we move away from the junction represented by the location of
the delta-function potential. Investigating the possibility of the appearance
of bound states, we find that their energies appear as pairs of $\pm |E_b |$,
reflecting the presence of the imaginary-valued wavevectors at both positive
and negative values of energies of the propagating quasiparticles.",['cond-mat.mes-hall'],False,,,,"Impact of charge transfer excitons on unidirectional exciton transport
  in lateral TMD heterostructures","Delta-function-potential junctions with quasiparticles occupying tilted
  bands with quadratic-in-momentum dispersion"
neg-d2-3,2025-02-16,,2502.11434," The KM3Net Collaboration has recently reported on the observation of a
remarkable event KM3-230213A that could have been produced by an ultra high
energy cosmic neutrino. The origin of this event is still unclear. In
particular, the cosmogenic neutrino scenario is not favoured due to the
non-observation of a similar event by the IceCube detector, and most galactic
scenarios are disfavoured as well. We show that the blazar PKS 0605-085 is a
viable source of the KM3-230213A event. In particular, even though this blazar
is located at 2.4$^{\circ}$ from the KM3-230213A event, the association between
the blazar and the event is not unlikely due to a sizable direction systematic
uncertainty of $\approx 1.5^{\circ}$ reported by the KM3Net Collaboration.
Furthermore, we show that the observation of a $\approx$72 PeV neutrino from
PKS 0605-085 is entirely possible given that a $\approx$7.5 PeV neutrino could
have been observed from another blazar TXS 0506+056. Finally, we consider
$\gamma$-ray constraints on the number of observable neutrino events and show
that for the case of the external photon field production mechanism these
constraints could be relaxed due to the often-neglected effect of the
isotropisation of the hadronically-produced electrons in the magnetic field of
the blob. We encourage further multi-wavelength observations of the blazar PKS
0605-085.",['astro-ph.HE'],2502.0115," This study analyzes the multi-wavelength flaring activity of the distant flat
spectrum radio quasar (FSRQ) OP 313 (z=0.997) during November 2023 to March
2024, using data from Fermi-Large Area Telescope, Swift X-ray Telescope, and
Ultraviolet and Optical Telescope. The analysis highlights two significant very
high energy(VHE) detection epochs and GeV gamma-ray flaring episodes, providing
insight into jet emission processes and radiative mechanisms. Key findings
include broadband spectral energy distribution (SED) evolution, including
enigmatic X-ray spectral changes. Modeling of the multi-wavelength SED with a
one-zone leptonic radiative processes attributes the emissions to synchrotron
radiation, Synchrotron Self-Compton (SSC), and External Compton (EC)
mechanisms, with torus photons as the primary source for EC processes. The
results suggest that the gamma-ray emitting region lies outside the broad-line
region but within the dusty torus. Furthermore, we find that the radiated power
is significantly smaller than the total jet power, suggesting that most of the
bulk energy remains within the jet even after passing through the blazar
emission zone. These findings advance our understanding of particle
acceleration, jet dynamics, and photon field interactions in FSRQs.",['astro-ph.HE'],False,,,,"The blazar PKS 0605-085 as the origin of the KM3-230213A ultra high
  energy neutrino event","Deciphering the Multi-Wavelength Flares of the Most Distant Very
  High-Energy (>100 GeV) Gamma-ray Emitting Blazar"
neg-d2-4,2025-03-07,,2503.05561," Chatbots are software typically embedded in Web and Mobile applications
designed to assist the user in a plethora of activities, from chit-chatting to
task completion. They enable diverse forms of interactions, like text and voice
commands. As any software, even chatbots are susceptible to bugs, and their
pervasiveness in our lives, as well as the underlying technological
advancements, call for tailored quality assurance techniques. However, test
case generation techniques for conversational chatbots are still limited. In
this paper, we present Chatbot Test Generator (CTG), an automated testing
technique designed for task-based chatbots. We conducted an experiment
comparing CTG with state-of-the-art BOTIUM and CHARM tools with seven chatbots,
observing that the test cases generated by CTG outperformed the competitors, in
terms of robustness and effectiveness.",['cs.SE'],2501.02875," Mutation testing may be used to guide test case generation and as a technique
to assess the quality of test suites. Despite being used frequently, mutation
testing is not so commonly applied in the mobile world. One critical challenge
in mutation testing is dealing with its computational cost. Generating mutants,
running test cases over each mutant, and analyzing the results may require
significant time and resources. This research aims to contribute to reducing
Android mutation testing costs. It implements mutation testing operators
(traditional and Android-specific) according to mutant schemata (implementing
multiple mutants into a single code file). It also describes an Android
mutation testing framework developed to execute test cases and determine
mutation scores. Additional mutation operators can be implemented in JavaScript
and easily integrated into the framework. The overall approach is validated
through case studies showing that mutant schemata have advantages over the
traditional mutation strategy (one file per mutant). The results show mutant
schemata overcome traditional mutation in all evaluated aspects with no
additional cost: it takes 8.50% less time for mutant generation, requires
99.78% less disk space, and runs, on average, 6.45% faster than traditional
mutation. Moreover, considering sustainability metrics, mutant schemata have
8,18% less carbon footprint than traditional strategy.",['cs.SE'],False,,,,Test Case Generation for Dialogflow Task-Based Chatbots,METFORD -- Mutation tEsTing Framework fOR anDroid
neg-d2-5,2025-02-04,,2502.02677," Research on ultra-high dose rate (UHDR) radiation therapy has indicated its
potential to spare normal tissue while maintaining equivalent tumor control
compared to conventional treatments. First clinical trials are underway. The
randomized phase II/III FEATHER clinical trial at the Paul Scherrer Institute
in collaboration with the University of Zurich Animal Hospital is one of the
first curative domestic animal trials to be attempted, and it is designed to
provide a good example for human trials. However, the lack of standardized
quality assurance (QA) guidelines for FLASH clinical trials presents a
significant challenge in trial design. This work aims to demonstrate the
development and testing of QA and reporting procedures implemented in the
FEATHER clinical trial. We have expanded the clinical QA program to include
UHDR-specific QA and additional patient-specific QA. Furthermore, we have
modified the monitor readout to enable time-resolved measurements, allowing
delivery log files to be used for dose and dose rate recalculations. Finally,
we developed a reporting strategy encompassing relevant parameters for
retrospective studies. We evaluated our QA and reporting procedures with
simulated treatments. This testing confirmed that our QA procedures effectively
ensure the correct and safe delivery of the planned dose. Additionally, we
demonstrated that we could reconstruct the delivered dose and dose rate using
the delivery log files. We developed and used in practice a comprehensive QA
and reporting protocol for a FLASH clinical trial at the Paul Scherrer
Institute. This work aims to establish guidelines and standardize reporting
practices for future advancements in the FLASH-RT field.",['physics.med-ph'],2501.08827," The key molecules such as triphosphate (ATP), glutathione (GSH), and
homocarnosine (hCs) - central to metabolic processes in the human brain remain
elusive or challenging to detect with upfield 1H-MRSI. Traditional 3D 1H-MRSI
in vivo faces challenges, including a low signal-to-noise ratio and
magnetization transfer effects with water, leading to prolonged measurement
times and reduced resolution. To address these limitations, we propose a
downfield 3D-MRSI method aimed at measuring downfield metabolites with enhanced
spatial resolution, and speed acceptable for clinical practice at 7T. The
CHEmical-shift selective Adiabatic Pulse (CHEAP) technique was integrated into
echo-planar spectroscopic imaging (EPSI) readout sequence for downfield
metabolite and water reference 3D-MRSI. Five healthy subjects and two glioma
patients were scanned to test the feasibility. In this work, CHEAP-EPSI
technique is shown to significantly enhance spatial the resolution to 0.37 ml
while simultaneously reducing the scan time to 10.5 minutes. Its distinct
advantages include low specific absorption rate, effective suppression of water
and lipid signals, and minimal baseline distortions, making it a valuable tool
for research or potentially diagnostic purposes. CHEAP-EPSI improves the
detection sensitivity of downfield metabolites like N-acetyl-aspartate (NAA+)
and DF8.18 (ATP&GSH+), and offers new possibilities for the study of metabolism
in healthy and diseased brain.",['physics.med-ph'],False,,,,"Quality assurance and reporting for FLASH clinical trials:the experience
  of the FEATHER trial","CHEmical-shift selective Adiabatic Pulse (CHEAP): Fast and High
  Resolution Downfield 3D 1H-MRSI at 7T"
neg-d2-6,2025-01-02,,2501.01344," This paper presents a suite of machine learning models, CRC-ML-Radio Metrics,
designed for modeling RSRP, RSRQ, and RSSI wireless radio metrics in 4G
environments. These models utilize crowdsourced data with local environmental
features to enhance prediction accuracy across both indoor at elevation and
outdoor urban settings. They achieve RMSE performance of 9.76 to 11.69 dB for
RSRP, 2.90 to 3.23 dB for RSRQ, and 9.50 to 10.36 dB for RSSI, evaluated on
over 300,000 data points in the Toronto, Montreal, and Vancouver areas. These
results demonstrate the robustness and adaptability of the models, supporting
precise network planning and quality of service optimization in complex
Canadian urban environments.",['cs.LG'],2501.01344," This paper presents a suite of machine learning models, CRC-ML-Radio Metrics,
designed for modeling RSRP, RSRQ, and RSSI wireless radio metrics in 4G
environments. These models utilize crowdsourced data with local environmental
features to enhance prediction accuracy across both indoor at elevation and
outdoor urban settings. They achieve RMSE performance of 9.76 to 11.69 dB for
RSRP, 2.90 to 3.23 dB for RSRQ, and 9.50 to 10.36 dB for RSSI, evaluated on
over 300,000 data points in the Toronto, Montreal, and Vancouver areas. These
results demonstrate the robustness and adaptability of the models, supporting
precise network planning and quality of service optimization in complex
Canadian urban environments.",['cs.LG'],False,,,,"Machine Learning for Modeling Wireless Radio Metrics with Crowdsourced
  Data and Local Environment Features","Machine Learning for Modeling Wireless Radio Metrics with Crowdsourced
  Data and Local Environment Features"
neg-d2-7,2025-02-14,,2502.1037," We provide explicit formulas for the Alexander polynomial of Pretzel knots
and establish several immediate corollaries, including the characterization of
Pretzel knots with a trivial Alexander polynomial.",['math.GT'],2502.1037," We provide explicit formulas for the Alexander polynomial of Pretzel knots
and establish several immediate corollaries, including the characterization of
Pretzel knots with a trivial Alexander polynomial.",['math.GT'],False,,,,Explicit Formulas for the Alexander Polynomial of Pretzel Knots,Explicit Formulas for the Alexander Polynomial of Pretzel Knots
neg-d2-8,2025-02-17,,2502.12487," We present the explicit form of the Regge trajectory relations for the doubly
heavy baryons $\Xi_{QQ'}$ and $\Omega_{QQ'}$ $(Q,Q'=b,c)$ in the diquark
picture. Using the derived Regge trajectory relations, we estimate the masses
of the $\lambda$-excited states and the $\rho$-excited states, which are
consistent with other theoretical predictions. Both the $\lambda$-trajectories
and $\rho$-trajectories are discussed. We show that the $\rho$-trajectories
behave differently from the $\lambda$-trajectories. Specifically, the
$\rho$-trajectories behave as $M{\sim}x_{\rho}^{2/3}$ $(x_{\rho}=n_r,l)$,
whereas the $\lambda$-trajectories follow $M{\sim}x_{\lambda}^{1/2}$
$(x_{\lambda}=N_r,L)$. By using the obtained relations, the baryon Regge
trajectory provides a straightforward and easy method for estimating the
spectra of both the $\lambda$-excited states and $\rho$-excited states.",['hep-ph'],2501.15649," The evidence of a Stochastic Gravitational Wave Background (SGWB) in the nHz
frequency range is posed to open a new window on the Universe. A preferred
explanation relies on a supercooled first order phase transition at the 100 MeV
- GeV scale. In this article, we address the feasibility going from the
particle physics model to the production of the gravitational waves. We take a
minimal approach for the dark sector model introducing the fewest ingredients
required, namely a new U(1) gauge group and a dark scalar that dynamically
breaks the symmetry. Supercooling poses challenges in the analysis that put
under question the feasibility of this explanation: we address them, going
beyond previous studies by carefully considering the effects of a vacuum
domination phase and explicitly tracking the phase transition from its onset to
its completion. We find that the proposed model can successfully give origin to
the observed PTA SGWB signal. The strong supercooling imposes a correlation
between the new gauge coupling and the scalar quartic one, leading to a
significant hierarchy between the (heavier) gauge boson and the dark scalar.
Ultimately, information on phase transitions from SGWB observations could
provide a direct probe of the microphysics of the Early Universe and be used to
guide future searches of dark sector in laboratories.",['hep-ph'],False,,,,"$\lambda$ and $\rho$ trajectories for the doubly heavy baryons in the
  diquark picture",Supercooled Dark Scalar Phase Transitions explanation of NANOGrav data
neg-d2-9,2025-01-11,,2501.06493," Efficient motion planning for Aerial Manipulators (AMs) is essential for
tackling complex manipulation tasks, yet achieving coupled trajectory planning
remains challenging. In this work, we propose, to the best of our knowledge,
the first whole-body integrated motion planning framework for aerial
manipulators, which is facilitated by an improved Safe Flight Corridor (SFC)
generation strategy and high-dimensional collision-free trajectory planning. In
particular, we formulate an optimization problem to generate feasible
trajectories for both the quadrotor and manipulator while ensuring collision
avoidance, dynamic feasibility, kinematic feasibility, and waypoint
constraints. To achieve collision avoidance, we introduce a variable geometry
approximation method, which dynamically models the changing collision volume
induced by different manipulator configurations. Moreover, waypoint constraints
in our framework are defined in $\mathrm{SE(3)\times\mathbb{R}^3}$, allowing
the aerial manipulator to traverse specified positions while maintaining
desired attitudes and end-effector states. The effectiveness of our framework
is validated through comprehensive simulations and real-world experiments
across various environments.",['cs.RO'],2501.15272," Transporting a heavy payload using multiple aerial robots (MARs) is an
efficient manner to extend the load capacity of a single aerial robot. However,
existing schemes for the multiple aerial robots transportation system (MARTS)
still lack the capability to generate a collision-free and dynamically feasible
trajectory in real-time and further track an agile trajectory especially when
there are no sensors available to measure the states of payload and cable.
Therefore, they are limited to low-agility transportation in simple
environments. To bridge the gap, we propose complete planning and control
schemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of
a cable-suspended payload in complex environments. Flatness maps for the aerial
robot considering the complete kinematical constraint and the dynamical
coupling between each aerial robot and payload are derived. To improve the
responsiveness for the generation of the safe, dynamically feasible, and agile
trajectory in complex environments, a real-time spatio-temporal trajectory
planning scheme is proposed for the MARTS. Besides, we break away from the
reliance on the state measurement for both the payload and cable, as well as
the closed-loop control for the payload, and propose a fully distributed
control scheme to track the agile trajectory that is robust against imprecise
payload mass and non-point mass payload. The proposed schemes are extensively
validated through benchmark comparisons, ablation studies, and simulations.
Finally, extensive real-world experiments are conducted on a MARTS integrated
by three aerial robots with onboard computers and sensors. The result validates
the efficiency and robustness of our proposed schemes for SAAT in complex
environments.",['cs.RO'],False,,,,Whole-Body Integrated Motion Planning for Aerial Manipulators,"Safe and Agile Transportation of Cable-Suspended Payload via Multiple
  Aerial Robots"
neg-d2-10,2025-03-20,,2503.1612," Multi-species animal pose estimation has emerged as a challenging yet
critical task, hindered by substantial visual diversity and uncertainty. This
paper challenges the problem by efficient prompt learning for Vision-Language
Pretrained (VLP) models, \textit{e.g.} CLIP, aiming to resolve the
cross-species generalization problem. At the core of the solution lies in the
prompt designing, probabilistic prompt modeling and cross-modal adaptation,
thereby enabling prompts to compensate for cross-modal information and
effectively overcome large data variances under unbalanced data distribution.
To this end, we propose a novel probabilistic prompting approach to fully
explore textual descriptions, which could alleviate the diversity issues caused
by long-tail property and increase the adaptability of prompts on unseen
category instance. Specifically, we first introduce a set of learnable prompts
and propose a diversity loss to maintain distinctiveness among prompts, thus
representing diverse image attributes. Diverse textual probabilistic
representations are sampled and used as the guidance for the pose estimation.
Subsequently, we explore three different cross-modal fusion strategies at
spatial level to alleviate the adverse impacts of visual uncertainty. Extensive
experiments on multi-species animal pose benchmarks show that our method
achieves the state-of-the-art performance under both supervised and zero-shot
settings. The code is available at https://github.com/Raojiyong/PPAP.",['cs.CV'],2501.08659," Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.",['cs.CV'],False,,,,Probabilistic Prompt Distribution Learning for Animal Pose Estimation,"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with
  Multi-modality Refinement Module"
neg-d2-11,2025-03-12,,2503.09359," We explore the mechanisms and regimes of mixing in yield-stress fluids by
simulating the stirring of an infinite, two-dimensional domain filled with a
Bingham fluid. A cylindrical stirrer moves along a circular path at constant
speed to stir the fluid, with an initially quiescent domain marked by a passive
dye in the lower half, facilitating the analysis of dye interface evolution and
mixing dynamics. We first examine the mixing process in Newtonian fluids,
identifying three key mechanisms: interface stretching and folding around the
stirrer's path, diffusion across streamlines, and dye advection and interface
stretching due to vortex shedding. Introducing yield stress into the system
leads to notable localization effects in mixing, manifesting through three
mechanisms: advection of vortices within a finite distance of the stirrer,
vortex entrapment near the stirrer, and complete suppression of vortex shedding
at high yield stresses. Based on these mechanisms, we classify three distinct
mixing regimes in yield-stress fluids: (i) Regime SE, where shed vortices
escape the central region, (ii) Regime ST, where shed vortices remain trapped
near the stirrer, and (iii) Regime NS, where no vortex shedding occurs. These
regimes are quantitatively distinguished through spectral analysis of energy
oscillations, revealing transitions and the critical Bingham and Reynolds
numbers. The transitions are captured through effective Reynolds numbers,
supporting a hypothesis that mixing regime transitions in yield-stress fluids
share fundamental characteristics with bluff-body flow dynamics. The findings
provide a mechanistic framework for understanding and predicting mixing
behaviors in yield-stress fluids, suggesting that the localization mechanisms
and mixing regimes observed here are archetypal for stirred-tank applications.",['physics.flu-dyn'],2502.10256," In this work, we investigate the application of an advanced nonlinear
torsion- and shear-free Kirchhoff rod model, enhanced with a penalty-based
barrier function (to simulate the seabed contact), intended for studying the
static and dynamic behavior of mooring lines. The formulation incorporates
conservative and non-conservative external loads, including those coming from
the surrounding flow (added mass, tangential drag, and normal drag). To
illustrate the favorable features of this model, we consider some key scenarios
such as static configurations, pulsating force applications at the fairlead,
and fluid-structure interaction between mooring lines and the surrounding flow.
Verification against well-established solutions, including catenary
configurations and OpenFAST simulations, shows excellent accuracy in predicting
mooring line responses for a floating offshore wind turbine. Among the most
important results, we can mention that under normal pulsating loads at the
fairlead, the mooring line exhibits a transition from a drag-dominated regime
at low frequencies to an added-mass-dominated regime at higher frequencies.
Furthermore, tangential forcing at the fairlead reveals a strong coupling
between axial and bending dynamics, contrasting with normal forcing scenarios
where axial dynamics remain largely unaffected. These findings underscore the
potential of the proposed approach for advanced mooring line simulations.",['physics.flu-dyn'],False,,,,"Yield-Stress Fluid Mixing: Localization Mechanisms and Regime
  Transitions",On the use of an advanced Kirchhoff rod model to study mooring lines
neg-d2-12,2025-03-03,,2503.01415," In response to the growing demand for high-quality videos, Versatile Video
Coding (VVC) was released in 2020, building on the hybrid coding architecture
of its predecessor, HEVC, achieving about 50% bitrate reduction for the same
visual quality. It introduces more flexible block partitioning, enhancing
compression efficiency at the cost of increased encoding complexity. To make
efficient use of VVC in practical applications, optimization is essential.
VVenC, an optimized open-source VVC encoder, introduces multiple presets to
address the trade-off between compression efficiency and encoder complexity.
Although an optimized set of encoding tools has been selected for each preset,
the rate-distortion (RD) search space in the encoder presets still poses a
challenge for efficient encoder implementations. In this paper, we propose
Early Termination using Reference Frames (ETRF), which improves the trade-off
between encoding efficiency and time complexity and positions itself as a new
preset between medium and fast presets. The CTU partitioning map of the
reference frames in lower temporal layers is employed to accelerate the
encoding of frames in higher temporal layers. The results show a reduction in
the encoding time of around 21% compared to the medium preset. Specifically,
for videos with high spatial and temporal complexities, which typically require
longer encoding times, the proposed method achieves a better trade-off between
bitrate savings and encoding time compared to the fast preset.",['cs.MM'],2503.01415," In response to the growing demand for high-quality videos, Versatile Video
Coding (VVC) was released in 2020, building on the hybrid coding architecture
of its predecessor, HEVC, achieving about 50% bitrate reduction for the same
visual quality. It introduces more flexible block partitioning, enhancing
compression efficiency at the cost of increased encoding complexity. To make
efficient use of VVC in practical applications, optimization is essential.
VVenC, an optimized open-source VVC encoder, introduces multiple presets to
address the trade-off between compression efficiency and encoder complexity.
Although an optimized set of encoding tools has been selected for each preset,
the rate-distortion (RD) search space in the encoder presets still poses a
challenge for efficient encoder implementations. In this paper, we propose
Early Termination using Reference Frames (ETRF), which improves the trade-off
between encoding efficiency and time complexity and positions itself as a new
preset between medium and fast presets. The CTU partitioning map of the
reference frames in lower temporal layers is employed to accelerate the
encoding of frames in higher temporal layers. The results show a reduction in
the encoding time of around 21% compared to the medium preset. Specifically,
for videos with high spatial and temporal complexities, which typically require
longer encoding times, the proposed method achieves a better trade-off between
bitrate savings and encoding time compared to the fast preset.",['cs.MM'],False,,,,Improving the Efficiency of VVC using Partitioning of Reference Frames,Improving the Efficiency of VVC using Partitioning of Reference Frames
neg-d2-13,2025-02-26,,2502.19111," We make an exposition of the proof of the Baum-Connes conjecture for the
infinite dihedral group following the ideas of Higson and Kasparov.",['math.KT'],2502.19111," We make an exposition of the proof of the Baum-Connes conjecture for the
infinite dihedral group following the ideas of Higson and Kasparov.",['math.KT'],False,,,,On the Baum-Connes conjecture for $D_{\infty}$,On the Baum-Connes conjecture for $D_{\infty}$
neg-d2-14,2025-02-04,,2502.02253," The maturity and commercial roll-out of 5G networks and its deployment for
private networks makes 5G a key enabler for various vertical industries and
applications, including robotics. Providing ultra-low latency, high data rates,
and ubiquitous coverage and wireless connectivity, 5G fully unlocks the
potential of robot autonomy and boosts emerging robotic applications,
particularly in the domain of autonomous mobile robots. Ensuring seamless,
efficient, and reliable navigation and operation of robots within a 5G network
requires a clear understanding of the expected network quality in the
deployment environment. However, obtaining real-time insights into network
conditions, particularly in highly dynamic environments, presents a significant
and practical challenge. In this paper, we present a novel framework for
building a Network Digital Twin (NDT) using real-time data collected by robots.
This framework provides a comprehensive solution for monitoring, controlling,
and optimizing robotic operations in dynamic network environments. We develop a
pipeline integrating robotic data into the NDT, demonstrating its evolution
with real-world robotic traces. We evaluate its performances in radio-aware
navigation use case, highlighting its potential to enhance energy efficiency
and reliability for 5Genabled robotic operations.",['cs.NI'],2501.08644," In future wireless communication systems, millimeter waves (mmWaves) will
play an important role in meeting high data rates. However, due to their short
wavelengths, these mmWaves present high propagation losses and are highly
attenuated by blocking. In this chapter, we seek to increase the indoor radio
coverage at 60 GHz in non line-of-sight (NLOS) environments. Firstly, a
metallic passive reflector is used in an L-shaped corridor. Secondly, an array
of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80
grooves) is used in a T-shaped corridor. Next, the study focuses on the
blockage losses caused by the human body. The results obtained in these
different configurations show that it is possible to use beamforming to exploit
a reflected path when the direct path is blocked.",['cs.NI'],False,,,,Network Digital Twin for 5G-Enabled Mobile Robots,"Extension of indoor mmW link radio coverage in non line-of-sight
  conditions"
neg-d2-15,2025-01-02,,2501.01634," There has been much work on the following question: given n how large can a
subset of {1,...,n} be that has no arithmetic progressions of length 3. We call
such sets 3-free. Most of the work has been asymptotic. In this paper we sketch
applications of large 3-free sets, review the literature of how to construct
large 3-free sets, and present empirical studies on how large such sets
actually are. The two main questions considered are (1) How large can a 3-free
set be when n is small, and (2) How do the methods in the literature compare to
each other? In particular, when do the ones that are asymptotically better
actually yield larger sets? (This paper overlaps with our previous paper with
the title { Finding Large 3-Free Sets I: the Small n Case}.)",['math.CO'],2502.19835," Let $B$ be a bidirected multigraph with signing $\sigma$, let $X$ be a set of
vertices in $B$, and let $k$ be a non-negative integer. For any pair of vertex
sets $S,T\subset V(B)$ satisfying $X\cap S = X\cap T$, we denote by $B_{S,T}$
the multigraph with the same vertex set as $B$ and with edge set consisting of
those edges $e$ of $B$ each of whose endvertices $v$ satisfies $v\notin S\cup
T$ or $v\in S\setminus T$, $\sigma(v,e)=-$ or $v\in T\setminus S$,
$\sigma(v,e)=+$. We prove that $B$ admits a set of $k$ pairwise disjoint
$X$-paths if and only if for any $S,T\subseteq V(B)$ with $X\cap S = X\cap T$,
the inequality $\left\lvert S\cap T \right\rvert +\sum \lfloor \tfrac{1}{2}
\left\lvert V(C)\cap (X\cup S\cup T) \right\rvert \rfloor \geq k$ holds where
the sum is indexed by the components of $B_{S,T}$. This result is a
generalization of a result of Gallai from undirected graphs to bidirected ones.
Furthermore, we will deduce from this a kind of an Erd\H{o}s-P\'osa property
for $X$-paths in bidirected multigraphs.",['math.CO'],False,,,,"Finding Large Sets Without Arithmetic Progressions of Length Three: An
  Empirical View and Survey II",Disjoint $X$-paths in bidirected graphs
neg-d2-16,2025-02-11,,2502.0808," Decomposition of text into atomic propositions is a flexible framework
allowing for the closer inspection of input and output text. We use atomic
decomposition of hypotheses in two natural language reasoning tasks,
traditional NLI and defeasible NLI, to form atomic sub-problems, or granular
inferences that models must weigh when solving the overall problem. These
atomic sub-problems serve as a tool to further understand the structure of both
NLI and defeasible reasoning, probe a model's consistency and understanding of
different inferences, and measure the diversity of examples in benchmark
datasets. Our results indicate that LLMs still struggle with logical
consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify
critical atomic sub-problems of defeasible NLI examples, or those that most
contribute to the overall label, and propose a method to measure the
inferential consistency of a model, a metric designed to capture the degree to
which a model makes consistently correct or incorrect predictions about the
same fact under different contexts.",['cs.CL'],2502.13646," In-context learning (ICL) has demonstrated significant potential in enhancing
the capabilities of large language models (LLMs) during inference. It's
well-established that ICL heavily relies on selecting effective demonstrations
to generate outputs that better align with the expected results. As for
demonstration selection, previous approaches have typically relied on intuitive
metrics to evaluate the effectiveness of demonstrations, which often results in
limited robustness and poor cross-model generalization capabilities. To tackle
these challenges, we propose a novel method, \textbf{D}emonstration
\textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration
validation perspective into this field. By introducing the demonstration
validation mechanism, our method effectively identifies demonstrations that are
both effective and highly generalizable. \textbf{D.Va} surpasses all existing
demonstration selection techniques across both natural language understanding
(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate
the robustness and generalizability of our approach across various language
models with different retrieval models.",['cs.CL'],False,,,,NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals,D.Va: Validate Your Demonstration First Before You Use It
neg-d2-17,2025-01-07,,2501.03654," Deep learning (DL) models have gained prominence in domains such as computer
vision and natural language processing but remain underutilized for regression
tasks involving tabular data. In these cases, traditional machine learning (ML)
models often outperform DL models. In this study, we propose and evaluate
various data augmentation (DA) techniques to improve the performance of DL
models for tabular data regression tasks. We compare the performance gain of
Neural Networks by different DA strategies ranging from a naive method of
duplicating existing observations and adding noise to a more sophisticated DA
strategy that preserves the underlying statistical relationship in the data.
Our analysis demonstrates that the advanced DA method significantly improves DL
model performance across multiple datasets and regression tasks, resulting in
an average performance increase of over 10\% compared to baseline models
without augmentation. The efficacy of these DA strategies was rigorously
validated across 30 distinct datasets, with multiple iterations and evaluations
using three different automated deep learning (AutoDL) frameworks: AutoKeras,
H2O, and AutoGluon. This study demonstrates that by leveraging advanced DA
techniques, DL models can realize their full potential in regression tasks,
thereby contributing to broader adoption and enhanced performance in practical
applications.",['cs.LG'],2501.12739," Stochastic Gradient Descent (SGD) is the foundation of modern deep learning
optimization but becomes increasingly inefficient when training convolutional
neural networks (CNNs) on high-resolution data. This paper introduces
Multiscale Stochastic Gradient Descent (Multiscale-SGD), a novel optimization
approach that exploits coarse-to-fine training strategies to estimate the
gradient at a fraction of the cost, improving the computational efficiency of
SGD type methods while preserving model accuracy. We derive theoretical
criteria for Multiscale-SGD to be effective, and show that while standard
convolutions can be used, they can be suboptimal for noisy data. This leads us
to introduce a new class of learnable, scale-independent Mesh-Free Convolutions
(MFCs) that ensure consistent gradient behavior across resolutions, making them
well-suited for multiscale training. Through extensive empirical validation, we
demonstrate that in practice, (i) our Multiscale-SGD approach can be used to
train various architectures for a variety of tasks, and (ii) when the noise is
not significant, standard convolutions benefit from our multiscale training
framework. Our results establish a new paradigm for the efficient training of
deep networks, enabling practical scalability in high-resolution and multiscale
learning tasks.",['cs.LG'],False,,,,"Data Augmentation for Deep Learning Regression Tasks by Machine Learning
  Models","Multiscale Stochastic Gradient Descent: Efficiently Training
  Convolutional Neural Networks"
neg-d2-18,2025-01-22,,2501.13053," Boron Neutron Capture Therapy (BNCT) is a form of radiotherapy based on the
irradiation of the tumour with a low energy neutron beam, after the
administration of a selective drug enriched in boron-10. The therapy exploits
the high cross section of thermal neutron capture in boron, generating two
low-range charged particles. The availability of accelerators able to generate
high-intensity neutron beams via proton nuclear interaction is boosting the
construction of new clinical centres. One of these is under development in
Italy, using a 5 MeV, 30 mA proton radiofrequency accelerator coupled to a
beryllium target, funded by the Complementary Plan to the Recovery and
Resilience National Plan, under the project ANTHEM. The present study focuses
on radiation protection aspects of patients undergoing BNCT, specifically on
the activation of their organs and tissues. A criterion to establish the
relevance of such activation after BNCT has been proposed. Based on the current
Italian regulatory framework, the level of patient activation following BNCT
treatment does not pose a significant radiological concern, even shortly after
irradiation. Another aspect is the activation of patient's excretions, which
can impact on the design of the building and requires a process for the
discharge. The described study contributes to the radiation protection study
for the ANTHEM BNCT centre in Italy.",['physics.med-ph'],2501.11793," The treatment of intracranial aneurysms (IA) relies on angiography guidance
using biplane views. However, accurate flow estimation and device sizing for
treatment are often compromised by vessel overlap and foreshortening, which can
obscure critical details. This study introduces an epipolar reconstruction
approach to enhance 3D rendering of the internal carotid artery (ICA) and
aneurysm dome using routinely acquired biplane angiographic data. Our method
aims to improve procedural guidance by overcoming the limitations of
traditional two-dimensional imaging techniques. This study employed three 3D
geometries of ICA aneurysms to simulate virtual angiograms. These angiograms
were generated using a computational fluid dynamics (CFD) solver, followed by
the simulation of biplane angiography using a cone-beam geometry.
Self-calibration was accomplished by matching contrast media position as a
function of time between biplane views. Feature-matching was used to
triangulate and reconstruct vascular structures in 3D. The projection data was
utilized to refine the 3D estimation, including elimination of erroneous
structures and ellipse-fitting. The accuracy of the reconstructions was
evaluated using the Dice-Sorensen coefficient, comparing the 3D reconstructions
to the original models. The proposed epipolar reconstruction method generalized
well across the three tested aneurysm models, with respective Dice-Sorensen
coefficients of 0.745, 0.759, and 0.654. Errors were primarily due to partial
vessel overlap. The average reconstruction time for all three volumes was
approximately 10 seconds. The proposed epipolar reconstruction method enhanced
3D visualization, addressing challenges such as projection-induced vessel
foreshortening. This method provides a solution to the complexity of IA
visualization, with the potential to provide more accurate analysis and device
sizing for treatment.",['physics.med-ph'],False,,,,"Evaluation of patient activation and dosimetry after Boron Neutron
  Capture Therapy","Self-Calibrated Epipolar Reconstruction for Assessment of Aneurysms in
  the Internal Carotid Artery Using In-Silico Biplane Angiograms"
neg-d2-19,2025-01-09,,2501.05145," We are examining a specific type of graph called a balanced bipartite graph.
The balanced bipartite graph, such as $\mathcal{B}$, has two parts, $V_1$ and
$V_2$, each containing $n$ vertices, for a total of $2n$ vertices. The degree
of a vertex $v$ in $V_1\cup\,V_2$ is denoted by $d_\mathcal{B}(v)$. The minimum
degree of any vertex in the graph $\mathcal{B}$ is represented by
$\delta(\mathcal{B})$. If $S$ is a subset of $V_1\cup\,V_2$, then the subgraph
of $\mathcal{B}$ induced by $S$ is the graph that has $S$ as its vertex set and
contains all the edges of $\mathcal{B}$ that have both endpoints in $S$. This
subgraph is denoted by $\mathcal{B}[S]$. The forest number of a graph
$\mathcal{B}$ is the size of the largest subset of vertices of $\mathcal{B}$
that form an induced forest. We use $f(\mathcal{B})$ to represent the forest
number of graph $\mathcal{B}$. A decycling set or a feedback vertex set of a
graph is a set of vertices whose removal results in a forest. The smallest
possible size of a decycling set of $\mathcal{B}$ is represented by
$\nabla(\mathcal{B})$. Finding the decycling number of $\mathcal{B}$ is
equivalent to determining the largest order of an induced forest, i.e.,
$f(\mathcal{B})+\nabla(\mathcal{B})=2n$. In this essay, we study the structure
and cardinality of the largest subsets of vertices of graph $\mathcal{B}$ that
form induced forests.",['math.CO'],2503.13722," The largest prime p that can be the order of an automorphism of a 2-(35,17,8)
design is p=17, and all 2-(35,17,8) designs with an automorphism of order 17
were classified by Tonchev. The symmetric 2-(35,17,8) designs with
automorphisms of odd prime order $p<17$ were also classified. In this paper we
give the classification of all symmetric 2-(35,17,8) designs that admit an
automorphism of order $p=2$. It is shown that there are exactly $11,642,495$
nonisomorphic such designs. Furthermore, it is shown that the number of
nonisomorphic 3-(36,18,8) designs which have at least one derived 2-$(35,17,8)$
design with an automorphism of order 2, is $1,015,225$.",['math.CO'],False,,,,On Maximum Induced Forests of the Balanced Bipartite Graphs,"Symmetric 2-(35,17,8) designs with an automorphism of order 2"
neg-d2-20,2025-01-17,,2501.10108," This paper investigates the effects of saturated thermal conduction (TC) and
thermal-driven winds (TDWs) on magnetized advection-dominated accretion onto a
rotating black hole (BH). We incorporate dissipative processes in the
magnetized accretion flow and expect the accretion disk to be threaded by
predominantly toroidal and turbulent magnetic fields. We solve the
magnetohydrodynamics equations and construct a self-consistent steady model of
the magnetized accretion flow surrounding a rotating BH, which includes TC and
TDWs. We seek global accretion solutions spanning from the BH horizon to a
large distance and analyze the solution's characteristics as a function of
dissipation parameters. Accretion solutions with multiple critical points may
exhibit shock waves if they meet the standing shock criteria. We found steady,
global transonic, and shocked accretion solutions around the rotating BH. In
particular, the wind parameter ($m$) and the saturated conduction parameter
($\Phi_{\rm s}$) significantly influence the dynamical behavior of shocks. The
shock location moves away from the BH horizon as $\Phi_{\rm s}$ and $m$
increase, assuming fixed conditions at the disk's outer edge. Our formalism
explains the declining phase of BH outbursts, characterized by a monotonic
decrease in QPO frequency as the burst decays. Based on our findings, we
conclude that the combined effect of $\Phi_{\rm s}$ and $m$ parameters
substantially alters the steady shock specific energy vs angular momentum
parameter space and also modifies the corresponding post-shock luminosity vs
QPO frequency parameter space. We propose, based on our theoretical model, that
the $\Phi_{\rm s}$ and $m$ parameters may significantly influence the evolution
of the BH outbursts.",['astro-ph.HE'],2502.0115," This study analyzes the multi-wavelength flaring activity of the distant flat
spectrum radio quasar (FSRQ) OP 313 (z=0.997) during November 2023 to March
2024, using data from Fermi-Large Area Telescope, Swift X-ray Telescope, and
Ultraviolet and Optical Telescope. The analysis highlights two significant very
high energy(VHE) detection epochs and GeV gamma-ray flaring episodes, providing
insight into jet emission processes and radiative mechanisms. Key findings
include broadband spectral energy distribution (SED) evolution, including
enigmatic X-ray spectral changes. Modeling of the multi-wavelength SED with a
one-zone leptonic radiative processes attributes the emissions to synchrotron
radiation, Synchrotron Self-Compton (SSC), and External Compton (EC)
mechanisms, with torus photons as the primary source for EC processes. The
results suggest that the gamma-ray emitting region lies outside the broad-line
region but within the dusty torus. Furthermore, we find that the radiated power
is significantly smaller than the total jet power, suggesting that most of the
bulk energy remains within the jet even after passing through the blazar
emission zone. These findings advance our understanding of particle
acceleration, jet dynamics, and photon field interactions in FSRQs.",['astro-ph.HE'],False,,,,"Thermal Conduction and Thermal-Driven Winds in Magnetized Viscous
  Accretion Disk Dynamics","Deciphering the Multi-Wavelength Flares of the Most Distant Very
  High-Energy (>100 GeV) Gamma-ray Emitting Blazar"
neg-d2-21,2025-02-17,,2502.12283," A recent study shows that if the power spectra (PS) of accreting compact
objects consist of a combination of Lorentzian functions that are coherent in
different energy bands but incoherent with each other, the same is true for the
Real and Imaginary parts of the cross spectrum (CS). Using this idea, we
discovered imaginary quasi-periodic oscillations (QPOs) in NICER observations
of the black hole candidate MAXI J1820+070. The imaginary QPOs appear as narrow
features with a small Real and large Imaginary part in the CS but are not
significantly detected in the PS when they overlap in frequency with other
variability components. The coherence function drops and the phase lags
increase abruptly at the frequency of the imaginary QPO. We show that the
multi-Lorentzian model that fits the PS and CS of the source in two energy
bands correctly reproduces the lags and the coherence, and that the narrow drop
of the coherence is caused by the interaction of the imaginary QPO with other
variability components. The imaginary QPO appears only in the decay of the
outburst, during the transition from the high-soft to the low-hard state of
MAXI J1820+070, and its frequency decreases from approximately 5 Hz to around 1
Hz as the source spectrum hardens. We also analysed the earlier observations of
the transition, where no narrow features were seen, and we identified a QPO in
the PS that appears to evolve into the imaginary QPO as the source hardens. As
for the type-B and C QPOs in this source, the rms spectrum of the imaginary QPO
increases with energy. The lags of the imaginary QPO are similar to those of
the type-B and C QPOs above 2 keV but differ from the lags of those other QPOs
below that energy. While the properties of this imaginary QPO resemble those of
type-C QPOs, we cannot rule out that it is a new type of QPO.",['astro-ph.HE'],2501.0131," Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range
of time scales. Traditionally, this variability can often be modeled well as a
stochastic process. However, in certain cases, jetted AGN variability displays
regular patterns, enabling us to conduct investigations aimed at understanding
its origins. Additionally, a novel type of variability has emerged in jetted
AGN lightcurves, specifically, the observation of a long-term trend
characterized by a linear increase of the flux with time in blazars such as PG
1553+113, which is among the objects most likely to display periodic behavior.
In this paper, we present the results of a systematic search for long-term
trends, spanning $\approx$10\, years, utilizing 12 years of Fermi-LAT
observations. The study is focused on detecting the presence of linear or
quadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has
identified 40 jetted AGN that exhibit long-term trends, each with distinct
properties, which we also characterize in this study. These long-term trends
may originate from the dynamics of a supermassive black hole binary system, or
they could be the result of intrinsic phenomena within the jet itself. Our
findings can help in addressing questions pertaining to the astrophysical
origins of variability and periodicity within jetted AGN.",['astro-ph.HE'],False,,,,"The nature of an imaginary quasi-periodic oscillation in the
  soft-to-hard transition of MAXI J1820+070","Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active
  Galactic Nuclei"
neg-d2-22,2025-02-11,,2502.0867," We study the effectiveness of the time-localised principal resolvent forcing
mode at actuating the near wall cycle of turbulence. The mode is restricted to
a wavelet pulse and computed from an SVD of the windowed wavelet-based
resolvent operator so that it produces the largest amplification via the
linearised Navier-Stokes equations. We then inject this time-localised mode
into the turbulent minimal flow unit at different intensities, and measure the
instantaneous deviation of the system's response from the optimal resolvent
response mode. This is possible under the new formulation, which enables the
modes to represent transient trajectories. For the most energetic spatial wave
numbers in the minimal flow unit -- constant in the streamwise direction and
once-periodic in the spanwise direction -- the forcing mode takes the shape of
streamwise rolls and produces a response mode in the form of streamwise streaks
that transiently grow and decay. For initial times and close to the wall, the
DNS response matches the principal response mode well, but due to
nonlinearities, the response across all forcing intensities decays prematurely,
and a higher forcing intensity leads to faster energy decay. The principal
forcing mode still leads to significant energy amplification and is more
effective than a randomly-generated forcing structure and the second suboptimal
resolvent forcing mode at amplifying the near-wall streaks. We compute the
nonlinear energy transfer to secondary modes and observe that the breakdown of
the actuated mode proceeds similarly across all forcing intensities: in the
near-wall region, the induced streak forks into a structure twice periodic in
the spanwise direction; in the outer region, the streak breaks up into a
structure that is once periodic in the streamwise direction. In both regions,
spanwise gradients account for the dominant share of nonlinear energy transfer.",['physics.flu-dyn'],2503.08331," Modern telescopes provide breathtaking images of nebulae, clouds and galaxies
shaped by gravity-driven interactions between complex bodies. While such
structures are prevalent on an astrophysical scale, they are rarely observed at
the human scale. In this letter, we report the observations of the complex
orbits, collision, and coalescence of droplets on a soap film, forming
structures such as bridges and spiral arms, reminiscent of their astrophysical
counterparts. These dynamics emerge from attractive forces caused by
gravito-capillary-driven distortions of the supporting soap film. Long orbits
and intricate coalescence mechanisms are enabled by the small dissipation in
the soap film and the fluidic nature of the droplets and supporting film,
respectively. The existence of stable droplets within the soap film featuring a
universal radius, as well as the attractive potentials, are explained through a
careful comparison of experimental data with models computing the distortions
of the supporting soap film. This work opens perspectives to",['physics.flu-dyn'],False,,,,"Transient growth and nonlinear breakdown of wavelet-based resolvent
  modes in turbulent channel flow","Orbiting, colliding and merging droplets on a soap film: toward
  gravitational analogues"
neg-d2-23,2025-02-19,,2502.13448," In this paper, we establish three criteria for the asymptotic behavior of
Markov-Feller semigroups. First, we present a criterion for convergence in
total variation to a unique invariant measure, requiring only $TV$-eventual
continuity of the semigroup at a single point. Second, we propose two new
criteria for asymptotic stability that require eventual continuity at a single
point. This localized condition is more practical and easier to check. To
illustrate the advantages of our framework, we provide an explicit example
where verifying eventual continuity at a single point is straightforward,
whereas establishing the corresponding global property is challenging.",['math.PR'],2503.04166," We study the composition of bivariate L\'evy process with bivariate inverse
subordinator. The explicit expressions for its dispersion and auto correlation
matrices are obtained. Also, the time-changed two parameter L\'evy processes
with rectangular increments are studied. We introduce some time-changed
variants of the Poisson random field in plane with and without drift, and
derive the associated fractional differential equations for their
distributions. Later, we consider some time-changed L\'evy processes where the
time-changing components are two parameter Poisson random fields with drifts.
Moreover, two parameter coordinatewise semigroup operators associated with some
of the introduced processes are discussed.",['math.PR'],False,,,,"Criteria for asymptotic stability of eventually continuous Markov-Feller
  semigroups",On Two Parameter Time-Changed Poisson Random Fields with Drifts
neg-d2-24,2025-02-27,,2502.19769," Significant advancements have been achieved in the realm of understanding
poses and interactions of two hands manipulating an object. The emergence of
augmented reality (AR) and virtual reality (VR) technologies has heightened the
demand for real-time performance in these applications. However, current
state-of-the-art models often exhibit promising results at the expense of
substantial computational overhead. In this paper, we present a query-optimized
real-time Transformer (QORT-Former), the first Transformer-based real-time
framework for 3D pose estimation of two hands and an object. We first limit the
number of queries and decoders to meet the efficiency requirement. Given
limited number of queries and decoders, we propose to optimize queries which
are taken as input to the Transformer decoder, to secure better accuracy: (1)
we propose to divide queries into three types (a left hand query, a right hand
query and an object query) and enhance query features (2) by using the contact
information between hands and an object and (3) by using three-step update of
enhanced image and query features with respect to one another. With proposed
methods, we achieved real-time pose estimation performance using just 108
queries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing
state-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right
hand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)
and 10.4% (object), our method excels in accuracy. Additionally, it sets the
state-of-the-art in interaction recognition, maintaining real-time efficiency
with an off-the-shelf action recognition module.",['cs.CV'],2501.08174," Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.",['cs.CV'],False,,,,"QORT-Former: Query-optimized Real-time Transformer for Understanding Two
  Hands Manipulating Objects","Object-Centric 2D Gaussian Splatting: Background Removal and
  Occlusion-Aware Pruning for Compact Object Models"
neg-d2-25,2025-03-19,,2503.1501," We propose using a dielectric beveled nozzle for electrospray and
electrohydrodynamic jet printing. This nozzle stabilizes the liquid ejection of
low-conductivity liquids, considerably reducing the minimum flow rate below
which the flow becomes unstable. This translates into a significant reduction
of the minimum jet diameter. Due to its dielectric character, electrochemical
reactions occurring in metallic beveled nozzles (e.g. hypodermic needles) do
not occur, preserving the purity of the liquid. This property makes this nozzle
appropriate for Electrospray Ionization Mass Spectrometry (ESI-MS) or
bioplotting. We illustrate the capabilities of this new technique by conducting
(i) electrospray experiments with Newtonian liquids and (ii)
electrohydrodynamic jet printing experiments with viscoelastic fluids. Jets
with diameters around 1 $\mu$m are produced with low-conductivity liquids such
as octanol and glycerine. Viscoelastic threads a few microns in diameter are
gently deposited on a moving substrate to print out uniform lines tens of
nanometers in height. Due to the strong stabilizing effect of the beveled
nozzle, the minimum flow rate and jet diameter were much smaller than the
respective values obtained with the cylindrical capillary in the electrospray
and electrohydrodynamic jet printing experiments. The proposed technique opens
new routes for electrospray and electrohydrodynamic jet printing.",['physics.flu-dyn'],2501.01137," Since the geometry structure of ultra-high-pressure (UHP) water-jet nozzle is
a critical factor to enhance its hydrodynamic performance, it is critical to
obtain a suitable geometry for a UHP water jet nozzle. In this study, a
CFD-based optimization loop for UHP nozzle structure has been developed by
integrating an approximate model to optimize nozzle structure for increasing
the radial peak wall shear stress. In order to improve the optimization
accuracy of the sparrow search algorithm (SSA), an enhanced version called the
Logistic-Tent chaotic sparrow search algorithm (LTC-SSA) is proposed. The
LTC-SSA algorithm utilizes the Logistic-Tent Chaotic (LTC) map, which is
designed by combining the Logistic and Tent maps. This new approach aims to
overcome the shortcoming of ""premature convergence"" for the SSA algorithm by
increasing the diversity of the sparrow population. In addition, to improve the
prediction accuracy of peak wall shear stress, a data prediction method based
on LTC-SSA-support vector machine (SVM) is proposed. Herein, LTC-SSA algorithm
is used to train the penalty coefficient C and parameter gamma g of SVM model.
In order to build LTC-SSA-SVM model, optimal Latin hypercube design (Opt LHD)
is used to design the sampling nozzle structures, and the peak wall shear
stress (objective function) of these nozzle structures are calculated by CFD
method. For the purpose of this article, this optimization framework has been
employed to optimize original nozzle structure. The results show that the
optimization framework developed in this study can be used to optimize nozzle
structure with significantly improved its hydrodynamic performance.",['physics.flu-dyn'],False,,,,A new emitter for electrospray and electrohydrodynamic jet printing,"Computational fluid dynamics-based structure optimization of
  ultra-high-pressure water-jet nozzle using approximation method"
neg-d2-26,2025-03-10,,2503.07141," We consider one-dimensional, integrable many-body classical and quantum
systems in thermal equilibrium. In the classical case, we use the classical
limit of the Bethe equations to obtain a self-consistent integral equation
whose solution gives the distribution of asymptotic Bethe momenta, or
rapidities, as well as the classical partition function in the canonical
ensemble, and the thermal energy dispersion. For quantum gases, we obtain a
similar integral equation, albeit in the grand canonical ensemble, with
completely analogous results. We apply our theory to the classical and quantum
Tonks and Calogero-Sutherland models, and our results are in perfect agreement
with standard calculations using Yang-Yang thermodynamics. Remarkably, we show
in a straightforward manner that the thermodynamics of the quantum
Calogero-Sutherland model is in one-to-one correspondence with the ideal Fermi
gas upon simple rescalings of chemical potential and density.",['cond-mat.quant-gas'],2503.08629," Due to their photonic components, exciton-polariton systems provide a
convenient platform to study the coherence properties of weakly-interacting
Bose gases and the Bose-Einstein condensate transition. In particular, optical
interferometry enables the measurement of the first-order coherence function
which provides insight into the phase of the system. In this paper, we analyze
the buildup of coherence in finite-sized, noninteracting, equilibrium Bose
gases through the condensate fraction and the related coherent fraction,
defined via the first-order coherence function. Our results provide a baseline
to compare against experimental data. Discrepancies may indicate where
interacting or nonequilibrium models are necessary to describe the system. In
the normal phase, before the Bose-Einstein condensate transition, Bose gases
exhibit partial spatial and temporal coherence. This significantly alters the
paraxial propagation and interference of optical signals from exciton-polariton
systems. Therefore, we also analyze diffraction related to the introduction of
apertures and time-delay between interferometry arms, given the partial
coherence of the source. Comparison to experiment shows remarkable agreement
with the noninteracting Bose gas theory, even approaching the quasi-condensate
regime.",['cond-mat.quant-gas'],False,,,,Exact Thermal Distributions in Integrable Classical and Quantum Gases,Optical probes of coherence in two dimensional Bose gases of polaritons
neg-d2-27,2025-03-01,,2503.00641," Post-hoc importance attribution methods are a popular tool for ""explaining""
Deep Neural Networks (DNNs) and are inherently based on the assumption that the
explanations can be applied independently of how the models were trained.
Contrarily, in this work we bring forward empirical evidence that challenges
this very notion. Surprisingly, we discover a strong dependency on and
demonstrate that the training details of a pre-trained model's classification
layer (less than 10 percent of model parameters) play a crucial role, much more
than the pre-training scheme itself. This is of high practical relevance: (1)
as techniques for pre-training models are becoming increasingly diverse,
understanding the interplay between these techniques and attribution methods is
critical; (2) it sheds light on an important yet overlooked assumption of
post-hoc attribution methods which can drastically impact model explanations
and how they are interpreted eventually. With this finding we also present
simple yet effective adjustments to the classification layers, that can
significantly enhance the quality of model explanations. We validate our
findings across several visual pre-training frameworks (fully-supervised,
self-supervised, contrastive vision-language training) and analyse how they
impact explanations for a wide range of attribution methods on a diverse set of
evaluation metrics.",['cs.CV'],2501.08837," Long-term dense action anticipation is very challenging since it requires
predicting actions and their durations several minutes into the future based on
provided video observations. To model the uncertainty of future outcomes,
stochastic models predict several potential future action sequences for the
same observation. Recent work has further proposed to incorporate uncertainty
modelling for observed frames by simultaneously predicting per-frame past and
future actions in a unified manner. While such joint modelling of actions is
beneficial, it requires long-range temporal capabilities to connect events
across distant past and future time points. However, the previous work
struggles to achieve such a long-range understanding due to its limited and/or
sparse receptive field. To alleviate this issue, we propose a novel MANTA
(MAmba for ANTicipation) network. Our model enables effective long-term
temporal modelling even for very long sequences while maintaining linear
complexity in sequence length. We demonstrate that our approach achieves
state-of-the-art results on three datasets - Breakfast, 50Salads, and
Assembly101 - while also significantly improving computational and memory
efficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .",['cs.CV'],False,,,,"How to Probe: Simple Yet Effective Techniques for Improving Post-hoc
  Explanations","MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation"
neg-d2-28,2025-03-21,,2503.17176," We proved that for every sufficiently large $n$, the complete graph $K_{2n}$
with an arbitrary edge signing $\sigma: E(K_{2n}) \to \{-1, +1\}$ admits a high
discrepancy $1$-factor decomposition. That is, there exists a universal
constant $c > 0$ such that every edge-signed $K_{2n}$ has a perfect matching
decomposition $\{\psi_1, \ldots, \psi_{2n-1}\}$, where for each perfect
matching $\psi_i$, the discrepancy $\lvert \frac{1}{n} \sum_{e\in E(\psi_i)}
\sigma(e) \rvert$ is at least $c$.",['math.CO'],2503.0882," Recently, Pan and Yu showed that Lascoux polynomials can be defined in terms
of certain collections of diagrams consisting of unit cells arranged in the
first quadrant. Starting from certain initial diagrams, one forms a finite set
of diagrams by applying two types of moves: Kohnert and ghost moves. Both moves
cause at most one cell to move to a lower row with ghost moves leaving a new
""ghost cell"" in its place. Each diagram formed in this way defines a monomial
in the associated Lascoux polynomial. Restricting attention to diagrams formed
by applying sequences of only Kohnert moves in the definition of Lascoux
polynomials, one obtains the family of key polynomials. Recent articles have
considered a poset structure on the collections of diagrams formed when one
uses only Kohnert moves. In general, these posets are not ""well-behaved,"" not
usually having desirable poset properties. Here, as an intermediate step to
studying the analogous posets associated with Lascoux polynomials, we consider
the posets formed by restricting attention to those diagrams formed by using
only ghost moves. Unlike in the case of Kohnert posets, we show that such
""ghost Kohnert posets"" are always ranked join semi-lattices. In addition, we
establish a necessary condition for when ghost Kohnert posets are bounded and,
consequently, lattices.",['math.CO'],False,,,,On high discrepancy $1$-factorizations of complete graphs,Ghost Kohnert posets
neg-d2-29,2025-01-07,,2501.0419," In the last decade, various works have used statistics on relations to
improve both the theory and practice of conjunctive query execution. Starting
with the AGM bound which took advantage of relation sizes, later works
incorporated statistics like functional dependencies and degree constraints.
Each new statistic prompted work along two lines; bounding the size of
conjunctive query outputs and worst-case optimal join algorithms. In this work,
we continue in this vein by introducing a new statistic called a
\emph{partition constraint}. This statistic captures latent structure within
relations by partitioning them into sub-relations which each have much tighter
degree constraints. We show that this approach can both refine existing
cardinality bounds and improve existing worst-case optimal join algorithms.",['cs.DB'],2501.0419," In the last decade, various works have used statistics on relations to
improve both the theory and practice of conjunctive query execution. Starting
with the AGM bound which took advantage of relation sizes, later works
incorporated statistics like functional dependencies and degree constraints.
Each new statistic prompted work along two lines; bounding the size of
conjunctive query outputs and worst-case optimal join algorithms. In this work,
we continue in this vein by introducing a new statistic called a
\emph{partition constraint}. This statistic captures latent structure within
relations by partitioning them into sub-relations which each have much tighter
degree constraints. We show that this approach can both refine existing
cardinality bounds and improve existing worst-case optimal join algorithms.",['cs.DB'],False,,,,"Partition Constraints for Conjunctive Queries: Bounds and Worst-Case
  Optimal Joins","Partition Constraints for Conjunctive Queries: Bounds and Worst-Case
  Optimal Joins"
neg-d2-30,2025-02-28,,2502.20812," The rapid growth of Large Language Models (LLMs) and AI-driven applications
has propelled Vector Database Management Systems (VDBMSs) into the spotlight as
a critical infrastructure component. VDBMS specializes in storing, indexing,
and querying dense vector embeddings, enabling advanced LLM capabilities such
as retrieval-augmented generation, long-term memory, and caching mechanisms.
However, the explosive adoption of VDBMS has outpaced the development of
rigorous software testing methodologies tailored for these emerging systems.
Unlike traditional databases optimized for structured data, VDBMS face unique
testing challenges stemming from the high-dimensional nature of vector data,
the fuzzy semantics in vector search, and the need to support dynamic data
scaling and hybrid query processing. In this paper, we begin by conducting an
empirical study of VDBMS defects and identify key challenges in test input
generation, oracle definition, and test evaluation. Drawing from these
insights, we propose the first comprehensive research roadmap for developing
effective testing methodologies tailored to VDBMS. By addressing these
challenges, the software testing community can contribute to the development of
more reliable and trustworthy VDBMS, enabling the full potential of LLMs and
data-intensive AI applications.",['cs.SE'],2501.06894," Quantum computing is an emerging field with significant potential, yet
software development and maintenance challenges limit its accessibility and
maturity. This work investigates the current state, evolution, and maintenance
practices in the quantum computing community by conducting a large-scale mining
analysis of over 21,000 quantum software repositories on GitHub, containing
more than 1.2 million commits contributed by over 10,000 unique developers.
Specifically, the focus of this paper is to: (i) assess the community's status
and growth by examining the popularity of quantum computing, trends in
programming languages and framework usage, growth of contributors, and insights
from repository documentation; and (ii) analyze maintenance practices through
commit patterns, issue classification, and maintenance levels. Our findings
indicate rapid growth in the quantum computing community, with a 200% increase
in the number of repositories and a 150% rise in contributors since 2017. Our
analysis of commits shows a strong focus on perfective updates, while the
relatively low number of corrective commits highlights potential gaps in bug
resolution. Furthermore, one-third of the quantum computing issues highlight
the need for specialized tools in addition to general software infrastructure.
In summary, this work provides a foundation for targeted improvements in
quantum software to support sustained growth and technical advancement. Based
on our analysis of development activity, community structure, and maintenance
practices, this study offers actionable recommendations to enhance quantum
programming tools, documentation, and resources. We are also open-sourcing our
dataset to support further analysis by the community and to guide future
research and tool development for quantum computing.",['cs.SE'],False,,,,"Towards Reliable Vector Database Management Systems: A Software Testing
  Roadmap for 2030","Analyzing the Evolution and Maintenance of Quantum Computing
  Repositories"
neg-d2-31,2025-01-01,,2501.01028," As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.",['cs.CL'],2502.12464," Deploying large language models (LLMs) in real-world applications requires
robust safety guard models to detect and block harmful user prompts. While
large safety guard models achieve strong performance, their computational cost
is substantial. To mitigate this, smaller distilled models are used, but they
often underperform on ""hard"" examples where the larger model provides accurate
predictions. We observe that many inputs can be reliably handled by the smaller
model, while only a small fraction require the larger model's capacity.
Motivated by this, we propose SafeRoute, a binary router that distinguishes
hard examples from easy ones. Our method selectively applies the larger safety
guard model to the data that the router considers hard, improving efficiency
while maintaining accuracy compared to solely using the larger safety guard
model. Experimental results on multiple benchmark datasets demonstrate that our
adaptive model selection significantly enhances the trade-off between
computational cost and safety performance, outperforming relevant baselines.",['cs.CL'],False,,,,KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model,"SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety
  Guardrails in Large Language Models"
neg-d2-32,2025-02-17,,2502.11479," Kitaev quantum spin liquids have attracted significant attention in condensed
matter physics over the past decade. To understand their emergent quantum
phenomena, high-quality single crystals of substantial size are essential.
Here, we report the synthesis of single crystals of the Kitaev quantum spin
liquid candidate RuBr3, achieving millimeter-sized crystals through a self-flux
method under high pressure and high temperature conditions. The crystals
exhibit well-defined cleavage planes with a lustrous appearance. Transport
characterizations exhibit a narrow band-gap semiconducting behavior with 0.13
eV and 0.11 eV band-gap in ab plane and along c axis, respectively. Magnetic
measurement shows a transition to antiferromagnetic (AFM) state at
approximately 29 K both in ab plane and along c axis. Notably, the N\'eel
temperature increases to 34 K with an applied magnetic field of up to 7 T in
the ab plane, but without any change along c axis. The large size and high
quality of RuBr3 single crystals provide a valuable platform for investigating
various interactions, particularly the Kitaev interaction, and for elucidating
the intrinsic physical properties of Kitaev quantum spin liquids.",['cond-mat.str-el'],2503.00484," Realizing a quantum critical point (QCP) in clean ferromagnetic (FM) metals
has remained elusive due to the coupling of magnetization to the electronic
soft modes that drive the transition to be of first order. However, by
introducing a suitable amount of quenched disorder, one can still establish a
QCP in ferromagnets. In this study, we ascertain that the itinerant ferromagnet
Ni$_{1-x}$Mo$_{x}$ exhibits a FM QCP at a critical doping of $x_c \simeq
0.125$. Through magnetization and muon-spin relaxation measurements, we
demonstrate that the FM ordering temperature is suppressed continuously to zero
at $x_c$, while the magnetic volume fraction remains $100\%$ up to $x_c$,
indicating a second-order phase transition. The QCP is accompanied by a
non-Fermi liquid behavior, as evidenced by the logarithmic divergence of the
specific heat and the linear temperature dependence of the low-temperature
resistivity. Our findings reveal a minimal effect of disorder on the critical
spin dynamics of Ni$_{1-x}$Mo$_{x}$ at $x_c$, highlighting it as one of the
rare systems to exhibit a clean FM QCP.",['cond-mat.str-el'],False,,,,"High Quality Single Crystal of Kitaev Spin Liquid Candidate Material
  RuBr3 Synthesized under High Pressure","Magnetic order and spin dynamics across the ferromagnetic quantum
  critical point in Ni\boldmath{$_{1-x}$}Mo\boldmath{$_{x}$}"
neg-d2-33,2025-01-20,,2501.11411," Coupling Large Language Models (LLMs) with Evolutionary Algorithms has
recently shown significant promise as a technique to design new heuristics that
outperform existing methods, particularly in the field of combinatorial
optimisation. An escalating arms race is both rapidly producing new heuristics
and improving the efficiency of the processes evolving them. However, driven by
the desire to quickly demonstrate the superiority of new approaches, evaluation
of the new heuristics produced for a specific domain is often cursory: testing
on very few datasets in which instances all belong to a specific class from the
domain, and on few instances per class. Taking bin-packing as an example, to
the best of our knowledge we conduct the first rigorous benchmarking study of
new LLM-generated heuristics, comparing them to well-known existing heuristics
across a large suite of benchmark instances using three performance metrics.
For each heuristic, we then evolve new instances won by the heuristic and
perform an instance space analysis to understand where in the feature space
each heuristic performs well. We show that most of the LLM heuristics do not
generalise well when evaluated across a broad range of benchmarks in contrast
to existing simple heuristics, and suggest that any gains from generating very
specialist heuristics that only work in small areas of the instance space need
to be weighed carefully against the considerable cost of generating these
heuristics.",['cs.NE'],2502.05824," Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base
stations (BSs) to improve terrestrial communications. However, the limited
onboard energy and antenna power of a UAV restrict its communication range and
transmission capability. To address these limitations, this work employs
collaborative beamforming through a UAV-enabled virtual antenna array to
improve transmission performance from the UAV to terrestrial mobile users,
under interference from non-associated BSs and dynamic channel conditions.
Specifically, we introduce a memory-based random walk model to more accurately
depict the mobility patterns of terrestrial mobile users. Following this, we
formulate a multi-objective optimization problem (MOP) focused on maximizing
the transmission rate while minimizing the flight energy consumption of the UAV
swarm. Given the NP-hard nature of the formulated MOP and the highly dynamic
environment, we transform this problem into a multi-objective Markov decision
process and propose an improved evolutionary multi-objective reinforcement
learning algorithm. Specifically, this algorithm introduces an evolutionary
learning approach to obtain the approximate Pareto set for the formulated MOP.
Moreover, the algorithm incorporates a long short-term memory network and
hyper-sphere-based task selection method to discern the movement patterns of
terrestrial mobile users and improve the diversity of the obtained Pareto set.
Simulation results demonstrate that the proposed method effectively generates a
diverse range of non-dominated policies and outperforms existing methods.
Additional simulations demonstrate the scalability and robustness of the
proposed CB-based method under different system parameters and various
unexpected circumstances.",['cs.NE'],False,,,,Beyond the Hype: Benchmarking LLM-Evolved Heuristics for Bin Packing,"Aerial Reliable Collaborative Communications for Terrestrial Mobile
  Users via Evolutionary Multi-Objective Deep Reinforcement Learning"
neg-d2-34,2025-02-25,,2502.18363," As robotics advances toward integrating soft structures, anthropomorphic
shapes, and complex tasks, soft and highly stretchable mechanotransducers are
becoming essential. To reliably measure tactile and proprioceptive data while
ensuring shape conformability, stretchability, and adaptability, researchers
have explored diverse transduction principles alongside scalable and versatile
manufacturing techniques. Nonetheless, many current methods for stretchable
sensors are designed to produce a single sensor configuration, thereby limiting
design flexibility. Here, we present an accessible, flexible, printing-based
fabrication approach for customizable, stretchable sensors. Our method employs
a custom-built printhead integrated with a commercial 3D printer to enable
direct ink writing (DIW) of conductive ink onto cured silicone substrates. A
layer-wise fabrication process, facilitated by stackable trays, allows for the
deposition of multiple liquid conductive ink layers within a silicone matrix.
To demonstrate the method's capacity for high design flexibility, we fabricate
and evaluate both capacitive and resistive strain sensor morphologies.
Experimental characterization showed that the capacitive strain sensor
possesses high linearity (R^2 = 0.99), high sensitivity near the 1.0
theoretical limit (GF = 0.95), minimal hysteresis (DH = 1.36%), and large
stretchability (550%), comparable to state-of-the-art stretchable strain
sensors reported in the literature.",['cs.RO'],2503.07547," In human-robot interactions, human and robot agents maintain internal mental
models of their environment, their shared task, and each other. The accuracy of
these representations depends on each agent's ability to perform theory of
mind, i.e. to understand the knowledge, preferences, and intentions of their
teammate. When mental models diverge to the extent that it affects task
execution, reconciliation becomes necessary to prevent the degradation of
interaction. We propose a framework for bi-directional mental model
reconciliation, leveraging large language models to facilitate alignment
through semi-structured natural language dialogue. Our framework relaxes the
assumption of prior model reconciliation work that either the human or robot
agent begins with a correct model for the other agent to align to. Through our
framework, both humans and robots are able to identify and communicate missing
task-relevant context during interaction, iteratively progressing toward a
shared mental model.",['cs.RO'],False,,,,"Stretchable Capacitive and Resistive Strain Sensors: Accessible
  Manufacturing Using Direct Ink Writing","Bi-Directional Mental Model Reconciliation for Human-Robot Interaction
  with Large Language Models"
neg-d2-35,2025-02-06,,2502.03959," The hard-sphere potential has become a cornerstone in the study of both
molecular and complex fluids. Despite its mathematical simplicity, its
implementation in fixed time-step molecular simulations remains a formidable
challenge due to the discontinuity at contact. To circumvent the issues
associated with the ill-defined force at contact, a continuous
potential--referred to here as the pseudo-hard-sphere (pHS) potential--has
recently been proposed [J. Chem, Phys. 149, 164907 (2018)]. This potential is
constructed to match the second virial coefficient of the hard-sphere potential
and is expected to mimic its thermodynamic properties. However, this hypothesis
has only been partially validated within the fluid region of the phase diagram
for hard-sphere dispersions in two and three dimensions. In this contribution,
we examine the ability of the continuous pHS potential to reproduce the
equation of state of a hard-sphere fluid, not only in the fluid phase but also
across the fluid-solid coexistence region. Our focus is primarily on
hard-sphere systems in three and four dimensions. We compare the results
obtained from Brownian dynamics simulations of the pHS potential with those
derived from refined event-driven simulations of the corresponding hard-sphere
potential. Furthermore, we provide a comparative analysis with theoretical
equations of state based on both mean-field and integral equation
approximations.",['cond-mat.soft'],2503.08894," Origami metamaterials made of repeating unit cells of parallelogram panels
joined at folds dramatically change their shape through a collective motion of
their cells. Here we develop an effective elastic model and numerical method to
study the large deformation response of these metamaterials under a broad class
of loads. The model builds on an effective plate theory derived in our prior
work [64]. The theory captures the overall shape change of all slightly
stressed parallelogram origami deformations through nonlinear geometric
compatibility constraints that couple the origami's (cell averaged) effective
deformation to an auxiliary angle field quantifying its cell-by-cell actuation.
It also assigns to each such origami deformation a plate energy associated to
these effective fields. Seeking a constitutive model that is faithful to the
theory but also practical to simulate, we relax the geometric constraints via
corresponding elastic energy penalties; we also simplify the plate energy
density to embrace its essential character as a regularization to the geometric
penalties. The resulting model for parallelogram origami is a generalized
elastic continuum that is nonlinear in the effective deformation gradient and
angle field and regularized by high-order gradients thereof. We provide a
finite element formulation of this model using the $C^0$ interior penalty
method to handle second gradients of deformation, and implement it using the
open source computing platform Firedrake. We end by using the model and
numerical method to study two canonical parallelogram origami patterns, in
Miura and Eggbox origami, under a variety of loading conditions.",['cond-mat.soft'],False,,,,"Phase diagram of the hard-sphere potential model in three and four
  dimensions using a pseudo-hard-sphere potential","Modeling and computation of the effective elastic behavior of
  parallelogram origami metamaterials"
neg-d2-36,2025-03-05,,2503.03613," Despite its prevalent use in image-text matching tasks in a zero-shot manner,
CLIP has been shown to be highly vulnerable to adversarial perturbations added
onto images. Recent studies propose to finetune the vision encoder of CLIP with
adversarial samples generated on the fly, and show improved robustness against
adversarial attacks on a spectrum of downstream datasets, a property termed as
zero-shot robustness. In this paper, we show that malicious perturbations that
seek to maximise the classification loss lead to `falsely stable' images, and
propose to leverage the pre-trained vision encoder of CLIP to counterattack
such adversarial images during inference to achieve robustness. Our paradigm is
simple and training-free, providing the first method to defend CLIP from
adversarial attacks at test time, which is orthogonal to existing methods
aiming to boost zero-shot adversarial robustness of CLIP. We conduct
experiments across 16 classification datasets, and demonstrate stable and
consistent gains compared to test-time defence methods adapted from existing
adversarial robustness studies that do not rely on external networks, without
noticeably impairing performance on clean images. We also show that our
paradigm can be employed on CLIP models that have been adversarially finetuned
to further enhance their robustness at test time. Our code is available
\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",['cs.CV'],2503.17453," This article presents our results for the eighth Affective Behavior Analysis
in-the-wild (ABAW) competition.Multimodal emotion recognition (ER) has
important applications in affective computing and human-computer interaction.
However, in the real world, compound emotion recognition faces greater issues
of uncertainty and modal conflicts. For the Compound Expression (CE)
Recognition Challenge,this paper proposes a multimodal emotion recognition
method that fuses the features of Vision Transformer (ViT) and Residual Network
(ResNet). We conducted experiments on the C-EXPR-DB and MELD datasets. The
results show that in scenarios with complex visual and audio cues (such as
C-EXPR-DB), the model that fuses the features of ViT and ResNet exhibits
superior performance.Our code are avalible on
https://github.com/MyGitHub-ax/8th_ABAW",['cs.CV'],False,,,,"CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards
  Zero-shot Adversarial Robustness of CLIP","Feature-Based Dual Visual Feature Extraction Model for Compound
  Multimodal Emotion Recognition"
neg-d2-37,2025-02-15,,2502.10901," This paper constructs two involutions on tip-augmented plane trees, as
defined by Donaghey, that interchange two distinct types of leaves while
preserving all other leaves. These two involutions provide bijective
explanations addressing a question posed by Dong, Du, Ji, and Zhang in their
work.",['math.CO'],2502.15647," We introduce two new families of permutation group polynomials over finite
fields of arbitrary characteristic, which are special types of bivariate local
permutation polynomials. For each family, we explicitly construct their
companions. Furthermore, we precisely determine the total number of permutation
group polynomials equivalent to the proposed families. Moreover, we resolve the
problem of enumerating permutation group polynomials that are equivalent to
$e$-Klenian polynomials over finite fields for $e\geq 1$, a problem previously
noted as nontrivial by Gutierrez and Urroz (2023).",['math.CO'],False,,,,Involutions on Tip-Augmented Plane Trees for Leaf Interchanging,"Bivariate local permutation polynomials, their companions, and related
  enumeration results"
neg-d2-38,2025-03-17,,2503.13226," The same real-world entity (e.g., a movie, a restaurant, a person) may be
described in various ways on different datasets. Entity Resolution (ER) aims to
find such different descriptions of the same entity, this way improving data
quality and, therefore, data value. However, an ER pipeline typically involves
several steps (e.g., blocking, similarity estimation, clustering), with each
step requiring its own configurations and tuning. The choice of the best
configuration, among a vast number of possible combinations, is a
dataset-specific and labor-intensive task both for novice and expert users,
while it often requires some ground truth knowledge of real matches. In this
work, we examine ways of automatically configuring a state of-the-art
end-to-end ER pipeline based on pre-trained language models under two settings:
(i) When ground truth is available. In this case, sampling strategies that are
typically used for hyperparameter optimization can significantly restrict the
search of the configuration space. We experimentally compare their relative
effectiveness and time efficiency, applying them to ER pipelines for the first
time. (ii) When no ground truth is available. In this case, labelled data
extracted from other datasets with available ground truth can be used to train
a regression model that predicts the relative effectiveness of parameter
configurations. Experimenting with 11 ER benchmark datasets, we evaluate the
relative performance of existing techniques that address each problem, but have
not been applied to ER before.",['cs.DB'],2501.0419," In the last decade, various works have used statistics on relations to
improve both the theory and practice of conjunctive query execution. Starting
with the AGM bound which took advantage of relation sizes, later works
incorporated statistics like functional dependencies and degree constraints.
Each new statistic prompted work along two lines; bounding the size of
conjunctive query outputs and worst-case optimal join algorithms. In this work,
we continue in this vein by introducing a new statistic called a
\emph{partition constraint}. This statistic captures latent structure within
relations by partitioning them into sub-relations which each have much tighter
degree constraints. We show that this approach can both refine existing
cardinality bounds and improve existing worst-case optimal join algorithms.",['cs.DB'],False,,,,Auto-Configuring Entity Resolution Pipelines,"Partition Constraints for Conjunctive Queries: Bounds and Worst-Case
  Optimal Joins"
neg-d2-39,2025-02-12,,2502.08713," In galaxies, the flattening of the spectrum at low radio frequencies below
300 MHz has been the subject of some debate. A turnover at low frequencies
could be caused by multiple physical processes, which can yield new insights
into the properties of the ionised gas in the interstellar medium. We
investigate the existence and nature of the low-frequency turnover in the HII
regions of M 101. We study the nearby galaxy M 101 using the LOw Frequency
ARray (LOFAR) at frequencies of 54 and 144 MHz, Apertif at 1370 MHz, and
published combined map from the Very Large Array (VLA) and Effelesberg
telescope at 4850 MHz. The spectral index between 54 and 144 MHz is inverted at
the centres of HII regions. We find a significant low-frequency flattening at
the centres of five out of six HII regions that we selected for this study. The
low frequency flattening in HII regions of M 101 can be explained with two
different free-free absorption models. The flattening is localised in a region
smaller than 1.5 kpc and can only be detected with high resolution (better than
45''). The detection of low frequency flattening has important consequences for
using radio continuum observations below 100 MHz to measure extinction-free
star-formation rates.",['astro-ph.GA'],2503.02204," The gas-phase abundances of deuterium (D) in the local interstellar medium
(ISM) exhibit considerable regional variations. Particularly, in some regions
the gas-phase D abundances are substantially lower than the primordial D
abundance generated in the Big Bang, after subtracting the astration reduction
caused by the Galactic chemical evolution. Deuterated polycyclic aromatic
hydrocarbon (PAH) molecules have been suggested as a potential reservoir of the
D atoms missing from the gas-phase. Recent observations from the James Webb
Space Telescope's Near Infrared Spectrograph have revealed the widespread of
deuterated PAHs in the Orion Bar through their aliphatic C--D emission at
4.65${\,{\rm \mu m}}$ and possibly aromatic C--D emission at 4.4${\,{\rm \mu
m}}$ as well. To examine the viability of deuterated PAHs as the D reservoir,
we model the infrared (IR) emission spectra of small PAH molecules containing
various aromatic and aliphatic D atoms in the Orion Bar. We find that small
deuterated PAHs exhibit a noticeable emission band at 4.4 or 4.65${\,{\rm \mu
m}}$ even if they contain only one aromatic or aliphatic D atom. We derive
${{N_{\rm D,ali}}}/{N_{\rm H}}\approx3.4\%$, the deuteration degree of PAHs
measured as the number of aliphatic D atoms (relative to H), from the observed
intensity ratios of the 4.65${\,{\rm \mu m}}$ band to the 3.3${\,{\rm \mu m}}$
aromatic C--H band. The deuteration degree for aromatically-deuterated PAHs is
less certain as C--N stretch also contributes to the observed emission around
4.4${\,{\rm \mu m}}$. If we attribute it exclusively to aromatic C--D, we
derive an upper limit of $\approx14\%$ on the deuteration degree, which is
capable of accounting for an appreciable fraction of the missing D budget.",['astro-ph.GA'],False,,,,"The low-frequency flattening of the radio spectrum of giant HII regions
  in M 101","Deuterated Polycyclic Aromatic Hydrocarbons in the Interstellar Medium:
  Constraints from the Orion Bar as Observed by the James Webb Space Telescope"
neg-d2-40,2025-02-03,,2502.01494," We demonstrate the ability to experimentally measure fluctuations of the
convective heat transfer coefficient at the wall in a turbulent boundary layer.
For this, we measure two-dimensional fields of wall-temperature fluctuations
beneath a zero-pressure-gradient turbulent boundary layer, at two moderate
friction Reynolds numbers ($Re_\tau \approx 990$ and $Re_\tau \approx 1800$).
Spatiotemporal data of wall-temperature are acquired by means of a
heated-thin-foil sensor as sensing hardware, and an infrared camera as
temperature detector. At low $Re_\tau$ conditions, the fields of the Nusselt
number fluctuations are populated by elongated structures comprising streamwise
and spanwise length scales comparable to those of near-wall streaks. At higher
$Re_\tau$ conditions, the effective width and length of the coherent $Nu$
fluctuations increases. These findings are based on two-point correlations, as
well as streamwise-spanwise energy spectra of $Nu$ fluctuations. The convective
velocities of the $Nu$ fluctuations are also computed with the available time
resolution from the measurements. This allows for resolving the multi-scale
nature of convective footprints of wall-bounded turbulence: our experimental
data reflect that larger streaks in the footprint convect at velocities in the
order of the free-stream velocity, while the more energetic smaller-scale
features move at velocities in the order of $10u_\tau$. Measurements of the
kind presented here offer a promising method for sensing, as they can be used
as input to flow control systems.",['physics.flu-dyn'],2503.09359," We explore the mechanisms and regimes of mixing in yield-stress fluids by
simulating the stirring of an infinite, two-dimensional domain filled with a
Bingham fluid. A cylindrical stirrer moves along a circular path at constant
speed to stir the fluid, with an initially quiescent domain marked by a passive
dye in the lower half, facilitating the analysis of dye interface evolution and
mixing dynamics. We first examine the mixing process in Newtonian fluids,
identifying three key mechanisms: interface stretching and folding around the
stirrer's path, diffusion across streamlines, and dye advection and interface
stretching due to vortex shedding. Introducing yield stress into the system
leads to notable localization effects in mixing, manifesting through three
mechanisms: advection of vortices within a finite distance of the stirrer,
vortex entrapment near the stirrer, and complete suppression of vortex shedding
at high yield stresses. Based on these mechanisms, we classify three distinct
mixing regimes in yield-stress fluids: (i) Regime SE, where shed vortices
escape the central region, (ii) Regime ST, where shed vortices remain trapped
near the stirrer, and (iii) Regime NS, where no vortex shedding occurs. These
regimes are quantitatively distinguished through spectral analysis of energy
oscillations, revealing transitions and the critical Bingham and Reynolds
numbers. The transitions are captured through effective Reynolds numbers,
supporting a hypothesis that mixing regime transitions in yield-stress fluids
share fundamental characteristics with bluff-body flow dynamics. The findings
provide a mechanistic framework for understanding and predicting mixing
behaviors in yield-stress fluids, suggesting that the localization mechanisms
and mixing regimes observed here are archetypal for stirred-tank applications.",['physics.flu-dyn'],False,,,,"Instantaneous convective heat transfer at the wall: a depiction of
  turbulent boundary layer structures","Yield-Stress Fluid Mixing: Localization Mechanisms and Regime
  Transitions"
neg-d2-41,2025-01-22,,2501.13052," The deployment of IoT (Internet of Things) sensor-based machine learning
models in industrial systems for anomaly classification tasks poses significant
challenges due to distribution shifts, as the training data acquired in
controlled laboratory settings may significantly differ from real-time data in
production environments. Furthermore, many real-world applications cannot
provide a substantial number of labeled examples for each anomalous class in
every new environment. It is therefore crucial to develop adaptable machine
learning models that can be effectively transferred from one environment to
another, enabling rapid adaptation using normal operational data. We extended
this problem setting to an arbitrary classification task and formulated the
one-class domain adaptation (OC-DA) problem setting. We took a meta-learning
approach to tackle the challenge of OC-DA, and proposed a task sampling
strategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified
the well-established model-agnostic meta-learning (MAML) algorithm and
introduced the OC-DA MAML algorithm. We provided a theoretical analysis showing
that OC-DA MAML optimizes for meta-parameters that enable rapid one-class
adaptation across domains. The OC-DA MAML algorithm is evaluated on the
Rainbow-MNIST meta-learning benchmark and on a real-world dataset of
vibration-based sensor readings. The results show that OC-DA MAML significantly
improves the performance on the target domains and outperforms MAML using the
standard task sampling strategy.",['cs.LG'],2503.01203," Hypergraph neural networks (HGNNs) effectively model complex high-order
relationships in domains like protein interactions and social networks by
connecting multiple vertices through hyperedges, enhancing modeling
capabilities, and reducing information loss. Developing foundation models for
hypergraphs is challenging due to their distinct data, which includes both
vertex features and intricate structural information. We present Hyper-FM, a
Hypergraph Foundation Model for multi-domain knowledge extraction, featuring
Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex
feature representation and Hierarchical Multi-Hypergraph Guided Structural
Knowledge Extraction for structural information. Additionally, we curate 10
text-attributed hypergraph datasets to advance research between HGNNs and LLMs.
Experiments on these datasets show that Hyper-FM outperforms baseline methods
by approximately 13.3\%, validating our approach. Furthermore, we propose the
first scaling law for hypergraph foundation models, demonstrating that
increasing domain diversity significantly enhances performance, unlike merely
augmenting vertex and hyperedge counts. This underscores the critical role of
domain diversity in scaling hypergraph models.",['cs.LG'],False,,,,One-Class Domain Adaptation via Meta-Learning,Hypergraph Foundation Model
neg-d2-42,2025-01-09,,2501.05331," Aerodynamic loads play a central role in many fluid dynamics applications,
and we present a method for identifying the structures (or modes) in a flow
that make dominant contributions to the time-varying aerodynamic loads in a
flow. The method results from the combination of the force partitioning method
(Menon and Mittal, J. Fluid Mech., 907:A37, 2021) and modal decomposition
techniques such as Reynolds decomposition, triple decomposition, and proper
orthogonal decomposition, and is applied here to three distinct flows -
two-dimensional flows past a circular cylinder and an airfoil, and the
three-dimensional flow over a revolving rectangular wing. We show that the
force partitioning method applied to modal decomposition of velocity fields
results in complex, and difficult to interpret inter-modal interactions. We
therefore propose and apply modal decomposition directly to the $Q$-field
associated with these flows. The variable $Q$ is a non-linear observable that
is typically used to identify vortices in a flow, and we find that the direct
decomposition of $Q$ leads to results that are more amenable to interpretation.
We also demonstrate that this modal force partitioning can be extended to
provide insights into the far-field aeroacoustic loading noise of these flows.",['physics.flu-dyn'],2502.0482," This paper presents a parallel and fully conservative adaptive mesh
refinement (AMR) implementation of a finite-volume-based kinetic solver for
compressible flows. Time-dependent H-type refinement is combined with a
two-population quasi-equilibrium Bhatnagar-Gross-Krook discrete velocity
Boltzmann model. A validation has shown that conservation laws are strictly
preserved through the application of refluxing operations at coarse-fine
interfaces. Moreover, the targeted macroscopic moments of Euler and
Navier-Stokes-Fourier level flows were accurately recovered with correct and
Galilean invariant dispersion rates for a temperature range over three orders
of magnitude and dissipation rates of all eigen-modes up to Mach of order 1.8.
Results for one- and two-dimensional benchmarks up to Mach numbers of 3.2 and
temperature ratios of 7, such as the Sod and Lax shock tubes, the Shu-Osher and
several Riemann problems, as well as viscous shock-vortex interactions, have
demonstrated that the solver precisely captures reference solutions. Excellent
performance in obtaining sensitive quantities was proven, for example in the
test case involving nonlinear acoustics, whilst, for the same accuracy and
fidelity of the solution, the AMR methodology significantly reduced
computational cost and memory footprints. Over all demonstrated two-dimensional
problems, up to a 4- to 9-fold reduction was achieved and an upper limit of the
AMR overhead of 30% was found in a case with very cost-intensive parameter
choice. The proposed solver marks an accurate, efficient and scalable framework
for kinetic simulations of compressible flows with moderate supersonic speeds
and discontinuities, offering a valuable tool for studying complex problems in
fluid dynamics.",['physics.flu-dyn'],False,,,,"Modal Force Partitioning -- A Method for Determining the Aerodynamic
  Loads for Decomposed Flow Modes with Application to Aeroacoustic Noise","A fully conservative discrete velocity Boltzmann solver with parallel
  adaptive mesh refinement for compressible flows"
neg-d2-43,2025-02-24,,2502.17072," This study examines insurance companies' financial performance and reporting
trends within the medical sector using advanced clustering techniques to
identify distinct patterns. Four clusters were identified by analyzing
financial ratios and time series data, each representing unique financial
performance and reporting consistency combinations. Dynamic Time Warping (DTW)
and KMeans clustering were employed to capture temporal variations and uncover
key insights into company behaviors. The findings reveal that resilient
performers consistently report and have financial stability, making them
reliable options for policyholders. In contrast, clusters of underperforming
companies and those with reporting gaps highlight operational challenges and
issues related to data consistency. These insights emphasize the importance of
transparency and timely reporting to ensure the sector's resilience. This study
contributes to the literature by integrating time series analysis into
financial clustering, offering practical recommendations for improving data
governance and financial stability in the insurance sector. Future research
could further investigate non-financial indicators and explore alternative
clustering methods to provide a deeper understanding of performance dynamics.",['q-fin.CP'],2502.14766," We propose a structural default model for portfolio-wide valuation
adjustments (xVAs) and represent it as a system of coupled backward stochastic
differential equations. The framework is divided into four layers, each
capturing a key component: (i) clean values, (ii) initial margin and Collateral
Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments
(CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding
Valuation Adjustment (FVA). Because these layers depend on one another through
collateral and default effects, a naive Monte Carlo approach would require
deeply nested simulations, making the problem computationally intractable.
  To address this challenge, we use an iterative deep BSDE approach, handling
each layer sequentially so that earlier outputs serve as inputs to the
subsequent layers. Initial margin is computed via deep quantile regression to
reflect margin requirements over the Margin Period of Risk. We also adopt a
change-of-measure method that highlights rare but significant defaults of the
bank or counterparty, ensuring that these events are accurately captured in the
training process.
  We further extend Han and Long's (2020) a posteriori error analysis to BSDEs
on bounded domains. Due to the random exit from the domain, we obtain an order
of convergence of $\mathcal{O}(h^{1/4-\epsilon})$ rather than the usual
$\mathcal{O}(h^{1/2})$.
  Numerical experiments illustrate that this method drastically reduces
computational demands and successfully scales to high-dimensional,
non-symmetric portfolios. The results confirm its effectiveness and accuracy,
offering a practical alternative to nested Monte Carlo simulations in
multi-counterparty xVA analyses.",['q-fin.CP'],False,,,,"Decoding Financial Health in Kenyas' Medical Insurance Sector: A
  Data-Driven Cluster Analysis","Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and
  Convergence Analysis"
neg-d2-44,2025-03-13,,2503.10141," Collision-free flight in cluttered environments is a critical capability for
autonomous quadrotors. Traditional methods often rely on detailed 3D map
construction, trajectory generation, and tracking. However, this cascade
pipeline can introduce accumulated errors and computational delays, limiting
flight agility and safety. In this paper, we propose a novel method for
enabling collision-free flight in cluttered environments without explicitly
constructing 3D maps or generating and tracking collision-free trajectories.
Instead, we leverage Model Predictive Control (MPC) to directly produce safe
actions from sparse waypoints and point clouds from a depth camera. These
sparse waypoints are dynamically adjusted online based on nearby obstacles
detected from point clouds. To achieve this, we introduce a dual KD-Tree
mechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for
avoidance, while the Edge KD-Tree provides a robust initial guess for the MPC
solver, preventing it from getting stuck in local minima during obstacle
avoidance. We validate our approach through extensive simulations and
real-world experiments. The results show that our approach significantly
outperforms the mapping-based methods and is also superior to imitation
learning-based methods, demonstrating reliable obstacle avoidance at up to 12
m/s in simulations and 6 m/s in real-world tests. Our method provides a simple
and robust alternative to existing methods.",['cs.RO'],2502.01256," With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.",['cs.RO'],False,,,,"Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered
  Environments",Soft is Safe: Human-Robot Interaction for Soft Robots
neg-d2-45,2025-01-11,,2501.06535," In this paper, we consider the classical coupon collector problem with
uniform probabilities. Since the seminal paper by P. Erd\""os and A. R\'enyi
\cite{ErRe}, it is well-known that the renormalized number of attempts required
to complete a collection of $n$ items distributed with uniform probability
tends to a Gumbel distribution when $n$ goes to infinity. We propose to
determine how fast this convergence takes place for a certain distance to be
introduced by using the so-called generator approach of Stein's method. To do
so, we introduce a semi-group similar to the classical Ornstein-Uhlenbeck
semi-group and whose stationary measure is the standard Gumbel distribution. We
give its essential properties and apply them to prove that the renormalized
number of attempts converges to the Gumbel distribution at rate $\log n/n$.",['math.PR'],2501.11077," Duplication-divergence models are a popular model for the evolution of gene
and protein interaction networks. However, existing duplication-divergence
models often neglect realistic features such as loss of interactions. Thus, in
this paper we present two novel models that incorporate random edge deletions
into the duplication-divergence framework. As in protein-protein interaction
networks, with proteins as vertices and interactions as edges, by design
isolated vertices tend to be rare, our main focus is on the number of isolated
vertices; our main result gives lower and upper bounds for the proportion of
isolated vertices, when the network size is large. Using these bounds we
identify the parameter regimes for which almost all vertices are typically
isolated; and also show that there are parameter regimes in which the
proportion of isolated vertices can be bounded away from 0 and 1 with high
probability. In addition, we find regimes in which the proportion of isolated
vertices tends to be small. The proof relies on a standard martingale argument,
which in turn requires a careful analysis of the first two moments of the
expected degree distribution. The theoretical findings are illustrated by
simulations, indicating that as the network size tends to infinity, the
proportion of isolated vertices can converge to a limit that is neither 0 or 1.",['math.PR'],False,,,,Convergence rate for the coupon collector's problem with Stein's method,"Isolated vertices in two duplication-divergence models with edge
  deletion"
neg-d2-46,2025-02-26,,2502.19561," This work explores the thermodynamic performance of a quantum Stirling heat
engine implemented with an anisotropic spin-1 Heisenberg dimer as the working
medium. Using the Hamiltonian of the system, we analyze the interplay of
anisotropy, magnetic field, and exchange interactions and their influence on
the energy spectrum and the quantum level crossing. Our results reveal that
double-degenerate point (DDP) and a triple-degenerate point (TDP) play pivotal
roles in shaping the operational regimes and efficiency of the quantum Stirling
engine. At those points, the Carnot efficiency reaches higher work output and
enhanced stability, making it a robust candidate for optimal thermodynamic
performance. These findings highlight the potential of anisotropic spin systems
as viable platforms for quantum heat engines and contribute to advancing the
field of quantum thermodynamics.",['cond-mat.stat-mech'],2503.17898," We provide a concise framework for generalized ensemble theory through a
matrix-based approach. By introducing an observation matrix, any discrete
probability distribution, including those for non-equilibrium steady states,
can be expressed as a generalized Boltzmann distribution, with observables and
conjugate variables as the basis and coordinates in a linear space. In this
framework, we identify the minimal sufficient statistics required for inferring
the Boltzmann distribution. Furthermore, we show that the Hadamard and
Vandermonde matrices are suitable observation matrices for spin systems and
random walks. In master equation systems, the probability flux observation
matrix facilitates the identification of detailed balance violations. Our
findings provide a new approach to developing generalized ensemble theory for
non-equilibrium steady-state systems.",['cond-mat.stat-mech'],False,,,,"Quantum Level-Crossing Induced by Anisotropy in Spin-1 Heisenberg
  Dimers: Applications to Quantum Stirling Engines",Matrix approach to generalized ensemble theory
neg-d2-47,2025-01-16,,2501.09404," I introduce an agent-based model of a Perpetual Futures market with
heterogeneous agents trading via a central limit order book. Perpetual Futures
(henceforth Perps) are financial derivatives introduced by the economist Robert
Shiller, designed to peg their price to that of the underlying Spot market.
This paper extends the limit order book model of Chiarella et al. (2002) by
taking their agent and orderbook parameters, designed for a simple stock
exchange, and applying it to the more complex environment of a Perp market with
long and short traders who exhibit both positional and basis-trading behaviors.
I find that despite the simplicity of the agent behavior, the simulation is
able to reproduce the most salient feature of a Perp market, the pegging of the
Perp price to the underlying Spot price. In contrast to fundamental simulations
of stock markets which aim to reproduce empirically observed stylized facts
such as the leptokurtosis and heteroscedasticity of returns, volatility
clustering and others, in derivatives markets many of these features are
provided exogenously by the underlying Spot price signal. This is especially
true of Perps since the derivative is designed to mimic the price of the Spot
market. Therefore, this paper will focus exclusively on analyzing how market
and agent parameters such as order lifetime, trading horizon and spread affect
the premiums at which Perps trade with respect to the underlying Spot market. I
show that this simulation provides a simple and robust environment for
exploring the dynamics of Perpetual Futures markets and their microstructure in
this regard. Lastly, I explore the ability of the model to reproduce the
effects of biasing long traders to trade positionally and short traders to
basis-trade, which was the original intention behind the market design, and is
a tendency observed empirically in real Perp markets.",['q-fin.TR'],2501.09404," I introduce an agent-based model of a Perpetual Futures market with
heterogeneous agents trading via a central limit order book. Perpetual Futures
(henceforth Perps) are financial derivatives introduced by the economist Robert
Shiller, designed to peg their price to that of the underlying Spot market.
This paper extends the limit order book model of Chiarella et al. (2002) by
taking their agent and orderbook parameters, designed for a simple stock
exchange, and applying it to the more complex environment of a Perp market with
long and short traders who exhibit both positional and basis-trading behaviors.
I find that despite the simplicity of the agent behavior, the simulation is
able to reproduce the most salient feature of a Perp market, the pegging of the
Perp price to the underlying Spot price. In contrast to fundamental simulations
of stock markets which aim to reproduce empirically observed stylized facts
such as the leptokurtosis and heteroscedasticity of returns, volatility
clustering and others, in derivatives markets many of these features are
provided exogenously by the underlying Spot price signal. This is especially
true of Perps since the derivative is designed to mimic the price of the Spot
market. Therefore, this paper will focus exclusively on analyzing how market
and agent parameters such as order lifetime, trading horizon and spread affect
the premiums at which Perps trade with respect to the underlying Spot market. I
show that this simulation provides a simple and robust environment for
exploring the dynamics of Perpetual Futures markets and their microstructure in
this regard. Lastly, I explore the ability of the model to reproduce the
effects of biasing long traders to trade positionally and short traders to
basis-trade, which was the original intention behind the market design, and is
a tendency observed empirically in real Perp markets.",['q-fin.TR'],False,,,,Agent-Based Simulation of a Perpetual Futures Market,Agent-Based Simulation of a Perpetual Futures Market
neg-d2-48,2025-02-06,,2502.04012," This chapter is about the fundamentals of fabrication, control, and
human-robot interaction of a new type of collaborative robotic manipulators,
called malleable robots, which are based on adjustable architectures of varying
stiffness for achieving high dexterity with lower mobility arms. Collaborative
robots, or cobots, commonly integrate six or more degrees of freedom (DOF) in a
serial arm in order to allow positioning in constrained spaces and adaptability
across tasks. Increasing the dexterity of robotic arms has been indeed
traditionally accomplished by increasing the number of degrees of freedom of
the system; however, once a robotic task has been established (e.g., a
pick-and-place operation), the motion of the end-effector can be normally
achieved using less than 6-DOF (i.e., lower mobility). The aim of malleable
robots is to close the technological gap that separates current cobots from
achieving flexible, accessible manufacturing automation with a reduced number
of actuators.",['cs.RO'],2502.01329," Quadratic Programs (QPs) are widely used in the control of walking robots,
especially in Model Predictive Control (MPC) and Whole-Body Control (WBC). In
both cases, the controller design requires the formulation of a QP and the
selection of a suitable QP solver, both requiring considerable time and
expertise. While computational performance benchmarks exist for QP solvers,
studies comparing optimal combinations of computational hardware (HW), QP
formulation, and solver performance are lacking. In this work, we compare dense
and sparse QP formulations, and multiple solving methods on different HW
architectures, focusing on their computational efficiency in dynamic walking of
four legged robots using MPC. We introduce the Solve Frequency per Watt (SFPW)
as a performance measure to enable a cross hardware comparison of the
efficiency of QP solvers. We also benchmark different QP solvers for WBC that
we use for trajectory stabilization in quadrupedal walking. As a result, this
paper provides recommendations for the selection of QP formulations and solvers
for different HW architectures in walking robots and indicates which problems
should be devoted the greater technical effort in this domain in future.",['cs.RO'],False,,,,Malleable Robots,"Benchmarking Different QP Formulations and Solvers for Dynamic
  Quadrupedal Walking"
neg-d2-49,2025-02-17,,2502.11889," As artificial intelligence (AI) systems become increasingly integrated into
critical domains, ensuring their responsible design and continuous development
is imperative. Effective AI quality management (QM) requires tools and
methodologies that address the complexities of the AI lifecycle. In this paper,
we propose an approach for AI lifecycle planning that bridges the gap between
generic guidelines and use case-specific requirements (MQG4AI). Our work aims
to contribute to the development of practical tools for implementing
Responsible AI (RAI) by aligning lifecycle planning with technical, ethical and
regulatory demands. Central to our approach is the introduction of a flexible
and customizable Methodology based on Quality Gates, whose building blocks
incorporate RAI knowledge through information linking along the AI lifecycle in
a continuous manner, addressing AIs evolutionary character. For our present
contribution, we put a particular emphasis on the Explanation stage during
model development, and illustrate how to align a guideline to evaluate the
quality of explanations with MQG4AI, contributing to overall Transparency.",['cs.CY'],2502.03472," The rapid advancement of Large Language Models (LLMs) has created a critical
gap in consumer protection due to the lack of standardized certification
processes for LLM-powered Artificial Intelligence (AI) systems. This paper
argues that current regulatory approaches, which focus on compute-level
thresholds and generalized model evaluations, are insufficient to ensure the
safety and effectiveness of specific LLM-based user experiences. We propose a
shift towards a certification process centered on actual user-facing
experiences and the curation of high-quality datasets for evaluation. This
approach offers several benefits: it drives consumer confidence in AI system
performance, enables businesses to demonstrate the credibility of their
products, and allows regulators to focus on direct consumer protection. The
paper outlines a potential certification workflow, emphasizing the importance
of domain-specific datasets and expert evaluation. By repositioning data as the
strategic center of regulatory efforts, this framework aims to address the
challenges posed by the probabilistic nature of AI systems and the rapid pace
of technological advancement. This shift in regulatory focus has the potential
to foster innovation while ensuring responsible AI development, ultimately
benefiting consumers, businesses, and government entities alike.",['cs.CY'],False,,,,"MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency
  Focusing on Explainability Techniques","Powering LLM Regulation through Data: Bridging the Gap from Compute
  Thresholds to Customer Experiences"
neg-d2-50,2025-02-14,,2502.10249," Changes are inherent in software development, often increasing developers'
perception of instability. Understanding the relationship between human factors
and Software Engineering processes is crucial to mitigating and preventing
issues. One such factor is burnout, a recognized disease that impacts
productivity, turnover, and, most importantly, developers' well-being.
Investigating the link between instability and burnout can help organizations
implement strategies to improve developers' work conditions and performance.
  This study aims to identify and describe the relationship between perceived
instability and burnout among software developers. A cross-sectional survey was
conducted with 411 respondents, using convenience sampling and self-selection.
In addition to analyzing variable relationships, confirmatory factor analysis
was applied.
  Key findings include: (1) A significant positive relationship between burnout
(exhaustion and cynicism) and team, technological, and task instability; (2) A
weak negative relationship between efficacy and technological/team instability,
with no correlation to task instability; (3) Exhaustion was the most frequently
reported burnout symptom, while task instability was the most perceived type of
instability.
  These results are valuable for both industry and academia, providing insights
to reduce burnout and instability among software engineers. Future research can
further explore the impact of instability, offering new perspectives on
monitoring and mitigating its effects in software development.",['cs.SE'],2503.10876," The proliferation of Large Language Models (LLMs) in recent years has
realized many applications in various domains. Being trained with a huge of
amount of data coming from various sources, LLMs can be deployed to solve
different tasks, including those in Software Engineering (SE). Though they have
been widely adopted, the potential of using LLMs cooperatively has not been
thoroughly investigated. In this paper, we proposed Metagente as a novel
approach to amplify the synergy of various LLMs. Metagente is a Multi-Agent
framework based on a series of LLMs to self-optimize the system through
evaluation, feedback, and cooperation among specialized agents. Such a
framework creates an environment where multiple agents iteratively refine and
optimize prompts from various perspectives. The results of these explorations
are then reviewed and aggregated by a teacher agent. To study its performance,
we evaluated Metagente with an SE task, i.e., summarization of README.MD files,
and compared it with three well-established baselines, i.e., GitSum, LLaMA-2,
and GPT-4o. The results show that our proposed approach works efficiently and
effectively, consuming a small amount of data for fine-tuning but still getting
a high accuracy, thus substantially outperforming the baselines. The
performance gain compared to GitSum, the most relevant benchmark, ranges from
27.63% to 60.43%. More importantly, compared to using only one LLM, Metagente
boots up the accuracy to multiple folds.",['cs.SE'],False,,,,"Understanding the relationships between the perceptions of burnout and
  instability in Software Engineering","Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD
  Summarization"
neg-d2-51,2025-03-02,,2503.01176," The Prognostics and Health Management Data Challenge (PHM) 2016 tracks the
health state of components of a semiconductor wafer polishing process. The
ultimate goal is to develop an ability to predict the measurement on the wafer
surface wear through monitoring the components health state. This translates to
cost saving in large scale production. The PHM dataset contains many time
series measurements not utilized by traditional physics based approach. On the
other hand task, applying a data driven approach such as deep learning to the
PHM dataset is non-trivial. The main issue with supervised deep learning is
that class label is not available to the PHM dataset. Second, the feature space
trained by an unsupervised deep learner is not specifically targeted at the
predictive ability or regression. In this work, we propose using the
autoencoder based clustering whereby the feature space trained is found to be
more suitable for performing regression. This is due to having a more compact
distribution of samples respective to their nearest cluster means. We justify
our claims by comparing the performance of our proposed method on the PHM
dataset with several baselines such as the autoencoder as well as
state-of-the-art approaches.",['cs.AI'],2503.17604," Large Language Models (LLMs) have demonstrated remarkable potential in
advancing scientific knowledge and addressing complex challenges. In this work,
we introduce OmniScience, a specialized large reasoning model for general
science, developed through three key components: (1) domain adaptive
pretraining on a carefully curated corpus of scientific literature, (2)
instruction tuning on a specialized dataset to guide the model in following
domain-specific tasks, and (3) reasoning-based knowledge distillation through
fine-tuning to significantly enhance its ability to generate contextually
relevant and logically sound responses. We demonstrate the versatility of
OmniScience by developing a battery agent that efficiently ranks molecules as
potential electrolyte solvents or additives. Comprehensive evaluations reveal
that OmniScience is competitive with state-of-the-art large reasoning models on
the GPQA Diamond and domain-specific battery benchmarks, while outperforming
all public reasoning and non-reasoning models with similar parameter counts. We
further demonstrate via ablation experiments that domain adaptive pretraining
and reasoning-based knowledge distillation are critical to attain our
performance levels, across benchmarks.",['cs.AI'],False,,,,"Prognostics and Health Management of Wafer Chemical-Mechanical Polishing
  System using Autoencoder","OmniScience: A Domain-Specialized LLM for Scientific Reasoning and
  Discovery"
neg-d2-52,2025-03-10,,2503.07456," Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding
clinicians and radiologists by automatically retrieving relevant patient cases
in the database given the query image and/or report, for more efficient
clinical diagnosis and treatment, especially for rare diseases. However
conventional ITR systems typically only rely on global image or text
representations for measuring patient image/report similarities, which overlook
local distinctiveness across patient cases. This often results in suboptimal
retrieval performance. In this paper, we propose an Anatomical
Location-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a
query image and the associated suspicious anatomical region(s), aims to
retrieve similar patient cases exhibiting the same disease or symptoms in the
same anatomical region. To perform location-conditioned multimodal retrieval,
we learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with
semantic global-level and region-/word-level alignment to produce
generalizable, well-aligned multi-modal representations. Additionally, we
perform location-conditioned contrastive learning to further utilize cross-pair
region-level contrastiveness for improved multi-modal retrieval. We show that
our proposed RRA-VL achieves state-of-the-art localization performance in
phase-grounding tasks, and satisfying multi-modal retrieval performance with or
without location conditioning. Finally, we thoroughly investigate the
generalizability and explainability of our proposed ALC-ITR system in providing
explanations and preliminary diagnosis reports given retrieved patient cases
(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.",['cs.CV'],2501.08837," Long-term dense action anticipation is very challenging since it requires
predicting actions and their durations several minutes into the future based on
provided video observations. To model the uncertainty of future outcomes,
stochastic models predict several potential future action sequences for the
same observation. Recent work has further proposed to incorporate uncertainty
modelling for observed frames by simultaneously predicting per-frame past and
future actions in a unified manner. While such joint modelling of actions is
beneficial, it requires long-range temporal capabilities to connect events
across distant past and future time points. However, the previous work
struggles to achieve such a long-range understanding due to its limited and/or
sparse receptive field. To alleviate this issue, we propose a novel MANTA
(MAmba for ANTicipation) network. Our model enables effective long-term
temporal modelling even for very long sequences while maintaining linear
complexity in sequence length. We demonstrate that our approach achieves
state-of-the-art results on three datasets - Breakfast, 50Salads, and
Assembly101 - while also significantly improving computational and memory
efficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .",['cs.CV'],False,,,,Anatomy-Aware Conditional Image-Text Retrieval,"MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation"
neg-d2-53,2025-02-24,,2502.17532," We investigate the spectral structure of multi-frequency quasi-periodic CMV
matrices with Verblunsky coefficients defined by shifts on the $d$-dimensional
torus. Under the positive Lyapunov exponent regime and standard Diophantine
frequency conditions, we establish that the spectrum of these operators
contains intervals on the unit circle.",['math.SP'],2502.17532," We investigate the spectral structure of multi-frequency quasi-periodic CMV
matrices with Verblunsky coefficients defined by shifts on the $d$-dimensional
torus. Under the positive Lyapunov exponent regime and standard Diophantine
frequency conditions, we establish that the spectrum of these operators
contains intervals on the unit circle.",['math.SP'],False,,,,"The spectrum of the multi-frequency quasi-periodic CMV matrices contains
  intervals","The spectrum of the multi-frequency quasi-periodic CMV matrices contains
  intervals"
neg-d2-54,2025-03-21,,2503.17604," Large Language Models (LLMs) have demonstrated remarkable potential in
advancing scientific knowledge and addressing complex challenges. In this work,
we introduce OmniScience, a specialized large reasoning model for general
science, developed through three key components: (1) domain adaptive
pretraining on a carefully curated corpus of scientific literature, (2)
instruction tuning on a specialized dataset to guide the model in following
domain-specific tasks, and (3) reasoning-based knowledge distillation through
fine-tuning to significantly enhance its ability to generate contextually
relevant and logically sound responses. We demonstrate the versatility of
OmniScience by developing a battery agent that efficiently ranks molecules as
potential electrolyte solvents or additives. Comprehensive evaluations reveal
that OmniScience is competitive with state-of-the-art large reasoning models on
the GPQA Diamond and domain-specific battery benchmarks, while outperforming
all public reasoning and non-reasoning models with similar parameter counts. We
further demonstrate via ablation experiments that domain adaptive pretraining
and reasoning-based knowledge distillation are critical to attain our
performance levels, across benchmarks.",['cs.AI'],2501.07071," As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning
their values with humans has become imperative for their responsible
development and customized applications. However, there still lack evaluations
of LLMs values that fulfill three desirable goals. (1) Value Clarification: We
expect to clarify the underlying values of LLMs precisely and comprehensively,
while current evaluations focus narrowly on safety risks such as bias and
toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are
prone to data contamination and quickly become obsolete as LLMs evolve.
Additionally, these discriminative evaluations uncover LLMs' knowledge about
values, rather than valid assessments of LLMs' behavioral conformity to values.
(3) Value Pluralism: The pluralistic nature of human values across individuals
and cultures is largely ignored in measuring LLMs value alignment. To address
these challenges, we presents the Value Compass Leaderboard, with three
correspondingly designed modules. It (i) grounds the evaluation on
motivationally distinct \textit{basic values to clarify LLMs' underlying values
from a holistic view; (ii) applies a \textit{generative evolving evaluation
framework with adaptive test items for evolving LLMs and direct value
recognition from behaviors in realistic scenarios; (iii) propose a metric that
quantifies LLMs alignment with a specific value as a weighted sum over multiple
dimensions, with weights determined by pluralistic values.",['cs.AI'],False,,,,"OmniScience: A Domain-Specialized LLM for Scientific Reasoning and
  Discovery","Value Compass Leaderboard: A Platform for Fundamental and Validated
  Evaluation of LLMs Values"
neg-d2-55,2025-02-10,,2502.06466," Kirigami offers unique opportunities for guided morphing by leveraging the
geometry of the cuts. This work presents inflatable kirigami crawlers created
by introducing cut patterns into heat-sealable textiles to achieve locomotion
upon cyclic pneumatic actuation. Inflating traditional air pouches results in
symmetric bulging and contraction. In inflated kirigami actuators, the
accumulated compressive forces uniformly break the symmetry, enhance
contraction compared to simple air pouches by two folds, and trigger local
rotation of the sealed edges that overlap and self-assemble into an architected
surface with emerging scale-like features. As a result, the inflatable kirigami
actuators exhibit a uniform, controlled contraction with asymmetric localized
out-of-plane deformations. This process allows us to harness the geometric and
material nonlinearities to imbue inflatable textile-based kirigami actuators
with predictable locomotive functionalities. We thoroughly characterized the
programmed deformations of these actuators and their impact on friction. We
found that the kirigami actuators exhibit directional anisotropic friction
properties when inflated, having higher friction coefficients against the
direction of the movement, enabling them to move across surfaces with varying
roughness. We further enhanced the functionality of inflatable kirigami
actuators by introducing multiple channels and segments to create functional
soft robotic prototypes with versatile locomotion capabilities.",['cs.RO'],2502.06466," Kirigami offers unique opportunities for guided morphing by leveraging the
geometry of the cuts. This work presents inflatable kirigami crawlers created
by introducing cut patterns into heat-sealable textiles to achieve locomotion
upon cyclic pneumatic actuation. Inflating traditional air pouches results in
symmetric bulging and contraction. In inflated kirigami actuators, the
accumulated compressive forces uniformly break the symmetry, enhance
contraction compared to simple air pouches by two folds, and trigger local
rotation of the sealed edges that overlap and self-assemble into an architected
surface with emerging scale-like features. As a result, the inflatable kirigami
actuators exhibit a uniform, controlled contraction with asymmetric localized
out-of-plane deformations. This process allows us to harness the geometric and
material nonlinearities to imbue inflatable textile-based kirigami actuators
with predictable locomotive functionalities. We thoroughly characterized the
programmed deformations of these actuators and their impact on friction. We
found that the kirigami actuators exhibit directional anisotropic friction
properties when inflated, having higher friction coefficients against the
direction of the movement, enabling them to move across surfaces with varying
roughness. We further enhanced the functionality of inflatable kirigami
actuators by introducing multiple channels and segments to create functional
soft robotic prototypes with versatile locomotion capabilities.",['cs.RO'],False,,,,Inflatable Kirigami Crawlers,Inflatable Kirigami Crawlers
neg-d2-56,2025-02-08,,2502.05558," Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.",['cs.IR'],2501.04364," The underlying data source for web usage mining (WUM) is commonly thought to
be server logs. However, access log files ensure quite limited data about the
clients. Identifying sessions from this messy data takes a considerable effort,
and operations performed for this purpose do not always yield excellent
results. Also, this data cannot be used for web analytics efficiently. This
study proposes an innovative method for user tracking, session management, and
collecting web usage data. The method is mainly based on a new approach for
using collected data for web analytics extraction as the data source in web
usage mining. An application-based API has been developed with a different
strategy from conventional client-side methods to obtain and process log data.
The log data has been successfully gathered by integrating the technique into
an enterprise web application. The results reveal that the homogeneous
structured data collected and stored with this method is more convenient to
browse, filter, and process than web server logs. This data stored on a
relational database can be used effortlessly as a reliable data source for
high-performance web usage mining activity, real-time web analytics, or a
functional recommendation system.",['cs.IR'],False,,,,Large Memory Network for Recommendation,"An innovative data collection method to eliminate the preprocessing
  phase in web usage mining"
neg-d2-57,2025-01-06,,2501.0339," The Pseudo-Boolean problem deals with linear or polynomial constraints with
integer coefficients over Boolean variables. The objective lies in optimizing a
linear objective function, or finding a feasible solution, or finding a
solution that satisfies as many constraints as possible. In the 2024
Pseudo-Boolean competition, solvers incorporating the SCIP framework won five
out of six categories it was competing in. From a total of 1,207 instances,
SCIP successfully solved 759, while its parallel version FiberSCIP solved 776.
Based on the results from the competition, we further enhanced SCIP's
Pseudo-Boolean capabilities. This article discusses the results and presents
the winning algorithmic ideas.",['math.OC'],2503.14981," This work is concerned with convex analysis of so-called spectral functions
of matrices that only depend on eigenvalues of the matrix. An abstract
framework of spectral decomposition systems is proposed that covers a wide
range of previously studied settings, including eigenvalue decomposition of
Hermitian matrices and singular value decomposition of rectangular matrices and
allows deriving new results in more general settings such as Euclidean Jordan
algebras. The main results characterize convexity, lower semicontinuity,
Fenchel conjugates, convex subdifferentials, and Bregman proximity operators of
spectral functions in terms of the reduced functions. As a byproduct, a
generalization of the Ky Fan majorization theorem is obtained.",['math.OC'],False,,,,State-of-the-art Methods for Pseudo-Boolean Solving with SCIP,Convex Analysis in Spectral Decomposition Systems
neg-d2-58,2025-01-15,,2501.08977," As Large Language Models (LLMs) are integrated into electronic health record
(EHR) workflows, validated instruments are essential to evaluate their
performance before implementation. Existing instruments for provider
documentation quality are often unsuitable for the complexities of
LLM-generated text and lack validation on real-world data. The Provider
Documentation Summarization Quality Instrument (PDSQI-9) was developed to
evaluate LLM-generated clinical summaries. Multi-document summaries were
generated from real-world EHR data across multiple specialties using several
LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson
correlation for substantive validity, factor analysis and Cronbach's alpha for
structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for
generalizability, a semi-Delphi process for content validity, and comparisons
of high-versus low-quality summaries for discriminant validity. Seven physician
raters evaluated 779 summaries and answered 8,329 questions, achieving over 80%
power for inter-rater reliability. The PDSQI-9 demonstrated strong internal
consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high
inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting
structural validity and generalizability. Factor analysis identified a 4-factor
model explaining 58% of the variance, representing organization, clarity,
accuracy, and utility. Substantive validity was supported by correlations
between note length and scores for Succinct (rho = -0.200, p = 0.029) and
Organized ($\rho = -0.190$, $p = 0.037$). Discriminant validity distinguished
high- from low-quality summaries ($p < 0.001$). The PDSQI-9 demonstrates robust
construct validity, supporting its use in clinical practice to evaluate
LLM-generated summaries and facilitate safer integration of LLMs into
healthcare workflows.",['cs.AI'],2502.07709," Open-ended learning agents must efficiently prioritize goals in vast
possibility spaces, focusing on those that maximize learning progress (LP).
When such autotelic exploration is achieved by LLM agents trained with online
RL in high-dimensional and evolving goal spaces, a key challenge for LP
prediction is modeling one's own competence, a form of metacognitive
monitoring. Traditional approaches either require extensive sampling or rely on
brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive
framework that lets LLM agents learn to predict their competence and LP online.
By capturing semantic relationships between goals, MAGELLAN enables
sample-efficient LP estimation and dynamic adaptation to evolving goal spaces
through generalization. In an interactive learning environment, we show that
MAGELLAN improves LP prediction efficiency and goal prioritization, being the
only method allowing the agent to fully master a large and evolving goal space.
These results demonstrate how augmenting LLM agents with a metacognitive
ability for LP predictions can effectively scale curriculum learning to
open-ended goal spaces.",['cs.AI'],False,,,,"Development and Validation of the Provider Documentation Summarization
  Quality Instrument for Large Language Models","MAGELLAN: Metacognitive predictions of learning progress guide autotelic
  LLM agents in large goal spaces"
neg-d2-59,2025-01-11,,2501.06555," Spin-orbit coupling (SOC) is one of the key factors that affect the chiral
symmetry of matter by causing the spatial symmetry breaking of the system. We
find that Raman-induced SOC can induce a chiral supersolid phase with a helical
antiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein
condensates (BECs) in a harmonic trap by modulating the Raman coupling
strength, strong contrast with the mirror symmetric supersolid phase containing
skyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state
phase diagrams are presented as a function of the Rydberg interaction strength
and the SOC strength, as well as that of the Rydberg interaction strength and
the Raman coupling strength, respectively. It is shown that the interplay among
Raman-induced SOC, soft-core long-range Rydberg interactions, and contact
interactions favors rich ground-state structures including half-quantum vortex
phase, stripe supersolid phase, toroidal stripe phase with a central
Anderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror
symmetric supersolid phase, chiral supersolid phase and standing-wave
supersolid phase. In addition, the effects of rotation and in-plane quadrupole
magnetic field on the ground state of the system are analyzed. In these two
cases, the chiral supersolid phase is broken and the ground state tends to form
a miscible phase. Furthermore, the stability and superfluid properties of the
two-component BECs with Raman-induced SOC and Rydberg interactions in free
space are revealed by solving the Bogoliubov-de Gennes equation. Finally, we
demonstrate that when the initial state is a chiral supersolid phase the
rotating harmonic trapped system sustains dissipative continuous time crystal
by studying the rotational dynamic behaviors of the system.",['cond-mat.quant-gas'],2503.08629," Due to their photonic components, exciton-polariton systems provide a
convenient platform to study the coherence properties of weakly-interacting
Bose gases and the Bose-Einstein condensate transition. In particular, optical
interferometry enables the measurement of the first-order coherence function
which provides insight into the phase of the system. In this paper, we analyze
the buildup of coherence in finite-sized, noninteracting, equilibrium Bose
gases through the condensate fraction and the related coherent fraction,
defined via the first-order coherence function. Our results provide a baseline
to compare against experimental data. Discrepancies may indicate where
interacting or nonequilibrium models are necessary to describe the system. In
the normal phase, before the Bose-Einstein condensate transition, Bose gases
exhibit partial spatial and temporal coherence. This significantly alters the
paraxial propagation and interference of optical signals from exciton-polariton
systems. Therefore, we also analyze diffraction related to the introduction of
apertures and time-delay between interferometry arms, given the partial
coherence of the source. Comparison to experiment shows remarkable agreement
with the noninteracting Bose gas theory, even approaching the quasi-condensate
regime.",['cond-mat.quant-gas'],False,,,,"Chiral supersolid and dissipative time crystal in Rydberg-dressed
  Bose-Einstein condensates with Raman-induced spin-orbit coupling",Optical probes of coherence in two dimensional Bose gases of polaritons
neg-d2-60,2025-02-03,,2503.15523," Children tend to be constantly exposed to technologies, such as smartphones,
tablets, and gaming consoles, drawn by the interactive and visually stimulating
nature of digital platforms. Thus, integrating the teaching process with
technological gadgets may enhance engagement and foster interactive learning
experiences, besides equipping students with the digital skills for today's
increasingly technology-driven world. The main goal of this work is to provide
an open-source and manageable tool that teachers can use as an everyday
activity and as an exergame. For this, we present a prototype of an interactive
platform that students use to answer a quiz by moving to segments available on
an interactive floor. All the platform design and implementation directions are
publicly available.",['cs.HC'],2503.07797," News reading helps individuals stay informed about events and developments in
society. Local residents and new immigrants often approach the same news
differently, prompting the question of how technology, such as LLM-powered
chatbots, can best enhance a reader-oriented news experience. The current paper
presents an empirical study involving 144 participants from three groups in
Virginia, United States: local residents born and raised there (N=48), Chinese
immigrants (N=48), and Vietnamese immigrants (N=48). All participants read
local housing news with the assistance of the Copilot chatbot. We collected
data on each participant's Q&A interactions with the chatbot, along with their
takeaways from news reading. While engaging with the news content, participants
in both immigrant groups asked the chatbot fewer analytical questions than the
local group. They also demonstrated a greater tendency to rely on the chatbot
when formulating practical takeaways. These findings offer insights into
technology design that aims to serve diverse news readers.",['cs.HC'],False,,,,"InteractiveEdu: An Open-source Interactive Floor for Exergame as a
  Learning Platform","The News Says, the Bot Says: How Immigrants and Locals Differ in
  Chatbot-Facilitated News Reading"
neg-d2-61,2025-02-13,,2502.09177," Quantum error correction plays a critical role in enabling fault-tolerant
quantum computing by protecting fragile quantum information from noise. While
general-purpose quantum error correction codes are designed to address a wide
range of noise types, they often require substantial resources, making them
impractical for near-term quantum devices. Approximate quantum error correction
provides an alternative by tailoring codes to specific noise environments,
reducing resource demands while maintaining effective error suppression.
Dynamical codes, including Floquet codes, introduce a dynamic approach to
quantum error correction, employing time-dependent operations to stabilize
logical qubits. In this work, we combine the flexibility of dynamical codes
with the efficiency of approximate quantum error correction to offer a
promising avenue for addressing dominant noise in quantum systems. We construct
several approximate dynamical codes using the recently developed strategic code
framework. As a special case, we recover the approximate static codes widely
studied in the existing literature. By analyzing these approximate dynamical
codes through semidefinite programming, we establish the uniqueness and
robustness of the optimal encoding, decoding, and check measurements. We also
develop a temporal Petz recovery map suited to approximate dynamical codes.",['quant-ph'],2501.1271," Microwave cavity modes with long coherence times are used in many different
quantum computing systems. During normal operation of such systems, these
modes, called memory modes, often need to be set to different coherent photonic
occupations. In this work we present a novel technique we call Rabi Driven
Reset in which the state of a memory mode is transferred into a decaying mode.
This is done through a Rabi driven qubit which is coupled to both modes via
sideband driving tones. The outcome of the method is the initialization of the
memory mode at any required coherent state. Simulations are presented to
demonstrate the effectiveness of this technique, along with a comparison to an
existing coupling method. Our simulations predict an improvement of an order of
magnitude in initialization times compared to existing methods.",['quant-ph'],False,,,,Approximate Dynamical Quantum Error-Correcting Codes,Cavity Mode Initialization via a Rabi Driven Qubit
neg-d2-62,2025-03-06,,2503.0437," This research addresses the challenges of handling unbalanced datasets for
binary classification tasks. In such scenarios, standard evaluation metrics are
often biased by the disproportionate representation of the minority class.
Conducting experiments across seven datasets, we uncovered inconsistencies in
evaluation metrics when determining the model that outperforms others for each
binary classification problem. This justifies the need for a metric that
provides a more consistent and unbiased evaluation across unbalanced datasets,
thereby supporting robust model selection. To mitigate this problem, we propose
a novel metric, the Unbiased Integration Coefficients (UIC), which exhibits
significantly reduced bias ($p < 10^{-4}$) towards the minority class compared
to conventional metrics. The UIC is constructed by aggregating existing metrics
while penalising those more prone to imbalance. In addition, we introduce the
Identical Partitions for Imbalance Problems (IPIP) algorithm for imbalanced ML
problems, an ensemble-based approach. Our experimental results show that IPIP
outperforms other baseline imbalance-aware approaches using Random Forest and
Logistic Regression models in three out of seven datasets as assessed by the
UIC metric, demonstrating its effectiveness in addressing imbalanced data
challenges in binary classification tasks. This new framework for dealing with
imbalanced datasets is materialized in the FILM (Framework for Imbalanced
Learning Machines) R Package, accessible at https://github.com/antoniogt/FILM.",['cs.LG'],2503.09533," Designing strategyproof mechanisms for multi-facility location that optimize
social costs based on agent preferences had been challenging due to the
extensive domain knowledge required and poor worst-case guarantees. Recently,
deep learning models have been proposed as alternatives. However, these models
require some domain knowledge and extensive hyperparameter tuning as well as
lacking interpretability, which is crucial in practice when transparency of the
learned mechanisms is mandatory. In this paper, we introduce a novel approach,
named LLMMech, that addresses these limitations by incorporating large language
models (LLMs) into an evolutionary framework for generating interpretable,
hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.
Our experimental results, evaluated on various problem settings where the
social cost is arbitrarily weighted across agents and the agent preferences may
not be uniformly distributed, demonstrate that the LLM-generated mechanisms
generally outperform existing handcrafted baselines and deep learning models.
Furthermore, the mechanisms exhibit impressive generalizability to
out-of-distribution agent preferences and to larger instances with more agents.",['cs.LG'],False,,,,"FILM: Framework for Imbalanced Learning Machines based on a new unbiased
  performance measure and a new ensemble-based technique",Large Language Models for Multi-Facility Location Mechanism Design
neg-d2-63,2025-01-08,,2501.0453," An intriguing phenomenon regarding Levi-degenerate hypersurfaces is the
existence of nontrivial infinitesimal symmetries with vanishing 2-jets at a
point. In this work we consider polynomial models of Levi-degenerate real
hypersurfaces in $\mathbb{C}^3$ of finite Catlin multitype. Exploiting the
structure of the corresponding Lie algebra, we characterize completely models
without 2-jet determination, including an explicit description of their
symmetry algebras.",['math.CV'],2501.04384," We explore the existence of closed geodesics and geodesic spirals for the
Szeg\""o metric in a $C^{\infty}$-smoothly bounded strongly pseudoconvex domain
$\Omega\subset\mathbb{C}^n$, which is not simply connected for $n \geq 2$.",['math.CV'],False,,,,"Classification of polynomial models without 2-jet determination in
  $\mathbb{C}^3$","On the geodesics of the Szeg\""o metric"
neg-d2-64,2025-02-04,,2502.02314," Exploring the photoinduced dynamics of chiral states offers promising avenues
for advanced control of condensed matter systems. Photoinduced or photoenhanced
chirality in 1T-TiSe$_{2}$ has been suggested as a fascinating platform for
optical manipulation of chiral states. However, the mechanisms underlying
chirality training and its interplay with the charge density wave (CDW) phase
remain elusive. Here, we use time-resolved X-ray diffraction (tr-XRD) with
circularly polarized pump lasers to probe the photoinduced dynamics of
chirality in 1T-TiSe$_{2}$. We observe a notable ($\sim$20%) difference in CDW
intensity suppression between left- and right-circularly polarized pumps.
Additionally, we reveal momentum-resolved circular dichroism arising from
domains of different chirality, providing a direct link between CDW and
chirality. An immediate increase in CDW correlation length upon laser pumping
is detected, suggesting the photoinduced expansion of chiral domains. These
results both advance the potential of light-driven chirality by elucidating the
mechanism driving chirality manipulation in TiSe$_2$, and they demonstrate that
tr-XRD with circularly polarized pumps is an effective tool for chirality
detection in condensed matter systems.",['cond-mat.str-el'],2503.14131," We investigate the emergence and transformation of pinch-point singularities
in the excitation spectrum of electronic flat band systems on kagome and
pyrochlore lattices with spin-orbit coupling (SOC) and Coulomb interactions.
While pinch points are widely recognized as signatures of classical spin
liquids, they also appear in electronic flat-band systems when there exists a
singular band-touching point to dispersive bands. We explore how SOC modifies
the pinch-point structure in the chiral spin flat-band metallic state, which we
term spinor-ice. The pinch point profile can rotate or redistribute its
spectral weight, governed by a prefactor in the spectral function that
primarily depends on the direction of the ground-state spin polarization, where
we show that SOC flat bands could be experimentally probed by rotating the spin
polarization of the injected electron to infer internal magnetic structures.
These observations are discussed in conjunction with the angle-resolved
photoemission spectroscopy (ARPES) and the application to the potential SOC
flat-band material $\rm CsW_2O_6$. We also demonstrate the persistent residual
pinch-point features under Coulomb interactions and deviations from the ideal
flat-band limit.",['cond-mat.str-el'],False,,,,"Photo-induced Dynamics and Momentum Distribution of Chiral Charge
  Density Waves in 1T-TiSe$_{2}$","Spinor ice correlation in flat-band electronic states on kagome and
  pyrochlore lattices with spin-orbit coupling"
neg-d2-65,2025-02-23,,2502.16826," Building on recent advances in Bayesian statistics and image denoising, we
propose Noise2Score3D, a fully unsupervised framework for point cloud denoising
that addresses the critical challenge of limited availability of clean data.
Noise2Score3D learns the gradient of the underlying point cloud distribution
directly from noisy data, eliminating the need for clean data during training.
By leveraging Tweedie's formula, our method performs inference in a single
step, avoiding the iterative processes used in existing unsupervised methods,
thereby improving both performance and efficiency. Experimental results
demonstrate that Noise2Score3D achieves state-of-the-art performance on
standard benchmarks, outperforming other unsupervised methods in Chamfer
distance and point-to-mesh metrics, and rivaling some supervised approaches.
Furthermore, Noise2Score3D demonstrates strong generalization ability beyond
training datasets. Additionally, we introduce Total Variation for Point Cloud,
a criterion that allows for the estimation of unknown noise parameters, which
further enhances the method's versatility and real-world utility.",['cs.CV'],2503.1476," This work aims to discuss the current landscape of kinematic analysis tools,
ranging from the state-of-the-art in sports biomechanics such as inertial
measurement units (IMUs) and retroreflective marker-based optical motion
capture (MoCap) to more novel approaches from the field of computing such as
human pose estimation and human mesh recovery. Primarily, this comparative
analysis aims to validate the use of marker-less MoCap techniques in a clinical
setting by showing that these marker-less techniques are within a reasonable
range for kinematics analysis compared to the more cumbersome and less portable
state-of-the-art tools. Not only does marker-less motion capture using human
pose estimation produce results in-line with the results of both the IMU and
MoCap kinematics but also benefits from a reduced set-up time and reduced
practical knowledge and expertise to set up. Overall, while there is still room
for improvement when it comes to the quality of the data produced, we believe
that this compromise is within the room of error that these low-speed actions
that are used in small clinical tests.",['cs.CV'],False,,,,Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising,"Validation of Human Pose Estimation and Human Mesh Recovery for
  Extracting Clinically Relevant Motion Data from Videos"
neg-d2-66,2025-02-24,,2502.16895," Teaching scientific concepts is essential but challenging, and analogies help
students connect new concepts to familiar ideas. Advancements in large language
models (LLMs) enable generating analogies, yet their effectiveness in education
remains underexplored. In this paper, we first conducted a two-stage study
involving high school students and teachers to assess the effectiveness of
LLM-generated analogies in biology and physics through a controlled in-class
test and a classroom field study. Test results suggested that LLM-generated
analogies could enhance student understanding particularly in biology, but
require teachers' guidance to prevent over-reliance and overconfidence.
Classroom experiments suggested that teachers could refine LLM-generated
analogies to their satisfaction and inspire new analogies from generated ones,
encouraged by positive classroom feedback and homework performance boosts.
Based on findings, we developed and evaluated a practical system to help
teachers generate and refine teaching analogies. We discussed future directions
for developing and evaluating LLM-supported teaching and learning by analogy.",['cs.HC'],2502.16895," Teaching scientific concepts is essential but challenging, and analogies help
students connect new concepts to familiar ideas. Advancements in large language
models (LLMs) enable generating analogies, yet their effectiveness in education
remains underexplored. In this paper, we first conducted a two-stage study
involving high school students and teachers to assess the effectiveness of
LLM-generated analogies in biology and physics through a controlled in-class
test and a classroom field study. Test results suggested that LLM-generated
analogies could enhance student understanding particularly in biology, but
require teachers' guidance to prevent over-reliance and overconfidence.
Classroom experiments suggested that teachers could refine LLM-generated
analogies to their satisfaction and inspire new analogies from generated ones,
encouraged by positive classroom feedback and homework performance boosts.
Based on findings, we developed and evaluated a practical system to help
teachers generate and refine teaching analogies. We discussed future directions
for developing and evaluating LLM-supported teaching and learning by analogy.",['cs.HC'],False,,,,"Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies
  for Student Understanding and Classroom Practice?","Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies
  for Student Understanding and Classroom Practice?"
neg-d2-67,2025-01-13,,2501.07348," Full Waveform Inversion (FWI) is a promising technique for achieving
high-resolution imaging in medical ultrasound. Traditional FWI methods suffer
from issues related to computational efficiency, dependence on initial models,
and the inability to quantify uncertainty. This study introduces the Stein
Variational Gradient Descent (SVGD) algorithm into FWI, aiming to improve
inversion performance and enhance uncertainty quantification. By deriving the
posterior gradient, the study explores the integration of SVGD with FWI and
demonstrates its ability to approximate complex priors. In-silico experiments
with synthetic data and real-world breast tissue data highlight the advantages
of the SVGD-based framework over conventional FWI. SVGD-based FWI improves
inversion quality, provides more reliable uncertainty quantification, and
offers a tighter bound for the prior distribution. These findings show that
probabilistic inversion is a promising tool for addressing the limitations of
traditional FWI methods in ultrasonic imaging of medical tissues.",['physics.med-ph'],2502.02677," Research on ultra-high dose rate (UHDR) radiation therapy has indicated its
potential to spare normal tissue while maintaining equivalent tumor control
compared to conventional treatments. First clinical trials are underway. The
randomized phase II/III FEATHER clinical trial at the Paul Scherrer Institute
in collaboration with the University of Zurich Animal Hospital is one of the
first curative domestic animal trials to be attempted, and it is designed to
provide a good example for human trials. However, the lack of standardized
quality assurance (QA) guidelines for FLASH clinical trials presents a
significant challenge in trial design. This work aims to demonstrate the
development and testing of QA and reporting procedures implemented in the
FEATHER clinical trial. We have expanded the clinical QA program to include
UHDR-specific QA and additional patient-specific QA. Furthermore, we have
modified the monitor readout to enable time-resolved measurements, allowing
delivery log files to be used for dose and dose rate recalculations. Finally,
we developed a reporting strategy encompassing relevant parameters for
retrospective studies. We evaluated our QA and reporting procedures with
simulated treatments. This testing confirmed that our QA procedures effectively
ensure the correct and safe delivery of the planned dose. Additionally, we
demonstrated that we could reconstruct the delivered dose and dose rate using
the delivery log files. We developed and used in practice a comprehensive QA
and reporting protocol for a FLASH clinical trial at the Paul Scherrer
Institute. This work aims to establish guidelines and standardize reporting
practices for future advancements in the FLASH-RT field.",['physics.med-ph'],False,,,,"Ultrasonic Medical Tissue Imaging Using Probabilistic Inversion:
  Leveraging Variational Inference for Speed Reconstruction and Uncertainty
  Quantification","Quality assurance and reporting for FLASH clinical trials:the experience
  of the FEATHER trial"
neg-d2-68,2025-01-22,,2501.12642," This report investigates Training Data Attribution (TDA) and its potential
importance to and tractability for reducing extreme risks from AI. First, we
discuss the plausibility and amount of effort it would take to bring existing
TDA research efforts from their current state, to an efficient and accurate
tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss
the numerous research benefits AI labs will expect to see from using such TDA
tooling. Then, we discuss a key outstanding bottleneck that would limit such
TDA tooling from being accessible publicly: AI labs' willingness to disclose
their training data. We suggest ways AI labs may work around these limitations,
and discuss the willingness of governments to mandate such access. Assuming
that AI labs willingly provide access to TDA inference, we then discuss what
high-level societal benefits you might see. We list and discuss a series of
policies and systems that may be enabled by TDA. Finally, we present an
evaluation of TDA's potential impact on mitigating large-scale risks from AI
systems.",['cs.CY'],2501.12642," This report investigates Training Data Attribution (TDA) and its potential
importance to and tractability for reducing extreme risks from AI. First, we
discuss the plausibility and amount of effort it would take to bring existing
TDA research efforts from their current state, to an efficient and accurate
tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss
the numerous research benefits AI labs will expect to see from using such TDA
tooling. Then, we discuss a key outstanding bottleneck that would limit such
TDA tooling from being accessible publicly: AI labs' willingness to disclose
their training data. We suggest ways AI labs may work around these limitations,
and discuss the willingness of governments to mandate such access. Assuming
that AI labs willingly provide access to TDA inference, we then discuss what
high-level societal benefits you might see. We list and discuss a series of
policies and systems that may be enabled by TDA. Finally, we present an
evaluation of TDA's potential impact on mitigating large-scale risks from AI
systems.",['cs.CY'],False,,,,Training Data Attribution (TDA): Examining Its Adoption & Use Cases,Training Data Attribution (TDA): Examining Its Adoption & Use Cases
neg-d2-69,2025-02-07,,2502.05342," The appropriate discount rate for evaluating policies is a critical
consideration in economic decision-making. This paper presents a new model for
calculating the derived discount rate for a society that includes different
groups with varying desirable discount rates. The model takes into account
equality in society and is designed to be used by social planners. The derived
discount rate is a useful tool for examining the social planner's approach to
policies related to the future of society. If the discount rate is determined
correctly, it can help determine the amount of growth and equality in society,
as well as the level of attention paid to long-term public projects. The model
can be customized for different distributions of wealth and discount rates,
allowing researchers to extract desired results. Analysis of the model shows
that when equality in society is considered, the derived discount rate is lower
than the result obtained using Hamilton's method. Social planners must consider
that this may increase disagreement in more consuming groups of society at
first.",['econ.TH'],2502.05342," The appropriate discount rate for evaluating policies is a critical
consideration in economic decision-making. This paper presents a new model for
calculating the derived discount rate for a society that includes different
groups with varying desirable discount rates. The model takes into account
equality in society and is designed to be used by social planners. The derived
discount rate is a useful tool for examining the social planner's approach to
policies related to the future of society. If the discount rate is determined
correctly, it can help determine the amount of growth and equality in society,
as well as the level of attention paid to long-term public projects. The model
can be customized for different distributions of wealth and discount rates,
allowing researchers to extract desired results. Analysis of the model shows
that when equality in society is considered, the derived discount rate is lower
than the result obtained using Hamilton's method. Social planners must consider
that this may increase disagreement in more consuming groups of society at
first.",['econ.TH'],False,,,,Discounting under inequality and lobbyists disagreement,Discounting under inequality and lobbyists disagreement
neg-d2-70,2025-02-12,,2502.08368," A Single Ensemble Empirical Mode Decomposition (SEEMD) is proposed for
locating the damage in rolling element bearings. The SEEMD does not require a
number of ensembles from the addition or subtraction of noise every time while
processing the signals. The SEEMD requires just a single sifting process of a
modified raw signal to reduce the computation time significantly. The other
advantage of the SEEMD method is its success in dealing with non-Gaussian or
non-stationary perturbing signals. In SEEMD, initially, a fractional Gaussian
noise (FGN) is added to the raw signal to emphasize on high frequencies of the
signal. Then, a convoluted white Gaussian noise is multiplied to the resulting
signal which changes the spectral content of the signal which helps in
extraction of the weak periodic signal. Finally, the obtained signal is
decomposed by using a single sifting process. The proposed methodology is
applied to the raw signals obtained from the mining industry. These signals are
difficult to analyze since cyclic impulsive components are obscured by noise
and other interference. Based on the results, the proposed method can
effectively detect the fault where the signal of interest (SOI) has been
extracted with good quality.",['eess.SP'],2501.11266," Upcoming Augmented Reality (AR) and Virtual Reality (VR) systems require high
data rates ($\geq$ 500 Mbps) and low power consumption for seamless experience.
With an increasing number of subscribing users, the total number of antennas
across all transmitting users far exceeds the number of antennas at the access
point (AP). This results in a low rank wireless channel, presenting a
bottleneck for uplink communication systems. The current uplink systems that
use orthogonal multiple access (OMA) and the proposed non-orthogonal multiple
access (NOMA), fail to achieve the required data rates / power consumption
under predominantly low rank channel scenarios. This paper introduces an
optimal power sub carrier allocation algorithm for multi-carrier NOMA, named
minPMAC, and an associated time-sharing algorithm that adaptively changes
successive interference cancellation decoding orders to maximize sum data rates
in these low rank channels. This Lagrangian based optimization technique,
although globally optimum, is prohibitive in terms of runtime, proving
inefficient for real-time scenarios. Hence, we propose a novel near-optimal
deep reinforcement learning-based energy sum optimization (DRL-minPMAC) which
achieves real-time efficiency. Extensive experimental evaluations show that
minPMAC achieves 28\% and 39\% higher data rates than NOMA and OMA baselines.
Furthermore, the proposed DRL-minPMAC runs ~5 times faster than minPMAC and
achieves 83\% of the global optimum data rates in real time",['eess.SP'],False,,,,"Local damage detection in rolling element bearings based on a Single
  Ensemble Empirical Mode Decomposition","Optimum Power Allocation for Low Rank Wi-Fi Channels: A Comparison with
  Deep RL Framework"
neg-d2-71,2025-03-06,,2503.04561," We consider the parametric family of elliptic curves over $\mathbb{Q}$ of the
form $E_{m} : y^{2} = x(x - n_{1})(x - n_{2}) + t^{2}$, where $n_{1}$, $n_{2}$
and $t$ are particular polynomial expressions in an integral variable $m$. In
this paper, we investigate the torsion group $E_{m}(\mathbb{Q})_{\rm{tors}}$, a
lower bound for the Mordell-Weil rank $r({E_{m}})$ and the $2$-Selmer group
${\rm{Sel}}_{2}(E_{m})$ under certain conditions on $m$. This extends the
previous works done in this direction, which are mostly concerned with the
Mordell-Weil ranks of various parametric families of elliptic curves.",['math.NT'],2501.18906," We solve the lifting problem for Galois representations in every dimension
and in every characteristic. That is, we determine all pairs $(n,k)$, where $n$
is a positive integer and $k$ is a field of characteristic $p>0$, such that for
every field $F$, every continuous homomorphism $\Gamma_F\to \mathrm{GL}_n(k)$
lifts to $\mathrm{GL}_n(W_2(k))$, where $\Gamma_F$ is the absolute Galois group
of $F$ and $W_2(k)$ is the ring of $p$-typical length $2$ Witt vectors of $k$.",['math.NT'],False,,,,"On the Mordell-Weil rank and $2$-Selmer group of a family of elliptic
  curves",The lifting problem for Galois representations
neg-d2-72,2025-01-26,,2501.15734," Network slicing aims to enhance flexibility and efficiency in next-generation
wireless networks by allocating the right resources to meet the diverse
requirements of various applications. Managing these slices with machine
learning (ML) algorithms has emerged as a promising approach however
explainability has been a challenge. To this end, several Explainable
Artificial Intelligence (XAI) frameworks have been proposed to address the
opacity in decision-making in many ML methods. In this paper, we propose a
Prioritized Value-Decomposition Network (PVDN) as an XAI-driven approach for
resource allocation in a multi-agent network slicing system. The PVDN method
decomposes the global value function into individual contributions and
prioritizes slice outputs, providing an explanation of how resource allocation
decisions impact system performance. By incorporating XAI, PVDN offers valuable
insights into the decision-making process, enabling network operators to better
understand, trust, and optimize slice management strategies. Through
simulations, we demonstrate the effectiveness of the PVDN approach with
improving the throughput by 67% and 16%, while reducing latency by 35% and 22%,
compared to independent and VDN-based resource allocation methods.",['cs.NI'],2501.12783," Serverless computing adopts a pay-as-you-go billing model where applications
are executed in stateless and shortlived containers triggered by events,
resulting in a reduction of monetary costs and resource utilization. However,
existing platforms do not provide an upper bound for the billing model which
makes the overall cost unpredictable, precluding many organizations from
managing their budgets. Due to the diverse ranges of serverless functions and
the heterogeneous capacity of edge devices, it is challenging to receive
near-optimal solutions for deployment cost in a polynomial time. In this paper,
we investigated the function scheduling problem with a budget constraint for
serverless computing in wireless networks. Users and IoT devices are sending
requests to edge nodes, improving the latency perceived by users. We propose
two online scheduling algorithms based on reinforcement learning, incorporating
several important characteristics of serverless functions. Via extensive
simulations, we justify the superiority of the proposed algorithm by comparing
with an ILP solver (Midaco). Our results indicate that the proposed algorithms
efficiently approximate the results of Midaco within a factor of 1.03 while our
decision-making time is 5 orders of magnitude less than that of Midaco.",['cs.NI'],False,,,,"Prioritized Value-Decomposition Network for Explainable AI-Enabled
  Network Slicing","Cost Optimization for Serverless Edge Computing with Budget Constraints
  using Deep Reinforcement Learning"
neg-d2-73,2025-02-18,,2502.12972," Beat and downbeat tracking models are predominantly developed using datasets
with music in 4/4 meter, which decreases their generalization to repertories in
other time signatures, such as Brazilian samba which is in 2/4. In this work,
we propose a simple augmentation technique to increase the representation of
time signatures beyond 4/4, namely 2/4 and 3/4. Our augmentation procedure
works by removing beat intervals from 4/4 annotated tracks. We show that the
augmented data helps to improve downbeat tracking for underrepresented meters
while preserving the overall performance of beat tracking in two different
models. We also show that this technique helps improve downbeat tracking in an
unseen samba dataset.",['cs.SD'],2502.12972," Beat and downbeat tracking models are predominantly developed using datasets
with music in 4/4 meter, which decreases their generalization to repertories in
other time signatures, such as Brazilian samba which is in 2/4. In this work,
we propose a simple augmentation technique to increase the representation of
time signatures beyond 4/4, namely 2/4 and 3/4. Our augmentation procedure
works by removing beat intervals from 4/4 annotated tracks. We show that the
augmented data helps to improve downbeat tracking for underrepresented meters
while preserving the overall performance of beat tracking in two different
models. We also show that this technique helps improve downbeat tracking in an
unseen samba dataset.",['cs.SD'],False,,,,"Skip That Beat: Augmenting Meter Tracking Models for Underrepresented
  Time Signatures","Skip That Beat: Augmenting Meter Tracking Models for Underrepresented
  Time Signatures"
neg-d2-74,2025-01-12,,2501.06773," Multi-objective decision-making problems have emerged in numerous real-world
scenarios, such as video games, navigation and robotics. Considering the clear
advantages of Reinforcement Learning (RL) in optimizing decision-making
processes, researchers have delved into the development of Multi-Objective RL
(MORL) methods for solving multi-objective decision problems. However, previous
methods either cannot obtain the entire Pareto front, or employ only a single
policy network for all the preferences over multiple objectives, which may not
produce personalized solutions for each preference. To address these
limitations, we propose a novel decomposition-based framework for MORL, Pareto
Set Learning for MORL (PSL-MORL), that harnesses the generation capability of
hypernetwork to produce the parameters of the policy network for each
decomposition weight, generating relatively distinct policies for various
scalarized subproblems with high efficiency. PSL-MORL is a general framework,
which is compatible for any RL algorithm. The theoretical result guarantees the
superiority of the model capacity of PSL-MORL and the optimality of the
obtained policy network. Through extensive experiments on diverse benchmarks,
we demonstrate the effectiveness of PSL-MORL in achieving dense coverage of the
Pareto front, significantly outperforming state-of-the-art MORL methods in the
hypervolume and sparsity indicators.",['cs.LG'],2503.03545," Patients with semantic dementia (SD) present with remarkably consistent
atrophy of neurons in the anterior temporal lobe and behavioural impairments,
such as graded loss of category knowledge. While relearning of lost knowledge
has been shown in acute brain injuries such as stroke, it has not been widely
supported in chronic cognitive diseases such as SD. Previous research has shown
that deep linear artificial neural networks exhibit stages of semantic learning
akin to humans. Here, we use a deep linear network to test the hypothesis that
relearning during disease progression rather than particular atrophy cause the
specific behavioural patterns associated with SD. After training the network to
generate the common semantic features of various hierarchically organised
objects, neurons are successively deleted to mimic atrophy while retraining the
model. The model with relearning and deleted neurons reproduced errors specific
to SD, including prototyping errors and cross-category confusions. This
suggests that relearning is necessary for artificial neural networks to
reproduce the behavioural patterns associated with SD in the absence of
\textit{output} non-linearities. Our results support a theory of SD progression
that results from continuous relearning of lost information. Future research
should revisit the role of relearning as a contributing factor to cognitive
diseases.",['cs.LG'],False,,,,Pareto Set Learning for Multi-Objective Reinforcement Learning,Revisiting the Role of Relearning in Semantic Dementia
neg-d2-75,2025-01-18,,2501.17877," In this paper, we carry out in-depth research centering around the $(p,
q)$-Sobolev inequality and Nash inequality on forward complete Finsler metric
measure manifolds under the condition that ${\rm Ric}_{\infty} \geq -K$ for
some $K \geq 0$. We first obtain a global $p$-Poincar\'{e} inequality on such
Finsler manifolds. Based on this, we can derive a $(p, q)$-Sobolev inequality.
Furthermore, we establish a global optimal $(p, q)$-Sobolev inequality with a
sharp Sobolev constant. Finally, as an application of the $p$-Poincar\'{e}
inequality, we prove a Nash inequality.",['math.DG'],2502.11914," We address the long-standing problem of the existence of a Riemannian metric
on $S^2\times T^2$ with strictly positive biorthogonal curvature ($
K_{\text{biort}}(\sigma) > 0 $), but in a weaker framework, by introducing an
affine connection with antisymmetric closed torsion, naturally encoded in the
cohomology of $S^2 \times T^2$ ($H^3(S^2 \times T^2; \mathbb{R}) \cong
\mathbb{R}^2$). This torsion, parametrized by non-trivial cohomology classes,
overcomes topological constraints imposed by the zero Euler characteristic,
ensuring $ K_{\text{biort}}(\sigma) > 0$ globally.",['math.DG'],False,,,,"$(p, q)$-Sobolev inequality and Nash inequality on forward complete
  Finsler metric measure manifolds","Positive biorthogonal curvature on $S^2 \times T^2$ via affine
  connection"
neg-d2-76,2025-01-19,,2501.11266," Upcoming Augmented Reality (AR) and Virtual Reality (VR) systems require high
data rates ($\geq$ 500 Mbps) and low power consumption for seamless experience.
With an increasing number of subscribing users, the total number of antennas
across all transmitting users far exceeds the number of antennas at the access
point (AP). This results in a low rank wireless channel, presenting a
bottleneck for uplink communication systems. The current uplink systems that
use orthogonal multiple access (OMA) and the proposed non-orthogonal multiple
access (NOMA), fail to achieve the required data rates / power consumption
under predominantly low rank channel scenarios. This paper introduces an
optimal power sub carrier allocation algorithm for multi-carrier NOMA, named
minPMAC, and an associated time-sharing algorithm that adaptively changes
successive interference cancellation decoding orders to maximize sum data rates
in these low rank channels. This Lagrangian based optimization technique,
although globally optimum, is prohibitive in terms of runtime, proving
inefficient for real-time scenarios. Hence, we propose a novel near-optimal
deep reinforcement learning-based energy sum optimization (DRL-minPMAC) which
achieves real-time efficiency. Extensive experimental evaluations show that
minPMAC achieves 28\% and 39\% higher data rates than NOMA and OMA baselines.
Furthermore, the proposed DRL-minPMAC runs ~5 times faster than minPMAC and
achieves 83\% of the global optimum data rates in real time",['eess.SP'],2503.15158," This paper focuses on an integrated sensing and communication (ISAC) system
in the presence of signal-dependent modulated jamming (SDMJ). Our goal is to
suppress jamming while carrying out simultaneous communications and sensing. We
minimize the integrated sidelobe level (ISL) of the mismatch filter output for
the transmitted waveform and the integrated level (IL) of the mismatch filter
output for the jamming, under the constraints of the loss in-processing gain
(LPG) and the peak-to-average power ratio (PAPR) of the transmitted waveform.
Meanwhile, the similarity constraint is introduced for information-bearing
transmit waveform. We develop a decoupled majorization minimization (DMM)
algorithm to solve the proposed multi-constrained optimization problem. In
contrast to the existing approaches, the proposed algorithm transforms the
difficult optimization problem involving two variables into two parallel
sub-problems with one variable, thus significantly speeding up the convergence
rate. Furthermore, fast Fourier transform (FFT) is introduced to compute the
closed-form solution of each sub-problem, giving rise to a greatly reduced
computation complexity. Simulation results demonstrate the capabilities of the
proposed ISAC system which strikes a proper trade-off among sensing and jamming
suppression.",['eess.SP'],False,,,,"Optimum Power Allocation for Low Rank Wi-Fi Channels: A Comparison with
  Deep RL Framework","Waveform and Filter Design for Integrated Sensing and Communication
  Against Signal-dependent Modulated Jamming"
neg-d2-77,2025-03-21,,2503.16985," We investigate the weak limit of the hyper-rough square-root process as the
Hurst index $H$ goes to $-1/2\,$. This limit corresponds to the fractional
kernel $t^{H - 1 / 2}$ losing integrability. We establish the joint convergence
of the couple $(X, M)\,$, where $X$ is the hyper-rough process and $M$ the
associated martingale, to a fully correlated Inverse Gaussian L\'evy jump
process. This unveils the existence of a continuum between hyper-rough
continuous models and jump processes, as a function of the Hurst index. Since
we prove a convergence of continuous to discontinuous processes, the usual
Skorokhod $J_1$ topology is not suitable for our problem. Instead, we obtain
the weak convergence in the Skorokhod $M_1$ topology for $X$ and in the
non-Skorokhod $S$ topology for $M$.",['math.PR'],2503.16912," The purpose of this paper is to introduce the construction of a stochastic
process called ``diffusion house-moving'' and to explore its properties. We
study the weak convergence of diffusion bridges conditioned to stay between two
curves, and we refer to this limit as diffusion house-moving. Applying this
weak convergence result, we give the sample path properties of diffusion
house-moving.",['math.PR'],False,,,,From Hyper Roughness to Jumps as $H \to -1/2$,"Construction and sample path properties of diffusion house-moving
  between two curves"
neg-d2-78,2025-01-28,,2501.16848," Biophysical models offer valuable insights into climate-phenology
relationships in both natural and agricultural settings. However, there are
substantial structural discrepancies across models which require site-specific
recalibration, often yielding inconsistent predictions under similar climate
scenarios. Machine learning methods offer data-driven solutions, but often lack
interpretability and alignment with existing knowledge. We present a phenology
model describing dormancy in fruit trees, integrating conventional biophysical
models with a neural network to address their structural disparities. We
evaluate our hybrid model in an extensive case study predicting cherry tree
phenology in Japan, South Korea and Switzerland. Our approach consistently
outperforms both traditional biophysical and machine learning models in
predicting blooming dates across years. Additionally, the neural network's
adaptability facilitates parameter learning for specific tree varieties,
enabling robust generalization to new sites without site-specific
recalibration. This hybrid model leverages both biophysical constraints and
data-driven flexibility, offering a promising avenue for accurate and
interpretable phenology modeling.",['cs.LG'],2501.08305," Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.",['cs.LG'],False,,,,"Hybrid Phenology Modeling for Predicting Temperature Effects on Tree
  Dormancy","Benchmarking Graph Representations and Graph Neural Networks for
  Multivariate Time Series Classification"
neg-d2-79,2025-02-17,,2502.12464," Deploying large language models (LLMs) in real-world applications requires
robust safety guard models to detect and block harmful user prompts. While
large safety guard models achieve strong performance, their computational cost
is substantial. To mitigate this, smaller distilled models are used, but they
often underperform on ""hard"" examples where the larger model provides accurate
predictions. We observe that many inputs can be reliably handled by the smaller
model, while only a small fraction require the larger model's capacity.
Motivated by this, we propose SafeRoute, a binary router that distinguishes
hard examples from easy ones. Our method selectively applies the larger safety
guard model to the data that the router considers hard, improving efficiency
while maintaining accuracy compared to solely using the larger safety guard
model. Experimental results on multiple benchmark datasets demonstrate that our
adaptive model selection significantly enhances the trade-off between
computational cost and safety performance, outperforming relevant baselines.",['cs.CL'],2503.01742," The rapid growth of Large Language Models (LLMs) presents significant
privacy, security, and ethical concerns. While much research has proposed
methods for defending LLM systems against misuse by malicious actors,
researchers have recently complemented these efforts with an offensive approach
that involves red teaming, i.e., proactively attacking LLMs with the purpose of
identifying their vulnerabilities. This paper provides a concise and practical
overview of the LLM red teaming literature, structured so as to describe a
multi-component system end-to-end. To motivate red teaming we survey the
initial safety needs of some high-profile LLMs, and then dive into the
different components of a red teaming system as well as software packages for
implementing them. We cover various attack methods, strategies for
attack-success evaluation, metrics for assessing experiment outcomes, as well
as a host of other considerations. Our survey will be useful for any reader who
wants to rapidly obtain a grasp of the major red teaming concepts for their own
use in practical applications.",['cs.CL'],False,,,,"SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety
  Guardrails in Large Language Models","Building Safe GenAI Applications: An End-to-End Overview of Red Teaming
  for Large Language Models"
neg-d2-80,2025-02-05,,2502.03755," In this paper, we address the task of characterizing the chemical composition
of planetary surfaces using convolutional neural networks (CNNs). Specifically,
we seek to predict the multi-oxide weights of rock samples based on
spectroscopic data collected under Martian conditions. We frame this problem as
a multi-target regression task and propose a novel regularization method based
on f-divergence. The f-divergence regularization is designed to constrain the
distributional discrepancy between predictions and noisy targets. This
regularizer serves a dual purpose: on the one hand, it mitigates overfitting by
enforcing a constraint on the distributional difference between predictions and
noisy targets. On the other hand, it acts as an auxiliary loss function,
penalizing the neural network when the divergence between the predicted and
target distributions becomes too large. To enable backpropagation during neural
network training, we develop a differentiable f-divergence and incorporate it
into the f-divergence regularization, making the network training feasible. We
conduct experiments using spectra collected in a Mars-like environment by the
remote-sensing instruments aboard the Curiosity and Perseverance rovers.
Experimental results on multi-oxide weight prediction demonstrate that the
proposed $f$-divergence regularization performs better than or comparable to
standard regularization methods including $L_1$, $L_2$, and dropout. Notably,
combining the $f$-divergence regularization with these standard regularization
further enhances performance, outperforming each regularization method used
independently.",['cs.LG'],2501.04099," Imbalanced multiclass datasets pose challenges for machine learning
algorithms. These datasets often contain minority classes that are important
for accurate prediction. Existing methods still suffer from sparse data and may
not accurately represent the original data patterns, leading to noise and poor
model performance. A hybrid method called Neighbor Displacement-based Enhanced
Synthetic Oversampling (NDESO) is proposed in this paper. This approach uses a
displacement strategy for noisy data points, computing the average distance to
their neighbors and moving them closer to their centroids. Random oversampling
is then performed to achieve dataset balance. Extensive evaluations compare 14
alternatives on nine classifiers across synthetic and 20 real-world datasets
with varying imbalance ratios. The results show that our method outperforms its
competitors regarding average G-mean score and achieves the lowest statistical
mean rank. This highlights its superiority and suitability for addressing data
imbalance in practical applications.",['cs.LG'],False,,,,"Regularization via f-Divergence: An Application to Multi-Oxide
  Spectroscopic Analysis","Neighbor displacement-based enhanced synthetic oversampling for
  multiclass imbalanced data"
neg-d2-81,2025-02-06,,2502.04542," Visual storytelling combines visuals and narratives to communicate important
insights. While web-based visual storytelling is well-established, leveraging
the next generation of digital technologies for visual storytelling,
specifically immersive technologies, remains underexplored. We investigated the
impact of the story viewpoint (from the audience's perspective) and navigation
(when progressing through the story) on spatial immersion and understanding.
First, we collected web-based 3D stories and elicited design considerations
from three VR developers. We then adapted four selected web-based stories to an
immersive format. Finally, we conducted a user study (N=24) to examine
egocentric and exocentric viewpoints, active and passive navigation, and the
combinations they form. Our results indicated significantly higher preferences
for egocentric+active (higher agency and engagement) and exocentric+passive
(higher focus on content). We also found a marginal significance of viewpoints
on story understanding and a strong significance of navigation on spatial
immersion.",['cs.HC'],2502.04542," Visual storytelling combines visuals and narratives to communicate important
insights. While web-based visual storytelling is well-established, leveraging
the next generation of digital technologies for visual storytelling,
specifically immersive technologies, remains underexplored. We investigated the
impact of the story viewpoint (from the audience's perspective) and navigation
(when progressing through the story) on spatial immersion and understanding.
First, we collected web-based 3D stories and elicited design considerations
from three VR developers. We then adapted four selected web-based stories to an
immersive format. Finally, we conducted a user study (N=24) to examine
egocentric and exocentric viewpoints, active and passive navigation, and the
combinations they form. Our results indicated significantly higher preferences
for egocentric+active (higher agency and engagement) and exocentric+passive
(higher focus on content). We also found a marginal significance of viewpoints
on story understanding and a strong significance of navigation on spatial
immersion.",['cs.HC'],False,,,,"Ego vs. Exo and Active vs. Passive: Investigating the Effects of
  Viewpoint and Navigation on Spatial Immersion and Understanding in Immersive
  Storytelling","Ego vs. Exo and Active vs. Passive: Investigating the Effects of
  Viewpoint and Navigation on Spatial Immersion and Understanding in Immersive
  Storytelling"
neg-d2-82,2025-03-19,,2503.15213," Automatic non-cooperative analysis of intercepted radar signals is essential
for intelligent equipment in both military and civilian domains. Accurate
modulation identification and parameter estimation enable effective signal
classification, threat assessment, and the development of countermeasures. In
this paper, we propose a symbolic approach for radar signal recognition and
parameter estimation based on a vision-language model that combines
context-free grammar with time-frequency representation of radar waveforms. The
proposed model, called Sig2text, leverages the power of vision transformers for
time-frequency feature extraction and transformer-based decoders for symbolic
parsing of radar waveforms. By treating radar signal recognition as a parsing
problem, Sig2text can effectively recognize and parse radar waveforms with
different modulation types and parameters. We evaluate the performance of
Sig2text on a synthetic radar signal dataset and demonstrate its effectiveness
in recognizing and parsing radar waveforms with varying modulation types and
parameters. The training code of the model is available at
https://github.com/Na-choneko/sig2text.",['eess.SP'],2502.02512," Recently, there has been an increasing interest in 6G technology for
integrated sensing and communications, where positioning stands out as a key
application. In the realm of 6G, cell-free massive multiple-input
multiple-output (MIMO) systems, featuring distributed base stations equipped
with a large number of antennas, present an abundant source of angle-of-arrival
(AOA) information that could be exploited for positioning applications. In this
paper we leverage this AOA information at the base stations using the multiple
signal classification (MUSIC) algorithm, in conjunction with received signal
strength (RSS) for positioning through Gaussian process regression (GPR). An
AOA fingerprint database is constructed by capturing the angle data from
multiple locations across the network area and is combined with RSS data from
the same locations to form a hybrid fingerprint which is then used to train a
GPR model employing a squared exponential kernel. The trained regression model
is subsequently utilized to estimate the location of a user equipment.
Simulations demonstrate that the GPR model with hybrid input achieves better
positioning accuracy than traditional GPR models utilizing RSS-only and
AOA-only inputs.",['eess.SP'],False,,,,"Sig2text, a Vision-language model for Non-cooperative Radar Signal
  Parsing",Hybrid Fingerprint-based Positioning in Cell-Free Massive MIMO Systems
neg-d2-83,2025-03-20,,2503.16164," The asymptotically optimal version of Rapidly-exploring Random Tree (RRT*) is
often used to find optimal paths in a high-dimensional configuration space. The
well-known issue of RRT* is its slow convergence towards the optimal solution.
A possible solution is to draw random samples only from a subset of the
configuration space that is known to contain configurations that can improve
the cost of the path (omniscient set). A fast convergence rate may be achieved
by approximating the omniscient with a low-volume set. In this letter, we
propose new methods to approximate the omniscient set and methods for their
effective sampling. First, we propose to approximate the omniscient set using
several (small) hyperellipsoids defined by sections of the current best
solution. The second approach approximates the omniscient set by a convex hull
computed from the current solution. Both approaches ensure asymptotical
optimality and work in a general n-dimensional configuration space. The
experiments have shown superior performance of our approaches in multiple
scenarios in 3D and 6D configuration spaces.",['cs.RO'],2502.15961," Planning paths that maximize information gain for robotic platforms has
wide-ranging applications and significant potential impact. To effectively
adapt to real-time data collection, informative path planning must be computed
online and be responsive to new observations. In this work, we present
IA-TIGRIS, an incremental and adaptive sampling-based informative path planner
that can be run efficiently with onboard computation. Our approach leverages
past planning efforts through incremental refinement while continuously
adapting to updated world beliefs. We additionally present detailed
implementation and optimization insights to facilitate real-world deployment,
along with an array of reward functions tailored to specific missions and
behaviors. Extensive simulation results demonstrate IA-TIGRIS generates
higher-quality paths compared to baseline methods. We validate our planner on
two distinct hardware platforms: a hexarotor UAV and a fixed-wing UAV, each
having unique motion models and configuration spaces. Our results show up to a
41% improvement in information gain compared to baseline methods, suggesting
significant potential for deployment in real-world applications.",['cs.RO'],False,,,,"Asymptotically Optimal Path Planning With an Approximation of the
  Omniscient Set","IA-TIGRIS: An Incremental and Adaptive Sampling-Based Planner for Online
  Informative Path Planning"
neg-d2-84,2025-01-22,,2501.12945," Quantum interference is known to become extinct with distinguishing
information, as illustrated by the ubiquitous double-slit experiment or the
two-photon HOM effect. In the former case single particle interference is
destroyed with which-path information while in the latter bunching interference
tails-off as photons become distinguishable. It has been observed that when
more than two particles are involved, these interference patterns are in
general a non monotonic function of the distinguishability. Here we perform a
comprehensive characterization, both theoretically and experimentally, of
four-photon interference by analyzing the corresponding correlation functions,
contemplating several degrees of distinguishability across different
parameters. This study provides all the necessary tools to quantify the impact
of multi-photon interference on precision measurements of parameters such as
phase, frequency, and time difference. We apply these insights to quantify the
precision in the estimation of an interferometric phase in a two-port
interferometer using a four-photon state. Our results reveal that, for certain
phase values, partially distinguishable multi-photon states can achieve higher
Fisher information values compared to the two-photon experiment. These findings
highlight the potential of distinguishable multi-photon states for enhanced
precision in quantum metrology and related applications.",['quant-ph'],2502.00496," The nodes are traditionally viewed as fixed points where the probability
density vanishes. However, this work demonstrates that these nodes exhibit
time-dependent oscillation in quantum superposition states. We derive this
effect for a fundamental system: the 1D particle in a box. It is shown that the
probability density in a superposition of two eigenstates evolves with a
time-dependent interference term, introducing an oscillation of the nodes at a
specific frequency equal to the energy difference between the states. This
result suggests a deeper dynamical role for nodes in quantum systems.",['quant-ph'],False,,,,"Unraveling quantum phase estimation: exploring the impact of
  multi-photon interference on the quantum Fisher information",Time evolution of nodes in quantum superposition states
neg-d2-85,2025-02-22,,2502.1622," We analyze the available observational data on the radial distribution of gas
and young stellar populations in the disks of low surface brightness (LSB)
galaxies and in the outer regions or the extended disks of normal brightness
(HSB) galaxies. These cases involve star formation under special conditions of
low volume and surface gas density. There is no well-defined boundary between
these subgroups of galaxies that we consider, but in non-dwarf LSB galaxies the
rate of current star formation within the wide range of radial distances
appears to be higher compared to the outer disks of most of HSB galaxies at
similar values of the surface gas density. The factors that could stimulate the
compression of the rarefied gas at the periphery of galaxies are briefly
discussed. Attention is drawn to the idea that the densities of LSB disks
estimated from their brightness may be underestimated.",['astro-ph.GA'],2501.09791," Effectively finding and identifying active galactic nuclei (AGNs) in dwarf
galaxies is an important step in studying black hole formation and evolution.
In this work, we examine four mid-IR-selected AGN candidates in dwarf galaxies
with stellar masses between $M_\star \sim 10^8 - 10^9 M_\odot$ , and find that
the galaxies are host to nuclear star clusters (NSCs) that are notably rare in
how young and massive they are. We perform photometric measurements on the
central star clusters in our target galaxies galaxies using Hubble Space
Telescope optical and near-IR imaging and compare their observed properties to
models of stellar population evolution. We find that these galaxies are host to
very massive ($\sim10^7 M_\odot$), extremely young ($\lesssim 8$ Myr), dusty
($0.6 \lesssim \mathrm{A_v} \lesssim 1.8$) nuclear star clusters. Our results
indicate that these galactic nuclei have ongoing star-formation, are still at
least partially obscured by clouds of gas and dust, and are most likely
producing the extremely red AGN-like mid-IR colors. Moreover, prior work has
shown that these galaxies do not exhibit X-ray or optical AGN signatures.
Therefore, we recommend caution when using mid-IR color-color diagnostics for
AGN selection in dwarf galaxies, since, as directly exemplified in this sample,
they can be contaminated by massive star clusters with ongoing star formation.",['astro-ph.GA'],False,,,,"Star formation in low brightness galaxies and in the extended gaseous
  disks of normal galaxies","Star-Forming Nuclear Clusters in Dwarf Galaxies Mimicking AGN Signatures
  in the Mid-Infrared"
neg-d2-86,2025-03-19,,2503.15279," The absorption of laser energy by plasma is of paramount importance for
various applications. Collisional and resonant processes are often invoked for
this purpose. However, in some contexts (e.g. in vacuum and the JxB heating),
the energy transfer occurs even when plasma is collisionless, and there is no
resonant process involved. The energy absorption in these cases has been
attributed to the sheath electrostatic fields that get generated as the
electrons are pulled out in the vacuum from the plasma medium. The origin of
irreversibility aiding the absorption, in these cases, remains to be
understood. Particle-In-Cell (PIC) simulations using the OSIRIS 4.0 platform
have been carried out. The nearby trajectories of lighter electron species
involved in the interaction with the laser show exponential separation. This is
confirmed by the positive Lyapunov index and also by other characterizations.
The observations in these cases are contrasted with the electron cyclotron
resonant case, which shows negligible chaos in the electron trajectories
despite the energy absorption percentage being high.",['physics.plasm-ph'],2501.0464," Advanced stellarators require convoluted modular coils to produce a plasma
with satisfactory performance. Moreover, the number of coils is sometimes high
to decrease the modular ripple created by the coils. For reactor stellarators,
these requirements imply relatively small ports for in-vessel access and
maintenance, i.e. in comparison with tokamaks. The blankets and divertor
modules will have to be replaced periodically (about each 1-4 years depending
on the design) due to neutron damage, and also erosion of divertor targets.
Blanket modules are activated, thus, all the maintenance operations have to be
produced remotely. In order to reduce the shutdown time and cost during
component replacement, and to reduce the number, speed and other specifications
of the remote maintenance equipment, the number of blanket modules in the
reactor should be low and thus, the blanket modules should be large (in
relation to the minor and major radius). Nevertheless, the size of the openings
between coils limits the maximum size of the blanket and divertor modules,
though several potential enhancements have been proposed in the past for
stellarators, like straightening the outboard segments of the coils and the
movement and/or expansion of certain coils to have wider access. The present
work reports on a coil geometry for the 'Helias Stellarator Reactor' (HSR) of
three periods (HSR3) with coils located far from the plasma at the outboard
region of the straight-like sector. This feature creates natural wide openings
at such regions of the coils, which may be utilized to allow access to large
blanket and divertor modules.",['physics.plasm-ph'],False,,,,Chaos aided regime of Laser/Electromagnetic Energy Absorption by plasma,"Coil geometry with large openings for a HSR3-like stellarator reactor
  for fast replacement of in-vessel components"
neg-d2-87,2025-01-27,,2501.16108," A general idea research is a lack of articles to estimate system indicator of
the effectiveness of the strategy of human resources management (HR) at an
economic object (enterprise). We are use the method of integral indicators for
a comprehensive assessment of the activities of an economic object
(enterprise). The economic object is formalized as a nonstationary dynamic
system. The system has a dimension of 1.2 million parameters. The parameters of
the object under research (business processes) are compared with staff
responsibilities. The sanctions mode is set by blocking staff responsibilities
in the interval in each time period t. The integral indicator is calculated
according to the standard mode (Strategy 1) of the economic object (enterprise)
and without blocking the staff responsibilities. Also, the integral indicator
is calculated according to the non-standard operating mode of the enterprise
(Strategy 2) with the blocking of staff responsibilities. The difference
between the integral indicator of Strategy 2 and Strategy 1 is an estimation of
the impact of the imposed sanctions. The resource consumption for the
restoration of the normal operation of an economic object after the imposed
sanctions is give (61.63 million rubles). Example 2 introduces equipment
sanctions from America for a new project. An analysis of the indicators of a
new project is carried out with the search and use of analog equipment.",['math.OC'],2501.153," In this article, we present a modified variant of the Dai-Liao spectral
conjugate gradient method, developed through an analysis of eigenvalues and
inspired by a modified secant condition. We show that the proposed method is
globally convergent for general nonlinear functions under standard assumptions.
By incorporating the new secant condition and a quasi-Newton direction, we
introduce updated spectral parameters. These changes ensure that the resulting
search direction satisfies the sufficient descent property without relying on
any line search. Numerical experiments show that the proposed algorithm
performs better than several existing methods in terms of convergence speed and
computational efficiency. Its effectiveness is further demonstrated through an
application in signal processing.",['math.OC'],False,,,,"Control of a non-stationary dynamic system with estimating a strategy of
  human resources management by the integral indicators method","Modified Dai-Liao Spectral Conjugate Gradient Method with Application to
  Signal Processing"
neg-d2-88,2025-02-11,,2502.08012," Here we present an efficient, visible-light, gigahertz-frequency
acousto-optic modulator fabricated on a 200 mm wafer in a volume CMOS foundry.
Our device combines a piezoelectric transducer and a photonic waveguide within
a single microstructure that confines both a propagating optical mode and an
electrically excitable breathing-mode mechanical resonance. By tuning the
device's geometry to optimize the optomechanical interaction, we achieve
modulation depths exceeding 2 rad with 15 mW applied microwave power at 2.31
GHz in a 2 mm long device. This corresponds to a modulation figure of merit of
$V_{\pi}\cdot L$ = 0.26 Vcm in a visible-light, integrated acousto-optics
platform that can be straightforwardly extended to a wide range of optical
wavelengths and modulation frequencies. For the important class of
gigahertz-frequency modulators that can handle hundreds of milliwatts of
visible-light optical power, which are critical for scalable quantum control
systems, this represents a 15x decrease in $V_{\pi}$ and a 100x decrease in
required microwave power compared to the commercial state-of-the-art and
existing work in the literature.",['physics.optics'],2503.0567," Epsilon-near-zero (ENZ) systems exhibit unconventional electromagnetic
response close to their zero permittivity regime. Here, we explore the ability
of ultrathin ENZ films to modulate the transmission of radiation from an
underlying quantum emitter through active control of the carrier density of the
ENZ film. The achievable on/off switching ratio is shown to be constrained by
the material's loss parameter, particularly in the ENZ regime, where
transmissivity increases with higher material loss. The finite loss in real
materials limit the more extraordinary potential of ideal near-zero-index
systems. Along with an in-depth discussion on the material parameters vis-a-vis
the underlying physics, this work provides avenues to overcome the shortcomings
of finite loss in real materials. These findings are intended to guide material
development and offer valuable insights for designing on-chip optical
modulators and beam steering devices operating in the near-infrared regime.",['physics.optics'],False,,,,"Gigahertz-Frequency, Acousto-Optic Phase Modulation of Visible Light in
  a CMOS-Fabricated Photonic Circuit",Leveraging Epsilon Near Zero phenomena for on-chip photonic modulation
neg-d2-89,2025-02-18,,2502.12669," The rapid advancement of perovskite solar cells (PSCs) has led to an
exponential growth in research publications, creating an urgent need for
efficient knowledge management and reasoning systems in this domain. We present
a comprehensive knowledge-enhanced system for PSCs that integrates three key
components. First, we develop Perovskite-KG, a domain-specific knowledge graph
constructed from 1,517 research papers, containing 23,789 entities and 22,272
relationships. Second, we create two complementary datasets: Perovskite-Chat,
comprising 55,101 high-quality question-answer pairs generated through a novel
multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully
curated materials science problems. Third, we introduce two specialized large
language models: Perovskite-Chat-LLM for domain-specific knowledge assistance
and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental
results demonstrate that our system significantly outperforms existing models
in both domain-specific knowledge retrieval and scientific reasoning tasks,
providing researchers with effective tools for literature review, experimental
design, and complex problem-solving in PSC research.",['cs.AI'],2502.10742," Despite excelling in high-level reasoning, current language models lack
robustness in real-world scenarios and perform poorly on fundamental
problem-solving tasks that are intuitive to humans. This paper argues that both
challenges stem from a core discrepancy between human and machine cognitive
development. While both systems rely on increasing representational power, the
absence of core knowledge-foundational cognitive structures in humans-prevents
language models from developing robust, generalizable abilities, where complex
skills are grounded in simpler ones within their respective domains. It
explores empirical evidence of core knowledge in humans, analyzes why language
models fail to acquire it, and argues that this limitation is not an inherent
architectural constraint. Finally, it outlines a workable proposal for
systematically integrating core knowledge into future multi-modal language
models through the large-scale generation of synthetic training data using a
cognitive prototyping strategy.",['cs.AI'],False,,,,"Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite
  Solar Cell Research",The Philosophical Foundations of Growing AI Like A Child
neg-d2-90,2025-02-04,,2502.02414," Although deep models have been widely explored in solving partial
differential equations (PDEs), previous works are primarily limited to data
only with up to tens of thousands of mesh points, far from the million-point
scale required by industrial simulations that involve complex geometries. In
the spirit of advancing neural PDE solvers to real industrial applications, we
present Transolver++, a highly parallel and efficient neural solver that can
accurately solve PDEs on million-scale geometries. Building upon previous
advancements in solving PDEs by learning physical states via Transolver,
Transolver++ is further equipped with an extremely optimized parallelism
framework and a local adaptive mechanism to efficiently capture eidetic
physical states from massive mesh points, successfully tackling the thorny
challenges in computation and physics learning when scaling up input mesh size.
Transolver++ increases the single-GPU input capacity to million-scale points
for the first time and is capable of continuously scaling input size in linear
complexity by increasing GPUs. Experimentally, Transolver++ yields 13% relative
promotion across six standard PDE benchmarks and achieves over 20% performance
gain in million-scale high-fidelity industrial simulations, whose sizes are
100$\times$ larger than previous benchmarks, covering car and 3D aircraft
designs.",['cs.LG'],2502.12898," This study examines the potential causal relationship between head injury and
the risk of developing Alzheimer's disease (AD) using Bayesian networks and
regression models. Using a dataset of 2,149 patients, we analyze key medical
history variables, including head injury history, memory complaints,
cardiovascular disease, and diabetes. Logistic regression results suggest an
odds ratio of 0.88 for head injury, indicating a potential but statistically
insignificant protective effect against AD. In contrast, memory complaints
exhibit a strong association with AD, with an odds ratio of 4.59. Linear
regression analysis further confirms the lack of statistical significance for
head injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive
importance of memory complaints. These findings highlight the complex interplay
of medical history factors in AD risk assessment and underscore the need for
further research utilizing larger datasets and advanced causal modeling
techniques.",['cs.LG'],False,,,,"Transolver++: An Accurate Neural Solver for PDEs on Million-Scale
  Geometries","The Relationship Between Head Injury and Alzheimer's Disease: A Causal
  Analysis with Bayesian Networks"
neg-d2-91,2025-02-18,,2502.1279," Entanglement of spin degree of freedom can drastically alter the orbital
exchange symmetry of electrons, switching their bunching and antibunching
behaviors and the resultant current correlations in the Hanbury-Brown-Twiss
interferometry. Here, we investigate the exchange correlation of quasiparticles
with entanglement encoded in the Nambu spinors, or the electron-hole degree of
freedom. In contrast to the conventional correspondence between spin
entanglement and current correlation, we find that singlet (triplet)
entanglement of Nambu spinors results in suppressed (enhanced) current
correlation. This effect arises because the charge degree of freedom itself
encodes the entanglement. We propose implementing this phenomenon in the edge
states of a quantum Hall system, where the entangled states of the Nambu
spinors can be continuously tuned by gate voltages. Our study reveals a novel
relationship between entanglement and charge correlations, offering an
effective approach for detecting entanglement of Nambu spinors.",['cond-mat.mes-hall'],2502.12931," We continue our explorations of the transport characteristics in
junction-configurations comprising semimetals with quadratic band-crossings,
observed in the bandstructures of both two- and three-dimensional materials.
Here, we consider short potential barriers/wells modelled by delta-function
potentials. We also generalize our analysis by incorporating tilts in the
dispersion. Due to the parabolic nature of the spectra, caused by
quadratic-in-momentum dependence, there exist evanescent waves, which decay
exponentially as we move away from the junction represented by the location of
the delta-function potential. Investigating the possibility of the appearance
of bound states, we find that their energies appear as pairs of $\pm |E_b |$,
reflecting the presence of the imaginary-valued wavevectors at both positive
and negative values of energies of the propagating quasiparticles.",['cond-mat.mes-hall'],False,,,,"Anomalous exchange correlation of quasiparticles with entangled Nambu
  spinors","Delta-function-potential junctions with quasiparticles occupying tilted
  bands with quadratic-in-momentum dispersion"
neg-d2-92,2025-02-04,,2502.02154," This article defines a partial order structure to study the relationship
between levels and contents of conscious subjective experience in a single
mathematical set-up. We understand phenomenal structure as extrapolated
relationships among experiences, instead of fixed properties of specific
experiences. Our mathematical account is based on multilayer network theory.
Multilayer theory is a generalization of graph and network theory, widely used
in several scientific domains. This structure is also the underlying conceptual
and mathematical structure of most current models of conscious experience. From
our simple set of assumptions, yet rigorous analysis, we conclude that assuming
the comparison and quantification among phenomenal experiences yield only
partial comparison, rather than commonly assumed absolute comparability. This
has implications for evolutionary and animal consciousness: evolution may
encompass diverse modes of experiencing, not necessarily implying larger ones
on an absolute scale. Our characterization elucidates structural constraints on
experiential comparisons imposed by assumptions and choices made by modellers
as active participants in the scientific process. In summary, in light of our
phenomenological intuitions, it might be right that some experiences carry
qualitative aspects that make them incompatible or non-comparable with other
experiences, quantitatively speaking. Some experiences are comparable (e.g. at
some experiential levels), but others are not. These results have direct
implications for consciousness science, evolution and animal consciousness.",['q-bio.NC'],2502.02154," This article defines a partial order structure to study the relationship
between levels and contents of conscious subjective experience in a single
mathematical set-up. We understand phenomenal structure as extrapolated
relationships among experiences, instead of fixed properties of specific
experiences. Our mathematical account is based on multilayer network theory.
Multilayer theory is a generalization of graph and network theory, widely used
in several scientific domains. This structure is also the underlying conceptual
and mathematical structure of most current models of conscious experience. From
our simple set of assumptions, yet rigorous analysis, we conclude that assuming
the comparison and quantification among phenomenal experiences yield only
partial comparison, rather than commonly assumed absolute comparability. This
has implications for evolutionary and animal consciousness: evolution may
encompass diverse modes of experiencing, not necessarily implying larger ones
on an absolute scale. Our characterization elucidates structural constraints on
experiential comparisons imposed by assumptions and choices made by modellers
as active participants in the scientific process. In summary, in light of our
phenomenological intuitions, it might be right that some experiences carry
qualitative aspects that make them incompatible or non-comparable with other
experiences, quantitatively speaking. Some experiences are comparable (e.g. at
some experiential levels), but others are not. These results have direct
implications for consciousness science, evolution and animal consciousness.",['q-bio.NC'],False,,,,Structural constraints to compare phenomenal experience,Structural constraints to compare phenomenal experience
neg-d2-93,2025-02-04,,2502.02568," M Dwarfs make up the majority of stars, offering an avenue for discovering
exoplanets due to their smaller sizes. However, their magnetic activity poses
challenges for exoplanet detection, characterization, and planetary
habitability. Understanding its magnetic activity, including surface starspots
and internal dynamos, is crucial for exoplanet research. In this study, we
present short-term variability in four Balmer emission lines \ha, \hb, \hg, and
\hd\ for a sample of 77 M dwarfs of varying spectral types, and binarity. Stars
were observed using the MDM Observatory's Ohio State Multi-Object Spectrograph
on the 2.4m Telescope and the Modular Spectrograph on the 1.3 m Telescope.
These data are combined with TESS photometry to explore the connection between
spectroscopic and photometric variability. We observe sporadic short-term
variability in Balmer lines for some stars, on timescale $\gtrsim$ 15-min, but
much shorter than the stellar rotation period. We calculate periods for stars
lacking those measurements, re-evaluated the relationship between amplitude
(\rvar)-activity relation for the \ha \ line from
\citet{garcia_soto_contemporaneous_2023}, and extended our analysis to the \hb,
\hg \ and \hd \ lines, which indicates that the relation becomes increasingly
dispersed for higher-order Balmer lines. This is consistent with increased
intrinsic variability from lower to higher order lines. Additionally, we
compute the Balmer decrement, using \hb \ as the fiducial, for stars where we
could measure \hg \ and/or \hd. The Balmer decrement can show distinct patterns
during white-light flares, with significant differences even for the same star.
We also find evidence for dark spots on \object{TIC 283866910}.",['astro-ph.SR'],2501.18763," Carbon stars (with atmospheric C/O$>1$) range widely in temperature and
luminosity, from low mass dwarfs to asymptotic giant branch stars (AGB). The
main sequence dwarf carbon (dC) stars have inherited carbon-rich material from
an AGB companion, which has since transitioned to a white dwarf. The dC stars
are far more common than C giants, but no reliable estimates of dC space
density have been published to date. We present results from an all-sky survey
for carbon stars using the low-resolution XP spectra from Gaia DR3. We
developed and measured a set of spectral indices contrasting C$_{\rm 2}$ and CN
molecular band strengths in carbon stars against common absorption features
found in normal (C/O$<1$) stars such as CaI, TiO and Balmer lines. We combined
these indices with the XP spectral coefficients as input to supervised
machine-learning algorithms trained on a vetted sample of known C stars from
LAMOST. We describe the selection of the carbon candidate sample, and provide a
catalog of 43,574 candidates dominated by cool C giants in the Magellanic
Clouds and at low galactic latitude in the Milky Way. We report the
confirmation of candidate C stars using intermediate ($R\sim 1800$) resolution
optical spectroscopy from the Fred Lawrence Whipple Observatory, and provide
estimates of sample purity and completeness. From a carefully-vetted sample of
over 600 dCs, we measure their local space density to be
$\rho_0\,=\,1.96^{+0.14}_{-0.12}\times10^{-6}\,\text{pc}^{-3}$ (about one dC in
every local disk volume of radius 50\,pc), with a relatively large disk scale
height of $H_z\,=\,856^{+49}_{-43}\,$pc.",['astro-ph.SR'],False,,,,Short-Term Balmer Line Emission Variability in M Dwarfs,Carbon Stars From Gaia DR3 and the Space Density of Dwarf Carbon Stars
neg-d2-94,2025-01-23,,2501.14044," This study contributes to ongoing research which aims to predict small
biomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C
NMR) spectrum data and machine learning (ML). The approach was demonstrated
using a bioassay on human dopamine D1 receptor antagonists. The Simplified
Molecular Input Line Entry System (SMILES) notations of compounds in this
bioassay were extracted and converted into spectroscopic data by software
designed for this purpose. The resulting data was then used for ML with
scikit-learn algorithms. The ML models were trained by 27,756 samples and
tested by 5,466. From the estimators K-Nearest neighbor, Decision Tree
Classifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost
Classifier, and Support Vector Classifier, the last performed the best,
achieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,
and 0.749 cross-validation score with 0.005 standard deviation. The methodology
can be applied to predict any functionality of any compound when relevant data
are available. It was hypothesized also that increasing the number of samples
would increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML
model, the time- , and cost-efficient CID_SID ML model was developed. This
model allows researchers who have developed a compound and obtained its PubChem
CID and SID to check whether their compound is also a human dopamine D1
receptor antagonist based solely on the PubChem identifiers. The metrics of the
CID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,
79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard
deviation.",['q-bio.OT'],2501.14044," This study contributes to ongoing research which aims to predict small
biomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C
NMR) spectrum data and machine learning (ML). The approach was demonstrated
using a bioassay on human dopamine D1 receptor antagonists. The Simplified
Molecular Input Line Entry System (SMILES) notations of compounds in this
bioassay were extracted and converted into spectroscopic data by software
designed for this purpose. The resulting data was then used for ML with
scikit-learn algorithms. The ML models were trained by 27,756 samples and
tested by 5,466. From the estimators K-Nearest neighbor, Decision Tree
Classifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost
Classifier, and Support Vector Classifier, the last performed the best,
achieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,
and 0.749 cross-validation score with 0.005 standard deviation. The methodology
can be applied to predict any functionality of any compound when relevant data
are available. It was hypothesized also that increasing the number of samples
would increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML
model, the time- , and cost-efficient CID_SID ML model was developed. This
model allows researchers who have developed a compound and obtained its PubChem
CID and SID to check whether their compound is also a human dopamine D1
receptor antagonist based solely on the PubChem identifiers. The metrics of the
CID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,
79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard
deviation.",['q-bio.OT'],False,,,,"Leveraging 13C NMR spectroscopic data derived from SMILES to predict the
  functionality of small biomolecules by machine learning: a case study on
  human Dopamine D1 receptor antagonists","Leveraging 13C NMR spectroscopic data derived from SMILES to predict the
  functionality of small biomolecules by machine learning: a case study on
  human Dopamine D1 receptor antagonists"
neg-d2-95,2025-02-11,,2502.08111," The obscuration of light from a distant galaxy has raised the possibility
that a type of carbon dust existed in the earliest epochs of the Universe --
challenging the idea that stars had not yet evolved enough to make such
material.",['astro-ph.GA'],2503.03219," Filament G37 exhibits a distinctive ""caterpillar"" shape, characterized by two
semicircular structures within its 40\,pc-long body, providing an ideal target
to investigate the formation and evolution of filaments. By analyzing multiple
observational data, such as CO spectral line, the H$\alpha$\,RRL, and
multi-wavelength continuum, we find that the expanding H\,{\scriptsize II}
regions surrounding filament G37 exert pressure on the structure of the
filament body, which kinetic process present as the gas flows in multiple
directions along its skeleton. The curved magnetic field structure of filament
G37 derived by employing the Velocity Gradient Technique with CO is found to be
parallel to the filament body and keeps against the pressure from expanded
H\,{\scriptsize II} regions. The multi-directional flows in the filament G37
could cause the accumulation and subsequent collapse of gas, resulting in the
formation of massive clumps. The curved structure and star formation observed
in filament G37 are likely a result of the filament body being squeezed by the
expanding H\,{\scriptsize II} region. This physical process occurs over a
timescale of approximately 5\,Myr. The filament G37 provides a potential
candidate for end-dominated collapse.",['astro-ph.GA'],False,,,,Interstellar dust revealed by light from cosmic dawn,"The Impact of Expanding HII Regions on Filament G37:Curved Magnetic
  Field and Multiple Direction Material Flows"
neg-d2-96,2025-03-05,,2503.0345," The medial axis transform is a well-known tool for shape recognition. Instead
of the object contour, it equivalently describes a binary object in terms of a
skeleton containing all centres of maximal inscribed discs. While this shape
descriptor is useful for many applications, it is also sensitive to noise:
Small boundary perturbations can result in large unwanted expansions of the
skeleton. Pruning offers a remedy by removing unwanted skeleton parts. In our
contribution, we generalise this principle to skeleton sparsification: We show
that subsequently removing parts of the skeleton simplifies the associated
shape in a hierarchical manner that obeys scale-space properties.
  To this end, we provide both a continuous and discrete theory that
incorporates architectural and simplification statements as well as
invariances. We illustrate how our skeletonisation scale-spaces can be employed
for practical applications with two proof-of-concept implementations for
pruning and compression.",['eess.IV'],2501.14718," Cancer grade is a critical clinical criterion that can be used to determine
the degree of cancer malignancy. Revealing the condition of the glands, a
precise gland segmentation can assist in a more effective cancer grade
classification. In machine learning, binary classification information about
glands (i.e., benign and malignant) can be utilized as a prompt for gland
segmentation and cancer grade classification. By incorporating prior knowledge
of the benign or malignant classification of the gland, the model can
anticipate the likely appearance of the target, leading to better segmentation
performance. We utilize Segment Anything Model to solve the segmentation task,
by taking advantage of its prompt function and applying appropriate
modifications to the model structure and training strategies. We improve the
results from fine-tuned Segment Anything Model and produce SOTA results using
this approach.",['eess.IV'],False,,,,Skeletonisation Scale-Spaces,Gland Segmentation Using SAM With Cancer Grade as a Prompt
neg-d2-97,2025-01-12,,2501.06846," As is well known, unital Pauli maps can be eternally non-CP-divisible. In
contrast, here we show that in the case of non-unital maps, eternal
non-Markovianity in the non-unital part is ruled out. In the unital case, the
eternal non-Markovianity can be obtained by a convex combination of two
dephasing semigroups, but not all three of them. We study these results and the
ramifications arising from them.",['quant-ph'],2501.06846," As is well known, unital Pauli maps can be eternally non-CP-divisible. In
contrast, here we show that in the case of non-unital maps, eternal
non-Markovianity in the non-unital part is ruled out. In the unital case, the
eternal non-Markovianity can be obtained by a convex combination of two
dephasing semigroups, but not all three of them. We study these results and the
ramifications arising from them.",['quant-ph'],False,,,,On the eternal non-Markovianity of qubit maps,On the eternal non-Markovianity of qubit maps
neg-d2-98,2025-03-15,,2503.15543," We investigate the geometry, dynamics, and collision mechanisms in the
ergoregion of KerrNewman-AdS black hole influenced by quintessential energy.
Particle splittings within the ergoregion are analyzed, demonstrating their
role in energy extraction via the Penrose process. Increased spin elongates the
ergosphere, while higher quintessential parameters expand static limits and
distort photon regions. Prograde orbits benefit from reduced energy and angular
momentum due to frame-dragging, whereas retrograde orbits require higher
energy. Quintessential energy weakens the gravitational pull, shifts stable
orbit radii, and enhances orbital chaos, as indicated by Lyapunov exponents.
The Penrose process demonstrates efficiencies ranging from 5% to 35%, with peak
efficiency achieved at high spin, but diminishing with increased charge or
quintessential energy due to reduced frame-dragging. We derive the exprssion
for irreducible mass and discuss its dependence on cosmological and
quintessence parameters, revealing their role in limiting extractable energy.",['gr-qc'],2503.15543," We investigate the geometry, dynamics, and collision mechanisms in the
ergoregion of KerrNewman-AdS black hole influenced by quintessential energy.
Particle splittings within the ergoregion are analyzed, demonstrating their
role in energy extraction via the Penrose process. Increased spin elongates the
ergosphere, while higher quintessential parameters expand static limits and
distort photon regions. Prograde orbits benefit from reduced energy and angular
momentum due to frame-dragging, whereas retrograde orbits require higher
energy. Quintessential energy weakens the gravitational pull, shifts stable
orbit radii, and enhances orbital chaos, as indicated by Lyapunov exponents.
The Penrose process demonstrates efficiencies ranging from 5% to 35%, with peak
efficiency achieved at high spin, but diminishing with increased charge or
quintessential energy due to reduced frame-dragging. We derive the exprssion
for irreducible mass and discuss its dependence on cosmological and
quintessence parameters, revealing their role in limiting extractable energy.",['gr-qc'],False,,,,"Energy Extraction from Rotating Black Hole with Quintessential Energy
  through the Penrose Process","Energy Extraction from Rotating Black Hole with Quintessential Energy
  through the Penrose Process"
neg-d2-99,2025-02-09,,2502.05913," GDL, a free interpreter for the IDL language, continues to develop smoothly,
driven by feedback and requests from an increasingly active and growing user
base, especially since GDL was made available on GitHub. Among the most notable
features introduced in recent years are stable Widgets; extensive testing on
M1, M2, and M3 processors; excellent computational performance (including
OpenMP support) demonstrated across a comprehensive benchmark; simplified
compilation and installation processes; and the availability of SHMMAP and
Bridge functions, which enable concurrent GDL runs on shared RAM in HPC
environments.
  As developers of GDL, we believe this language holds a valuable place in
today's world, where efficiency and low-power computing are essential. GDL (not
to mention IDL), written in C/C++, demonstrates exceptional efficiency in
""real-world"" benchmarks, making it one of the few interpreted languages that
can truly be considered ""green."" Moreover, it is likely the only interpreter
accompanied by a vast collection of free, well-tested, and proven astronomical
procedures developed by colleagues over the years. GDL also stands out for its
suitability for long-term projects, thanks to its stable and reliable syntax.",['astro-ph.IM'],2501.17997," Developing algorithms to search through data efficiently is a challenging
part of searching for signs of technology beyond our solar system. We have
built a digital signal processing system and computer cluster on the backend of
the Karl G. Jansky Very Large Array (VLA) in New Mexico in order to search for
signals throughout the Galaxy consistent with our understanding of artificial
radio emissions. In our first paper, we described the system design and
software pipelines. In this paper, we describe a postprocessing pipeline to
identify persistent sources of interference, filter out false positives, and
search for signals not immediately identifiable as anthropogenic radio
frequency interference during the VLA Sky Survey. As of 01 September 2024, the
Commensal Open-source Multi-mode Interferometric Cluster had observed more than
950,000 unique pointings. This paper presents the strategy we employ when
commensally observing during the VLA Sky Survey and a postprocessing strategy
for the data collected during the survey. To test this postprocessing pipeline,
we searched toward 511 stars from the $Gaia$ catalog with coherent beams. This
represents about 30 minutes of observation during VLASS, where we typically
observe about 2000 sources per hour in the coherent beamforming mode. We did
not detect any unidentifiable signals, setting isotropic power limits ranging
from 10$^{11}$ to 10$^{16}$W.",['astro-ph.IM'],False,,,,"GDL 1.1, a smart and green language","COSMIC's Large-Scale Search for Technosignatures during the VLA sky
  Survey: Survey Description and First Results"
neg-d2-100,2025-03-12,,2503.09342," We present GASPACHO: a method for generating photorealistic controllable
renderings of human-object interactions. Given a set of multi-view RGB images
of human-object interactions, our method reconstructs animatable templates of
the human and object as separate sets of Gaussians simultaneously. Different
from existing work, which focuses on human reconstruction and ignores objects
as background, our method explicitly reconstructs both humans and objects,
thereby allowing for controllable renderings of novel human object interactions
in different poses from novel-camera viewpoints. During reconstruction, we
constrain the Gaussians that generate rendered images to be a linear function
of a set of canonical Gaussians. By simply changing the parameters of the
linear deformation functions after training, our method can generate renderings
of novel human-object interaction in novel poses from novel camera viewpoints.
We learn the 3D Gaussian properties of the canonical Gaussians on the
underlying 2D manifold of the canonical human and object templates. This in
turns requires a canonical object template with a fixed UV unwrapping. To
define such an object template, we use a feature based representation to track
the object across the multi-view sequence. We further propose an occlusion
aware photometric loss that allows for reconstructions under significant
occlusions. Several experiments on two human-object datasets - BEHAVE and
DNA-Rendering - demonstrate that our method allows for high-quality
reconstruction of human and object templates under significant occlusion and
the synthesis of controllable renderings of novel human-object interactions in
novel human poses from novel camera views.",['cs.CV'],2503.10982," Occlusion poses a significant challenge in pedestrian detection from a single
view. To address this, multi-view detection systems have been utilized to
aggregate information from multiple perspectives. Recent advances in multi-view
detection utilized an early-fusion strategy that strategically projects the
features onto the ground plane, where detection analysis is performed. A
promising approach in this context is the use of 3D feature-pulling technique,
which constructs a 3D feature volume of the scene by sampling the corresponding
2D features for each voxel. However, it creates a 3D feature volume of the
whole scene without considering the potential locations of pedestrians. In this
paper, we introduce a novel model that efficiently leverages traditional 3D
reconstruction techniques to enhance deep multi-view pedestrian detection. This
is accomplished by complementing the 3D feature volume with probabilistic
occupancy volume, which is constructed using the visual hull technique. The
probabilistic occupancy volume focuses the model's attention on regions
occupied by pedestrians and improves detection accuracy. Our model outperforms
state-of-the-art models on the MultiviewX dataset, with an MODA of 97.3%, while
achieving competitive performance on the Wildtrack dataset.",['cs.CV'],False,,,,GASPACHO: Gaussian Splatting for Controllable Humans and Objects,"Enhanced Multi-View Pedestrian Detection Using Probabilistic Occupancy
  Volume"
neg-d2-101,2025-01-09,,2501.05025," We investigate the magnetoelectric properties of the monolayer NiX$_{2}$ (X =
Br, I) through first-principles calculations. Our calculations predict that the
NiBr$_{2}$ monolayer exhibits a cycloidal magnetic ground state. For the
NiI$_{2}$ monolayer, a proper-screw helical magnetic ground state with
modulation vector \(\boldsymbol{Q} = (q, 0, 0)\) is adopted, approximated based
on experimental observations. The electric polarization in NiBr$_{2}$ shows a
linear dependence on the spin-orbit coupling strength \(\lambda_{\text{SOC}}\),
which can be adequately described by the generalized Katsura-Nagaosa-Balatsky
(gKNB) model, considering contributions from up to the third nearest-neighbor
spin pairs. In contrast, the electric polarization in NiI$_{2}$ exhibits a
distinct dependence on \(q\) and \(\lambda_{\text{SOC}}\), which cannot be
fully explained by the gKNB mechanism alone. To address this, the \(p\)-\(d\)
hybridization mechanism is extended to NiI$_{2}$ to explain the observed
behavior. The respective contributions from the \(p\)-\(d\) hybridization and
the gKNB mechanism in NiI$_{2}$ are then quantitatively evaluated. Overall, our
work elucidates the microscopic mechanisms underlying multiferroicity in
NiBr$_{2}$ and NiI$_{2}$ monolayers, with the conclusions readily applicable to
their bulk forms.",['cond-mat.mtrl-sci'],2503.13187," Magnetoelectric composites integrate the coupling between magnetic and
piezoelectric materials to create new functionalities for potential
technological applications. This coupling is typically achieved through the
exchange of magnetic, electric, or elastic energy across the interfaces between
the different constituent materials. Tailoring the strength of the
magnetoelectric effect is primarily accomplished by selecting suitable
materials for each constituent and by optimizing geometrical and
microstructural designs. Various composite architectures, such as (0-3), (2-2),
(1-3) and core-shell connectivities, have been studied to enhance
magnetoelectric coupling and other required physical properties in composites.
This review examines the latest advancements in magnetoelectric materials,
focusing on the impact of different interphase connectivity types on their
properties and performance. Before exploring magnetic-electric coupling, a
brief overview of the historical background of multiferroic magnetoelectric
composites is provided. Fundamental concepts underlying the magnetoelectric
effect, piezoelectricity, and the magnetostrictive effect are explained,
including their origins and examples of these materials' properties. So far,
three types of magnetoelectric composite connectivities have been investigated
experimentally: particulate composites (0-3), laminated and thin films (2-2),
sticks embedded in matrix, core-shell particles, and coaxial fibers. An outlook
on the prospects and scientific challenges in the field of multiferroic
magnetoelectric composites is given at the end of this review.",['cond-mat.mtrl-sci'],False,,,,"Microscopic origin of magnetoferroelectricity in monolayer NiBr$_{2}$
  and NiI$_{2}$","Current Advances in Magnetoelectric Composites with Various Interphase
  Connectivity Types"
neg-d2-102,2025-01-13,,2501.07295," This paper introduces GestLLM, an advanced system for human-robot interaction
that enables intuitive robot control through hand gestures. Unlike conventional
systems, which rely on a limited set of predefined gestures, GestLLM leverages
large language models and feature extraction via MediaPipe to interpret a
diverse range of gestures. This integration addresses key limitations in
existing systems, such as restricted gesture flexibility and the inability to
recognize complex or unconventional gestures commonly used in human
communication.
  By combining state-of-the-art feature extraction and language model
capabilities, GestLLM achieves performance comparable to leading
vision-language models while supporting gestures underrepresented in
traditional datasets. For example, this includes gestures from popular culture,
such as the ``Vulcan salute"" from Star Trek, without any additional
pretraining, prompt engineering, etc. This flexibility enhances the naturalness
and inclusivity of robot control, making interactions more intuitive and
user-friendly.
  GestLLM provides a significant step forward in gesture-based interaction,
enabling robots to understand and respond to a wide variety of hand gestures
effectively. This paper outlines its design, implementation, and evaluation,
demonstrating its potential applications in advanced human-robot collaboration,
assistive robotics, and interactive entertainment.",['cs.RO'],2502.0738," Simulation has been pivotal in recent robotics milestones and is poised to
play a prominent role in the field's future. However, recent robotic advances
often rely on expensive and high-maintenance platforms, limiting access to
broader robotics audiences. This work introduces Wheeled Lab, a framework for
the low-cost, open-source wheeled platforms that are already widely established
in education and research. Through integration with Isaac Lab, Wheeled Lab
introduces modern techniques in Sim2Real, such as domain randomization, sensor
simulation, and end-to-end learning, to new user communities. To kickstart
education and demonstrate the framework's capabilities, we develop three
state-of-the-art policies for small-scale RC cars: controlled drifting,
elevation traversal, and visual navigation, each trained in simulation and
deployed in the real world. By bridging the gap between advanced Sim2Real
methods and affordable, available robotics, Wheeled Lab aims to democratize
access to cutting-edge tools, fostering innovation and education in a broader
robotics context. The full stack, from hardware to software, is low cost and
open-source.",['cs.RO'],False,,,,"GestLLM: Advanced Hand Gesture Interpretation via Large Language Models
  for Human-Robot Interaction","Demonstrating Wheeled Lab: Modern Sim2Real for Low-cost, Open-source
  Wheeled Robotics"
neg-d2-103,2025-03-18,,2503.14022," As the scale of data centers continues to grow, there is an increasing demand
for interconnection networks to resist malicious attacks. Hence, it is
necessary to evaluate the reliability of networks under various fault patterns.
The family of generalized $K_4$-hypercubes serve as interconnection networks of
data centers, characterized by topological structures with exceptional
properties. The $h$-extra edge-connectivity $\lambda_h$, the $l$-super
edge-connectivity $\lambda^l$, the $l$-average degree edge-connectivity
$\overline{\lambda^l}$, the $l$-embedded edge-connectivity $\eta_l$ and the
cyclic edge-connectivity $\lambda_c$ are vital parameters to accurately assess
the reliability of interconnection networks. Let integer $n\geq3$. This paper
obtains the optimal solution of the edge isoperimetric problem and its explicit
representation, which offers an upper bound of the $h$-extra edge-connectivity
of an $n$-dimensional $K_4$-hypercube $H_n^4$. As an application, we presents
$\lambda_h(H_n^4)$ for $1\leq h\leq 2^{\lceil n/2 \rceil }$. Moreover, for
$2^{\lceil n/2\rceil+t}-g_t \le h\le2^{\lceil n/2\rceil+t}$,
$g_t=\lceil(2^{2t+2+\gamma})/3\rceil$,
  $0\leq t \leq\lfloor n/2\rfloor-1 $, $\gamma=0$ for even $n$ and $\gamma=1$
for odd $n$, $\lambda_h(H_n^4)$ is a constant $(\lfloor n/2\rfloor-t)2^{\lceil
n/2\rceil+t}$. The above lower and upper bounds of the integer $h$ are both
sharp. Furthermore, $\lambda^l(H_n^4)$, $\overline{\lambda^l}(H_n^4)$,
$\lambda_{2^l}(H_n^4)$, and $\eta_l(H_n^4)$ share a common value $(n-l)2^l$ for
$2\leq l\leq n-1$, and we determines the values of $\lambda_c(H_n^4)$.",['math.CO'],2501.02726," In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),
which are drawn on the torus so that every edge crosses another edge at most
once, and has $n$ vertices and exactly $4n$ edges. We first consider
connectivity of O1TGs, and give the characterization of O1TGs having
connectivity exactly $k$ for each $k\in \{4, 5, 6, 8\}$. In our argument, we
also show that there exists no O1TG having connectivity exactly $7$.
Furthermore, using the result above, we discuss extendability of matchings, and
give the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",['math.CO'],False,,,,"Reliability Evaluation of Generalized $K_4$-Hypercubes Based on Five
  Link Fault Patterns","Connectivity and matching extendability of optimal $1$-embedded graphs
  on the torus"
neg-d2-104,2025-01-21,,2501.12034," Like most computer systems, a manycore can also be the target of security
attacks. It is essential to ensure the security of the NoC since all
information travels through its channels, and any interference in the traffic
of messages can reflect on the entire chip, causing communication problems.
Among the possible attacks on NoC, Denial of Service (DoS) attacks are the most
cited in the literature. The state of the art shows a lack of work that can
detect such attacks through learning techniques. On the other hand, these
techniques are widely explored in computer network security via an Intrusion
Detection System (IDS). In this context, the main goal of this document is to
present the progress of a work that explores an IDS technique using machine
learning and temporal series for detecting DoS attacks in NoC-based manycore
systems. To fulfill this goal, it is necessary to extract traffic data from a
manycore NoC and execute the learning techniques in the extracted data.
However, while low-level platforms offer precision and slow execution,
high-level platforms offer higher speed and data incompatible with reality.
Therefore, a platform is being developed using the OVP tool, which has a higher
level of abstraction. To solve the low precision problem, the developed
platform will have its data validated with a low-level platform.",['cs.CR'],2503.02499," CONTEXT. Attack treesare a recommended threat modeling tool, but there is no
established method to compare them. OBJECTIVE. We aim to establish a method to
compare ""real"" attack trees, based on both the structure of the tree itself and
the meaning of the node labels. METHOD. We define four methods of comparison
(three novel and one established) and compare them to a dataset of attack trees
created from a study run on students (n = 39). These attack trees all follow
from the same scenario, but have slightly different labels. RESULTS. We find
that applying semantic similarity as a means of comparing node labels is a
valid approach. Further, we find that treeedit distance (established) and
radical distance (novel) are themost promising methods of comparison in most
circumstances. CONCLUSION. We show that these two methods are valid as means of
comparing attack trees, and suggest a novel technique for using semantic
similarity to compare node labels. We further suggest that these methods can be
used to compare attack trees in a real-world scenario, and that they can be
used to identify similar attack trees.",['cs.CR'],False,,,,"Application of Machine Learning Techniques for Secure Traffic in
  NoC-based Manycores","Attack Tree Distance: a practical examination of tree difference
  measurement within cyber security"
neg-d2-105,2025-02-07,,2502.05324," The prevailing methodologies for visualizing AI risks have focused on
technical issues such as data biases and model inaccuracies, often overlooking
broader societal risks like job loss and surveillance. Moreover, these
visualizations are typically designed for tech-savvy individuals, neglecting
those with limited technical skills. To address these challenges, we propose
the Atlas of AI Risks-a narrative-style tool designed to map the broad risks
associated with various AI technologies in a way that is understandable to
non-technical individuals as well. To both develop and evaluate this tool, we
conducted two crowdsourcing studies. The first, involving 40 participants,
identified the design requirements for visualizing AI risks for decision-making
and guided the development of the Atlas. The second study, with 140
participants reflecting the US population in terms of age, sex, and ethnicity,
assessed the usability and aesthetics of the Atlas to ensure it met those
requirements. Using facial recognition technology as a case study, we found
that the Atlas is more user-friendly than a baseline visualization, with a more
classic and expressive aesthetic, and is more effective in presenting a
balanced assessment of the risks and benefits of facial recognition. Finally,
we discuss how our design choices make the Atlas adaptable for broader use,
allowing it to generalize across the diverse range of technology applications
represented in a database that reports various AI incidents.",['cs.HC'],2502.04542," Visual storytelling combines visuals and narratives to communicate important
insights. While web-based visual storytelling is well-established, leveraging
the next generation of digital technologies for visual storytelling,
specifically immersive technologies, remains underexplored. We investigated the
impact of the story viewpoint (from the audience's perspective) and navigation
(when progressing through the story) on spatial immersion and understanding.
First, we collected web-based 3D stories and elicited design considerations
from three VR developers. We then adapted four selected web-based stories to an
immersive format. Finally, we conducted a user study (N=24) to examine
egocentric and exocentric viewpoints, active and passive navigation, and the
combinations they form. Our results indicated significantly higher preferences
for egocentric+active (higher agency and engagement) and exocentric+passive
(higher focus on content). We also found a marginal significance of viewpoints
on story understanding and a strong significance of navigation on spatial
immersion.",['cs.HC'],False,,,,Atlas of AI Risks: Enhancing Public Understanding of AI Risks,"Ego vs. Exo and Active vs. Passive: Investigating the Effects of
  Viewpoint and Navigation on Spatial Immersion and Understanding in Immersive
  Storytelling"
neg-d2-106,2025-02-12,,2502.08311," Inference in linear panel data models is complicated by the presence of fixed
effects when (some of) the regressors are not strictly exogenous. Under
asymptotics where the number of cross-sectional observations and time periods
grow at the same rate, the within-group estimator is consistent but its limit
distribution features a bias term. In this paper we show that a panel version
of the moving block bootstrap, where blocks of adjacent cross-sections are
resampled with replacement, replicates the limit distribution of the
within-group estimator. Confidence ellipsoids and hypothesis tests based on the
reverse-percentile bootstrap are thus asymptotically valid without the need to
take the presence of bias into account.",['econ.EM'],2502.08311," Inference in linear panel data models is complicated by the presence of fixed
effects when (some of) the regressors are not strictly exogenous. Under
asymptotics where the number of cross-sectional observations and time periods
grow at the same rate, the within-group estimator is consistent but its limit
distribution features a bias term. In this paper we show that a panel version
of the moving block bootstrap, where blocks of adjacent cross-sections are
resampled with replacement, replicates the limit distribution of the
within-group estimator. Confidence ellipsoids and hypothesis tests based on the
reverse-percentile bootstrap are thus asymptotically valid without the need to
take the presence of bias into account.",['econ.EM'],False,,,,"Inference in dynamic models for panel data using the moving block
  bootstrap","Inference in dynamic models for panel data using the moving block
  bootstrap"
neg-d2-107,2025-03-04,,2503.02934," We propose an approach to generative quantum machine learning that overcomes
the fundamental scaling issues of variational quantum circuits. The core idea
is to use a class of generative models based on instantaneous quantum
polynomial circuits, which we show can be trained efficiently on classical
hardware. Although training is classically efficient, sampling from these
circuits is widely believed to be classically hard, and so computational
advantages are possible when sampling from the trained model on quantum
hardware. By combining our approach with a data-dependent parameter
initialisation strategy, we do not encounter issues of barren plateaus and
successfully circumvent the poor scaling of gradient estimation that plagues
traditional approaches to quantum circuit optimisation. We investigate and
evaluate our approach on a number of real and synthetic datasets, training
models with up to one thousand qubits and hundreds of thousands of parameters.
We find that the quantum models can successfully learn from high dimensional
data, and perform surprisingly well compared to simple energy-based classical
generative models trained with a similar amount of hyperparameter optimisation.
Overall, our work demonstrates that a path to scalable quantum generative
machine learning exists and can be investigated today at large scales.",['quant-ph'],2502.11253," Quantum error correction is a cornerstone of reliable quantum computing, with
surface codes emerging as a prominent method for protecting quantum
information. Surface codes are efficient for Clifford gates but require magic
state distillation protocols to process non-Clifford gates, such as T gates,
essential for universal quantum computation. In large-scale quantum
architectures capable of correcting arbitrary circuits, specialized surface
codes for data qubits and distinct codes for magic state distillation are
needed. These architectures can be organized into data blocks and distillation
blocks. The system works by having distillation blocks produce magic states and
data blocks consume them, causing stalls due to either a shortage or excess of
magic states. This bottleneck presents an opportunity to optimize quantum space
by balancing data and distillation blocks. While prior research offers insights
into selecting distillation protocols and estimating qubit requirements, it
lacks a tailored optimization approach. We present a framework for optimizing
large-scale quantum architectures, focusing on data block layouts and magic
state distillation protocols. We evaluate three data block layouts and four
distillation protocols under three optimization strategies: minimizing tiles,
minimizing steps, and achieving a balanced trade-off. Through a comparative
analysis of brute force, dynamic programming, greedy, and random algorithms, we
find that brute force delivers optimal results, while greedy deviates by 7% for
minimizing steps and dynamic programming matches brute force in tile
minimization. We observe that total steps increase with columns, while total
tiles scale with qubits. Finally, we propose a heuristic to help users select
algorithms suited to their objectives, enabling scalable and efficient quantum
architectures.",['quant-ph'],False,,,,"Train on classical, deploy on quantum: scaling generative quantum
  machine learning to a thousand qubits","The Q-Spellbook: Crafting Surface Code Layouts and Magic State Protocols
  for Large-Scale Quantum Computing"
neg-d2-108,2025-01-10,,2501.06179," Classically hard to simulate quantum states, or ""magic states"", are
prerequisites to quantum advantage, highlighting an apparent separation between
classically and quantumly tractable problems. Classically simulable states such
as Clifford circuits on stabilizer states, free bosonic states, free fermions,
and matchgate circuits are all in some sense Gaussian. While free bosons and
fermions arise from quadratic Hamiltonians, recent works have demonstrated that
bosonic and qudit systems converge to Gaussians and stabilizers under
convolution. In this work, we similarly identify convolution for fermions and
find efficient measures of non-Gaussian magic in pure fermionic states. We
demonstrate that three natural notions for the Gaussification of a state, (1)
the Gaussian state with the same covariance matrix, (2) the fixed point of
convolution, and (3) the closest Gaussian in relative entropy, coincide by
proving a central limit theorem for fermionic systems. We then utilize the
violation of Wick's theorem and the matchgate identity to quantify non-Gaussian
magic in addition to a SWAP test.",['quant-ph'],2502.04425," We present a quantum solver for partial differential equations based on a
flexible matrix product operator representation. Utilizing mid-circuit
measurements and a state-dependent norm correction, this scheme overcomes the
restriction of unitary operators. Hence, it allows for the direct
implementation of a broad class of differential equations governing the
dynamics of classical and quantum systems. The capabilities of the framework
are demonstrated for an example system governed by Euler equations with
absorbing boundaries.",['quant-ph'],False,,,,"Measuring Non-Gaussian Magic in Fermions: Convolution, Entropy, and the
  Violation of Wick's Theorem and the Matchgate Identity",Tensor-Programmable Quantum Circuits for Solving Differential Equations
neg-d2-109,2025-01-08,,2501.0464," Advanced stellarators require convoluted modular coils to produce a plasma
with satisfactory performance. Moreover, the number of coils is sometimes high
to decrease the modular ripple created by the coils. For reactor stellarators,
these requirements imply relatively small ports for in-vessel access and
maintenance, i.e. in comparison with tokamaks. The blankets and divertor
modules will have to be replaced periodically (about each 1-4 years depending
on the design) due to neutron damage, and also erosion of divertor targets.
Blanket modules are activated, thus, all the maintenance operations have to be
produced remotely. In order to reduce the shutdown time and cost during
component replacement, and to reduce the number, speed and other specifications
of the remote maintenance equipment, the number of blanket modules in the
reactor should be low and thus, the blanket modules should be large (in
relation to the minor and major radius). Nevertheless, the size of the openings
between coils limits the maximum size of the blanket and divertor modules,
though several potential enhancements have been proposed in the past for
stellarators, like straightening the outboard segments of the coils and the
movement and/or expansion of certain coils to have wider access. The present
work reports on a coil geometry for the 'Helias Stellarator Reactor' (HSR) of
three periods (HSR3) with coils located far from the plasma at the outboard
region of the straight-like sector. This feature creates natural wide openings
at such regions of the coils, which may be utilized to allow access to large
blanket and divertor modules.",['physics.plasm-ph'],2503.15279," The absorption of laser energy by plasma is of paramount importance for
various applications. Collisional and resonant processes are often invoked for
this purpose. However, in some contexts (e.g. in vacuum and the JxB heating),
the energy transfer occurs even when plasma is collisionless, and there is no
resonant process involved. The energy absorption in these cases has been
attributed to the sheath electrostatic fields that get generated as the
electrons are pulled out in the vacuum from the plasma medium. The origin of
irreversibility aiding the absorption, in these cases, remains to be
understood. Particle-In-Cell (PIC) simulations using the OSIRIS 4.0 platform
have been carried out. The nearby trajectories of lighter electron species
involved in the interaction with the laser show exponential separation. This is
confirmed by the positive Lyapunov index and also by other characterizations.
The observations in these cases are contrasted with the electron cyclotron
resonant case, which shows negligible chaos in the electron trajectories
despite the energy absorption percentage being high.",['physics.plasm-ph'],False,,,,"Coil geometry with large openings for a HSR3-like stellarator reactor
  for fast replacement of in-vessel components",Chaos aided regime of Laser/Electromagnetic Energy Absorption by plasma
neg-d2-110,2025-01-15,,2501.08915," In this work we investigate, through a Bayesian study, the ability of a local
low matter density $\Omega_{\rm M}$, in discrepancy with the value usually
inferred from the CMB angular power spectrum, to accommodate observations from
local probes without being in tension with the local values of the Hubble
constant $H_0$ or the matter fluctuation $\sigma_8$ parameters. For that, we
combine multiple local probes, with the criteria that they either can constrain
the matter density parameter independently from the CMB constraints, or can
help in doing so after making their relevant observations more model
independent by relaxing their relevant calibration parameters. We assume
however, either a dynamical dark energy model, or the standard $\Lambda$CDM
model, when computing the corresponding theoretical observables. We also add,
in almost all of our Monte Carlo runs, the latest Baryonic acoustic
oscillations (BAO) measurements from the DESI year one release to our core
group. We found that, within $\Lambda$CDM model, for different combinations of
our probes, we can accommodate a low matter density along with the $H_0$ and
$\sigma_8$ values usually obtained from local probes, providing we promote the
sound drag $r_s$ component in BAO calculations to a free parameter, and that
even if we combine with the Pantheon+ Supernova sample. Assuming $w_0w_a$CDM,
we also found that relaxing $r_s$ allow us to accommodate $\Omega_{\rm M}$,
$H_0$ and $\sigma_8$ within their local values, with still however a preference
for $w_0w_a$ values far from $\Lambda$CDM. However, when including Pantheon+
Supernova sample, we found that the latter preference for high matter density
pushes $\sigma_8$ to much smaller values, mitigating by then a low matter
density solution to the two common tensions. We conclude that a low matter
density value, helps in preserving the concordance within $\Lambda$CDM model.
(abridged)",['astro-ph.CO'],2502.01751," We consider unified dark sector models in which the fluid can collapse and
cluster into halos, allowing for hierarchical structure formation to proceed as
in standard cosmology. We show that both background evolution and linear
perturbations tend towards those in $\LCDM$ as the clustered fraction $f
\rightarrow 1$. We confront such models with various observational datasets,
with emphasis on the relatively well motivated standard Chaplygin gas. We show
that the strongest constraints come from secondary anisotropies in the CMB
spectrum, which prefer models with $f \rightarrow 1$. However, as a larger
Hubble constant is allowed for smaller $f$, values of $f \simeq 0.99$ (rather
than tending to exact unity) are favored when late universe expansion data is
included, with $f \simeq 0.97$ and $H_0 \simeq 70 {\rm km/s/Mpc}$ allowed at
the 2-$\sigma$ level. Such values of $f$ imply extremely efficient clustering
into nonlinear structures. They may nevertheless be compatible with clustered
fractions in warm dark matter based cosmologies, which have similar minimal
halo mass scales as the models considered here. Tight CMB constraints on $f$
also apply to the generalized Chaplygin gas, except for models that are already
quite close to $\LCDM$, in which case all values of $0 \le f \le 1$ are
allowed. In contrast to the CMB, large scale structure data, which were
initially used to rule out unclustered unified dark matter models, are far less
constraining. Indeed, late universe data, including the large scale galaxy
distribution, prefer models that are far from $\LCDM$. But these are in tension
with the CMB data.",['astro-ph.CO'],False,,,,"The case for a low dark matter density in dynamical dark energy model
  from local probes","Clustered unified dark sector cosmology: Background evolution and linear
  perturbations in light of observations"
neg-d2-111,2025-03-17,,2503.12953," Text-video prediction (TVP) is a downstream video generation task that
requires a model to produce subsequent video frames given a series of initial
video frames and text describing the required motion. In practice TVP methods
focus on a particular category of videos depicting manipulations of objects
carried out by human beings or robot arms. Previous methods adapt models
pre-trained on text-to-image tasks, and thus tend to generate video that lacks
the required continuity. A natural progression would be to leverage more recent
pre-trained text-to-video (T2V) models. This approach is rendered more
challenging by the fact that the most common fine-tuning technique, low-rank
adaptation (LoRA), yields undesirable results. In this work, we propose an
adaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).
Within the module, we devise a sub-module that produces frame-wise text
embeddings from the input text, which acts as an additional text condition to
aid generation. We use FCA to fine-tune the T2V model, which incorporates the
initial frame(s) as an extra condition. We compare and discuss the more
effective strategy for injecting such embeddings into the T2V model. We conduct
extensive ablation studies on our design choices with quantitative and
qualitative performance analysis. Our approach establishes a new
state-of-the-art for the task of TVP. The project page is at
https://github.com/Cuberick-Orion/FCA .",['cs.CV'],2503.0074," In this paper, we present FaceShot, a novel training-free portrait animation
framework designed to bring any character into life from any driven video
without fine-tuning or retraining. We achieve this by offering precise and
robust reposed landmark sequences from an appearance-guided landmark matching
module and a coordinate-based landmark retargeting module. Together, these
components harness the robust semantic correspondences of latent diffusion
models to produce facial motion sequence across a wide range of character
types. After that, we input the landmark sequences into a pre-trained
landmark-driven animation model to generate animated video. With this powerful
generalization capability, FaceShot can significantly extend the application of
portrait animation by breaking the limitation of realistic portrait landmark
detection for any stylized character and driven video. Also, FaceShot is
compatible with any landmark-driven animation model, significantly improving
overall performance. Extensive experiments on our newly constructed character
benchmark CharacBench confirm that FaceShot consistently surpasses
state-of-the-art (SOTA) approaches across any character domain. More results
are available at our project website https://faceshot2024.github.io/faceshot/.",['cs.CV'],False,,,,"Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in
  Text-to-Video Prediction",FaceShot: Bring Any Character into Life
neg-d2-112,2025-01-17,,2501.1028," The hierarchical interplay among gravity, magnetic fields, and turbulent gas
flows in delivering the necessary material to form massive protostellar
clusters remains enigmatic. We have performed high-resolution (beam size
$\sim$14 arcsec $\simeq$ 0.05 pc at a distance 725 pc) 850 $\mu$m dust
polarization and C$^{18}$O molecular line observations of Cepheus A (Cep A),
the second closest massive star-forming region, using the 15-meter James Clerk
Maxwell Telescope (JCMT) along with the SCUBA-2/POL-2 and HARP instruments. Our
key analyses reveal that (i) morphologically, all three fields--gravitational
(G), magnetic (B), and kinetic (K) fields--are aligned with each other, and
(ii) energetically, they exhibit a hierarchical relationship with gravitational
($E_{\mathrm{G}}$) > magnetic ($E_{\mathrm{B}}$) > kinetic ($E_{\mathrm{K}}$).
Gravity dominates in Cep A clump and, as a primary active player, dictates the
other two agents. Consequently, gravity plays two active roles: (i) induces gas
flows and (ii) drags B-field lines toward the gravitational trough. Since
magnetic energy dominates kinetic energy, $E_{\mathrm{B}}$ > $E_{\mathrm{K}}$,
the ""dragged-in"" B-field as a secondary active player can mitigate turbulence
and instabilities, thereby regularizing gas flows into a more ordered
configuration. At the $\sim$0.60 pc clump scale, these flows deliver material
at a substantially high rate of $\sim$ 2.1$\times$10$^{-4}$ M$_{\odot}$
yr$^{-1}$ toward the cluster center. Our study, for the first time, presents
new insights into how B-fields and turbulent gas flows passively assist the
active role of gravity in the formation of a protostellar cluster, contrasting
with the standard notion that these agents primarily oppose gravitational
collapse.",['astro-ph.GA'],2502.08712," We present an analysis of deep $\textit{JWST}$/NIRSpec spectra of
star-forming galaxies at $z\simeq1.4-10$, observed as part of the AURORA
survey. We infer median low-ionization electron densities of
$268_{-49}^{+45}~\rm cm^{-3}$, $350_{-76}^{+140}~\rm cm^{-3}$, and
$480_{-310}^{+390}~\rm cm^{-3}$ at redshifts z$=2.3$, $z=3.2$, and $z=5.3$,
respectively, revealing an evolutionary trend following $(1+z)^{1.5\pm0.6}$. We
identify weak positive correlations between electron density and star formation
rate (SFR) as well as SFR surface density, but no significant trends with
stellar mass or specific SFR. Correlations with rest-optical emission line
ratios show densities increasing with $\rm [NeIII]\lambda3869/[OII]\lambda3727$
and, potentially, $\rm [OIII]\lambda5007/[OII]\lambda3727$, although variations
in dust attenuation complicate the latter. Additionally, electron density is
more strongly correlated with distance from the local BPT sequence than can be
explained by simple photoionization models. We further derive electron
densities from the CIII] doublet probing higher-ionization gas, and find a
median value of $1.4_{-0.5}^{+0.7}\times10^4~\rm cm^{-3}$, $\sim30$ times
higher than densities inferred from [SII]. This comparison suggests a
consistent HII region structure across cosmic time with dense, high-ionization
interiors surrounded by less dense, low-ionization gas. We compare measurements
of AURORA galaxies to predictions from the SPHINX galaxy formations,
highlighting the interplay between residual molecular cloud pressure in young
galaxies and feedback from stellar winds and supernovae as galaxies mature.",['astro-ph.GA'],False,,,,"Evidence for the gravity-driven and magnetically-regularized gas flows
  feeding the massive protostellar cluster in Cep A","The AURORA Survey: The Evolution of Multi-phase Electron Densities at
  High Redshift"
neg-d2-113,2025-03-10,,2503.07798," Understanding the risk and protective factors associated with Parkinsons
disease (PD) is crucial for improving outcomes for patients, individuals at
risk, healthcare providers, and healthcare systems. Studying these factors not
only enhances our knowledge of the disease but also aids in developing
effective prevention, management, and treatment strategies. This paper reviews
the key risk and protective factors associated with PD, with a particular focus
on the biological mechanisms underlying these factors. Risk factors include
genetic mutations, racial predispositions, and environmental exposures, all of
which contribute to an increased likelihood of developing PD or accelerating
disease progression. Conversely, protective factors such as regular physical
exercise, adherence to a Mediterranean diet, and higher urate levels have
demonstrated potential to reduce inflammation and support mitochondrial
function, thereby mitigating disease risk. However, identifying and validating
these factors presents significant challenges. To overcome challenges, we
propose several solutions and recommendations. Future research should
prioritize the development of standardized biomarkers for early diagnosis,
investigate gene-environment interactions in greater depth, and refine animal
models to better mimic human PD pathology. Additionally, we offer actionable
recommendations for PD prevention and management, tailored to healthy
individuals, patients diagnosed with PD, and healthcare systems. These
strategies aim to improve clinical outcomes, enhance quality of life, and
optimize healthcare delivery for PD.",['q-bio.NC'],2503.07798," Understanding the risk and protective factors associated with Parkinsons
disease (PD) is crucial for improving outcomes for patients, individuals at
risk, healthcare providers, and healthcare systems. Studying these factors not
only enhances our knowledge of the disease but also aids in developing
effective prevention, management, and treatment strategies. This paper reviews
the key risk and protective factors associated with PD, with a particular focus
on the biological mechanisms underlying these factors. Risk factors include
genetic mutations, racial predispositions, and environmental exposures, all of
which contribute to an increased likelihood of developing PD or accelerating
disease progression. Conversely, protective factors such as regular physical
exercise, adherence to a Mediterranean diet, and higher urate levels have
demonstrated potential to reduce inflammation and support mitochondrial
function, thereby mitigating disease risk. However, identifying and validating
these factors presents significant challenges. To overcome challenges, we
propose several solutions and recommendations. Future research should
prioritize the development of standardized biomarkers for early diagnosis,
investigate gene-environment interactions in greater depth, and refine animal
models to better mimic human PD pathology. Additionally, we offer actionable
recommendations for PD prevention and management, tailored to healthy
individuals, patients diagnosed with PD, and healthcare systems. These
strategies aim to improve clinical outcomes, enhance quality of life, and
optimize healthcare delivery for PD.",['q-bio.NC'],False,,,,Risk and Protective Factors in Parkinsons Disease,Risk and Protective Factors in Parkinsons Disease
neg-d2-114,2025-01-14,,2501.08262," Large language models (LLMs) have demonstrated significant capabilities, but
their widespread deployment and more advanced applications raise critical
sustainability challenges, particularly in inference energy consumption. We
propose the concept of the Sustainable AI Trilemma, highlighting the tensions
between AI capability, digital equity, and environmental sustainability.
Through a systematic case study of LLM agents and retrieval-augmented
generation (RAG), we analyze the energy costs embedded in memory module designs
and introduce novel metrics to quantify the trade-offs between energy
consumption and system performance. Our experimental results reveal significant
energy inefficiencies in current memory-augmented frameworks and demonstrate
that resource-constrained environments face disproportionate efficiency
penalties. Our findings challenge the prevailing LLM-centric paradigm in agent
design and provide practical insights for developing more sustainable AI
systems.",['cs.CY'],2501.12642," This report investigates Training Data Attribution (TDA) and its potential
importance to and tractability for reducing extreme risks from AI. First, we
discuss the plausibility and amount of effort it would take to bring existing
TDA research efforts from their current state, to an efficient and accurate
tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss
the numerous research benefits AI labs will expect to see from using such TDA
tooling. Then, we discuss a key outstanding bottleneck that would limit such
TDA tooling from being accessible publicly: AI labs' willingness to disclose
their training data. We suggest ways AI labs may work around these limitations,
and discuss the willingness of governments to mandate such access. Assuming
that AI labs willingly provide access to TDA inference, we then discuss what
high-level societal benefits you might see. We list and discuss a series of
policies and systems that may be enabled by TDA. Finally, we present an
evaluation of TDA's potential impact on mitigating large-scale risks from AI
systems.",['cs.CY'],False,,,,"Addressing the sustainable AI trilemma: a case study on LLM agents and
  RAG",Training Data Attribution (TDA): Examining Its Adoption & Use Cases
neg-d2-115,2025-03-10,,2503.06908," This paper concentrates on positive definite functions on finite abelian
groups, which are central to harmonic analysis and related fields. By
leveraging the group structure and employing Fourier analysis, we establish a
lower bound for the second largest value of positive definite functions. For
illustrative purposes, we present three applications of our lower bound: (a) We
obtain both lower and upper bounds for arbitrary functions on finite abelian
groups; (b) We derive lower bounds for the relaxation and mixing times of
random walks on finite abelian groups. Notably, our bound for the relaxation
time achieves a quadratic improvement over the previously known one; (c) We
determine a new lower bound for the size of the sumset of two subsets of finite
abelian groups.",['math.FA'],2501.12707, We construct a weak Hilbert space that is a twisted Hilbert space.,['math.FA'],False,,,,"An estimate for positive definite functions on finite abelian groups and
  its applications",A weak Hilbert space that is a twisted HIlbert space
neg-d2-116,2025-02-09,,2502.05875," The extended weak order on a Coxeter group $W$ is the poset of biclosed sets
in its root system. In (Barkley-Speyer 2024), it was shown that when
$W=\widetilde{S}_n$ is the affine symmetric group, then the extended weak order
is a quotient of the lattice $L_n$ of translation-invariant total orderings of
the integers. In this article, we give a combinatorial introduction to $L_n$
and the extended weak order on $\widetilde{S}_n$. We show that $L_n$ is an
algebraic completely semidistributive lattice. We describe its canonical join
representations using a cyclic version of Reading's non-crossing arc diagrams.
We also show analogous statements for the lattice of all total orders of the
integers, which is the extended weak order on the symmetric group $S_\infty$. A
key property of both of these lattices is that they are profinite; we also
prove that a profinite lattice is join semidistributive if and only if its
compact elements have canonical join representations. We conjecture that the
extended weak order of any Coxeter group is a profinite semidistributive
lattice.",['math.CO'],2503.14022," As the scale of data centers continues to grow, there is an increasing demand
for interconnection networks to resist malicious attacks. Hence, it is
necessary to evaluate the reliability of networks under various fault patterns.
The family of generalized $K_4$-hypercubes serve as interconnection networks of
data centers, characterized by topological structures with exceptional
properties. The $h$-extra edge-connectivity $\lambda_h$, the $l$-super
edge-connectivity $\lambda^l$, the $l$-average degree edge-connectivity
$\overline{\lambda^l}$, the $l$-embedded edge-connectivity $\eta_l$ and the
cyclic edge-connectivity $\lambda_c$ are vital parameters to accurately assess
the reliability of interconnection networks. Let integer $n\geq3$. This paper
obtains the optimal solution of the edge isoperimetric problem and its explicit
representation, which offers an upper bound of the $h$-extra edge-connectivity
of an $n$-dimensional $K_4$-hypercube $H_n^4$. As an application, we presents
$\lambda_h(H_n^4)$ for $1\leq h\leq 2^{\lceil n/2 \rceil }$. Moreover, for
$2^{\lceil n/2\rceil+t}-g_t \le h\le2^{\lceil n/2\rceil+t}$,
$g_t=\lceil(2^{2t+2+\gamma})/3\rceil$,
  $0\leq t \leq\lfloor n/2\rfloor-1 $, $\gamma=0$ for even $n$ and $\gamma=1$
for odd $n$, $\lambda_h(H_n^4)$ is a constant $(\lfloor n/2\rfloor-t)2^{\lceil
n/2\rceil+t}$. The above lower and upper bounds of the integer $h$ are both
sharp. Furthermore, $\lambda^l(H_n^4)$, $\overline{\lambda^l}(H_n^4)$,
$\lambda_{2^l}(H_n^4)$, and $\eta_l(H_n^4)$ share a common value $(n-l)2^l$ for
$2\leq l\leq n-1$, and we determines the values of $\lambda_c(H_n^4)$.",['math.CO'],False,,,,Extended weak order for the affine symmetric group,"Reliability Evaluation of Generalized $K_4$-Hypercubes Based on Five
  Link Fault Patterns"
neg-d2-117,2025-02-26,,2502.19713," Small-angle neutron scattering (SANS) is a powerful technique for probing the
nanoscale structure of materials. However, the fundamental limitations of
neutron flux pose significant challenges for rapid, high-fidelity data
acquisition required in many experiments. To circumvent this difficulty, we
introduce a Bayesian statistical framework based on Gaussian process regression
(GPR) to infer high-quality SANS intensity profiles from measurements with
suboptimal signal-to-noise ratios (SNR). Unlike machine learning approaches
that depend on extensive training datasets, the proposed one-shot method
leverages the intrinsic mathematical properties of the scattering function,
smoothness and continuity, offering a generalizable solution beyond the
constraints of data-intensive techniques. By examining existing SANS
experimental data, we demonstrate that this approach can reduce measurement
time by between one and two orders of magnitude while maintaining accuracy and
adaptability across different SANS instruments. By improving both efficiency
and reliability, this method extends the capabilities of SANS, enabling broader
applications in time-sensitive and low-flux experimental conditions.",['physics.app-ph'],2501.09812," The Benguela Upwelling System (BUS), off the south-western African coast, is
one of the four major eastern boundary upwelling ecosystems in the oceans.
However, despite its very interesting characteristics, this area has been
almost overlooked in the field of environmental radioactivity. In this work, it
has been carried out for the first time the combined study of $^{236}$U and
$^{237}$Np in the coast of Namibia within the northern BUS. Surface seawater
exhibited similar $^{236}$U and $^{237}$Np concentrations, ranging from
$3.9\times10^6$ to $5.6\times10^6$ atoms/kg and from $4.6\times10^6$ to
$8.5\times10^6$ atoms/kg, respectively. The observed inventories in a water
column from the continental margin, of $(2.10 \pm 0.11)\times10^12$ atoms
m$^{-2}$ for $^{236}$U and $(3.48 \pm 0.13)\times10^{12}$ atoms/m$^{-2}$ for
$^{237}$Np, were in agreement with the global fallout (GF) source term in the
Southern Hemisphere, which was recognized as the main source of actinides to
the region. A pattern was observed in the surface samples, with $^{237}$Np
concentrations that decreased by 25-30% when moving from inshore to offshore
stations, but such an effect could not be clearly discerned in the case of
$^{236}$U within the data uncertainties. An explanation based on the larger
particle reactivity of GF $^{237}$Np compared to GF $^{236}$U was proposed.
Such an effect would have been important at the studied site due to the enhance
presence of particles in the continental shelf triggered by the upwelling
phenomenon. A value of $1.77 \pm 0.20$ was obtained for the
$^{237}$Np/$^{236}$U atom ratio for the GF source term in the marine
environment.",['physics.app-ph'],False,,,,"Unlocking Hidden Information in Sparse Small-Angle Neutron Scattering
  Measurement","Presence of $^{236}$U and $^{237}$Np in a marine ecosystem: the northern
  Benguela Upwelling System, a case study"
neg-d2-118,2025-01-21,,2501.12101," We consider viscosity solution to one-phase free boundary problems for
general fully nonlinear operators and free boundary condition depending on the
normal vector. We show existence of viscosity solutions via the Perron's method
and we prove $C^{2,\alpha}$ regularity of flat free boundaries via a quadratic
improvement of flatness. Finally, we obtain the higher regularity of the free
boundary via an hodograph transform.",['math.AP'],2501.06544," In the mean-field regime, a gas of quantum particles with Boltzmann
statistics can be described by the Hartree-Fock equation. This dynamics becomes
trivial if the initial distribution of particle is invariant by translation.
However, the first correction is given on time of order $O(N)$ by the quantum
Lenard--Balescu equation. In the first part of the present article, we justify
this equation until time of order $O((\log N)^{1-\delta})$ (for any
$\delta\in(0,1)$).
  A similar phenomenon exists in the classical setting (with a similar validity
time obtained by Duerinckx \cite{Duerinckx}). In a second time, we prove the
convergence for dimension $d\geq 2$ of the solutions of the quantum
Lenard--Balescu equation to the solutions of its classical counterpart in the
semi-classical limit. This problem can be interpreted as a grazing collision
limit: the quantum Lenard--Balescu equation looks like a cut-off Boltzmann
equation, when the classical one looks like the Landau equation.",['math.AP'],False,,,,"Existence and regularity in the fully nonlinear one-phase free boundary
  problem",Around the Quantum Lenard-Balescu equation
neg-d2-119,2025-03-03,,2503.01778," This work presents a novel formulation for a redefinition of the second based
on the weighted arithmetic mean of multiple normalized frequencies. We
demonstrate that it is mathematically equivalent to the previously discussed
implementation employing a geometric mean. In our reformulation, the
normalization of frequencies provides the defining constants with immediate
physical meaning, while maintaining the decoupling of assigned weights from the
frequencies of the reference transitions. We believe that a definition based on
this formulation would be significantly more accessible to both experts and
non-specialists, enhancing understanding and facilitating broader acceptance.
We hope that this approach will help overcome barriers to the adoption of a
redefinition that effectively values all state-of-the-art atomic clocks.",['physics.atom-ph'],2501.13537," Precision determination of the hyperfine splitting of cadmium ions is
essential to study space-time variation of fundamental physical constants and
isotope shifts. In this work, we present the precision frequency measurement of
the excited-state $^2{P}_{3/2}$ hyperfine splitting of
$^{111,113}\mathrm{Cd}^+$ ions using the laser-induced fluorescence technique.
By introducing the technology of sympathetic cooling and setting up free-space
beat detection unit based on the optical comb, the uncertainties are improved
to 14.8 kHz and 10.0 kHz, respectively, two orders of magnitude higher than the
reported results from the linear transformation of isotope shifts. The magnetic
dipole constants $A_{P_{3/2}}$ of $^{111}\mathrm{Cd}^+$ and
$^{113}\mathrm{Cd}^+$ are estimated to be 395 938.8(7.4) kHz and 411 276.0(5.0)
kHz, respectively. The difference between the measured and theoretical
hyperfine structure constants indicates that more physical effects are required
to be considered in the theoretical calculation, and provides critical data for
the examination of deviation from King-plot linearity in isotope shifts.",['physics.atom-ph'],False,,,,"An Accessible Formulation for Defining the SI Second Based on Multiple
  Atomic Transitions","Precision determination of the excited-state hyperfine splitting of
  Cadmium ions"
neg-d2-120,2025-02-03,,2502.01823," We consider a system of two indistinguishable fermions (with four accessible
states each) that suffers decoherence without dissipation due to its coupling
with a global bosonic bath at a fixed temperature. Using an appropriate measure
of fermionic entanglement, we identify families of two-fermion states whose
entanglement persists throughout the evolution, either fully or partially,
despite the noisy effects of the interaction with the bath, and independently
of its temperature. The identified resilience to decoherence provides valuable
insights into the entanglement dynamics of open systems of indistinguishable
fermions, and into the conditions under which long-lived entanglement emerges
under more general decoherence channels.",['quant-ph'],2503.02934," We propose an approach to generative quantum machine learning that overcomes
the fundamental scaling issues of variational quantum circuits. The core idea
is to use a class of generative models based on instantaneous quantum
polynomial circuits, which we show can be trained efficiently on classical
hardware. Although training is classically efficient, sampling from these
circuits is widely believed to be classically hard, and so computational
advantages are possible when sampling from the trained model on quantum
hardware. By combining our approach with a data-dependent parameter
initialisation strategy, we do not encounter issues of barren plateaus and
successfully circumvent the poor scaling of gradient estimation that plagues
traditional approaches to quantum circuit optimisation. We investigate and
evaluate our approach on a number of real and synthetic datasets, training
models with up to one thousand qubits and hundreds of thousands of parameters.
We find that the quantum models can successfully learn from high dimensional
data, and perform surprisingly well compared to simple energy-based classical
generative models trained with a similar amount of hyperparameter optimisation.
Overall, our work demonstrates that a path to scalable quantum generative
machine learning exists and can be investigated today at large scales.",['quant-ph'],False,,,,Persistent fermionic entanglement under decoherence,"Train on classical, deploy on quantum: scaling generative quantum
  machine learning to a thousand qubits"
neg-d2-121,2025-03-13,,2503.11053," This paper develops general approaches for pricing various types of
American-style Parisian options (down-in/-out, perpetual/finite-maturity) with
general payoff functions based on continuous-time Markov chain (CTMC)
approximation under general 1D time-inhomogeneous Markov models. For the
down-in types, by conditioning on the Parisian stopping time, we reduce the
pricing problem to that of a series of vanilla American options with different
maturities and their prices integrated with the distribution function of the
Parisian stopping time yield the American Parisian down-in option price. This
facilitates an efficient application of CTMC approximation to obtain the
approximate option price by calculating the required quantities. For the
perpetual down-in cases under time-homogeneous models, significant
computational cost can be reduced. The down-out cases are more complicated, for
which we use the state augmentation approach to record the excursion duration
and then the approximate option price is obtained by solving a series of
variational inequalities recursively with the Lemke's pivoting method. We show
the convergence of CTMC approximation for all the types of American Parisian
options under general time-inhomogeneous Markov models, and the accuracy and
efficiency of our algorithms are confirmed with extensive numerical
experiments.",['q-fin.CP'],2501.11164," Recorded option pricing datasets are not always freely available.
Additionally, these datasets often contain numerous prices which are either
higher or lower than can reasonably be expected. Various reasons for these
unexpected observations are possible, including human error in the recording of
the details associated with the option in question. In order for the analyses
performed on these datasets to be reliable, it is necessary to identify and
remove these options from the dataset. In this paper, we list three distinct
problems often found in recorded option price datasets alongside means of
addressing these. The methods used are justified using sound statistical
reasoning and remove option prices violating the standard assumption of no
arbitrage. An attractive aspect of the proposed technique is that no option
pricing model-based assumptions are used. Although the discussion is restricted
to European options, the procedure is easily modified for use with exotic
options as well. As a final contribution, the paper contains a link to six
option pricing datasets which have already been cleaned using the proposed
methods and can be freely used by researchers.",['q-fin.CP'],False,,,,"Pricing American Parisian Options under General Time-Inhomogeneous
  Markov Models",A statistical technique for cleaning option price data
neg-d2-122,2025-01-26,,2501.15647," In this paper, we investigate the question of how one can recover the
homology of a simplicial complex $X$ equipped with a regular action of a finite
group $G$ from the structure of its quotient space $X/G.$ Specifically, we
describe a process for enriching the structure of the chain complex
$C_\ast(X/G; \mathbb{F})$ using the data of a complex of groups, a framework
developed by Bridson and Corsen for encoding the local structure of a group
action. We interpret this data through the lens of matrix representations of
the acting group, and combine this structure with the standard simplicial
boundary matrices for $X/G$ to construct a surrogate chain complex. In the case
$G = \mathbb{Z}_k,$ the group ring $\mathbb{F}G$ is commutative and matrices
over $\mathbb{F}G$ admit a Smith normal form, allowing us to recover the
homology of $G$ from this surrogate complex. This algebraic approach
complements the geometric compression algorithm for equivariant simplicial
complexes described by Carbone, Nanda, and Naqvi.",['math.AT'],2502.11503," For an elliptic CW-complex $X$, we denote its group of self-homotopy
equivalences as $\E(X)$, and its subgroup consisting of elements inducing the
identity on the homology groups as $\E_{*}(X)$. This paper aims to investigate
how the homology groups of $X$ influence the finiteness of $\E(X)$ and
$\E_{*}(X)$.",['math.AT'],False,,,,Computing homology of $\mathbb{Z}_k$-complexes from their quotients,"Impact of the Homology Groups on the Finiteness of the group of
  Self-Homotopy Equivalences of an Elliptic Space"
neg-d2-123,2025-01-06,,2501.03355," In this work we introduce a criterion for testing general covariance in
effective quantum gravity theories. It adapts the criterion based on general
spacetime diffeomorphisms of the Einstein-Hilbert action to the case of
effective canonical models. While the main purpose is to test models obtained
in Loop Quantum Gravity, the criterion is not limited to those physical systems
and may be applied to any canonically formulated modified theory of gravity.
The approach here is hence not that of finding an effective model, but rather
to analyze a given one represented by a quantum corrected Hamiltonian.
Specifically, we will apply the criterion to spherically symmetric spacetimes
in the vacuum with inverse triad and holonomy modifications that arise as a
consequence of the loop quantization procedure. It is found that, in addition
to the initial modifications of the Hamiltonian, quantum corrections of the
classical metric itself are needed as well in order to obtain generally
covariant models. A comparison with a recent alternative criterion is included
in the discussion.",['gr-qc'],2501.03125," We revise the role that ultraviolet divergences of quantum fields play in
slow-roll inflation, and discuss the renormalization of cosmological
observables from a space-time perspective, namely the angular power spectrum.
We first derive an explicit expression for the multipole coefficients
$C_{\ell}$ in the Sachs-Wolfe regime in terms of the two-point function of
primordial perturbations. We then analyze the ultraviolet behavior, and point
out that the standard result in the literature is equivalent to a
renormalization of $C_{\ell}$ at zero ``adiabatic'' order. We further argue
that renormalization at second ``adiabatic'' order may be more appropriate from
the viewpoint of standard quantum field theory. This may change significantly
the predictions for $C_{\ell}$, while maintaining scale invariance.",['gr-qc'],False,,,,"Testing general covariance in effective models motivated by Loop Quantum
  Gravity","On the renormalization of ultraviolet divergences in the inflationary
  angular power spectrum"
neg-d2-124,2025-01-22,,2501.12753," In previous studies, the propagation of extreme events across nodes in
monolayer networks has been extensively studied. In this work, we extend this
investigation to explore the propagation of extreme events between two distinct
layers in a multiplex network. We consider a two-layer network, where one layer
is globally coupled and exhibits extreme events, while the second layer remains
uncoupled. The interlayer connections between the layers are either
unidirectional or bidirectional. We find that unidirectional coupling between
the layers can induce extreme events in the uncoupled layer, whereas
bidirectional coupling tends to mitigate extreme events in the globally coupled
layer. To characterize extreme and non-extreme states, we use probability plots
to identify distinct regions in the parameter space. Additionally, we study the
robustness of extreme events emergence by examining various network topologies
in the uncoupled layer. The mechanism behind the occurrence of extreme events
is explored, with a particular focus on the transition from asynchronous states
to a fully synchronized excitable state. For numerical simulations, we use
nonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical
behavior of both coupled and uncoupled layers. Our findings suggest that
extreme events in the uncoupled layer emerge through the gradual disappearance
of disorder, accompanied by occasional bursts of synchronized activity. Results
obtained in this work will serve a starting point in understanding the dynamics
behind the propagation of extreme events in real-world networks.",['nlin.CD'],2502.08618," The Fitzhugh-Nagumo neuronal model is used to explore the influence of the
electric field on thermosensitive neurons' dynamics. This study investigates
how the electric field affects polarization modulation in cell media induced by
changes in ion charge density by adding electrical field as a new variable.
Driven by a voltage source acting as an external stimulus current, different
firing mode responses of the proposed model are analyzed when an external
electrical field is applied. Through computational analysis, the study
evaluates the impact of parameters such as cell radius, stimulus voltage source
amplitude, frequency, and as well as the presence of an external electric
field. The results demonstrate distinct mode transitions of isolated neurons
ranging from spiking to bursting, regular and chaotic oscillations. These
findings suggest that the firing mode is triggered by periodic external
electric fields and cell radius, with the electric field's involvement enhanced
to regulate neuron activity and control the dynamics. External electric fields
and stimuli play a crucial role in neuronal firing dynamics, affecting the
transition between different firing modes. Understanding these effects
contributes to the comprehension of neural processes and the potential
manipulation of neural activity for various applications in neuroscience and
biophysics.",['nlin.CD'],False,,,,Propagation of extreme events in multiplex neuronal networks,"Modulation of Neuronal Firing Modes by Electric Fields in a
  Thermosensitive FitzHugh-Nagumo Model"
neg-d2-125,2025-02-16,,2502.14886," Recent advancements in machine learning (ML) and deep learning (DL),
particularly through the introduction of foundational models (FMs), have
significantly enhanced surgical scene understanding within minimally invasive
surgery (MIS). This paper surveys the integration of state-of-the-art ML and DL
technologies, including Convolutional Neural Networks (CNNs), Vision
Transformers (ViTs), and foundational models like the Segment Anything Model
(SAM), into surgical workflows. These technologies improve segmentation
accuracy, instrument tracking, and phase recognition in surgical endoscopic
video analysis. The paper explores the challenges these technologies face, such
as data variability and computational demands, and discusses ethical
considerations and integration hurdles in clinical settings. Highlighting the
roles of FMs, we bridge the technological capabilities with clinical needs and
outline future research directions to enhance the adaptability, efficiency, and
ethical alignment of AI applications in surgery. Our findings suggest that
substantial progress has been made; however, more focused efforts are required
to achieve seamless integration of these technologies into clinical workflows,
ensuring they complement surgical practice by enhancing precision, reducing
risks, and optimizing patient outcomes.",['cs.CV'],2502.0376," Multi-object tracking (MOT) in UAV-based video is challenging due to
variations in viewpoint, low resolution, and the presence of small objects.
While other research on MOT dedicated to aerial videos primarily focuses on the
academic aspect by developing sophisticated algorithms, there is a lack of
attention to the practical aspect of these systems. In this paper, we propose a
novel real-time MOT framework that integrates Apache Kafka and Apache Spark for
efficient and fault-tolerant video stream processing, along with
state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for
accurate object detection and tracking. Our work highlights the importance of
not only the advanced algorithms but also the integration of these methods with
scalable and distributed systems. By leveraging these technologies, our system
achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set
while maintaining a real-time processing speed of 28 FPS on a single GPU. Our
work demonstrates the potential of big data technologies and deep learning for
addressing the challenges of MOT in UAV applications.",['cs.CV'],False,,,,"Surgical Scene Understanding in the Era of Foundation AI Models: A
  Comprehensive Review","RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on
  Deep Learning and Big Data Technology"
neg-d2-126,2025-01-10,,2501.0582," Modular Arrays (MAs) are a promising architecture to enable multi-user
communications in next-generation multiple-input multiple-output (MIMO) systems
based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an
improved spatial resolution with characteristic interference patterns
associated to grating lobes. In this work, we analyze whether MAs can
outperform conventional collocated deployments, in terms of achievable sum-rate
and served users in a multi-user downlink set-up. First, we provide a rigorous
analytical characterization of the inter-user interference for modular gMIMO
systems operating in the near field. Then, we leverage these results to
optimize the user selection and precoding mechanisms, designing two algorithms
that largely outperform existing alternatives in the literature, with different
algorithmic complexities. Results show that the proposed algorithms yield over
70% improvements in achievable sum-spectral efficiencies compared to the state
of the art. We also illustrate how MAs allow to serve a larger number of users
thanks to their improved spatial resolution, compared to the collocated
counterpart.",['eess.SP'],2502.02512," Recently, there has been an increasing interest in 6G technology for
integrated sensing and communications, where positioning stands out as a key
application. In the realm of 6G, cell-free massive multiple-input
multiple-output (MIMO) systems, featuring distributed base stations equipped
with a large number of antennas, present an abundant source of angle-of-arrival
(AOA) information that could be exploited for positioning applications. In this
paper we leverage this AOA information at the base stations using the multiple
signal classification (MUSIC) algorithm, in conjunction with received signal
strength (RSS) for positioning through Gaussian process regression (GPR). An
AOA fingerprint database is constructed by capturing the angle data from
multiple locations across the network area and is combined with RSS data from
the same locations to form a hybrid fingerprint which is then used to train a
GPR model employing a squared exponential kernel. The trained regression model
is subsequently utilized to estimate the location of a user equipment.
Simulations demonstrate that the GPR model with hybrid input achieves better
positioning accuracy than traditional GPR models utilizing RSS-only and
AOA-only inputs.",['eess.SP'],False,,,,User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays,Hybrid Fingerprint-based Positioning in Cell-Free Massive MIMO Systems
neg-d2-127,2025-01-16,,2501.09534," In this paper, we elaborate on how AI can support diversity and inclusion and
exemplify research projects conducted in that direction. We start by looking at
the challenges and progress in making large language models (LLMs) more
transparent, inclusive, and aware of social biases. Even though LLMs like
ChatGPT have impressive abilities, they struggle to understand different
cultural contexts and engage in meaningful, human like conversations. A key
issue is that biases in language processing, especially in machine translation,
can reinforce inequality. Tackling these biases requires a multidisciplinary
approach to ensure AI promotes diversity, fairness, and inclusion. We also
highlight AI's role in identifying biased content in media, which is important
for improving representation. By detecting unequal portrayals of social groups,
AI can help challenge stereotypes and create more inclusive technologies.
Transparent AI algorithms, which clearly explain their decisions, are essential
for building trust and reducing bias in AI systems. We also stress AI systems
need diverse and inclusive training data. Projects like the Child Growth
Monitor show how using a wide range of data can help address real world
problems like malnutrition and poverty. We present a project that demonstrates
how AI can be applied to monitor the role of search engines in spreading
disinformation about the LGBTQ+ community. Moreover, we discuss the SignON
project as an example of how technology can bridge communication gaps between
hearing and deaf people, emphasizing the importance of collaboration and mutual
trust in developing inclusive AI. Overall, with this paper, we advocate for AI
systems that are not only effective but also socially responsible, promoting
fair and inclusive interactions between humans and machines.",['cs.AI'],2502.06656," The recent development of powerful AI systems has highlighted the need for
robust risk management frameworks in the AI industry. Although companies have
begun to implement safety frameworks, current approaches often lack the
systematic rigor found in other high-risk industries. This paper presents a
comprehensive risk management framework for the development of frontier AI that
bridges this gap by integrating established risk management principles with
emerging AI-specific practices. The framework consists of four key components:
(1) risk identification (through literature review, open-ended red-teaming, and
risk modeling), (2) risk analysis and evaluation using quantitative metrics and
clearly defined thresholds, (3) risk treatment through mitigation measures such
as containment, deployment controls, and assurance processes, and (4) risk
governance establishing clear organizational structures and accountability.
Drawing from best practices in mature industries such as aviation or nuclear
power, while accounting for AI's unique challenges, this framework provides AI
developers with actionable guidelines for implementing robust risk management.
The paper details how each component should be implemented throughout the
life-cycle of the AI system - from planning through deployment - and emphasizes
the importance and feasibility of conducting risk management work prior to the
final training run to minimize the burden associated with it.",['cs.AI'],False,,,,AI in Support of Diversity and Inclusion,"A Frontier AI Risk Management Framework: Bridging the Gap Between
  Current AI Practices and Established Risk Management"
neg-d2-128,2025-01-29,,2501.17511," On a complete Riemannian manifold $M$, we study the spectral flow of a family
of Callias operators on $M$. We derive a codimension zero formula when
dimension of $M$ is odd and a codimension one formula when dimension of $M$ is
even. These can be seen as analogues of Gromov--Lawson's relative index theorem
and classical Callias index theorem, respectively. Secondly, we introduce an
intrinsic definition of K-cowaist on odd-dimensional manifolds, making use of
the odd Chern character of a smooth map from $M$ to a unitary group. It behaves
just like the usual K-cowaist on even-dimensional manifolds. We then apply the
notion of odd K-cowaist and the tool of spectral flow to investigate problems
related to positive scalar curvature on spin manifolds. In particular, we prove
infinite odd K-cowaist to be an obstruction to the existence of PSC metrics. We
obtain quantitative scalar curvature estimates on complete non-compact
manifolds and scalar-mean curvature estimates on compact manifolds with
boundary. They extend several previous results optimally, which unfolds a major
advantage of our method via spectral flow and odd K-cowaist.",['math.DG'],2501.17511," On a complete Riemannian manifold $M$, we study the spectral flow of a family
of Callias operators on $M$. We derive a codimension zero formula when
dimension of $M$ is odd and a codimension one formula when dimension of $M$ is
even. These can be seen as analogues of Gromov--Lawson's relative index theorem
and classical Callias index theorem, respectively. Secondly, we introduce an
intrinsic definition of K-cowaist on odd-dimensional manifolds, making use of
the odd Chern character of a smooth map from $M$ to a unitary group. It behaves
just like the usual K-cowaist on even-dimensional manifolds. We then apply the
notion of odd K-cowaist and the tool of spectral flow to investigate problems
related to positive scalar curvature on spin manifolds. In particular, we prove
infinite odd K-cowaist to be an obstruction to the existence of PSC metrics. We
obtain quantitative scalar curvature estimates on complete non-compact
manifolds and scalar-mean curvature estimates on compact manifolds with
boundary. They extend several previous results optimally, which unfolds a major
advantage of our method via spectral flow and odd K-cowaist.",['math.DG'],False,,,,"Spectral flow of Callias operators, odd K-cowaist, and positive scalar
  curvature","Spectral flow of Callias operators, odd K-cowaist, and positive scalar
  curvature"
neg-d2-129,2025-03-12,,2503.09531," The Quasi-harmonic Approximation (QHA) is a widely used method for
calculating the temperature dependence of lattice parameters and the thermal
expansion coefficients from first principles. However, applying QHA to
anisotropic systems typically requires several dozens or even hundreds of
phonon band structure calculations, leading to high computational costs. The
Zero Static Internal Stress Approximation (ZSISA) QHA method partly addresses
such caveat, but the computational load of its implementation remains high, so
that its volumetric-only counterpart v-ZSISA-QHA is preferred. In this work, we
present an efficient implementation of the ZSISA-QHA, enabling its application
across a wide range of crystal structures under varying temperature (T) and
pressure (P) conditions. By incorporating second-order derivatives of the
vibrational free energy with respect to lattice degrees of freedom, we
significantly reduce the number of required phonon band structure calculations
for the determination of all lattice parameters and angles. For hexagonal,
trigonal, and tetragonal systems, only six phonon band structure calculations
are needed, while 10, 15, and 28 calculations suffice for orthorhombic,
monoclinic, and triclinic systems, respectively. This method is tested for a
variety of non-cubic materials, from uniaxial ones like ZnO and CaCO3 to
monoclinic or triclinic materials such as ZrO2, HfO2, and Al2SiO5,
demonstrating a significant reduction in computational effort while maintaining
accuracy in modeling anisotropic thermal expansion, unlike the v-ZSISA-QHA. The
method is also applied to the first-principles calculation of
temperature-dependent elastic constants, with only up to six more phonon band
structure calculations, depending on the crystallographic system.",['cond-mat.mtrl-sci'],2502.2008," Recently synthesized Porous 12-Atom-Wide Armchair Graphene Nanoribbons Nano
Lett. 2024, 24, 10718-10723 exhibit tunable properties through periodic
porosity, enabling precise control over their electronic, optical, thermal, and
mechanical behavior. This work presents a comprehensive theoretical
characterization of pristine and porous 12-AGNRs based on density functional
theory (DFT) and molecular dynamics (MD) simulations. DFT calculations reveal
substantial electronic modifications, including band gap widening and the
emergence of localized states. Analyzed within the Bethe-Salpeter equation
(BSE) framework, optical properties highlight strong excitonic effects and
significant absorption shifts. Thermal transport simulations indicate a
pronounced reduction in conductivity due to enhanced phonon scattering at
nanopores. At the same time, MD-based mechanical analysis shows decreased
stiffness and strength while maintaining structural integrity. Despite these
modifications, porous 12-AGNRs remain mechanically and thermally stable. These
findings establish porosity engineering as a powerful strategy for tailoring
graphene nanoribbons' functional properties, reinforcing their potential for
nanoelectronic, optoelectronic, and thermal management applications.",['cond-mat.mtrl-sci'],False,,,,"Anisotropic temperature-dependent lattice parameters and elastic
  constants from first principles","Computational Characterization of the Recently Synthesized Pristine and
  Porous 12-Atom-Wide Armchair Graphene Nanoribbon"
neg-d2-130,2025-01-11,,2501.06548," Repeated waves of emerging variants during the SARS-CoV-2 pandemics have
highlighted the urge of collecting longitudinal genomic data and developing
statistical methods based on time series analyses for detecting new threatening
lineages and estimating their fitness early in time. Most models study the
evolution of the prevalence of particular lineages over time and require a
prior classification of sequences into lineages. Such process is prone to
induce delays and bias. More recently, few authors studied the evolution of the
prevalence of mutations over time with alternative clustering approaches,
avoiding specific lineage classification. Most of the aforementioned methods
are however either non parametric or unsuited to pooled data characterizing,
for instance, wastewater samples. In this context, we propose an alternative
unsupervised method for clustering mutations according to their frequency
trajectory over time and estimating group fitness from time series of pooled
mutation prevalence data. Our model is a mixture of observed count data and
latent group assignment and we use the expectation-maximization algorithm for
model selection and parameter estimation. The application of our method to time
series of SARS-CoV-2 sequencing data collected from wastewater treatment plants
in France from October 2020 to April 2021 shows its ability to agnostically
group mutations according to their probability of belonging to B.1.160, Alpha,
Beta, B.1.177 variants with selection coefficient estimates per group in
coherence with the viral dynamics in France reported by Nextstrain. Moreover,
our method detected the Alpha variant as threatening as early as supervised
methods (which track specific mutations over time) with the noticeable
difference that, since unsupervised, it does not require any prior information
on the set of mutations.",['q-bio.QM'],2501.06548," Repeated waves of emerging variants during the SARS-CoV-2 pandemics have
highlighted the urge of collecting longitudinal genomic data and developing
statistical methods based on time series analyses for detecting new threatening
lineages and estimating their fitness early in time. Most models study the
evolution of the prevalence of particular lineages over time and require a
prior classification of sequences into lineages. Such process is prone to
induce delays and bias. More recently, few authors studied the evolution of the
prevalence of mutations over time with alternative clustering approaches,
avoiding specific lineage classification. Most of the aforementioned methods
are however either non parametric or unsuited to pooled data characterizing,
for instance, wastewater samples. In this context, we propose an alternative
unsupervised method for clustering mutations according to their frequency
trajectory over time and estimating group fitness from time series of pooled
mutation prevalence data. Our model is a mixture of observed count data and
latent group assignment and we use the expectation-maximization algorithm for
model selection and parameter estimation. The application of our method to time
series of SARS-CoV-2 sequencing data collected from wastewater treatment plants
in France from October 2020 to April 2021 shows its ability to agnostically
group mutations according to their probability of belonging to B.1.160, Alpha,
Beta, B.1.177 variants with selection coefficient estimates per group in
coherence with the viral dynamics in France reported by Nextstrain. Moreover,
our method detected the Alpha variant as threatening as early as supervised
methods (which track specific mutations over time) with the noticeable
difference that, since unsupervised, it does not require any prior information
on the set of mutations.",['q-bio.QM'],False,,,,"Unsupervised detection and fitness estimation of emerging SARS-CoV-2
  variants. Application to wastewater samples (ANRS0160)","Unsupervised detection and fitness estimation of emerging SARS-CoV-2
  variants. Application to wastewater samples (ANRS0160)"
neg-d2-131,2025-01-19,,2501.10947," Amorphous solids are dynamically inhomogeneous due to in lack of
translational symmetry and hence exhibit vibrational properties different from
crystalline solids with anomalous low frequency vibrational density of states
(VDOS) and related low temperature thermal properties. However, an
interpretation of their origin from basic physical laws is still needed
compared with rapidly progressed particle level investigations. In this work,
we start with the quasi-equilibrium condition, which requires elastic potential
energy to be homogeneously distributed even in an inhomogeneous elastic solid
over long time observation. Analytical result shows that the anomalous low
frequency VDOS behavior $D(\omega) \propto \omega^4$ can be obtained when the
quasi-equilibrium condition is satisfied on an inhomogeneous elastic system.
Under high frequency after a crossover depending on the length scale of
inhomogeneity, the power law of VDOS is changed to square $D(\omega) \propto
\omega^2$ which is Debye's law for crystalline solids. These features agree
with recent particle level investigations. Our work suggest that the universal
low frequency anomaly of amorphous solids can be considered as a result of
homogeneous thermalization.",['cond-mat.soft'],2503.00559," Brain tissue accommodates non-linear deformations and exhibits time-dependent
mechanical behaviour. The latter is one of the most pronounced features of
brain tissue, manifesting itself primarily through viscoelastic effects such as
stress relaxation. To investigate its viscoelastic behaviour, we performed
ramp-and-hold relaxation tests in torsion on freshly slaughtered cylindrical
ovine brain samples ($25\,\,\text{mm}$ diameter and $\sim 10\,\,\text{mm}$
height). The tests were conducted using a commercial rheometer at varying twist
rates of $\{40,240,400\}\,\,\text{rad}\,\,\text{m}^{-1}\,\,\text{s}^{-1}$, with
the twist remaining fixed at $\sim 88\,\,\text{rad}\,\,\text{m}^{-1}$, which
generated two independent datasets for torque and normal force. The complete
set of viscoelastic material parameters was estimated via a simultaneous fit to
the analytical expressions for the torque and normal force predicted by the
modified quasi-linear viscoelastic model. The model's predictions were further
validated through finite element simulations in FEniCS. Our results show that
the modified quasi-linear viscoelastic model - recently reappraised and largely
unexploited - accurately fits the experimental data. Moreover, the estimated
material parameters are in line with those obtained in previous studies on
brain samples under torsion. When coupled with bespoke finite element models,
these material parameters could enhance our understanding of the forces and
deformations involved in traumatic brain injury and contribute to the design of
improved headgear for sports such as boxing and motorsports. On the other hand,
our novel testing protocol offers new insights into the mechanical behaviour of
soft tissues other than the brain.",['cond-mat.soft'],False,,,,"The anomalous density of states and quasi-localized vibration through
  homogeneous thermalization of an inhomogeneous elastic system","Modelling the non-linear viscoelastic behaviour of brain tissue in
  torsion"
neg-d2-132,2025-02-19,,2502.13646," In-context learning (ICL) has demonstrated significant potential in enhancing
the capabilities of large language models (LLMs) during inference. It's
well-established that ICL heavily relies on selecting effective demonstrations
to generate outputs that better align with the expected results. As for
demonstration selection, previous approaches have typically relied on intuitive
metrics to evaluate the effectiveness of demonstrations, which often results in
limited robustness and poor cross-model generalization capabilities. To tackle
these challenges, we propose a novel method, \textbf{D}emonstration
\textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration
validation perspective into this field. By introducing the demonstration
validation mechanism, our method effectively identifies demonstrations that are
both effective and highly generalizable. \textbf{D.Va} surpasses all existing
demonstration selection techniques across both natural language understanding
(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate
the robustness and generalizability of our approach across various language
models with different retrieval models.",['cs.CL'],2501.16616," Hallucination detection in text generation remains an ongoing struggle for
natural language processing (NLP) systems, frequently resulting in unreliable
outputs in applications such as machine translation and definition modeling.
Existing methods struggle with data scarcity and the limitations of unlabeled
datasets, as highlighted by the SHROOM shared task at SemEval-2024. In this
work, we propose a novel framework to address these challenges, introducing
DeepSeek Few-shot optimization to enhance weak label generation through
iterative prompt engineering. We achieved high-quality annotations that
considerably enhanced the performance of downstream models by restructuring
data to align with instruct generative models. We further fine-tuned the
Mistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to
accurately detect hallucinations in resource-limited settings. Combining this
fine-tuned model with ensemble learning strategies, our approach achieved 85.5%
accuracy on the test set, setting a new benchmark for the SHROOM task. This
study demonstrates the effectiveness of data restructuring, few-shot
optimization, and fine-tuning in building scalable and robust hallucination
detection frameworks for resource-constrained NLP systems.",['cs.CL'],False,,,,D.Va: Validate Your Demonstration First Before You Use It,"Few-Shot Optimized Framework for Hallucination Detection in
  Resource-Limited NLP Systems"
neg-d2-133,2025-01-25,,2501.15201," Training semantic segmenter with synthetic data has been attracting great
attention due to its easy accessibility and huge quantities. Most previous
methods focused on producing large-scale synthetic image-annotation samples and
then training the segmenter with all of them. However, such a solution remains
a main challenge in that the poor-quality samples are unavoidable, and using
them to train the model will damage the training process. In this paper, we
propose a training-free Synthetic Data Selection (SDS) strategy with CLIP to
select high-quality samples for building a reliable synthetic dataset.
Specifically, given massive synthetic image-annotation pairs, we first design a
Perturbation-based CLIP Similarity (PCS) to measure the reliability of
synthetic image, thus removing samples with low-quality images. Then we propose
a class-balance Annotation Similarity Filter (ASF) by comparing the synthetic
annotation with the response of CLIP to remove the samples related to
low-quality annotations. The experimental results show that using our method
significantly reduces the data size by half, while the trained segmenter
achieves higher performance. The code is released at
https://github.com/tanghao2000/SDS.",['cs.CV'],2503.10982," Occlusion poses a significant challenge in pedestrian detection from a single
view. To address this, multi-view detection systems have been utilized to
aggregate information from multiple perspectives. Recent advances in multi-view
detection utilized an early-fusion strategy that strategically projects the
features onto the ground plane, where detection analysis is performed. A
promising approach in this context is the use of 3D feature-pulling technique,
which constructs a 3D feature volume of the scene by sampling the corresponding
2D features for each voxel. However, it creates a 3D feature volume of the
whole scene without considering the potential locations of pedestrians. In this
paper, we introduce a novel model that efficiently leverages traditional 3D
reconstruction techniques to enhance deep multi-view pedestrian detection. This
is accomplished by complementing the 3D feature volume with probabilistic
occupancy volume, which is constructed using the visual hull technique. The
probabilistic occupancy volume focuses the model's attention on regions
occupied by pedestrians and improves detection accuracy. Our model outperforms
state-of-the-art models on the MultiviewX dataset, with an MODA of 97.3%, while
achieving competitive performance on the Wildtrack dataset.",['cs.CV'],False,,,,"A Training-free Synthetic Data Selection Method for Semantic
  Segmentation","Enhanced Multi-View Pedestrian Detection Using Probabilistic Occupancy
  Volume"
neg-d2-134,2025-03-18,,2503.14494," Flow based generative models have charted an impressive path across multiple
visual generation tasks by adhering to a simple principle: learning velocity
representations of a linear interpolant. However, we observe that training
velocity solely from the final layer output underutilizes the rich inter layer
representations, potentially impeding model convergence. To address this
limitation, we introduce DeepFlow, a novel framework that enhances velocity
representation through inter layer communication. DeepFlow partitions
transformer layers into balanced branches with deep supervision and inserts a
lightweight Velocity Refiner with Acceleration (VeRA) block between adjacent
branches, which aligns the intermediate velocity features within transformer
blocks. Powered by the improved deep supervision via the internal velocity
alignment, DeepFlow converges 8 times faster on ImageNet with equivalent
performance and further reduces FID by 2.6 while halving training time compared
to previous flow based models without a classifier free guidance. DeepFlow also
outperforms baselines in text to image generation tasks, as evidenced by
evaluations on MSCOCO and zero shot GenEval.",['cs.CV'],2502.13855," Geometric diagrams are critical in conveying mathematical and scientific
concepts, yet traditional diagram generation methods are often manual and
resource-intensive. While text-to-image generation has made strides in
photorealistic imagery, creating accurate geometric diagrams remains a
challenge due to the need for precise spatial relationships and the scarcity of
geometry-specific datasets. This paper presents MagicGeo, a training-free
framework for generating geometric diagrams from textual descriptions. MagicGeo
formulates the diagram generation process as a coordinate optimization problem,
ensuring geometric correctness through a formal language solver, and then
employs coordinate-aware generation. The framework leverages the strong
language translation capability of large language models, while formal
mathematical solving ensures geometric correctness. We further introduce
MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and
demonstrate that MagicGeo outperforms current methods in both qualitative and
quantitative evaluations. This work provides a scalable, accurate solution for
automated diagram generation, with significant implications for educational and
academic applications.",['cs.CV'],False,,,,Deeply Supervised Flow-Based Generative Models,MagicGeo: Training-Free Text-Guided Geometric Diagram Generation
neg-d2-135,2025-02-27,,2502.19953," Regular updates are essential for maintaining up-to-date knowledge in large
language models (LLMs). Consequently, various model editing methods have been
developed to update specific knowledge within LLMs. However, training-based
approaches often struggle to effectively incorporate new knowledge while
preserving unrelated general knowledge. To address this challenge, we propose a
novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes
the geometric relationships of parameter updates from fine-tuning to
differentiate between neurons associated with new knowledge updates and those
related to general knowledge perturbations. By employing a direction-aware
knowledge identification method, we avoid updating neurons with directions
approximately orthogonal to existing knowledge, thus preserving the model's
generalization ability. For the remaining neurons, we integrate both old and
new knowledge for aligned directions and apply a ""forget-then-learn"" editing
strategy for opposite directions. Additionally, we introduce an
importance-guided task vector fusion technique that filters out redundant
information and provides adaptive neuron-level weighting, further enhancing
model editing performance. Extensive experiments on two publicly available
datasets demonstrate the superiority of GeoEdit over existing state-of-the-art
methods.",['cs.CL'],2501.04249," Despite the remarkable advancements and widespread applications of deep
neural networks, their ability to perform reasoning tasks remains limited,
particularly in domains requiring structured, abstract thought. In this paper,
we investigate the linguistic reasoning capabilities of state-of-the-art large
language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from
International Linguistics Olympiad (IOL) problems. This dataset encompasses
diverse problems testing syntax, morphology, phonology, and semantics, all
carefully designed to be self-contained and independent of external knowledge.
These tasks challenge models to engage in metacognitive linguistic reasoning,
requiring the deduction of linguistic rules and patterns from minimal examples.
Through extensive benchmarking of leading LLMs, we find that even the most
advanced models struggle to handle the intricacies of linguistic complexity,
particularly in areas demanding compositional generalization and rule
abstraction. Our analysis highlights both the strengths and persistent
limitations of current models in linguistic problem-solving, offering valuable
insights into their reasoning capabilities. By introducing IOLBENCH, we aim to
foster further research into developing models capable of human-like reasoning,
with broader implications for the fields of computational linguistics and
artificial intelligence.",['cs.CL'],False,,,,GeoEdit: Geometric Knowledge Editing for Large Language Models,IOLBENCH: Benchmarking LLMs on Linguistic Reasoning
neg-d2-136,2025-03-18,,2503.14406," We investigate the influence of a time-periodic drive on three-dimensional
Weyl and multi-Weyl semimetals in planar-Hall/planar-thermal-Hall set-ups. The
drive is modelled here by circularly-polarized electromagnetic fields, whose
effects are incorporated by a combination of the Floquet theorem and the van
Vleck perturbation theory, applicable in the high-frequency limit. We evaluate
the longitudinal and in-plane transverse components of the linear-response
coefficients using the semiclassical Boltzmann formalism. We demonstrate the
explicit expressions of these transport coefficients in certain limits of the
system parameters, where it is possible to derive the explicit analytical
expressions. Our results demonstrate that the topological charges of the
corresponding semimetals etch their trademark signatures in these transport
properties, which can be observed in experiments.",['cond-mat.mes-hall'],2502.12931," We continue our explorations of the transport characteristics in
junction-configurations comprising semimetals with quadratic band-crossings,
observed in the bandstructures of both two- and three-dimensional materials.
Here, we consider short potential barriers/wells modelled by delta-function
potentials. We also generalize our analysis by incorporating tilts in the
dispersion. Due to the parabolic nature of the spectra, caused by
quadratic-in-momentum dependence, there exist evanescent waves, which decay
exponentially as we move away from the junction represented by the location of
the delta-function potential. Investigating the possibility of the appearance
of bound states, we find that their energies appear as pairs of $\pm |E_b |$,
reflecting the presence of the imaginary-valued wavevectors at both positive
and negative values of energies of the propagating quasiparticles.",['cond-mat.mes-hall'],False,,,,"Effects of time-periodic drive in the linear response for planar-Hall
  set-ups with Weyl and multi-Weyl semimetals","Delta-function-potential junctions with quasiparticles occupying tilted
  bands with quadratic-in-momentum dispersion"
neg-d2-137,2025-03-13,,2503.10982," Occlusion poses a significant challenge in pedestrian detection from a single
view. To address this, multi-view detection systems have been utilized to
aggregate information from multiple perspectives. Recent advances in multi-view
detection utilized an early-fusion strategy that strategically projects the
features onto the ground plane, where detection analysis is performed. A
promising approach in this context is the use of 3D feature-pulling technique,
which constructs a 3D feature volume of the scene by sampling the corresponding
2D features for each voxel. However, it creates a 3D feature volume of the
whole scene without considering the potential locations of pedestrians. In this
paper, we introduce a novel model that efficiently leverages traditional 3D
reconstruction techniques to enhance deep multi-view pedestrian detection. This
is accomplished by complementing the 3D feature volume with probabilistic
occupancy volume, which is constructed using the visual hull technique. The
probabilistic occupancy volume focuses the model's attention on regions
occupied by pedestrians and improves detection accuracy. Our model outperforms
state-of-the-art models on the MultiviewX dataset, with an MODA of 97.3%, while
achieving competitive performance on the Wildtrack dataset.",['cs.CV'],2502.06973," This paper presents a novel application for directly estimating indoor light
and heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.
In our image-based rendering method, the indoor panorama is used to estimate
the 3D room layout, while the corresponding outdoor panorama serves as an
environment map to infer spatially-varying light and material properties. We
establish a connection between indoor light transport and heat transport and
implement transient heat simulation to generate indoor heat panoramas. The
sensitivity analysis of various thermal parameters is conducted, and the
resulting heat maps are compared with the images captured by the thermal camera
in real-world scenarios. This digital application enables automatic indoor
light and heat estimation without manual inputs and cumbersome field
measurements.",['cs.CV'],False,,,,"Enhanced Multi-View Pedestrian Detection Using Probabilistic Occupancy
  Volume",Indoor Light and Heat Estimation from a Single Panorama
neg-d2-138,2025-03-07,,2503.06021," Federated Learning (FL) enables collaborative training of models across
distributed clients without sharing local data, addressing privacy concerns in
decentralized systems. However, the gradient-sharing process exposes private
data to potential leakage, compromising FL's privacy guarantees in real-world
applications. To address this issue, we propose Federated Error Minimization
(FedEM), a novel algorithm that incorporates controlled perturbations through
adaptive noise injection. This mechanism effectively mitigates gradient leakage
attacks while maintaining model performance. Experimental results on benchmark
datasets demonstrate that FedEM significantly reduces privacy risks and
preserves model accuracy, achieving a robust balance between privacy protection
and utility preservation.",['cs.LG'],2502.03755," In this paper, we address the task of characterizing the chemical composition
of planetary surfaces using convolutional neural networks (CNNs). Specifically,
we seek to predict the multi-oxide weights of rock samples based on
spectroscopic data collected under Martian conditions. We frame this problem as
a multi-target regression task and propose a novel regularization method based
on f-divergence. The f-divergence regularization is designed to constrain the
distributional discrepancy between predictions and noisy targets. This
regularizer serves a dual purpose: on the one hand, it mitigates overfitting by
enforcing a constraint on the distributional difference between predictions and
noisy targets. On the other hand, it acts as an auxiliary loss function,
penalizing the neural network when the divergence between the predicted and
target distributions becomes too large. To enable backpropagation during neural
network training, we develop a differentiable f-divergence and incorporate it
into the f-divergence regularization, making the network training feasible. We
conduct experiments using spectra collected in a Mars-like environment by the
remote-sensing instruments aboard the Curiosity and Perseverance rovers.
Experimental results on multi-oxide weight prediction demonstrate that the
proposed $f$-divergence regularization performs better than or comparable to
standard regularization methods including $L_1$, $L_2$, and dropout. Notably,
combining the $f$-divergence regularization with these standard regularization
further enhances performance, outperforming each regularization method used
independently.",['cs.LG'],False,,,,"FedEM: A Privacy-Preserving Framework for Concurrent Utility
  Preservation in Federated Learning","Regularization via f-Divergence: An Application to Multi-Oxide
  Spectroscopic Analysis"
neg-d2-139,2025-02-16,,2502.11072," This work presents a novel simulation-based approach for constructing
confidence regions in parametric models, which is particularly suited for
generative models and situations where limited data and conventional asymptotic
approximations fail to provide accurate results. The method leverages the
concept of data depth and depends on creating random hyper-rectangles, i.e.
boxes, in the sample space generated through simulations from the model,
varying the input parameters. A probabilistic acceptance rule allows to
retrieve a Depth-Confidence Distribution for the model parameters from which
point estimators as well as calibrated confidence sets can be read-off. The
method is designed to address cases where both the parameters and test
statistics are multivariate.",['stat.ME'],2503.04093," Developing tools for estimating heterogeneous treatment effects (HTE) and
individualized treatment effects has been an area of active research in recent
years. While these tools have proven to be useful in many contexts, a concern
when deploying such methods is the degree to which incorporating HTE into a
prediction model provides an advantage over predictive methods which do not
allow for variation in treatment effect across individuals. To address this
concern, we propose a procedure which evaluates the extent to which an HTE
model provides a predictive advantage. Specifically, our procedure targets the
gain in predictive performance from using a flexible predictive model
incorporating HTE versus an alternative model which is similar to the
HTE-utilizing model except that it is constrained to not allow variation in
treatment effect. By drawing upon recent work in using nested cross-validation
techniques for prediction error inference, we generate confidence intervals for
this measure of gain in predictive performance which allows one to directly
calculate the level at which one is confident of a substantial HTE-modeling
gain in prediction -- a quantity which we refer to as the h-value. Our
procedure is generic and can be directly used to assess the benefit of modeling
HTE for any method that incorporates treatment effect variation.",['stat.ME'],False,,,,Box Confidence Depth: simulation-based inference with hyper-rectangles,Evaluating and Testing for Actionable Treatment Effect Heterogeneity
neg-d2-140,2025-03-20,,2503.16314," Understanding the impact of disturbances in quantum channels is of paramount
importance for the implementation of many quantum technologies, as noise can be
detrimental to quantum correlations. Among the various types of disturbances,
we explore the effects of white and colored noise and experimentally test the
resilience of a quantum ghost spectrometer against these two types of noise,
showing that it is always robust against white noise, whereas colored noise
introduces a huge impact on the process.",['quant-ph'],2503.16314," Understanding the impact of disturbances in quantum channels is of paramount
importance for the implementation of many quantum technologies, as noise can be
detrimental to quantum correlations. Among the various types of disturbances,
we explore the effects of white and colored noise and experimentally test the
resilience of a quantum ghost spectrometer against these two types of noise,
showing that it is always robust against white noise, whereas colored noise
introduces a huge impact on the process.",['quant-ph'],False,,,,"An experimental investigation of quantum frequency correlations
  resilience against white and colored noise","An experimental investigation of quantum frequency correlations
  resilience against white and colored noise"
neg-d2-141,2025-03-14,,2503.11923," In this paper, we introduce the concept of bikernel by monochromatic paths of
a bicolored digraph. This concept is strongly motivated by the existing notions
of kernels, kernels by monochromatic paths, and double stable augmented
categories. We establish sufficient and necessary conditions for several
families of bicolored digraphs to have a bikernel by monochromatic paths. Also,
we characterize bicolored digraphs without monochromatic cycles that possess a
bikernel by monochromatic paths. Similarly, we characterize bicolored digraphs
with monochromatic cycles that also have a bikernel by monochromatic paths.
Furthermore, we prove sufficient and necessary conditions for some families of
digraphs to be $BK$-colorable. This means that a bicoloration of the digraph
exists where the resulting bicolored digraph has a bikernel.",['math.CO'],2501.02557," We construct and study new generalisations to rooted trees and forests of
some properties of shuffles of words. First, we build a coproduct on rooted
trees which, together with their shuffle, endow them with bialgebra structure.
We then caracterize the coproduct dual to the shuffle product of rooted forests
and build a product on rooted trees to obtain the bialgebra dual to the shuffle
bialgebra. We then characterize and enumerate primitive trees for the dual
coproduct. Finally, using modified shuffles of rooted forests, we prove a
property in the category of Rota-Baxter algebras.",['math.CO'],False,,,,Bikernels by monochromatic paths,"Coalgebras, bialgebras and Rota-Baxter algebras from shuffles of rooted
  forests"
neg-d2-142,2025-01-05,,2501.02515," Although multiferroics have undergone extensive examination for several
decades, the occurrence of ferroelectricity induced by orbital order is only
scarcely documented. In this study, we propose the existence of spontaneous
ferroelectric states featuring a finite out-of-plane polarization in monolayer
compounds of magnetic transition metal di-halides. Our first principles
analysis reveals that partially occupied d-orbital states within octahedra
exhibit a preference for spatial orbital order within a two-dimensional
lattice. The absence of inversion symmetry, arising from orbital order, serves
as the driving force introducing additional electric polarization along the
out-of-plane direction. Unlike previous reported orbital orders arising from
metal states in lattice, the non-colinear ones we studied in this work relate
to the transition between two insulator states. The resultant asymmetric
Jahn-Teller distortions are accompanied as the consequence producing additional
ionic polarization. Importantly, our findings indicate that this mechanism is
not confined to a specific material but is a shared characteristic among a
series of monolayer transition metal magnetic di-halides, proposing an
innovative form of intrinsic two-dimensional multiferroic physics.",['cond-mat.mtrl-sci'],2502.2008," Recently synthesized Porous 12-Atom-Wide Armchair Graphene Nanoribbons Nano
Lett. 2024, 24, 10718-10723 exhibit tunable properties through periodic
porosity, enabling precise control over their electronic, optical, thermal, and
mechanical behavior. This work presents a comprehensive theoretical
characterization of pristine and porous 12-AGNRs based on density functional
theory (DFT) and molecular dynamics (MD) simulations. DFT calculations reveal
substantial electronic modifications, including band gap widening and the
emergence of localized states. Analyzed within the Bethe-Salpeter equation
(BSE) framework, optical properties highlight strong excitonic effects and
significant absorption shifts. Thermal transport simulations indicate a
pronounced reduction in conductivity due to enhanced phonon scattering at
nanopores. At the same time, MD-based mechanical analysis shows decreased
stiffness and strength while maintaining structural integrity. Despite these
modifications, porous 12-AGNRs remain mechanically and thermally stable. These
findings establish porosity engineering as a powerful strategy for tailoring
graphene nanoribbons' functional properties, reinforcing their potential for
nanoelectronic, optoelectronic, and thermal management applications.",['cond-mat.mtrl-sci'],False,,,,"Orbital Order Triggered Out-of-Plane Ferroelectricity in Magnetic
  Transition Metal di-halide Monolayers","Computational Characterization of the Recently Synthesized Pristine and
  Porous 12-Atom-Wide Armchair Graphene Nanoribbon"
neg-d2-143,2025-03-17,,2503.13638," The Standard Model Effective Field Theory (SMEFT) is a widely utilized
framework for exploring new physics effects in a model-independent manner. In
previous studies, Drell-Yan collider data has emerged as a promising signature
due to its energy enhancement relative to Standard Model predictions. We
present recent works, extending this approach by also considering the ""missing
energy + jet"" signature, which can constrain related dineutrino couplings and
similarly benefits from energy enhancement. The combination of these
observables allows for constraining a broader selection of operators and helps
resolve flat directions in a global analysis. Overall, the bounds probe the
multi-TeV range, with the strongest reaching up to $10\; \text{TeV}$ for
four-fermion interactions and $7 \;\text{TeV}$ for gluonic dipole interactions.
Furthermore, we find that low energy flavor observables improve limits by up to
a factor of three for dipole operators. We also estimate sensitivities to new
physics at future hadron colliders including the $\sqrt{s} = 27 \;\text{TeV}$
HE-LHC and the $\sqrt{s} = 100 \; \text{TeV}$ FCC-hh.",['hep-ph'],2502.01668," The literature establishes that the light fermions contributions to the
decays $H\to Z\gamma$ and $H\to\gamma\gamma$ are negligible since their
coupling with the Higgs is proportional to $m_f$. In the present letter, we
show that although such a conclusion is true for leptons, the light quark
contributions are zero when we consider their non-perturbative effects.",['hep-ph'],False,,,,Teaming up MET plus jet with Drell-Yan in the SMEFT,Light quark contributions to Higgs decays
neg-d2-144,2025-02-27,,2502.19896," Existing point cloud completion methods, which typically depend on predefined
synthetic training datasets, encounter significant challenges when applied to
out-of-distribution, real-world scans. To overcome this limitation, we
introduce a zero-shot completion framework, termed GenPC, designed to
reconstruct high-quality real-world scans by leveraging explicit 3D generative
priors. Our key insight is that recent feed-forward 3D generative models,
trained on extensive internet-scale data, have demonstrated the ability to
perform 3D generation from single-view images in a zero-shot setting. To
harness this for completion, we first develop a Depth Prompting module that
links partial point clouds with image-to-3D generative models by leveraging
depth images as a stepping stone. To retain the original partial structure in
the final results, we design the Geometric Preserving Fusion module that aligns
the generated shape with input by adaptively adjusting its pose and scale.
Extensive experiments on widely used benchmarks validate the superiority and
generalizability of our approach, bringing us a step closer to robust
real-world scan completion.",['cs.CV'],2502.18417," While the task of face swapping has recently gained attention in the research
community, a related problem of head swapping remains largely unexplored. In
addition to skin color transfer, head swap poses extra challenges, such as the
need to preserve structural information of the whole head during synthesis and
inpaint gaps between swapped head and background. In this paper, we address
these concerns with GHOST 2.0, which consists of two problem-specific modules.
First, we introduce enhanced Aligner model for head reenactment, which
preserves identity information at multiple scales and is robust to extreme pose
variations. Secondly, we use a Blender module that seamlessly integrates the
reenacted head into the target background by transferring skin color and
inpainting mismatched regions. Both modules outperform the baselines on the
corresponding tasks, allowing to achieve state of the art results in head
swapping. We also tackle complex cases, such as large difference in hair styles
of source and target. Code is available at
https://github.com/ai-forever/ghost-2.0",['cs.CV'],False,,,,GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors,GHOST 2.0: generative high-fidelity one shot transfer of heads
neg-d2-145,2025-01-31,,2501.1938," Background: Software engineering requires both technical skills and creative
problem-solving. Blind and low-vision software professionals (BLVSPs) encounter
numerous workplace challenges, including inaccessible tools and collaboration
hurdles with sighted colleagues. Objective: This study explores the innovative
strategies employed by BLVSPs to overcome these accessibility barriers,
focusing on their custom solutions and the importance of supportive
communities. Methodology: We conducted semi-structured interviews with 30
BLVSPs and used reflexive thematic analysis to identify key themes. Results:
Findings reveal that BLVSPs are motivated to develop creative and adaptive
solutions, highlighting the vital role of collaborative communities in
fostering shared problem-solving. Conclusion: For BLVSPs, creative
problem-solving is essential for navigating inaccessible work environments, in
contrast to sighted peers, who pursue optimization. This study enhances
understanding of how BLVSPs navigate accessibility challenges through
innovation.",['cs.SE'],2503.04359," Current advanced long-context language models offer great potential for
real-world software engineering applications. However, progress in this
critical domain remains hampered by a fundamental limitation: the absence of a
rigorous evaluation framework for long code understanding. To gap this
obstacle, we propose a long code understanding benchmark LONGCODEU from four
aspects (8 tasks) to evaluate LCLMs' long code understanding ability required
for practical applications, including code unit perception, intra-code unit
understanding, inter-code unit relation understanding, and long code
documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6
general models and 3 code models). Our experimental results reveal key
limitations in current LCLMs' capabilities for long code understanding.
Particularly, the performance of LCLMs drops dramatically when the long code
length is greater than 32K, falling far short of their claimed 128K-1M context
windows. In the four aspects, inter-code unit relation understanding is the
most challenging for LCLMs. Our study provides valuable insights for optimizing
LCLMs and driving advancements in software engineering.",['cs.SE'],False,,,,"Creative Problem-Solving: A Study with Blind and Low Vision Software
  Professionals","LONGCODEU: Benchmarking Long-Context Language Models on Long Code
  Understanding"
neg-d2-146,2025-03-13,,2503.1081," We investigate the factorization properties of the massive fermion form
factor in QED, to next-to-leading power in the fermion mass, and up to two-loop
order. For this purpose we define new jet functions that have multiple
connections to the hard part as operator matrix elements, and compute them to
second order in the coupling. We test our factorization formula using these new
jet functions in a region-based analysis and find that factorization indeed
holds. We address a number of subtle aspects such as rapidity regulators and
external line corrections, and we find an interesting sequence of relations
among the jet functions.",['hep-ph'],2503.09937," We calculate the spin density matrix for neutral $\rho$ mesons from the
spectral function and thermal shear tensor by Kubo formula in the linear
response theory, which contributes to the $\gamma$ correlator for the CME
search. We derive the spectral function of neutral $\rho$ mesons with
$\rho\pi\pi$ and $\rho\rho\pi\pi$ interactions using the Dyson-Schwinger
equation. The thermal shear tensor contribution is obtained from the Kubo
formula in the linear response theory. We numerically calculate $\rho_{00}-1/3$
and $\mathrm{Re}\rho_{-1,1}$ using the simulation results for the thermal shear
tensor by the hydrodynamical model, which are of the order
$10^{-3}\sim10^{-2}$.",['hep-ph'],False,,,,Next-to-leading power jet functions in the small-mass limit in QED,"Spin density matrix for neutral $\rho$ mesons in a pion gas in linear
  response theory"
neg-d2-147,2025-01-17,,2501.10622," Cosmological simulations are an important method for investigating the
evolution of the Universe. In order to gain further insight into the processes
of structure formation, it is necessary to identify isolated bound objects
within the simulations, namely, the dark matter halos. The continuous wavelet
transform (CWT) is an effective tool used as a halo finder due to its ability
to extract clustering information from the input data. In this study, we
introduce CWTHF (Continuous Wavelet Transform Halo Finder), the first
wavelet-based, MPI-parallelized halo finder, marking a novel approach in the
field of cosmology. We calculate the CWT from the cloud-in-cell (CIC) grid and
segment the grid based on the local CWT maxima. We then investigate the effects
of the parameters that influence our program and identify the default settings.
A comparison with the conventional friends-of-friends (FOF) method demonstrates
the viability of CWT for halo finding. Although the actual performance is not
faster than FOF, the linear time complexity of $\mathcal{O}(N)$ of our
identification scheme indicates its significant potential for future
optimization and application.",['astro-ph.CO'],2501.08718," We conduct a comprehensive study into the impact of pixelization on cosmic
shear, uncovering several sources of bias in standard pseudo-$C_\ell$
estimators based on discrete catalogues. We derive models that can bring
residual biases to the percent level on small scales. We elucidate the impact
of aliasing and the varying shape of HEALPix pixels on power spectra and show
how the HEALPix pixel window function approximation is made in the discrete
spin-2 setting. We propose several improvements to the standard estimator and
its modelling, based on the principle that source positions and weights are to
be considered fixed. We show how empty pixels can be accounted for either by
modifying the mixing matrices or applying correction factors that we derive. We
introduce an approximate interlacing scheme for the HEALPix grid and show that
it can mitigate the effects of aliasing. We introduce bespoke pixel window
functions adapted to the survey footprint and show that, for band-limited
spectra, biases from using an isotropic window function can be effectively
reduced to zero. This work partly intends to serve as a useful reference for
pixel-related effects in angular power spectra, which are of relevance for
ongoing and forthcoming lensing and clustering surveys.",['astro-ph.CO'],False,,,,CWTHF: Identifying Dark Matter Halos with Continuous Wavelet Transform,Pixelization effects in cosmic shear angular power spectra
neg-d2-148,2025-02-27,,2502.20545," Large Language Models (LLMs) have achieved human-level proficiency across
diverse tasks, but their ability to perform rigorous mathematical problem
solving remains an open challenge. In this work, we investigate a fundamental
yet computationally intractable problem: determining whether a given
multivariate polynomial is nonnegative. This problem, closely related to
Hilbert's Seventeenth Problem, plays a crucial role in global polynomial
optimization and has applications in various fields. First, we introduce
SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials,
along with expert-designed reasoning instructions based on five progressively
challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that
without structured guidance, all models perform only slightly above the random
guess baseline 50%. However, high-quality reasoning instructions significantly
improve accuracy, boosting performance up to 81%. Furthermore, our 7B model,
SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3
and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation
time needed for letters, respectively. Our findings highlight the potential of
LLMs to push the boundaries of mathematical reasoning and tackle NP-hard
problems.",['cs.LG'],2503.01203," Hypergraph neural networks (HGNNs) effectively model complex high-order
relationships in domains like protein interactions and social networks by
connecting multiple vertices through hyperedges, enhancing modeling
capabilities, and reducing information loss. Developing foundation models for
hypergraphs is challenging due to their distinct data, which includes both
vertex features and intricate structural information. We present Hyper-FM, a
Hypergraph Foundation Model for multi-domain knowledge extraction, featuring
Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex
feature representation and Hierarchical Multi-Hypergraph Guided Structural
Knowledge Extraction for structural information. Additionally, we curate 10
text-attributed hypergraph datasets to advance research between HGNNs and LLMs.
Experiments on these datasets show that Hyper-FM outperforms baseline methods
by approximately 13.3\%, validating our approach. Furthermore, we propose the
first scaling law for hypergraph foundation models, demonstrating that
increasing domain diversity significantly enhances performance, unlike merely
augmenting vertex and hyperedge counts. This underscores the critical role of
domain diversity in scaling hypergraph models.",['cs.LG'],False,,,,SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers,Hypergraph Foundation Model
neg-d2-149,2025-03-19,,2503.15256," Planetary growth within protoplanetary disks involves accreting material from
their surroundings, yet the underlying mechanisms and physical conditions of
the accreting gas remain debated. This study aims to investigate the dynamics
and thermodynamic properties of accreting gas giants, and to characterize the
envelope that forms near the planet during accretion. We employ
three-dimensional hydrodynamical simulations of a Jupiter-mass planet embedded
in a viscous gaseous disk. Our models incorporate a non-isothermal energy
equation to compute gas and radiation energy diffusion and include radiative
feedback from the planet. Results indicate that gas accretion occurs
supersonically towards the planet, forming an ionized envelope that extends
from the planetary surface up to 0.2 times the Hill radius in the no-feedback
model, and up to 0.4 times the Hill radius in the feedback model. The
envelope's radius, or ionization radius, acts as a boundary halting supersonic
gas inflow and is pivotal for estimating accretion rates and H$\alpha$ emission
luminosities. Including radiative feedback increases accretion rates,
especially within the ionization radius and from areas to the right of the
planet when the star is positioned to the left. The accretion luminosities
calculated at the ionization radius are substantially lower than those
calculated at the Hill radius, highlighting potential misinterpretations in the
non-detection of H$\alpha$ emissions as indicators of ongoing planet formation.",['astro-ph.EP'],2502.06966," Recent discoveries of transiting giant exoplanets around M dwarfs (GEMS)
present an opportunity to investigate their atmospheric compositions and
explore how such massive planets can form around low-mass stars contrary to
canonical formation models. Here, we present the first transmission spectra of
TOI-5205b, a short-period ($P=1.63~\mathrm{days}$) Jupiter-like planet
($M_p=1.08~\mathrm{M_J}$ and $R_p=0.94~\mathrm{R_J}$) orbiting an M4 dwarf. We
obtained three transits using the PRISM mode of the JWST Near Infrared
Spectrograph (NIRSpec) spanning $0.6-5.3$ um. Our data reveal significant
stellar contamination that is evident in the light curves as spot-crossing
events and in the transmission spectra as a larger transit depth at bluer
wavelengths. Atmospheric retrievals demonstrate that stellar contamination from
unocculted star spots is the dominant component of the transmission spectrum at
wavelengths $\lambda\lesssim3.0$ um, which reduced the sensitivity to the
presence of clouds or hazes in our models. The degree of stellar contamination
also prevented the definitive detection of any $\mathrm{H_2O}$, which has
primary absorption features at these shorter wavelengths. The broad wavelength
coverage of NIRSpec PRISM enabled a robust detection of $\mathrm{CH_4}$ and
$\mathrm{H_2S}$, which have detectable molecular features between $3.0-5.0$ um.
Our gridded and Bayesian retrievals consistently favored an atmosphere with
both sub-solar metallicity ($\log\mathrm{[M/H]}\sim-2$ for a clear atmosphere)
and super-solar C/O ratio ($\log\mathrm{[C/O]}\sim3$ for a clear or cloudy
atmosphere). This contrasts with estimates from planetary interior models that
predict a bulk metallicity of 10--20%, which is $\sim100\times$ the atmospheric
metallicity, and suggests that the planetary interior for TOI-5205b is
decoupled from its atmosphere and not well mixed.",['astro-ph.EP'],False,,,,"Ionized envelopes around protoplanets and the role of radiative feedback
  in gas accretion","GEMS JWST: Transmission spectroscopy of TOI-5205b reveals significant
  stellar contamination and a metal-poor atmosphere"
neg-d2-150,2025-02-25,,2502.18636," In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.",['eess.SP'],2501.0582," Modular Arrays (MAs) are a promising architecture to enable multi-user
communications in next-generation multiple-input multiple-output (MIMO) systems
based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an
improved spatial resolution with characteristic interference patterns
associated to grating lobes. In this work, we analyze whether MAs can
outperform conventional collocated deployments, in terms of achievable sum-rate
and served users in a multi-user downlink set-up. First, we provide a rigorous
analytical characterization of the inter-user interference for modular gMIMO
systems operating in the near field. Then, we leverage these results to
optimize the user selection and precoding mechanisms, designing two algorithms
that largely outperform existing alternatives in the literature, with different
algorithmic complexities. Results show that the proposed algorithms yield over
70% improvements in achievable sum-spectral efficiencies compared to the state
of the art. We also illustrate how MAs allow to serve a larger number of users
thanks to their improved spatial resolution, compared to the collocated
counterpart.",['eess.SP'],False,,,,"Transfer Learning Assisted Fast Design Migration Over Technology Nodes:
  A Study on Transformer Matching Network",User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays
neg-d2-151,2025-01-17,,2501.10101," In this paper, we establish sharp bounds for a family of Kantorovich-type
neural network operators within the general frameworks of Sobolev-Orlicz and
Orlicz spaces. We establish both strong (in terms of the Luxemburg norm) and
weak (in terms of the modular functional) estimates, using different
approaches. The strong estimates are derived for spaces generated by
$\varphi$-functions that are $N$-functions or satisfy the
$\Delta^\prime$-condition. Such estimates also lead to convergence results with
respect to the Luxemburg norm in several instances of Orlicz spaces, including
the exponential case. Meanwhile, the weak estimates are achieved under less
restrictive assumptions on the involved $\varphi$-function. To obtain these
results, we introduce some new tools and techniques in Orlicz spaces. Central
to our approach is the Orlicz Minkowski inequality, which allows us to obtain
unified strong estimates for the operators. We also present a weak (modular)
version of this inequality holding under weaker conditions. Additionally, we
introduce a novel notion of discrete absolute $\varphi$-moments of hybrid type,
and we employ the Hardy-Littlewood maximal operator within Orlicz spaces for
the asymptotic analysis. Furthermore, we introduce the new space
$\mathcal{W}^{1,\varphi}(I)$, which is embedded in the Sobolev-Orlicz space
$W^{1,\varphi}(I)$ and modularly dense in $L^\varphi(I)$. This allows to
achieve asymptotic estimates for a wider class of $\varphi$-functions,
including those that do not meet the $\Delta_2$-condition. For the extension to
the whole Orlicz-setting, we generalize a Sobolev-Orlicz density result given
by H. Musielak using Steklov functions, providing a modular counterpart.
Finally, we explore the relationships between weak and strong Orlicz Lipschitz
classes, providing qualitative results for the rate of convergence of the
operators.",['math.FA'],2502.15486," In this note, we present an alternative proof of a quantified Tauberian
theorem for vector-valued sequences first proved in \cite{Sei15_Tauberian}. The
theorem relates the decay rate of a bounded sequence with properties of a
certain boundary function. We present a slightly strengthened version of this
result, and illustrate how it can be used to obtain quantified versions of the
Katznelson--Tzafriri theorem as well as results on Ritt operators.",['math.FA'],False,,,,"Strong and weak sharp bounds for Neural Network Operators in
  Sobolev-Orlicz spaces and their quantitative extensions to Orlicz spaces",Tauberian theorems for sequences and the Katznelson--Tzafriri theorem
neg-d2-152,2025-02-21,,2502.15613," Current robotic pick-and-place policies typically require consistent gripper
configurations across training and inference. This constraint imposes high
retraining or fine-tuning costs, especially for imitation learning-based
approaches, when adapting to new end-effectors. To mitigate this issue, we
present a diffusion-based policy with a hybrid learning-optimization framework,
enabling zero-shot adaptation to novel grippers without additional data
collection for retraining policy. During training, the policy learns
manipulation primitives from demonstrations collected using a base gripper. At
inference, a diffusion-based optimization strategy dynamically enforces
kinematic and safety constraints, ensuring that generated trajectories align
with the physical properties of unseen grippers. This is achieved through a
constrained denoising procedure that adapts trajectories to gripper-specific
parameters (e.g., tool-center-point offsets, jaw widths) while preserving
collision avoidance and task feasibility. We validate our method on a Franka
Panda robot across six gripper configurations, including 3D-printed fingertips,
flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a
93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion
policy baselines), supporting tool-center-point variations of 16-23.5 cm and
jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion
enables robust cross-gripper manipulation while maintaining the sample
efficiency of imitation learning, eliminating the need for gripper-specific
retraining. Video and code are available at https://github.com/yaoxt3/GADP.",['cs.RO'],2503.16164," The asymptotically optimal version of Rapidly-exploring Random Tree (RRT*) is
often used to find optimal paths in a high-dimensional configuration space. The
well-known issue of RRT* is its slow convergence towards the optimal solution.
A possible solution is to draw random samples only from a subset of the
configuration space that is known to contain configurations that can improve
the cost of the path (omniscient set). A fast convergence rate may be achieved
by approximating the omniscient with a low-volume set. In this letter, we
propose new methods to approximate the omniscient set and methods for their
effective sampling. First, we propose to approximate the omniscient set using
several (small) hyperellipsoids defined by sections of the current best
solution. The second approach approximates the omniscient set by a convex hull
computed from the current solution. Both approaches ensure asymptotical
optimality and work in a general n-dimensional configuration space. The
experiments have shown superior performance of our approaches in multiple
scenarios in 3D and 6D configuration spaces.",['cs.RO'],False,,,,"Pick-and-place Manipulation Across Grippers Without Retraining: A
  Learning-optimization Diffusion Policy Approach","Asymptotically Optimal Path Planning With an Approximation of the
  Omniscient Set"
neg-d2-153,2025-01-27,,2501.15966," PS16dtm is one of the earliest reported candidate tidal disruption events
(TDEs) in active galactic nuclei (AGNs) and displays a remarkably bright and
long-lived infrared (IR) echo revealed by multi-epoch photometry from the
Wide-field Infrared Survey Explorer (WISE). After a rapid rise in the first
year, the echo remains persistently at a high state from July 2017 to July
2024, the latest epoch, and keeps an almost constant color. We have fitted the
extraordinary IR emission with a refined dust echo model by taking into account
the dust sublimation process. The fitting suggests that an extremely giant dust
structure with a new inner radius of $\sim1.6$ pc and an ultra-high peak
bolometric luminosity, i.e., $\sim6\times10^{46} \rm erg~s^{-1}$ for typical
0.1$\mu$m-sized silicate grain, is required to account for the IR echo. This
work highlights the distinctive value of IR echoes in measuring the accurate
intrinsic bolometric luminosity, and thus the total radiated energy of TDEs,
which could be severely underestimated by traditional methods, i.e. probably by
more than an order of magnitude in PS16dtm. Such large energetic output
compared to normal TDEs could be boosted by the pre-existing accretion disk and
gas clouds around the black hole. Our model can be validated in the near future
by IR time-domain surveys such as Near-Earth Object (NEO) Surveyor, given the
recent retirement of WISE. In addition, the potential for spatially resolving a
receding dusty torus after a TDE could also be an exciting subject in the era
of advanced IR interferometry.",['astro-ph.HE'],2502.16626," The non-detection of periodicity related to rotation challenges the magnetar
model for fast radio bursts (FRBs). Moreover, a bimodal distribution of the
burst waiting times is widely observed in hyper-active FRBs, a significant
deviation from the exponential distribution expected from stationary Poisson
processes. By combining the epidemic-type aftershock sequence (ETAS) earthquake
model and the rotating vector model (RVM) involving the rotation of the
magnetar and orientations of the spin and magnetic axes, we find that starquake
events modulated by the rotation of FRB-emitting magnetar can explain the
bimodal distribution of FRB waiting times, as well as the non-detection of
periodicity in active repeating FRBs. We analyze data from multiple FRB
sources, demonstrating that differences in waiting time distributions and
observed energies can be explained by varying parameters related to magnetar
properties and starquake dynamics. Our results suggest that rotation-modulated
starquakes on magnetars can possibly be a unified source for FRBs. Notably, we
find that active repeaters tend to have small magnetic inclination angles in
order to hide their periodicity. We also show that our model can reproduce the
waiting time distribution of a pulsar phase of the galactic magnetar SGR
J1935+2154 with a larger inclination angle than the active repeaters, which
could explain the detection of spin period and the relatively low observed
energy for FRBs from the magnetar. The spin periods of active repeaters are not
well constrained, but most likely fall in the valley region between the two
peaks of the waiting time distributions.",['astro-ph.HE'],False,,,,"The Extraordinary Long-lasting Infrared Echo of PS16dtm Reveals an
  Extremely Energetic Nuclear Outburst","Hyper-active repeating fast radio bursts from rotation modulated
  starquakes on magnetars"
neg-d2-154,2025-02-09,,2502.05824," Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base
stations (BSs) to improve terrestrial communications. However, the limited
onboard energy and antenna power of a UAV restrict its communication range and
transmission capability. To address these limitations, this work employs
collaborative beamforming through a UAV-enabled virtual antenna array to
improve transmission performance from the UAV to terrestrial mobile users,
under interference from non-associated BSs and dynamic channel conditions.
Specifically, we introduce a memory-based random walk model to more accurately
depict the mobility patterns of terrestrial mobile users. Following this, we
formulate a multi-objective optimization problem (MOP) focused on maximizing
the transmission rate while minimizing the flight energy consumption of the UAV
swarm. Given the NP-hard nature of the formulated MOP and the highly dynamic
environment, we transform this problem into a multi-objective Markov decision
process and propose an improved evolutionary multi-objective reinforcement
learning algorithm. Specifically, this algorithm introduces an evolutionary
learning approach to obtain the approximate Pareto set for the formulated MOP.
Moreover, the algorithm incorporates a long short-term memory network and
hyper-sphere-based task selection method to discern the movement patterns of
terrestrial mobile users and improve the diversity of the obtained Pareto set.
Simulation results demonstrate that the proposed method effectively generates a
diverse range of non-dominated policies and outperforms existing methods.
Additional simulations demonstrate the scalability and robustness of the
proposed CB-based method under different system parameters and various
unexpected circumstances.",['cs.NE'],2501.11411," Coupling Large Language Models (LLMs) with Evolutionary Algorithms has
recently shown significant promise as a technique to design new heuristics that
outperform existing methods, particularly in the field of combinatorial
optimisation. An escalating arms race is both rapidly producing new heuristics
and improving the efficiency of the processes evolving them. However, driven by
the desire to quickly demonstrate the superiority of new approaches, evaluation
of the new heuristics produced for a specific domain is often cursory: testing
on very few datasets in which instances all belong to a specific class from the
domain, and on few instances per class. Taking bin-packing as an example, to
the best of our knowledge we conduct the first rigorous benchmarking study of
new LLM-generated heuristics, comparing them to well-known existing heuristics
across a large suite of benchmark instances using three performance metrics.
For each heuristic, we then evolve new instances won by the heuristic and
perform an instance space analysis to understand where in the feature space
each heuristic performs well. We show that most of the LLM heuristics do not
generalise well when evaluated across a broad range of benchmarks in contrast
to existing simple heuristics, and suggest that any gains from generating very
specialist heuristics that only work in small areas of the instance space need
to be weighed carefully against the considerable cost of generating these
heuristics.",['cs.NE'],False,,,,"Aerial Reliable Collaborative Communications for Terrestrial Mobile
  Users via Evolutionary Multi-Objective Deep Reinforcement Learning",Beyond the Hype: Benchmarking LLM-Evolved Heuristics for Bin Packing
neg-d2-155,2025-02-17,,2502.12429," One-way quantum computation is a promising approach to achieving universal,
scalable, and fault-tolerant quantum computation. However, a main challenge
lies in the creation of universal, scalable three-dimensional cluster states.
Here, an experimental scheme is proposed for building large-scale canonical
three-dimensional cubic cluster states, which are compatible with the majority
of qubit error-correcting codes, using the spatiospectral modes of an optical
parametric oscillator. Combining with Gottesman-Kitaev-Preskill states, one-way
fault-tolerant optical quantum computation can be achieved with a lower
fault-tolerant squeezing threshold. Our scheme drastically simplify
experimental configurations, paving the way for compact realizations of one-way
fault-tolerant optical quantum computation.",['quant-ph'],2502.09177," Quantum error correction plays a critical role in enabling fault-tolerant
quantum computing by protecting fragile quantum information from noise. While
general-purpose quantum error correction codes are designed to address a wide
range of noise types, they often require substantial resources, making them
impractical for near-term quantum devices. Approximate quantum error correction
provides an alternative by tailoring codes to specific noise environments,
reducing resource demands while maintaining effective error suppression.
Dynamical codes, including Floquet codes, introduce a dynamic approach to
quantum error correction, employing time-dependent operations to stabilize
logical qubits. In this work, we combine the flexibility of dynamical codes
with the efficiency of approximate quantum error correction to offer a
promising avenue for addressing dominant noise in quantum systems. We construct
several approximate dynamical codes using the recently developed strategic code
framework. As a special case, we recover the approximate static codes widely
studied in the existing literature. By analyzing these approximate dynamical
codes through semidefinite programming, we establish the uniqueness and
robustness of the optimal encoding, decoding, and check measurements. We also
develop a temporal Petz recovery map suited to approximate dynamical codes.",['quant-ph'],False,,,,A Compact One-Way Fault-Tolerant Optical Quantum Computation,Approximate Dynamical Quantum Error-Correcting Codes
neg-d2-156,2025-01-13,,2501.07811," Code generation aims to produce code that fulfills requirements written in
natural languages automatically. Large language Models (LLMs) like ChatGPT have
demonstrated promising effectiveness in this area. Nonetheless, these LLMs
often fail to ensure the syntactic and semantic correctness of the generated
code. Recently, researchers proposed multi-agent frameworks that guide LLMs
with different prompts to analyze programming tasks, generate code, perform
testing in a sequential workflow. However, the performance of the workflow is
not robust as the code generation depends on the performance of each agent. To
address this challenge, we propose CodeCoR, a self-reflective multi-agent
framework that evaluates the effectiveness of each agent and their
collaborations. Specifically, for a given task description, four agents in
CodeCoR generate prompts, code, test cases, and repair advice, respectively.
Each agent generates more than one output and prunes away the low-quality ones.
The generated code is tested in the local environment: the code that fails to
pass the generated test cases is sent to the repair agent and the coding agent
re-generates the code based on repair advice. Finally, the code that passes the
most number of generated test cases is returned to users. Our experiments on
four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,
demonstrate that CodeCoR significantly outperforms existing baselines (e.g.,
CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.",['cs.SE'],2502.05108," [Background] Emotional Intelligence (EI) can impact Software Engineering (SE)
outcomes through improved team communication, conflict resolution, and stress
management. SE workers face increasing pressure to develop both technical and
interpersonal skills, as modern software development emphasizes collaborative
work and complex team interactions. Despite EI's documented importance in
professional practice, SE education continues to prioritize technical knowledge
over emotional and social competencies. [Objective] This paper analyzes SE
students' self-perceptions of their EI after a two-month cooperative learning
project, using Mayer and Salovey's four-ability model to examine how students
handle emotions in collaborative development. [Method] We conducted a case
study with 29 SE students organized into four squads within a project-based
learning course, collecting data through questionnaires and focus groups that
included brainwriting and sharing circles, then analyzing the data using
descriptive statistics and open coding. [Results] Students demonstrated
stronger abilities in managing their own emotions compared to interpreting
others' emotional states. Despite limited formal EI training, they developed
informal strategies for emotional management, including structured planning and
peer support networks, which they connected to improved productivity and
conflict resolution. [Conclusion] This study shows how SE students perceive EI
in a collaborative learning context and provides evidence-based insights into
the important role of emotional competencies in SE education.",['cs.SE'],False,,,,"CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code
  Generation","Towards Emotionally Intelligent Software Engineers: Understanding
  Students' Self-Perceptions After a Cooperative Learning Experience"
neg-d2-157,2025-02-21,,2502.15647," We introduce two new families of permutation group polynomials over finite
fields of arbitrary characteristic, which are special types of bivariate local
permutation polynomials. For each family, we explicitly construct their
companions. Furthermore, we precisely determine the total number of permutation
group polynomials equivalent to the proposed families. Moreover, we resolve the
problem of enumerating permutation group polynomials that are equivalent to
$e$-Klenian polynomials over finite fields for $e\geq 1$, a problem previously
noted as nontrivial by Gutierrez and Urroz (2023).",['math.CO'],2502.13518," We generalize the Rubik's cube, together with its group of configurations, to
any abstract regular polytope. After discussing general aspects, we study the
Rubik's simplex of arbitrary dimension and provide a complete description of
the associated group. We sketch an analogous argument for the Rubik's hypercube
as well.",['math.CO'],False,,,,"Bivariate local permutation polynomials, their companions, and related
  enumeration results",Rubik's Abstract Polytopes
neg-d2-158,2025-01-06,,2501.02807," Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.",['cs.CV'],2503.04522," Assessing the quality of automatic image segmentation is crucial in clinical
practice, but often very challenging due to the limited availability of ground
truth annotations. In this paper, we introduce In-Context Reverse
Classification Accuracy (In-Context RCA), a novel framework for automatically
estimating segmentation quality in the absence of ground-truth annotations. By
leveraging recent in-context learning segmentation models and incorporating
retrieval-augmentation techniques to select the most relevant reference images,
our approach enables efficient quality estimation with minimal reference data.
Validated across diverse medical imaging modalities, our method demonstrates
robust performance and computational efficiency, offering a promising solution
for automated quality control in clinical workflows, where fast and reliable
segmentation assessment is essential. The code is available at
https://github.com/mcosarinsky/In-Context-RCA.",['cs.CV'],False,,,,"AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal
  Conditions and Larger Scene","In-Context Reverse Classification Accuracy: Efficient Estimation of
  Segmentation Quality without Ground-Truth"
neg-d2-159,2025-01-24,,2501.15062," EEG-based fatigue monitoring can effectively reduce the incidence of related
traffic accidents. In the past decade, with the advancement of deep learning,
convolutional neural networks (CNN) have been increasingly used for EEG signal
processing. However, due to the data's non-Euclidean characteristics, existing
CNNs may lose important spatial information from EEG, specifically channel
correlation. Thus, we propose the node-holistic graph convolutional network
(NHGNet), a model that uses graphic convolution to dynamically learn each
channel's features. With exact fit attention optimization, the network captures
inter-channel correlations through a trainable adjacency matrix. The
interpretability is enhanced by revealing critical areas of brain activity and
their interrelations in various mental states. In validations on two public
datasets, NHGNet outperforms the SOTAs. Specifically, in the intra-subject,
NHGNet improved detection accuracy by at least 2.34% and 3.42%, and in the
inter-subjects, it improved by at least 2.09% and 15.06%. Visualization
research on the model revealed that the central parietal area plays an
important role in detecting fatigue levels, whereas the frontal and temporal
lobes are essential for maintaining vigilance.",['cs.LG'],2502.05376," Post-training quantization (PTQ) is a promising approach to reducing the
storage and computational requirements of large language models (LLMs) without
additional training cost. Recent PTQ studies have primarily focused on
quantizing only weights to sub-8-bits while maintaining activations at 8-bits
or higher. Accurate sub-8-bit quantization for both weights and activations
without relying on quantization-aware training remains a significant challenge.
We propose a novel quantization method called block clustered quantization
(BCQ) wherein each operand tensor is decomposed into blocks (a block is a group
of contiguous scalars), blocks are clustered based on their statistics, and a
dedicated optimal quantization codebook is designed for each cluster. As a
specific embodiment of this approach, we propose a PTQ algorithm called
Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block
clustering and codebook design to greedily minimize the quantization mean
squared error. When weight and activation scalars are encoded to W4A4 format
(with 0.5-bits of overhead for storing scaling factors and codebook selectors),
we advance the current state-of-the-art by demonstrating <1% loss in inference
accuracy across several LLMs and downstream tasks.",['cs.LG'],False,,,,"Exact Fit Attention in Node-Holistic Graph Convolutional Network for
  Improved EEG-Based Driver Fatigue Detection",BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference
neg-d2-160,2025-03-10,,2503.07307," While diffusion models have achieved remarkable progress in style transfer
tasks, existing methods typically rely on fine-tuning or optimizing pre-trained
models during inference, leading to high computational costs and challenges in
balancing content preservation with style integration. To address these
limitations, we introduce AttenST, a training-free attention-driven style
transfer framework. Specifically, we propose a style-guided self-attention
mechanism that conditions self-attention on the reference style by retaining
the query of the content image while substituting its key and value with those
from the style image, enabling effective style feature integration. To mitigate
style information loss during inversion, we introduce a style-preserving
inversion strategy that refines inversion accuracy through multiple resampling
steps. Additionally, we propose a content-aware adaptive instance
normalization, which integrates content statistics into the normalization
process to optimize style fusion while mitigating the content degradation.
Furthermore, we introduce a dual-feature cross-attention mechanism to fuse
content and style features, ensuring a harmonious synthesis of structural
fidelity and stylistic expression. Extensive experiments demonstrate that
AttenST outperforms existing methods, achieving state-of-the-art performance in
style transfer dataset.",['cs.CV'],2503.08005," 3D object reconstruction from single-view image is a fundamental task in
computer vision with wide-ranging applications. Recent advancements in Large
Reconstruction Models (LRMs) have shown great promise in leveraging multi-view
images generated by 2D diffusion models to extract 3D content. However,
challenges remain as 2D diffusion models often struggle to produce dense images
with strong multi-view consistency, and LRMs tend to amplify these
inconsistencies during the 3D reconstruction process. Addressing these issues
is critical for achieving high-quality and efficient 3D reconstruction. In this
paper, we present CDI3D, a feed-forward framework designed for efficient,
high-quality image-to-3D generation with view interpolation. To tackle the
aforementioned challenges, we propose to integrate 2D diffusion-based view
interpolation into the LRM pipeline to enhance the quality and consistency of
the generated mesh. Specifically, our approach introduces a Dense View
Interpolation (DVI) module, which synthesizes interpolated images between main
views generated by the 2D diffusion model, effectively densifying the input
views with better multi-view consistency. We also design a tilt camera pose
trajectory to capture views with different elevations and perspectives.
Subsequently, we employ a tri-plane-based mesh reconstruction strategy to
extract robust tokens from these interpolated and original views, enabling the
generation of high-quality 3D meshes with superior texture and geometry.
Extensive experiments demonstrate that our method significantly outperforms
previous state-of-the-art approaches across various benchmarks, producing 3D
content with enhanced texture fidelity and geometric accuracy.",['cs.CV'],False,,,,"AttenST: A Training-Free Attention-Driven Style Transfer Framework with
  Pre-Trained Diffusion Models",CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction
neg-d2-161,2025-03-03,,2503.01599," Of the ~25 directly imaged planets to date, all are younger than 500Myr and
all but 6 are younger than 100Myr. Eps Ind A (HD209100, HIP108870) is a K5V
star of roughly solar age (recently derived as 3.7-5.7Gyr and
3.5$^{+0.8}_{-1.3}$Gyr). A long-term radial velocity trend as well as an
astrometric acceleration led to claims of a giant planet orbiting the nearby
star (3.6384$\pm$0.0013pc). Here we report JWST coronagraphic images that
reveal a giant exoplanet which is consistent with these radial and astrometric
measurements, but inconsistent with the previously claimed planet properties.
The new planet has temperature ~275K, and is remarkably bright at 10.65um and
15.50um. Non-detections between 3.5-5um indicate an unknown opacity source in
the atmosphere, possibly suggesting a high metallicity, high carbon-to-oxygen
ratio planet. The best-fit temperature of the planet is consistent with
theoretical thermal evolution models, which are previously untested at this
temperature range. The data indicates that this is likely the only giant planet
in the system and we therefore refer to it as ``b"", despite it having
significantly different orbital properties than the previously claimed planet
``b"".",['astro-ph.EP'],2503.10441," We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density
sub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of
observational data (2009-2022), our study incorporates 7 new ground-based
photometric transit observations, three sectors of Transiting Exoplanet Survey
Satellite (TESS) data, and 23 previously published light curves. A total of 46
light curves were analyzed using various analytical models, such as linear,
orbital decay, apsidal precession, and sinusoidal models to investigate the
presence of additional planets. The stellar tidal quality factor ($Q_\star'
\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay
model an unlikely explanation. The apsidal precession model with a $\chi_r^2$
of 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession
rate of 0.0045 rad/epoch. Frequency analysis using the Generalized Lomb-Scargle
(GLS) periodogram identified a significant periodic signal at 0.00415
cycles/day (FAP = 5.1$\times$10$^{-6}$ %), suggesting the influence of an
additional planetary companion. The sinusoidal model provides the lowest
reduced chi-squared value ($\chi_r^2$) of 3.2. Sinusoidal fitting of the timing
residuals estimated this companion to have a mass of approximately 0.02 $M_J$ ,
assuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.
Additionally, the Applegate mechanism, with an amplitude much smaller than the
observed TTV amplitude of 156 s, confirms that stellar activity is not
responsible for the observed variations.",['astro-ph.EP'],False,,,,A temperate super-Jupiter imaged with JWST in the mid-infrared,Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b
neg-d2-162,2025-03-02,,2503.0074," In this paper, we present FaceShot, a novel training-free portrait animation
framework designed to bring any character into life from any driven video
without fine-tuning or retraining. We achieve this by offering precise and
robust reposed landmark sequences from an appearance-guided landmark matching
module and a coordinate-based landmark retargeting module. Together, these
components harness the robust semantic correspondences of latent diffusion
models to produce facial motion sequence across a wide range of character
types. After that, we input the landmark sequences into a pre-trained
landmark-driven animation model to generate animated video. With this powerful
generalization capability, FaceShot can significantly extend the application of
portrait animation by breaking the limitation of realistic portrait landmark
detection for any stylized character and driven video. Also, FaceShot is
compatible with any landmark-driven animation model, significantly improving
overall performance. Extensive experiments on our newly constructed character
benchmark CharacBench confirm that FaceShot consistently surpasses
state-of-the-art (SOTA) approaches across any character domain. More results
are available at our project website https://faceshot2024.github.io/faceshot/.",['cs.CV'],2503.03144," Spiking Neural Networks (SNNs), inspired by the human brain, offer
significant computational efficiency through discrete spike-based information
transfer. Despite their potential to reduce inference energy consumption, a
performance gap persists between SNNs and Artificial Neural Networks (ANNs),
primarily due to current training methods and inherent model limitations. While
recent research has aimed to enhance SNN learning by employing knowledge
distillation (KD) from ANN teacher networks, traditional distillation
techniques often overlook the distinctive spatiotemporal properties of SNNs,
thus failing to fully leverage their advantages. To overcome these challenge,
we propose a novel logit distillation method characterized by temporal
separation and entropy regularization. This approach improves existing SNN
distillation techniques by performing distillation learning on logits across
different time steps, rather than merely on aggregated output features.
Furthermore, the integration of entropy regularization stabilizes model
optimization and further boosts the performance. Extensive experimental results
indicate that our method surpasses prior SNN distillation strategies, whether
based on logit distillation, feature distillation, or a combination of both.
The code will be available on GitHub.",['cs.CV'],False,,,,FaceShot: Bring Any Character into Life,"Temporal Separation with Entropy Regularization for Knowledge
  Distillation in Spiking Neural Networks"
neg-d2-163,2025-03-20,,2503.16407," Deep Feynman-Kac method was first introduced to solve parabolic partial
differential equations(PDE) by Beck et al. (SISC, V.43, 2021), named Deep
Splitting method since they trained the Neural Networks step by step in the
time direction. In this paper, we propose a new training approach with two
different features. Firstly, neural networks are trained at all time steps
globally, instead of step by step. Secondly, the training data are generated in
a new way, in which the method is consistent with a direct Monte Carlo scheme
when dealing with a linear parabolic PDE. Numerical examples show that our
method has significant improvement both in efficiency and accuracy.",['cs.CE'],2503.16407," Deep Feynman-Kac method was first introduced to solve parabolic partial
differential equations(PDE) by Beck et al. (SISC, V.43, 2021), named Deep
Splitting method since they trained the Neural Networks step by step in the
time direction. In this paper, we propose a new training approach with two
different features. Firstly, neural networks are trained at all time steps
globally, instead of step by step. Secondly, the training data are generated in
a new way, in which the method is consistent with a direct Monte Carlo scheme
when dealing with a linear parabolic PDE. Numerical examples show that our
method has significant improvement both in efficiency and accuracy.",['cs.CE'],False,,,,"Deep Feynman-Kac Methods for High-dimensional Semilinear Parabolic
  Equations: Revisit","Deep Feynman-Kac Methods for High-dimensional Semilinear Parabolic
  Equations: Revisit"
neg-d2-164,2025-01-10,,2501.05959," The restoration of nonlinearly distorted audio signals, alongside the
identification of the applied memoryless nonlinear operation, is studied. The
paper focuses on the difficult but practically important case in which both the
nonlinearity and the original input signal are unknown. The proposed method
uses a generative diffusion model trained unconditionally on guitar or speech
signals to jointly model and invert the nonlinear system at inference time.
Both the memoryless nonlinear function model and the restored audio signal are
obtained as output. Successful example case studies are presented including
inversion of hard and soft clipping, digital quantization, half-wave
rectification, and wavefolding nonlinearities. Our results suggest that, out of
the nonlinear functions tested here, the cubic Catmull-Rom spline is best
suited to approximating these nonlinearities. In the case of guitar recordings,
comparisons with informed and supervised methods show that the proposed blind
method is at least as good as they are in terms of objective metrics.
Experiments on distorted speech show that the proposed blind method outperforms
general-purpose speech enhancement techniques and restores the original voice
quality. The proposed method can be applied to audio effects modeling,
restoration of music and speech recordings, and characterization of analog
recording media.",['eess.AS'],2501.05959," The restoration of nonlinearly distorted audio signals, alongside the
identification of the applied memoryless nonlinear operation, is studied. The
paper focuses on the difficult but practically important case in which both the
nonlinearity and the original input signal are unknown. The proposed method
uses a generative diffusion model trained unconditionally on guitar or speech
signals to jointly model and invert the nonlinear system at inference time.
Both the memoryless nonlinear function model and the restored audio signal are
obtained as output. Successful example case studies are presented including
inversion of hard and soft clipping, digital quantization, half-wave
rectification, and wavefolding nonlinearities. Our results suggest that, out of
the nonlinear functions tested here, the cubic Catmull-Rom spline is best
suited to approximating these nonlinearities. In the case of guitar recordings,
comparisons with informed and supervised methods show that the proposed blind
method is at least as good as they are in terms of objective metrics.
Experiments on distorted speech show that the proposed blind method outperforms
general-purpose speech enhancement techniques and restores the original voice
quality. The proposed method can be applied to audio effects modeling,
restoration of music and speech recordings, and characterization of analog
recording media.",['eess.AS'],False,,,,"Estimation and Restoration of Unknown Nonlinear Distortion using
  Diffusion","Estimation and Restoration of Unknown Nonlinear Distortion using
  Diffusion"
neg-d2-165,2025-01-31,,2501.19084," In this work, we propose a method that leverages CLIP feature distillation,
achieving efficient 3D segmentation through language guidance. Unlike previous
methods that rely on multi-scale CLIP features and are limited by processing
speed and storage requirements, our approach aims to streamline the workflow by
directly and effectively distilling dense CLIP features, thereby achieving
precise segmentation of 3D scenes using text. To achieve this, we introduce an
adapter module and mitigate the noise issue in the dense CLIP feature
distillation process through a self-cross-training strategy. Moreover, to
enhance the accuracy of segmentation edges, this work presents a low-rank
transient query attention mechanism. To ensure the consistency of segmentation
for similar colors under different viewpoints, we convert the segmentation task
into a classification task through label volume, which significantly improves
the consistency of segmentation in color-similar areas. We also propose a
simplified text augmentation strategy to alleviate the issue of ambiguity in
the correspondence between CLIP features and text. Extensive experimental
results show that our method surpasses current state-of-the-art technologies in
both training speed and performance. Our code is available on:
https://github.com/xingy038/Laser.git.",['cs.CV'],2503.16591," Monocular 3D estimation is crucial for visual perception. However, current
methods fall short by relying on oversimplified assumptions, such as pinhole
camera models or rectified images. These limitations severely restrict their
general applicability, causing poor performance in real-world scenarios with
fisheye or panoramic images and resulting in substantial context loss. To
address this, we present UniK3D, the first generalizable method for monocular
3D estimation able to model any camera. Our method introduces a spherical 3D
representation which allows for better disentanglement of camera and scene
geometry and enables accurate metric 3D reconstruction for unconstrained camera
models. Our camera component features a novel, model-independent representation
of the pencil of rays, achieved through a learned superposition of spherical
harmonics. We also introduce an angular loss, which, together with the camera
module design, prevents the contraction of the 3D outputs for wide-view
cameras. A comprehensive zero-shot evaluation on 13 diverse datasets
demonstrates the state-of-the-art performance of UniK3D across 3D, depth, and
camera metrics, with substantial gains in challenging large-field-of-view and
panoramic settings, while maintaining top accuracy in conventional pinhole
small-field-of-view domains. Code and models are available at
github.com/lpiccinelli-eth/unik3d .",['cs.CV'],False,,,,Laser: Efficient Language-Guided Segmentation in Neural Radiance Fields,UniK3D: Universal Camera Monocular 3D Estimation
neg-d2-166,2025-01-08,,2501.04698," Text-to-video generation has made remarkable advancements through diffusion
models. However, Multi-Concept Video Customization (MCVC) remains a significant
challenge. We identify two key challenges in this task: 1) the identity
decoupling problem, where directly adopting existing customization methods
inevitably mix attributes when handling multiple concepts simultaneously, and
2) the scarcity of high-quality video-entity pairs, which is crucial for
training such a model that represents and decouples various concepts well. To
address these challenges, we introduce ConceptMaster, an innovative framework
that effectively tackles the critical issues of identity decoupling while
maintaining concept fidelity in customized videos. Specifically, we introduce a
novel strategy of learning decoupled multi-concept embeddings that are injected
into the diffusion models in a standalone manner, which effectively guarantees
the quality of customized videos with multiple identities, even for highly
similar visual concepts. To further overcome the scarcity of high-quality MCVC
data, we carefully establish a data construction pipeline, which enables
systematic collection of precise multi-concept video-entity data across diverse
concepts. A comprehensive benchmark is designed to validate the effectiveness
of our model from three critical dimensions: concept fidelity, identity
decoupling ability, and video generation quality across six different concept
composition scenarios. Extensive experiments demonstrate that our ConceptMaster
significantly outperforms previous approaches for this task, paving the way for
generating personalized and semantically accurate videos across multiple
concepts.",['cs.CV'],2501.12976," In commonly used sub-quadratic complexity modules, linear attention benefits
from simplicity and high parallelism, making it promising for image synthesis
tasks. However, the architectural design and learning strategy for linear
attention remain underexplored in this field. In this paper, we offer a suite
of ready-to-use solutions for efficient linear diffusion Transformers. Our core
contributions include: (1) Simplified Linear Attention using few heads,
observing the free-lunch effect of performance without latency increase. (2)
Weight inheritance from a fully pre-trained diffusion Transformer: initializing
linear Transformer using pre-trained diffusion Transformer and loading all
parameters except for those related to linear attention. (3) Hybrid knowledge
distillation objective: using a pre-trained diffusion Transformer to help the
training of the student linear Transformer, supervising not only the predicted
noise but also the variance of the reverse diffusion process. These guidelines
lead to our proposed Linear Diffusion Transformer (LiT), an efficient
text-to-image Transformer that can be deployed offline on a laptop. Experiments
show that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT
achieves highly competitive FID while reducing training steps by 80% and 77%
compared to DiT. LiT also rivals methods based on Mamba or Gated Linear
Attention. Besides, for text-to-image generation, LiT allows for the rapid
synthesis of up to 1K resolution photorealistic images. Project page:
https://techmonsterwang.github.io/LiT/.",['cs.CV'],False,,,,"ConceptMaster: Multi-Concept Video Customization on Diffusion
  Transformer Models Without Test-Time Tuning","LiT: Delving into a Simplified Linear Diffusion Transformer for Image
  Generation"
neg-d2-167,2025-01-08,,2501.0444," Rotated object detection has made significant progress in the optical remote
sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are
laggard behind, primarily due to the absence of a large-scale dataset.
Annotating such a dataset is inefficient and costly. A promising solution is to
employ a weakly supervised model (e.g., trained with available horizontal boxes
only) to generate pseudo-rotated boxes for reference before manual calibration.
Unfortunately, the existing weakly supervised models exhibit limited accuracy
in predicting the object's angle. Previous works attempt to enhance angle
prediction by using angle resolvers that decouple angles into cosine and sine
encodings. In this work, we first reevaluate these resolvers from a unified
perspective of dimension mapping and expose that they share the same
shortcomings: these methods overlook the unit cycle constraint inherent in
these encodings, easily leading to prediction biases. To address this issue, we
propose the Unit Cycle Resolver, which incorporates a unit circle constraint
loss to improve angle prediction accuracy. Our approach can effectively improve
the performance of existing state-of-the-art weakly supervised methods and even
surpasses fully supervised models on existing optical benchmarks (i.e.,
DOTA-v1.0 dataset). With the aid of UCR, we further annotate and introduce
RSAR, the largest multi-class rotated SAR object detection dataset to date.
Extensive experiments on both RSAR and optical datasets demonstrate that our
UCR enhances angle prediction accuracy. Our dataset and code can be found at:
https://github.com/zhasion/RSAR.",['cs.CV'],2502.19769," Significant advancements have been achieved in the realm of understanding
poses and interactions of two hands manipulating an object. The emergence of
augmented reality (AR) and virtual reality (VR) technologies has heightened the
demand for real-time performance in these applications. However, current
state-of-the-art models often exhibit promising results at the expense of
substantial computational overhead. In this paper, we present a query-optimized
real-time Transformer (QORT-Former), the first Transformer-based real-time
framework for 3D pose estimation of two hands and an object. We first limit the
number of queries and decoders to meet the efficiency requirement. Given
limited number of queries and decoders, we propose to optimize queries which
are taken as input to the Transformer decoder, to secure better accuracy: (1)
we propose to divide queries into three types (a left hand query, a right hand
query and an object query) and enhance query features (2) by using the contact
information between hands and an object and (3) by using three-step update of
enhanced image and query features with respect to one another. With proposed
methods, we achieved real-time pose estimation performance using just 108
queries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing
state-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right
hand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)
and 10.4% (object), our method excels in accuracy. Additionally, it sets the
state-of-the-art in interaction recognition, maintaining real-time efficiency
with an off-the-shelf action recognition module.",['cs.CV'],False,,,,RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark,"QORT-Former: Query-optimized Real-time Transformer for Understanding Two
  Hands Manipulating Objects"
neg-d2-168,2025-02-05,,2502.03734," Estimating free-energy differences using nonequilibrium work relations, such
as the Jarzynski equality, is hindered by poor convergence when work
fluctuations are large. For systems governed by overdamped Langevin dynamics,
we propose the counterintuitive approach of adding noise in order to increase
the precision of such calculations. By introducing additional stochastic
fluctuations to the system and rescaling its potential energy, we leave the
thermodynamics of the system unchanged while increasing its relaxation rate.
For a given time-dependent protocol this modification reduces dissipated work,
leading to more accurate free-energy estimates. We demonstrate this principle
using computer simulations applied to two model systems. However, the regime of
applicability of this strategy is likely limited, because it requires control
of the system's potential energy in a way that is feasible in only a few
experimental settings.",['cond-mat.stat-mech'],2502.19561," This work explores the thermodynamic performance of a quantum Stirling heat
engine implemented with an anisotropic spin-1 Heisenberg dimer as the working
medium. Using the Hamiltonian of the system, we analyze the interplay of
anisotropy, magnetic field, and exchange interactions and their influence on
the energy spectrum and the quantum level crossing. Our results reveal that
double-degenerate point (DDP) and a triple-degenerate point (TDP) play pivotal
roles in shaping the operational regimes and efficiency of the quantum Stirling
engine. At those points, the Carnot efficiency reaches higher work output and
enhanced stability, making it a robust candidate for optimal thermodynamic
performance. These findings highlight the potential of anisotropic spin systems
as viable platforms for quantum heat engines and contribute to advancing the
field of quantum thermodynamics.",['cond-mat.stat-mech'],False,,,,Improving noisy free-energy measurements by adding more noise,"Quantum Level-Crossing Induced by Anisotropy in Spin-1 Heisenberg
  Dimers: Applications to Quantum Stirling Engines"
neg-d2-169,2025-03-08,,2503.06126," We perform a complete analysis of the limiting behaviour of a class of
quasilinear problems with Dirichlet boundary data g. We show that the Lipschitz
constant of g plays a role in controlling the Gamma-convergence of the natural
energies. However the solutions converge uniformly to solution of a limiting
equation irrelevant to the Lipschitz constant of g. The limiting equation has
no coercivity in u. We prove that the limiting equation admits a weak
comparison principle and has a unique viscosity solution. We also obtain a
Poincare inequality in the Sobolev-Orlicz space for discontinuous operator,
which paves the way for our study of an extremal problem where its operator
becomes unbounded in a subdomain. Upon giving proper meaning to its solution,
we show that the extremal problem has a unique solution. It turns out the
solution has sufficient continuity, although operator is discontinuous. In the
appendix we provide some technical inequalities which play crucial roles in the
proof of uniqueness and we believe will be of independent interest.",['math.AP'],2503.03209," Isolated skyrmion solutions to the 2D Landau-Lifshitz equation with the
Dzyaloshinskii-Moriya interaction, Zeeman interaction, and easy-plane
anisotropy are considered. In a wide range of parameters illustrating the
various interaction strengths, we construct exact solutions and examine their
monotonicity, exponential decay, and stability using a careful mathematical
analysis. We also estimate the distance between the constructed solutions and
the harmonic maps by exploiting the structure of the linearized equation and by
proving a resolvent estimate for the linearized operator that is uniform in
extra implicit potentials.",['math.AP'],False,,,,Limit of quasilinear equations and related extremal problems,"Global perturbation of isolated equivariant chiral skyrmions from the
  harmonic maps"
neg-d2-170,2025-01-26,,2501.15722," Implicit neural representations (INRs) have become an important method for
encoding various data types, such as 3D objects or scenes, images, and videos.
They have proven to be particularly effective at representing 3D content, e.g.,
3D scene reconstruction from 2D images, novel 3D content creation, as well as
the representation, interpolation, and completion of 3D shapes. With the
widespread generation of 3D data in an INR format, there is a need to support
effective organization and retrieval of INRs saved in a data store. A key
aspect of retrieval and clustering of INRs in a data store is the formulation
of similarity between INRs that would, for example, enable retrieval of similar
INRs using a query INR. In this work, we propose INRet, a method for
determining similarity between INRs that represent shapes, thus enabling
accurate retrieval of similar shape INRs from an INR data store. INRet flexibly
supports different INR architectures such as INRs with octree grids, triplanes,
and hash grids, as well as different implicit functions including
signed/unsigned distance function and occupancy field. We demonstrate that our
method is more general and accurate than the existing INR retrieval method,
which only supports simple MLP INRs and requires the same architecture between
the query and stored INRs. Furthermore, compared to converting INRs to other
representations (e.g., point clouds or multi-view images) for 3D shape
retrieval, INRet achieves higher accuracy while avoiding the conversion
overhead.",['cs.LG'],2503.03545," Patients with semantic dementia (SD) present with remarkably consistent
atrophy of neurons in the anterior temporal lobe and behavioural impairments,
such as graded loss of category knowledge. While relearning of lost knowledge
has been shown in acute brain injuries such as stroke, it has not been widely
supported in chronic cognitive diseases such as SD. Previous research has shown
that deep linear artificial neural networks exhibit stages of semantic learning
akin to humans. Here, we use a deep linear network to test the hypothesis that
relearning during disease progression rather than particular atrophy cause the
specific behavioural patterns associated with SD. After training the network to
generate the common semantic features of various hierarchically organised
objects, neurons are successively deleted to mimic atrophy while retraining the
model. The model with relearning and deleted neurons reproduced errors specific
to SD, including prototyping errors and cross-category confusions. This
suggests that relearning is necessary for artificial neural networks to
reproduce the behavioural patterns associated with SD in the absence of
\textit{output} non-linearities. Our results support a theory of SD progression
that results from continuous relearning of lost information. Future research
should revisit the role of relearning as a contributing factor to cognitive
diseases.",['cs.LG'],False,,,,INRet: A General Framework for Accurate Retrieval of INRs for Shapes,Revisiting the Role of Relearning in Semantic Dementia
neg-d2-171,2025-01-23,,2501.13738," We compute the Euler characteristic of the moduli space of quadratic rational
maps with a periodic marked critical point of a given period.",['math.DS'],2501.04294," We construct a $C^\infty$-flow on the four-dimensional sphere whose
nonwandering set contains an attached hyperbolic singularity yet possesses the
standard shadowing property. This gives a counterexample to a conjecture given
by Arbieto, L\'{o}pez, Rego and S\'{a}nchez (Math. Annalen 390:417-437).",['math.DS'],False,,,,"Asymptotics of Transversality in Periodic Curves of Quadratic Rational
  Maps",A shadowable chain recurrent set with an attached hyperbolic singularity
neg-d2-172,2025-02-10,,2502.06397," A novel unsupervised learning method is proposed in this paper for
biclustering large-dimensional matrix-valued time series based on an entirely
new latent two-way factor structure. Each block cluster is characterized by its
own row and column cluster-specific factors in addition to some common matrix
factors which impact on all the matrix time series. We first estimate the
global loading spaces by projecting the observation matrices onto the row or
column loading space corresponding to common factors. The loading spaces for
cluster-specific factors are then further recovered by projecting the
observation matrices onto the orthogonal complement space of the estimated
global loading spaces. To identify the latent row/column clusters
simultaneously for matrix-valued time series, we provide a $K$-means algorithm
based on the estimated row/column factor loadings of the cluster-specific weak
factors. Theoretically, we derive faster convergence rates for global loading
matrices than those of the state-of-the-art methods available in the literature
under mild conditions. We also propose an one-pass eigenvalue-ratio method to
estimate the numbers of global and cluster-specific factors. The consistency
with explicit convergence rates is also established for the estimators of the
local loading matrices, the factor numbers and the latent cluster memberships.
Numerical experiments with both simulated data as well as a real data example
are also reported to illustrate the usefulness of our proposed method.",['stat.ME'],2503.13689," Traditional hypothesis testing methods for differences in binomial
proportions can either be too liberal (Wald test) or overly conservative
(Fisher's exact test), especially in small samples. Regulators favour
conservative approaches for robust type I error control, though excessive
conservatism may significantly reduce statistical power. We offer fundamental
theoretical contributions that extend an approach proposed in 1969, resulting
in the derivation of a family of exact tests designed to maximize a specific
type of power. We establish theoretical guarantees for controlling type I error
despite the discretization of the null parameter space. This theoretical
advancement is supported by a comprehensive series of experiments to
empirically quantify the power advantages compared to traditional hypothesis
tests. The approach determines the rejection region through a binary decision
for each outcome dataset and uses integer programming to find an optimal
decision boundary that maximizes power subject to type I error constraints. Our
analysis provides new theoretical properties and insights into this approach's
comparative advantages. When optimized for average power over all possible
parameter configurations under the alternative, the method exhibits remarkable
robustness, performing optimally or near-optimally across specific alternatives
while maintaining exact type I error control. The method can be further
customized for particular prior beliefs by using a weighted average. The
findings highlight both the method's practical utility and how techniques from
combinatorial optimization can enhance statistical methodology.",['stat.ME'],False,,,,"Factor Modelling for Biclustering Large-dimensional Matrix-valued Time
  Series","Exact statistical tests using integer programming: Leveraging an
  overlooked approach for maximizing power for differences between binomial
  proportions"
neg-d2-173,2025-03-11,,2503.08629," Due to their photonic components, exciton-polariton systems provide a
convenient platform to study the coherence properties of weakly-interacting
Bose gases and the Bose-Einstein condensate transition. In particular, optical
interferometry enables the measurement of the first-order coherence function
which provides insight into the phase of the system. In this paper, we analyze
the buildup of coherence in finite-sized, noninteracting, equilibrium Bose
gases through the condensate fraction and the related coherent fraction,
defined via the first-order coherence function. Our results provide a baseline
to compare against experimental data. Discrepancies may indicate where
interacting or nonequilibrium models are necessary to describe the system. In
the normal phase, before the Bose-Einstein condensate transition, Bose gases
exhibit partial spatial and temporal coherence. This significantly alters the
paraxial propagation and interference of optical signals from exciton-polariton
systems. Therefore, we also analyze diffraction related to the introduction of
apertures and time-delay between interferometry arms, given the partial
coherence of the source. Comparison to experiment shows remarkable agreement
with the noninteracting Bose gas theory, even approaching the quasi-condensate
regime.",['cond-mat.quant-gas'],2503.08629," Due to their photonic components, exciton-polariton systems provide a
convenient platform to study the coherence properties of weakly-interacting
Bose gases and the Bose-Einstein condensate transition. In particular, optical
interferometry enables the measurement of the first-order coherence function
which provides insight into the phase of the system. In this paper, we analyze
the buildup of coherence in finite-sized, noninteracting, equilibrium Bose
gases through the condensate fraction and the related coherent fraction,
defined via the first-order coherence function. Our results provide a baseline
to compare against experimental data. Discrepancies may indicate where
interacting or nonequilibrium models are necessary to describe the system. In
the normal phase, before the Bose-Einstein condensate transition, Bose gases
exhibit partial spatial and temporal coherence. This significantly alters the
paraxial propagation and interference of optical signals from exciton-polariton
systems. Therefore, we also analyze diffraction related to the introduction of
apertures and time-delay between interferometry arms, given the partial
coherence of the source. Comparison to experiment shows remarkable agreement
with the noninteracting Bose gas theory, even approaching the quasi-condensate
regime.",['cond-mat.quant-gas'],False,,,,Optical probes of coherence in two dimensional Bose gases of polaritons,Optical probes of coherence in two dimensional Bose gases of polaritons
neg-d2-174,2025-02-25,,2502.18739," Real time, singleshot multispectral imaging systems are crucial for
environment monitoring and biomedical imaging. Most singleshot multispectral
imagers rely on complex computational backends, which precludes real time
operations. In this work, we leverage the spectral selectivity afforded by
engineered photonic materials to perform bulk of the multispectral data
extraction in the optical domain, thereby circumventing the need for heavy
backend computation. We use our imager to extract multispectral data for two
real world objects at 8 predefined spectral channels in the 400 to 900 nm
wavelength range. For both objects, an RGB image constructed using extracted
multispectral data shows good agreement with an image taken using a phone
camera, thereby validating our imaging approach. We believe that the proposed
system can provide new avenues for the development of highly compact and low
latency multispectral imaging technologies.",['physics.optics'],2501.0735," Photochemistry in the earth's atmosphere is driven by the sun, continuously
altering the concentration and spatial distribution of pollutants. Precisely
monitoring their atmospheric abundance relies predominantly on optical sensing,
which requires the knowledge of exact absorption cross sections. One key
pollutant which impacts many photochemical reaction-pathways is formaldehyde.
Agreement on formaldehyde absolute absorption cross section remains elusive in
the photochemically-relevant ultraviolet spectral region, hampering sensitive
concentration tracking. Here, we introduce free-running ultraviolet dual comb
spectroscopy, combining high spectral resolution (1 GHz), broad spectral
coverage (12 THz), and fast acquisition speed (500 ms), as a novel method for
absolute absorption cross section determination with unprecedented fidelity.
Within this bandwidth, our method uncovers almost one order of magnitude more
rovibrational transitions than detected before which leads to refined
rotational constants for high-level quantum simulations of molecular
eigenstates. This ultra-resolution method can be generalized to provide a
universal tool for fast electronic fingerprinting of atmospherically-relevant
species, both for sensing applications and to benchmark improvements of
ab-initio quantum theory.",['physics.optics'],False,,,,Singleshot Multispectral Imaging via a Chromatic Metalens Array,Ultra-resolution photochemical sensing
neg-d2-175,2025-03-01,,2503.00485," Graph spectra are an important class of structural features on graphs that
have shown promising results in enhancing Graph Neural Networks (GNNs). Despite
their widespread practical use, the theoretical understanding of the power of
spectral invariants -- particularly their contribution to GNNs -- remains
incomplete. In this paper, we address this fundamental question through the
lens of homomorphism expressivity, providing a comprehensive and quantitative
analysis of the expressive power of spectral invariants. Specifically, we prove
that spectral invariant GNNs can homomorphism-count exactly a class of specific
tree-like graphs which we refer to as parallel trees. We highlight the
significance of this result in various contexts, including establishing a
quantitative expressiveness hierarchy across different architectural variants,
offering insights into the impact of GNN depth, and understanding the subgraph
counting capabilities of spectral invariant GNNs. In particular, our results
significantly extend Arvind et al. (2024) and settle their open questions.
Finally, we generalize our analysis to higher-order GNNs and answer an open
question raised by Zhang et al. (2024).",['cs.LG'],2501.16656," Data mining in transportation networks (DMTNs) refers to using diverse types
of spatio-temporal data for various transportation tasks, including pattern
analysis, traffic prediction, and traffic controls. Graph neural networks
(GNNs) are essential in many DMTN problems due to their capability to represent
spatial correlations between entities. Between 2016 and 2024, the notable
applications of GNNs in DMTNs have extended to multiple fields such as traffic
prediction and operation. However, existing reviews have primarily focused on
traffic prediction tasks. To fill this gap, this study provides a timely and
insightful summary of GNNs in DMTNs, highlighting new progress in prediction
and operation from academic and industry perspectives since 2023. First, we
present and analyze various DMTN problems, followed by classical and recent GNN
models. Second, we delve into key works in three areas: (1) traffic prediction,
(2) traffic operation, and (3) industry involvement, such as Google Maps, Amap,
and Baidu Maps. Along these directions, we discuss new research opportunities
based on the significance of transportation problems and data availability.
Finally, we compile resources such as data, code, and other learning materials
to foster interdisciplinary communication. This review, driven by recent trends
in GNNs in DMTN studies since 2023, could democratize abundant datasets and
efficient GNN methods for various transportation problems including prediction
and operation.",['cs.LG'],False,,,,Homomorphism Expressivity of Spectral Invariant Graph Neural Networks,"Data Mining in Transportation Networks with Graph Neural Networks: A
  Review and Outlook"
neg-d2-176,2025-03-19,,2503.15222," With the massive surge in ML models on platforms like Hugging Face, users
often lose track and struggle to choose the best model for their downstream
tasks, frequently relying on model popularity indicated by download counts,
likes, or recency. We investigate whether this popularity aligns with actual
model performance and how the comprehensiveness of model documentation
correlates with both popularity and performance. In our study, we evaluated a
comprehensive set of 500 Sentiment Analysis models on Hugging Face. This
evaluation involved massive annotation efforts, with human annotators
completing nearly 80,000 annotations, alongside extensive model training and
evaluation. Our findings reveal that model popularity does not necessarily
correlate with performance. Additionally, we identify critical inconsistencies
in model card reporting: approximately 80\% of the models analyzed lack
detailed information about the model, training, and evaluation processes.
Furthermore, about 88\% of model authors overstate their models' performance in
the model cards. Based on our findings, we provide a checklist of guidelines
for users to choose good models for downstream tasks.",['cs.CL'],2502.11405," Despite being pretrained on multilingual corpora, large language models
(LLMs) exhibit suboptimal performance on low-resource languages. Recent
approaches have leveraged multilingual encoders alongside LLMs by introducing
trainable parameters connecting the two models. However, these methods
typically focus on the encoder's output, overlooking valuable information from
other layers. We propose \aname (\mname), a framework that integrates
representations from all encoder layers, coupled with the \attaname mechanism
to enable layer-wise interaction between the LLM and the multilingual encoder.
Extensive experiments on multilingual reasoning tasks, along with analyses of
learned representations, show that our approach consistently outperforms
existing baselines.",['cs.CL'],False,,,,"Model Hubs and Beyond: Analyzing Model Popularity, Performance, and
  Documentation","LayAlign: Enhancing Multilingual Reasoning in Large Language Models via
  Layer-Wise Adaptive Fusion and Alignment Strategy"
neg-d2-177,2025-03-12,,2503.09323," In the present paper, we establish a multiplicity result for a following
class of nonlocal Neumann eigenvalue problems involving the fractional
p-Laplacian.
  \begin{align} \begin{cases}
  (-\Delta)^{s}_{p}u + a(x) \abs{u}^{p-2}u =\lambda h(x,u) & \text {in }
\Omega,
  \mathcal{N}_{s,p}u=0 & \text {in } \mathbb{R}^N \setminus \overline{\Omega},
  \end{cases}
  \end{align}
  Precisely, we demonstrate the existence of an open interval for positive
eigenvalues $\lambda$, for which the problem has at least three non-zero
solutions in $W^{s,p}_{\Omega}.$",['math.AP'],2501.11523," In this work, our interest lies in proving the existence of solutions to the
following Fractional Lane-Emden Hamiltonian system: $$ \begin{cases}
(-\Delta)^s u = H_v(x,u,v) & \text{in }\Omega,\\ (-\Delta)^s v = H_u(x,u,v) &
\text{in }\Omega,\\ u=v=0 & \text{in } \R^n\setminus\Omega. \end{cases} $$ The
method, that can be traced back to the work of De Figueiredo and Felmer
\cite{DF-F}, is flexible enough to deal with more general nonlocal operators
and make use of a combination of fractional order Sobolev spaces together with
functional calculus for self-adjoint operators.",['math.AP'],False,,,,"Three non-zero solutions of a Neumann eigenvalue problems involving the
  fractional p-Laplacian",Fractional Lane-Emden Hamiltonian systems
neg-d2-178,2025-02-09,,2502.05876," \begin{equation*}
  \left\{
  \begin{array}{l}
  u'' + \lambda h(x,\alpha) e^u = 0, \quad x \in (-1,1), \\[1ex]
  u(-1) = u(1) = 0,
  \end{array}
  \right. \end{equation*} where $\lambda>0$, $0<\alpha<1$, $h(x,\alpha)=0$ for
$|x|<\alpha$, and $h(x,\alpha)=1$ for $\alpha \le |x| \le 1$. We compute the
Morse index of positive even solutions, and then we prove the existence of an
unbounded connected set of positive non-even solutions emanating from a
symmetry-breaking bifurcation point.",['math.AP'],2501.11523," In this work, our interest lies in proving the existence of solutions to the
following Fractional Lane-Emden Hamiltonian system: $$ \begin{cases}
(-\Delta)^s u = H_v(x,u,v) & \text{in }\Omega,\\ (-\Delta)^s v = H_u(x,u,v) &
\text{in }\Omega,\\ u=v=0 & \text{in } \R^n\setminus\Omega. \end{cases} $$ The
method, that can be traced back to the work of De Figueiredo and Felmer
\cite{DF-F}, is flexible enough to deal with more general nonlocal operators
and make use of a combination of fractional order Sobolev spaces together with
functional calculus for self-adjoint operators.",['math.AP'],False,,,,"Morse index and symmetry-breaking bifurcation of positive solutions to
  the one-dimensional Liouville type equation with a step function weight",Fractional Lane-Emden Hamiltonian systems
neg-d2-179,2025-01-29,,2501.17679," Among the numerous molecular systems found in the interstellar medium (ISM),
vinyl cyanide is the first identified olephinic nitrile. While it has been
observed in various sources, its detection in Sgr B2 is notable as the
2$_{11}$-2$_{12}$ rotational transition exhibits maser features. This indicates
that local thermodynamic equilibrium conditions are not fulfilled, and an
accurate estimation of the molecular abundance in such conditions involves
solving the statistical equilibrium equations taking into account the
competition between the radiative and collisional processes. This in turn
requires the knowledge of rotational excitation data for collisions with the
most abundant species - He or H$_2$. In this paper the first three-dimensional
CH$_2$CHCN - He potential energy surface is computed using explicitly
correlated coupled-cluster theory [(CCSD(T)-F12] with a combination of two
basis sets. Scattering calculations of the rotational (de-)excitation of
CH$_2$CHCN induced by He atoms are performed with the quantum-mechanical
close-coupling method in the low-energy regime. Rotational state-to-state cross
sections derived from these calculations are used to compute the corresponding
rate coefficients. The interaction potential exhibits a high anisotropy, with a
global minimum of $-53.5$ cm$^{-1}$ and multiple local minima. Collisional
cross sections are calculated for total energies up to 100 cm$^{-1}$. By
thermally averaging the cross-sections, collisional rate coefficients are
determined for temperatures up to 20 K. A propensity favouring the transitions
with $\Delta k_a=0$ is observed.",['astro-ph.GA'],2502.09707," We assess the impact of CaII 3934,3969 and NaI 5891,5897 absorption arising
in the interstellar medium (ISM) on the SDSS-IV MaNGA Stellar Library (MaStar)
and produce corrected spectroscopy for 80% of the 24,162-star catalog. We model
the absorption strength of these transitions as a function of stellar distance,
Galactic latitude, and dust reddening based upon high-spectral resolution
studies. With this model, we identify 6342 MaStar stars that have negligible
ISM absorption ($W^\mathrm{ISM}$(CaII K) $<0.07$ Ang and $W^\mathrm{ISM}$(NaI
5891) $<0.05$ Ang). For 12,110 of the remaining stars, we replace their NaI D
profile (and their CaII profile for effective temperatures $T_{\rm eff}>9000$
K) with a coadded spectrum of low-ISM stars with similar $T_{\rm eff}$, surface
gravity, and metallicity. For 738 additional stars with $T_{\rm eff}>9000$ K,
we replace these spectral regions with a matching ATLAS9-based BOSZ model. This
results in a mean reduction in $W$(CaII K) ($W$(NaI D)) of $0.4-0.7$ Ang
($0.6-1.1$ Ang) for hot stars ($T_{\rm eff}>7610$ K), and a mean reduction in
$W$(NaI D) of $0.1-0.2$ Ang for cooler stars. We show that interstellar
absorption in simple stellar population (SSP) model spectra constructed from
the original library artificially enhances $W$(CaII K) by $\gtrsim20\%$ at
young ages ($<400$ Myr); dramatically enhances the strength of stellar NaI D in
starbursting systems (by ${\gtrsim}50\%$); and enhances stellar NaI D in older
stellar populations (${\gtrsim}10$ Gyr) by ${\gtrsim}10\%$. We provide SSP
spectra constructed from the cleaned library, and discuss the implications of
these effects for stellar population synthesis analyses constraining stellar
age, [Na/Fe] abundance, and the initial mass function.",['astro-ph.GA'],False,,,,"Rotational Excitation of Vinyl Cyanide by Collisions with Helium Atoms
  at Low Temperature","SDSS-IV MaStar: Quantification and Abatement of Interstellar Absorption
  in the Largest Empirical Stellar Spectral Library"
neg-d2-180,2025-02-24,,2502.16906," While logical reasoning evaluation of Large Language Models (LLMs) has
attracted significant attention, existing benchmarks predominantly rely on
multiple-choice formats that are vulnerable to random guessing, leading to
overestimated performance and substantial performance fluctuations. To obtain
more accurate assessments of models' reasoning capabilities, we propose an
automated method for synthesizing open-ended logic puzzles, and use it to
develop a bilingual benchmark, AutoLogi. Our approach features program-based
verification and controllable difficulty levels, enabling more reliable
evaluation that better distinguishes models' reasoning abilities. Extensive
evaluation of eight modern LLMs shows that AutoLogi can better reflect true
model capabilities, with performance scores spanning from 35% to 73% compared
to the narrower range of 21% to 37% on the source multiple-choice dataset.
Beyond benchmark creation, this synthesis method can generate high-quality
training data by incorporating program verifiers into the rejection sampling
process, enabling systematic enhancement of LLMs' reasoning capabilities across
diverse datasets.",['cs.CL'],2502.17927," Alignment techniques enable Large Language Models (LLMs) to generate outputs
that align with human preferences and play a crucial role in their
effectiveness. However, their impact often diminishes when applied to Small
Language Models (SLMs), likely due to the limited capacity of these models.
Instead of directly applying existing alignment techniques to SLMs, we propose
to utilize a well-aligned teacher LLM to guide the alignment process for these
models, thereby facilitating the transfer of the teacher's knowledge of human
preferences to the student model. To achieve this, we first explore a
straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that
employs knowledge distillation with two KL-divergence constraints from the
aligned teacher to the unaligned student. To further enhance the student's
ability to distinguish between preferred and dispreferred responses, we then
propose Advantage-Guided Distillation for Preference Alignment (ADPA), which
leverages an advantage function from the aligned teacher to deliver more
nuanced, distribution-level reward signals for the student's alignment. Our
experimental results show that these two approaches appreciably improve the
alignment of SLMs and narrow the performance gap with larger counterparts.
Among them, ADPA demonstrates superior performance and achieves even greater
effectiveness when integrated with DCKD. Our code is available at
https://github.com/SLIT-AI/ADPA.",['cs.CL'],False,,,,"AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning
  Abilities of Large Language Models","Advantage-Guided Distillation for Preference Alignment in Small Language
  Models"
neg-d2-181,2025-03-12,,2503.0968," The Spitzer Survey of Stellar Structure in Galaxies (S$^4$G), together with
its Early Type Galaxy (ETG) extension, stand as the most extensive dataset of
deep, uniform mid-infrared (mid-IR; 3.6 and 4.5$\,\mu$m) imaging for a sample
of $2817$ nearby ($d<40 \,$Mpc) galaxies. However, the velocity criterion used
to select the original sample results in an additional 422 galaxies without HI
detection that ought to have been included in the S$^4$G on the basis of their
optical recession velocities. In order to create a complete magnitude-, size-
and volume-limited sample of nearby galaxies, we collect $3.6\,\mu$m and
$i$-band images using archival data from different surveys and complement it
with new observations for the missing galaxies. We denote the sample of these
additional galaxies as Disc Galaxy (DG) extension. We present the Complete
Spitzer Survey of Stellar Structure in Galaxies (CS$^4$G), encompassing a
sample of $3239$ galaxies with consistent imaging, surface brightness profiles,
photometric parameters, and revised morphological classification. Following the
original strategy of the S$^4$G survey, we produce masks, surface brightness
profiles, and curves of growth using masked $3.6\,\mu$m and $i$-band images.
From these profiles, we derive the integrated quantities: total magnitude,
stellar mass, concentration parameter, and galaxy size, converting to
$3.6\,\mu$m. We re-measure these parameters also for the S$^4$G and ETG to
create a homogenous sample. We present new morphological revised $T$-types, and
we showcase mid-IR scaling relations for the photometric parameters. We
complete the S$^4$G sample by incorporating 422 galaxies. The CS$^4$G includes
at least 99.94\% of the complete sample of nearby galaxies, meeting the
original selection criteria, and it will enable a wide set of investigations
into galaxy structure and evolution.",['astro-ph.GA'],2502.13029," Many spiral galaxies host magnetic fields with energy densities comparable to
those of the turbulent and thermal motions of their interstellar gas. However,
quantitative comparison between magnetic field properties inferred from
observation and those obtained from theoretical modeling has been lacking. In
Paper I we developed a simple, axisymmetric galactic dynamo model that uses
various observational data as input. Here we apply our model to calculate
radial profiles of azimuthally and vertically averaged magnetic field strength
and pitch angle, gas velocity dispersion and scale height, turbulent
correlation time and length, and the sizes of supernova remnants for the
galaxies M31, M33, M51, and NGC 6946, using input data collected from the
literature. Scaling factors are introduced to account for a lack of precision
in both theory and observation. Despite the simplicity of our model, its
outputs agree fairly well with galaxy properties inferred from observation.
Additionally, we find that most of the parameter values are similar between
galaxies. We extend the model to predict the magnetic field pitch angles
arising from a combination of mean-field dynamo action and the winding up of
the random small-scale field owing to the large-scale radial shear. We find
their magnitudes to be much smaller than those of the pitch angles measured in
polarized radio and far infrared emission. This suggests that effects not
included in our model, such as effects associated with spiral arms, are needed
to explain the pitch angle values.",['astro-ph.GA'],False,,,,The Complete Spitzer Survey of Stellar Structure in Galaxies (CS$^4$G),Galactic magnetic fields II. Applying the model to nearby galaxies
neg-d2-182,2025-03-14,,2503.1183," We study the time-optimal robust control of a two-level quantum system
subjected to field inhomogeneities. We apply the Pontryagin Maximum Principle
and we introduce a reduced space onto which the optimal dynamics is projected
down. This reduction leads to a complete analytical derivation of the optimal
solution in terms of elliptic functions and elliptic integrals. Necessary
optimality conditions are then obtained for the original system. These
conditions are verified numerically and lead to the optimal control protocol.
Various examples, ranging from state-to-state transfer to the generation of a
Not gate, illustrate this study. The connection with other geometric
optimization approaches that have been used to solve this problem is also
discussed.",['quant-ph'],2502.2071," One of the predominant causes of program distortion in the real quantum
computing system may be attributed to the probability deviation caused by
thermal relaxation. We introduce Barber (Balancing reAdout Results using
Bit-invErted ciRcuits), a method designed to counteract the asymmetric thermal
relaxation deviation and improve the reliability of near-term quantum programs.
Barber collaborates with a bit-inverted quantum circuit, where the excited
quantum state of qubits is assigned to the $\lvert 0 \rangle$ and the unexcited
state to the $\lvert 1 \rangle$. In doing so, bit-inverted quantum circuits can
experience thermal relaxation in the opposite direction compared to standard
quantum circuits. Barber can effectively suppress the thermal relaxation
deviation in program's readout results by selectively merging distributions
from the standard and bit-inverted circuits.",['quant-ph'],False,,,,"Application of the Pontryagin Maximum Principle to the robust
  time-optimal control of two-level quantum systems","Balancing Thermal Relaxation Deviations of Near-Future Quantum Computing
  Results via Bit-Inverted Programs"
neg-d2-183,2025-01-14,,2501.08229," This research proposes a system as a solution for the challenges faced by Sri
Lanka' s historic railway system, such as scheduling delays, overcrowding,
manual ticketing, and management inefficiencies. It proposes a multi-subsystem
approach, incorporating GPS tracking, RFID-based e-ticketing, seat reservation,
and vision-based people counting. The GPS based real time train tracking system
performs accurately within 24 meters, with the MQTT protocol showing twice the
speed of the HTTP-based system. All subsystems use the MQTT protocol to enhance
efficiency, reliability, and passenger experience. The study's data and
methodology demonstrate the effectiveness of these innovations in improving
scheduling, passenger flow, and overall system performance, offering promising
solutions for modernizing Sri Lanka's railway infrastructure.",['cs.NI'],2501.12783," Serverless computing adopts a pay-as-you-go billing model where applications
are executed in stateless and shortlived containers triggered by events,
resulting in a reduction of monetary costs and resource utilization. However,
existing platforms do not provide an upper bound for the billing model which
makes the overall cost unpredictable, precluding many organizations from
managing their budgets. Due to the diverse ranges of serverless functions and
the heterogeneous capacity of edge devices, it is challenging to receive
near-optimal solutions for deployment cost in a polynomial time. In this paper,
we investigated the function scheduling problem with a budget constraint for
serverless computing in wireless networks. Users and IoT devices are sending
requests to edge nodes, improving the latency perceived by users. We propose
two online scheduling algorithms based on reinforcement learning, incorporating
several important characteristics of serverless functions. Via extensive
simulations, we justify the superiority of the proposed algorithm by comparing
with an ILP solver (Midaco). Our results indicate that the proposed algorithms
efficiently approximate the results of Midaco within a factor of 1.03 while our
decision-making time is 5 orders of magnitude less than that of Midaco.",['cs.NI'],False,,,,"Enhancing Train Transportation in Sri Lanka: A Smart IOT based
  Multi-Subsystem Approach using MQTT","Cost Optimization for Serverless Edge Computing with Budget Constraints
  using Deep Reinforcement Learning"
neg-d2-184,2025-02-26,,2502.19743," During thermonuclear bursts, it is suspected that {\bf the cooling of the
corona by the burst emission}
  may be the cause of hard X-ray {\bf deficits}. Although this {\bf deficit}
has been observed in nine sources, it has not been observed {\bf from}
4U~1608--52, a nearby prolific burster. Therefore, the authenticity and
universality of the hard X-ray {\bf deficit} may be in question. To investigate
this suspicion, Insight-HXMT performed cadence observations during the low/hard
state of 4U~1608--52 in September 2022 and detected 10 thermonuclear X-ray
bursts. Two of these bursts show a double-peaked structure in the soft X-ray
band, which could be caused by the high temperature of the burst emission and a
marginal photospheric radius expansion (PRE) around the burst peak time. This
is indicated by their peak fluxes being up to the Eddington limit and having a
large color factor at the peak of the bursts. The hard X-ray deficit is
significantly observed during bursts at $>$ 30 keV. Furthermore, the fraction
of this deficit shows saturation at 50\% for the first 8 bursts. This
saturation may indicate that the corona is layered and only a part of the
corona is cooled by the bursts. For example, the part close to the NS surface
is cooled while the rest remains intact during bursts. This result provides a
clue to the geometry of the corona, e.g., a possible scenario is that the
corona has two forms: a quasi-spheric corona between the NS and the disk, and a
disk-corona on both surfaces of the disk.",['astro-ph.HE'],2502.16626," The non-detection of periodicity related to rotation challenges the magnetar
model for fast radio bursts (FRBs). Moreover, a bimodal distribution of the
burst waiting times is widely observed in hyper-active FRBs, a significant
deviation from the exponential distribution expected from stationary Poisson
processes. By combining the epidemic-type aftershock sequence (ETAS) earthquake
model and the rotating vector model (RVM) involving the rotation of the
magnetar and orientations of the spin and magnetic axes, we find that starquake
events modulated by the rotation of FRB-emitting magnetar can explain the
bimodal distribution of FRB waiting times, as well as the non-detection of
periodicity in active repeating FRBs. We analyze data from multiple FRB
sources, demonstrating that differences in waiting time distributions and
observed energies can be explained by varying parameters related to magnetar
properties and starquake dynamics. Our results suggest that rotation-modulated
starquakes on magnetars can possibly be a unified source for FRBs. Notably, we
find that active repeaters tend to have small magnetic inclination angles in
order to hide their periodicity. We also show that our model can reproduce the
waiting time distribution of a pulsar phase of the galactic magnetar SGR
J1935+2154 with a larger inclination angle than the active repeaters, which
could explain the detection of spin period and the relatively low observed
energy for FRBs from the magnetar. The spin periods of active repeaters are not
well constrained, but most likely fall in the valley region between the two
peaks of the waiting time distributions.",['astro-ph.HE'],False,,,,"Insight-HXMT observations on thermonuclear X-ray bursts from 4U~1608--52
  in the low/hard state: the energy-dependant hard X-ray deficit and cooling
  saturation of the corona","Hyper-active repeating fast radio bursts from rotation modulated
  starquakes on magnetars"
neg-d2-185,2025-03-01,,2503.00708," In this paper, we consider the radial symmetry, uniqueness and non-degeneracy
of solutions to the degenerate nonlinear elliptic equation $$ -\nabla \cdot
\left(|x|^{2a} \nabla u\right) + \omega u=|u|^{p-2}u \quad \mbox{in} \,\,
\mathbb{R}^d, $$ where $d \geq 2$, $0<a<1$, $\omega>0$ and
$2<p<\frac{2d}{d-2(1-a)}$. We proved that any ground state is radially
symmetric and strictly decreasing in the radial direction. Moreover, we
establish the uniqueness of ground states and derive the non-degeneracy of
ground states in the corresponding radially symmetric Sobolev space. This
affirms the nature conjectures posed recently in \cite{IS}.",['math.AP'],2503.03209," Isolated skyrmion solutions to the 2D Landau-Lifshitz equation with the
Dzyaloshinskii-Moriya interaction, Zeeman interaction, and easy-plane
anisotropy are considered. In a wide range of parameters illustrating the
various interaction strengths, we construct exact solutions and examine their
monotonicity, exponential decay, and stability using a careful mathematical
analysis. We also estimate the distance between the constructed solutions and
the harmonic maps by exploiting the structure of the linearized equation and by
proving a resolvent estimate for the linearized operator that is uniform in
extra implicit potentials.",['math.AP'],False,,,,"Radial symmetry, uniqueness and non-degeneracy of solutions to
  degenerate nonlinear Schr\""odinger equations","Global perturbation of isolated equivariant chiral skyrmions from the
  harmonic maps"
neg-d2-186,2025-01-27,,2501.16312," Volumetric rendering has become central to modern novel view synthesis
methods, which use differentiable rendering to optimize 3D scene
representations directly from observed views. While many recent works build on
NeRF or 3D Gaussians, we explore an alternative volumetric scene
representation. More specifically, we introduce two new scene representations
based on linear primitives-octahedra and tetrahedra-both of which define
homogeneous volumes bounded by triangular faces. This formulation aligns
naturally with standard mesh-based tools, minimizing overhead for downstream
applications. To optimize these primitives, we present a differentiable
rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based
optimization while maintaining realtime rendering capabilities. Through
experiments on real-world datasets, we demonstrate comparable performance to
state-of-the-art volumetric methods while requiring fewer primitives to achieve
similar reconstruction fidelity. Our findings provide insights into the
geometry of volumetric rendering and suggest that adopting explicit polyhedra
can expand the design space of scene representations.",['cs.CV'],2502.14886," Recent advancements in machine learning (ML) and deep learning (DL),
particularly through the introduction of foundational models (FMs), have
significantly enhanced surgical scene understanding within minimally invasive
surgery (MIS). This paper surveys the integration of state-of-the-art ML and DL
technologies, including Convolutional Neural Networks (CNNs), Vision
Transformers (ViTs), and foundational models like the Segment Anything Model
(SAM), into surgical workflows. These technologies improve segmentation
accuracy, instrument tracking, and phase recognition in surgical endoscopic
video analysis. The paper explores the challenges these technologies face, such
as data variability and computational demands, and discusses ethical
considerations and integration hurdles in clinical settings. Highlighting the
roles of FMs, we bridge the technological capabilities with clinical needs and
outline future research directions to enhance the adaptability, efficiency, and
ethical alignment of AI applications in surgery. Our findings suggest that
substantial progress has been made; however, more focused efforts are required
to achieve seamless integration of these technologies into clinical workflows,
ensuring they complement surgical practice by enhancing precision, reducing
risks, and optimizing patient outcomes.",['cs.CV'],False,,,,LinPrim: Linear Primitives for Differentiable Volumetric Rendering,"Surgical Scene Understanding in the Era of Foundation AI Models: A
  Comprehensive Review"
neg-d2-187,2025-02-16,,2502.11405," Despite being pretrained on multilingual corpora, large language models
(LLMs) exhibit suboptimal performance on low-resource languages. Recent
approaches have leveraged multilingual encoders alongside LLMs by introducing
trainable parameters connecting the two models. However, these methods
typically focus on the encoder's output, overlooking valuable information from
other layers. We propose \aname (\mname), a framework that integrates
representations from all encoder layers, coupled with the \attaname mechanism
to enable layer-wise interaction between the LLM and the multilingual encoder.
Extensive experiments on multilingual reasoning tasks, along with analyses of
learned representations, show that our approach consistently outperforms
existing baselines.",['cs.CL'],2502.14451," Natural Language Generation (NLG) popularity has increased owing to the
progress in Large Language Models (LLMs), with zero-shot inference
capabilities. However, most neural systems utilize decoder-only causal
(unidirectional) transformer models, which are effective for English but may
reduce the richness of languages with less strict word order, subject omission,
or different relative clause attachment preferences. This is the first work
that analytically addresses optimal text generation order for non-causal
language models. We present a novel Viterbi algorithm-based methodology for
maximum likelihood word order estimation. We analyze the non-causal
most-likelihood order probability for NLG in Spanish and, then, the probability
of generating the same phrases with Spanish causal NLG. This comparative
analysis reveals that causal NLG prefers English-like SVO structures. We also
analyze the relationship between optimal generation order and causal
left-to-right generation order using Spearman's rank correlation. Our results
demonstrate that the ideal order predicted by the maximum likelihood estimator
is not closely related to the causal order and may be influenced by the
syntactic structure of the target sentence.",['cs.CL'],False,,,,"LayAlign: Enhancing Multilingual Reasoning in Large Language Models via
  Layer-Wise Adaptive Fusion and Alignment Strategy","Optimal word order for non-causal text generation with Large Language
  Models: the Spanish case"
neg-d2-188,2025-03-03,,2503.0121," Multi-modality image fusion, particularly infrared and visible image fusion,
plays a crucial role in integrating diverse modalities to enhance scene
understanding. Early research primarily focused on visual quality, yet
challenges remain in preserving fine details, making it difficult to adapt to
subsequent tasks. Recent approaches have shifted towards task-specific design,
but struggle to achieve the ``The Best of Both Worlds'' due to inconsistent
optimization goals. To address these issues, we propose a novel method that
leverages the semantic knowledge from the Segment Anything Model (SAM) to Grow
the quality of fusion results and Establish downstream task adaptability,
namely SAGE. Specifically, we design a Semantic Persistent Attention (SPA)
Module that efficiently maintains source information via the persistent
repository while extracting high-level semantic priors from SAM. More
importantly, to eliminate the impractical dependence on SAM during inference,
we introduce a bi-level optimization-driven distillation mechanism with triplet
losses, which allow the student network to effectively extract knowledge at the
feature, pixel, and contrastive semantic levels, thereby removing reliance on
the cumbersome SAM model. Extensive experiments show that our method achieves a
balance between high-quality visual results and downstream task adaptability
while maintaining practical deployment efficiency.",['cs.CV'],2501.05862," Depicting novel classes with language descriptions by observing few-shot
samples is inherent in human-learning systems. This lifelong learning
capability helps to distinguish new knowledge from old ones through the
increase of open-world learning, namely Few-Shot Class-Incremental Learning
(FSCIL). Existing works to solve this problem mainly rely on the careful tuning
of visual encoders, which shows an evident trade-off between the base knowledge
and incremental ones. Motivated by human learning systems, we propose a new
Language-inspired Relation Transfer (LRT) paradigm to understand objects by
joint visual clues and text depictions, composed of two major steps. We first
transfer the pretrained text knowledge to the visual domains by proposing a
graph relation transformation module and then fuse the visual and language
embedding by a text-vision prototypical fusion module. Second, to mitigate the
domain gap caused by visual finetuning, we propose context prompt learning for
fast domain alignment and imagined contrastive learning to alleviate the
insufficient text data during alignment. With collaborative learning of domain
alignments and text-image transfer, our proposed LRT outperforms the
state-of-the-art models by over $13\%$ and $7\%$ on the final session of
mini-ImageNet and CIFAR-100 FSCIL benchmarks.",['cs.CV'],False,,,,"Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality
  Image Fusion and Beyond","Language-Inspired Relation Transfer for Few-shot Class-Incremental
  Learning"
neg-d2-189,2025-03-17,,2503.12934," Distributed optimization problems have received much attention due to their
privacy preservation, parallel computation, less communication, and strong
robustness. This paper presents and studies the time-varying distributed
optimization problem for a class of stochastic multi-agent systems for the
first time. For this, we initially propose a protocol in the centralized case
that allows the tracking error of the agent with respect to the optimal
trajectory to be exponentially ultimately bounded in a mean-square sense by
stochastic Lyapunov theory. We then generalize this to the distributed case.
Therein, the global variable can be accurately estimated in a fixed-time by our
proposed estimator. Based on this estimator, we design a new distributed
protocol, and the results demonstrate that the tracking error of all agents
with respect to the optimal trajectory is exponentially ultimately bound in a
mean-square sense by stochastic Lyapunov theory. Finally, simulation
experiments are conducted to validate the findings.",['math.OC'],2503.14981," This work is concerned with convex analysis of so-called spectral functions
of matrices that only depend on eigenvalues of the matrix. An abstract
framework of spectral decomposition systems is proposed that covers a wide
range of previously studied settings, including eigenvalue decomposition of
Hermitian matrices and singular value decomposition of rectangular matrices and
allows deriving new results in more general settings such as Euclidean Jordan
algebras. The main results characterize convexity, lower semicontinuity,
Fenchel conjugates, convex subdifferentials, and Bregman proximity operators of
spectral functions in terms of the reduced functions. As a byproduct, a
generalization of the Ky Fan majorization theorem is obtained.",['math.OC'],False,,,,"Time-Varying Distributed Optimization for A Class of Stochastic
  Multi-Agent Systems",Convex Analysis in Spectral Decomposition Systems
neg-d2-190,2025-01-29,,2501.18644," This paper examines the output of cultural items generated by Chat Generative
PreTrained Transformer Pro in response to three structured prompts to translate
three anthologies of African poetry. The first prompt was broad, the second
focused on poetic structure, and the third prompt emphasized cultural
specificity. To support this analysis, four comparative tables were created.
The first table presents the results of the cultural items produced after the
three prompts, the second categorizes these outputs based on Aixela framework
of Proper nouns and Common expressions, the third table summarizes the cultural
items generated by human translators, a custom translation engine, and a Large
Language Model. The final table outlines the strategies employed by Chat
Generative PreTrained Transformer Pro following the culture specific prompt.
Compared to the outputs of cultural items from reference human translation and
the custom translation engine in prior studies the findings indicate that the
culture oriented prompts used with Chat Generative PreTrained Transformer Pro
did not yield significant enhancements of cultural items during the translation
of African poetry from English to French. Among the fifty four cultural items,
the human translation produced thirty three cultural items in repetition, the
custom translation engine generated Thirty eight cultural items in repetition
while Chat Generative PreTrained Transformer Pro produced forty one cultural
items in repetition. The untranslated cultural items revealed inconsistencies
in Large language models approach to translating cultural items in African
poetry from English to French.",['cs.CL'],2501.01028," As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.",['cs.CL'],False,,,,"Prompt-oriented Output of Culture-Specific Items in Translated African
  Poetry by Large Language Model: An Initial Multi-layered Tabular Review",KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
neg-d2-191,2025-02-24,,2502.17851," In this paper, we compute the cohomology sheaves of the $\ell$-adic nearby
cycles on the local model of the PEL $\mathrm{GU}(n-1,1)$ Shimura variety over
a ramified prime. The local model is known to have isolated singularities. If
$n=2$ it has semi-stable reduction, and if $n\geq 3$ the blow-up at the
singular point has semi-stable reduction. Thus, in principle one may compute
the nearby cycles at least on the blow-up, then use proper base change to
describe them on the original local model. As a result, we prove that the
nearby cycles are trivial when $n$ is odd, and that only a single higher
cohomology sheaf does not vanish when $n$ is even. In this case, we also
describe the Galois action by computing the associated eigenvalue of the
Frobenius.",['math.NT'],2502.08263," We study the twofold structure of the vector space of Drinfeld quasi-modular
forms for Hecke congruence subgroups. We provide representations as polynomials
in the false Eisenstein series with coefficients in the space of Drinfeld
modular forms, and as sums of hyperderivatives of Drinfeld modular forms
(whenever possible). Moreover, we offer a well-defined formula (i.e.
independent of the chosen representatives) for Hecke operators, and prove that
they preserve the space of Drinfeld quasi-modular forms of given weight and
type.",['math.NT'],False,,,,"Nearby cycles on the local model for the $\mathrm{GU}(n-1,1)$ PEL
  Shimura variety over a ramified prime",Drinfeld Quasi-Modular Forms of Higher Level
neg-d2-192,2025-03-20,,2503.16655," The discovery of novel antibiotics is critical to address the growing
antimicrobial resistance (AMR). However, pharmaceutical industries face high
costs (over $1 billion), long timelines, and a high failure rate, worsened by
the rediscovery of known compounds. We propose an LLM-based pipeline that acts
as an alarm system, detecting prior evidence of antibiotic activity to prevent
costly rediscoveries. The system integrates organism and chemical literature
into a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,
and multi-level evidence classification. We tested the pipeline on a private
list of 73 potential antibiotic-producing organisms, disclosing 12 negative
hits for evaluation. The results highlight the effectiveness of the pipeline
for evidence reviewing, reducing false negatives, and accelerating
decision-making. The KG for negative hits and the user interface for
interactive exploration will be made publicly available.",['cs.CL'],2502.19953," Regular updates are essential for maintaining up-to-date knowledge in large
language models (LLMs). Consequently, various model editing methods have been
developed to update specific knowledge within LLMs. However, training-based
approaches often struggle to effectively incorporate new knowledge while
preserving unrelated general knowledge. To address this challenge, we propose a
novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes
the geometric relationships of parameter updates from fine-tuning to
differentiate between neurons associated with new knowledge updates and those
related to general knowledge perturbations. By employing a direction-aware
knowledge identification method, we avoid updating neurons with directions
approximately orthogonal to existing knowledge, thus preserving the model's
generalization ability. For the remaining neurons, we integrate both old and
new knowledge for aligned directions and apply a ""forget-then-learn"" editing
strategy for opposite directions. Additionally, we introduce an
importance-guided task vector fusion technique that filters out redundant
information and provides adaptive neuron-level weighting, further enhancing
model editing performance. Extensive experiments on two publicly available
datasets demonstrate the superiority of GeoEdit over existing state-of-the-art
methods.",['cs.CL'],False,,,,"Accelerating Antibiotic Discovery with Large Language Models and
  Knowledge Graphs",GeoEdit: Geometric Knowledge Editing for Large Language Models
neg-d2-193,2025-03-10,,2503.08016," Predicting pedestrian trajectories is essential for autonomous driving
systems, as it significantly enhances safety and supports informed
decision-making. Accurate predictions enable the prevention of collisions,
anticipation of crossing intent, and improved overall system efficiency. In
this study, we present SGNetPose+, an enhancement of the SGNet architecture
designed to integrate skeleton information or body segment angles with bounding
boxes to predict pedestrian trajectories from video data to avoid hazards in
autonomous driving. Skeleton information was extracted using a pose estimation
model, and joint angles were computed based on the extracted joint data. We
also apply temporal data augmentation by horizontally flipping video frames to
increase the dataset size and improve performance. Our approach achieves
state-of-the-art results on the JAAD and PIE datasets using pose data with the
bounding boxes, outperforming the SGNet model. Code is available on Github:
SGNetPose+.",['cs.CV'],2501.02807," Compared to frame-based methods, computational neuromorphic imaging using
event cameras offers significant advantages, such as minimal motion blur,
enhanced temporal resolution, and high dynamic range. The multi-view
consistency of Neural Radiance Fields combined with the unique benefits of
event cameras, has spurred recent research into reconstructing NeRF from data
captured by moving event cameras. While showing impressive performance,
existing methods rely on ideal conditions with the availability of uniform and
high-quality event sequences and accurate camera poses, and mainly focus on the
object level reconstruction, thus limiting their practical applications. In
this work, we propose AE-NeRF to address the challenges of learning event-based
NeRF from non-ideal conditions, including non-uniform event sequences, noisy
poses, and various scales of scenes. Our method exploits the density of event
streams and jointly learn a pose correction module with an event-based NeRF
(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.
To generalize to larger scenes, we propose hierarchical event distillation with
a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine
the reconstruction process. We further propose an event reconstruction loss and
a temporal loss to improve the view consistency of the reconstructed scene. We
established a comprehensive benchmark that includes large-scale scenes to
simulate practical non-ideal conditions, incorporating both synthetic and
challenging real-world event datasets. The experimental results show that our
method achieves a new state-of-the-art in event-based 3D reconstruction.",['cs.CV'],False,,,,"SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for
  Trajectory Prediction in Autonomous Driving","AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal
  Conditions and Larger Scene"
neg-d2-194,2025-02-15,,2502.1091," Photoacoustic (PA) waves are strongly distorted and attenuated in skull bone.
To study these effects on PA imaging, we designed and 3D-printed
tissue-mimicking phantoms of human skull. We present a comparison of results in
phantom and ex vivo skull.",['physics.med-ph'],2501.10764," Photon-counting detector based computed tomography (PCCT) has greatly
advanced in recent years. However, the spectral inconsistency is still a
serious challenge for PCCT that could directly introduce obvious artifacts and
severe inaccuracies. This work attempts to overcome the challenge by modeling
the spectral inconsistency in a novel, unified, and two-term factorized
framework, with a spectral skew term independent of the energy threshold, and
an energy-threshold bias analytical characterization term. To solve the
spectral inconsistency, a two-step decomposition algorithm called
energy-threshold bias calculator (ETB-Cal) is derived here, in which the
spectral skew term is grossly determined at a relatively low energy threshold
and only the energy-threshold bias is needed to be calculated as the energy
threshold changes. After the two terms being computed out in calibration stage,
they will be incorporated into our spectral model to generate the spectral
correction vectors as well as the material decomposition vectors if needed, for
PCCT projection data. To validate our method, both numerical simulations
physics experiments were carried out on a tabletop PCCT system. Preliminary
results showed that the spectral inconsistency can be significantly reduced,
for example, with an non-uniformity quantitative indicators decreasing from
26.27 to 5.80 HU for Gammex multi-energy phantom and from 27.88 to 3.16 HU for
kyoto head phantom. The considerable improvements consistently demonstrate a
great potential of the proposed novel physics-model based correction scheme in
practical applications, as computationally efficient, calibration-wise
convenient with high degree of generality, and substantially avoiding the use
of X-ray florescence material in the energy-threshold calibration.",['physics.med-ph'],False,,,,3D printed human skull phantoms for transcranial photoacoustic imaging,"Energy-Threshold Bias Calculator: A Physics-Model Based Adaptive
  Correction Scheme for Photon-Counting CT"
neg-d2-195,2025-02-06,,2502.04507," Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art
video generation, but suffer from prohibitive compute cost -- when generating
just a 5-second 720P video, attention alone takes 800 out of 945 seconds of
total inference time. This paper introduces sliding tile attention (STA) to
address this challenge. STA leverages the observation that attention scores in
pretrained video diffusion models predominantly concentrate within localized 3D
windows. By sliding and attending over the local spatial-temporal region, STA
eliminates redundancy from full attention. Unlike traditional token-wise
sliding window attention (SWA), STA operates tile-by-tile with a novel
hardware-aware sliding window design, preserving expressiveness while being
hardware-efficient. With careful kernel-level optimizations, STA offers the
first efficient 2D/3D sliding-window-like attention implementation, achieving
58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over
FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading
video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s
without quality degradation, requiring no training. Enabling finetuning further
lowers latency to 268s with only a 0.09% drop on VBench.",['cs.CV'],2503.04983," In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.",['cs.CV'],False,,,,Fast Video Generation with Sliding Tile Attention,"Leveraging Large Language Models For Scalable Vector Graphics
  Processing: A Review"
neg-d2-196,2025-01-29,,2501.17809," Given a manifold $M$, some closed $\beta\in\Omega^1(M)$ and a map $f\in
C^\infty(M)$, a $\beta$-critical point is some $x\in M$ such that $d_\beta
f_{x}=0$ for the Lichnerowicz derivative $d_\beta$. In this paper, we will give
a lower bound for the number of $\beta$-critical points of index $i$ of a
$\beta$-Morse function $f$ in terms of the Morse-Novikov homology, and we
generalize this result to generating functions (quadratic at infinity). We also
give an application to the detection of essential Liouville chords of a set
length. These are a type of chords that appear in locally conformally
symplectic geometry as even-dimensional analogues to Reeb chords.",['math.SG'],2501.17809," Given a manifold $M$, some closed $\beta\in\Omega^1(M)$ and a map $f\in
C^\infty(M)$, a $\beta$-critical point is some $x\in M$ such that $d_\beta
f_{x}=0$ for the Lichnerowicz derivative $d_\beta$. In this paper, we will give
a lower bound for the number of $\beta$-critical points of index $i$ of a
$\beta$-Morse function $f$ in terms of the Morse-Novikov homology, and we
generalize this result to generating functions (quadratic at infinity). We also
give an application to the detection of essential Liouville chords of a set
length. These are a type of chords that appear in locally conformally
symplectic geometry as even-dimensional analogues to Reeb chords.",['math.SG'],False,,,,Morse-Novikov homology and $\beta$-critical points,Morse-Novikov homology and $\beta$-critical points
neg-d2-197,2025-02-17,,2502.12361," A reliable resume-job matching system helps a company recommend suitable
candidates from a pool of resumes and helps a job seeker find relevant jobs
from a list of job posts. However, since job seekers apply only to a few jobs,
interaction labels in resume-job datasets are sparse. We introduce ConFit v2,
an improvement over ConFit to tackle this sparsity problem. We propose two
techniques to enhance the encoder's contrastive training process: augmenting
job data with hypothetical reference resume generated by a large language
model; and creating high-quality hard negatives from unlabeled resume/job pairs
using a novel hard-negative mining strategy. We evaluate ConFit v2 on two
real-world datasets and demonstrate that it outperforms ConFit and prior
methods (including BM25 and OpenAI text-embedding-003), achieving an average
absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking
and resume-ranking tasks.",['cs.CL'],2503.15737," Named Entity Recognition (NER) is a fundamental task in Natural Language
Processing (NLP) that plays a crucial role in information extraction, question
answering, and knowledge-based systems. Traditional deep learning-based NER
models often struggle with domain-specific generalization and suffer from data
sparsity issues. In this work, we introduce Knowledge Graph distilled for Named
Entity Recognition (KoGNER), a novel approach that integrates Knowledge Graph
(KG) distillation into NER models to enhance entity recognition performance.
Our framework leverages structured knowledge representations from KGs to enrich
contextual embeddings, thereby improving entity classification and reducing
ambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge
Distillation, where external knowledge sources are distilled into a lightweight
representation for seamless integration with NER models, and (2) Entity-Aware
Augmentation, which integrates contextual embeddings that have been enriched
with knowledge graph information directly into GNN, thereby improving the
model's ability to understand and represent entity relationships. Experimental
results on benchmark datasets demonstrate that KoGNER achieves state-of-the-art
performance, outperforming finetuned NER models and LLMs by a significant
margin. These findings suggest that leveraging knowledge graphs as auxiliary
information can significantly improve NER accuracy, making KoGNER a promising
direction for future research in knowledge-aware NLP.",['cs.CL'],False,,,,"ConFit v2: Improving Resume-Job Matching using Hypothetical Resume
  Embedding and Runner-Up Hard-Negative Mining","KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical
  Named Entity Recognition"
neg-d2-198,2025-03-05,,2503.03994," Let $p$ be an odd prime, and $\mathbf{Q}_{p^f}$ the unramified extension of
$\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of
constructing strongly divisible modules for $2$-dimensional semi-stable
non-crystalline representations of
$\mathrm{Gal}(\overline{\mathbf{Q}}_p/\mathbf{Q}_{p^f})$ with Hodge--Tate
weights in the Fontaine--Laffaille range to solving systems of linear equations
and inequalities. We also determine the Breuil modules corresponding to the
mod-$p$ reduction of the strongly divisible modules. We expect our method to
produce at least one Galois-stable lattice in each such representation for
general $f$. Moreover, when the mod-$p$ reduction is an extension of distinct
characters, we further expect our method to provide the two non-homothetic
lattices. As applications, we show that our approach recovers previously known
results for $f=1$ and determine the mod-$p$ reduction of the semi-stable
representations with some small Hodge--Tate weights when $f=2$.",['math.NT'],2501.09697," In this paper, we consider the family of monic polynomials with prime
coefficients and the family of all polynomials with prime coefficients. We
determine the number of $f(x)$ in each of these families having: squarefree
discriminant; $\mathbb{Z}[x]/(f(x))$ as the maximal order in
$\mathbb{Q}[x]/(f(x))$.",['math.NT'],False,,,,On families of strongly divisible modules of rank 2,Squarefree discriminants of polynomials with prime coefficients
neg-d2-199,2025-01-30,,2501.18333," Deep learning and convolutional neural networks in particular are powerful
and promising tools for cosmological analysis of large-scale structure surveys.
They are already providing similar performance to classical analysis methods
using fixed summary statistics, are showing potential to break key degeneracies
by better probe combination and will likely improve rapidly in the coming years
as progress is made in the physical modelling through both software and
hardware improvement. One key issue remains: unlike classical analysis, a
convolutional neural network's decision process is hidden from the user as the
network optimises millions of parameters with no direct physical meaning. This
prevents a clear understanding of the potential limitations and biases of the
analysis, making it hard to rely on as a main analysis method. In this work, we
explore the behaviour of such a convolutional neural network through a novel
method. Instead of trying to analyse a network a posteriori, i.e. after
training has been completed, we study the impact on the constraining power of
training the network and predicting parameters with degraded data where we
removed part of the information. This allows us to gain an understanding of
which parts and features of a large-scale structure survey are most important
in the network's prediction process. We find that the network's prediction
process relies on a mix of both Gaussian and non-Gaussian information, and
seems to put an emphasis on structures whose scales are at the limit between
linear and non-linear regimes.",['astro-ph.CO'],2502.02239," The first SRG/eROSITA all-sky X-ray survey, eRASS1, resulted in a catalogue
of over twelve thousand optically-confirmed galaxy groups and clusters in the
western Galactic hemisphere. Using the eROSITA images of these objects, we
measure and study their morphological properties, including their
concentration, central density and slope, ellipticity, power ratios, photon
asymmetry, centroid shift and Gini coefficient. We also introduce new
forward-modelled parameters which take account of the instrument point spread
function (PSF), which are slosh, which measures how asymmetric the surface
brightness distribution is, and multipole magnitudes, which are analogues to
power ratios. Using simulations, we find some non forward-modelled parameters
are strongly biased due to PSF and data quality. For the same clusters, we find
similar values of concentration and central density compared to results by
ourselves using Chandra and previous results from XMM-Newton. The population as
a whole has log concentrations which are typically around 0.3 dex larger than
South Pole Telescope or Planck-selected samples and the deeper eFEDS sample.
The exposure time, detection likelihood threshold, extension likelihood
threshold and number of counts affect the concentration distribution, but
generally not enough to reduce the concentration to match the other samples.
The concentration of clusters in the survey strongly affects whether they are
detected as a function of redshift and luminosity. We introduce a combined
disturbance score based on a Gaussian mixture model fit to several of the
parameters. For brighter clusters, around 1/4 of objects are classified as
disturbed using this score, which may be due to our sensitivity to concentrated
objects.",['astro-ph.CO'],False,,,,"Interpretability of deep-learning methods applied to large-scale
  structure surveys","The SRG/eROSITA all-sky survey: The morphologies of clusters of galaxies
  I: A catalogue of morphological parameters"
neg-d2-200,2025-02-13,,2502.09239," Using orbital angular momentum (OAM) currents in nanoelectronics, for
example, for magnetization manipulation via spin-orbit torque (SOT), represents
a growing field known as ""spin-orbitronics"". Here, using the density functional
theory (DFT) and the real-time dynamics of electronic wave packets, we explore
a possibility of generation and propagation of orbital currents in two
representative systems: an oxidized Cu surface (where large OAMs are known to
form at the Cu/O interface) and a model molecular junction made of two carbon
chains connected by a chiral molecule. In the Cu/O system, the orbital
polarization of an incident wave packet from the Cu lead is strongly enhanced
at the Cu/O interface but then rapidly decays in the bulk Cu due to orbital
quenching of asymptotic bulk states. Interestingly, if a finite transmission
across the oxygen layer is allowed (in a tunnel junction geometry, for
example), a significant spin-polarization of transmitted (or reflected)
currents is instead predicted which persists at a much longer distance and can
be further tuned by an applied in-plane voltage. For the molecular junction,
the mixing of the carbon $p_x$ and $p_y$ (degenerate) channels by the chiral
molecular orbital gives rise not only to an efficient generation of orbital
current but also to its long-range propagation along the carbon chain.",['cond-mat.mtrl-sci'],2501.02515," Although multiferroics have undergone extensive examination for several
decades, the occurrence of ferroelectricity induced by orbital order is only
scarcely documented. In this study, we propose the existence of spontaneous
ferroelectric states featuring a finite out-of-plane polarization in monolayer
compounds of magnetic transition metal di-halides. Our first principles
analysis reveals that partially occupied d-orbital states within octahedra
exhibit a preference for spatial orbital order within a two-dimensional
lattice. The absence of inversion symmetry, arising from orbital order, serves
as the driving force introducing additional electric polarization along the
out-of-plane direction. Unlike previous reported orbital orders arising from
metal states in lattice, the non-colinear ones we studied in this work relate
to the transition between two insulator states. The resultant asymmetric
Jahn-Teller distortions are accompanied as the consequence producing additional
ionic polarization. Importantly, our findings indicate that this mechanism is
not confined to a specific material but is a shared characteristic among a
series of monolayer transition metal magnetic di-halides, proposing an
innovative form of intrinsic two-dimensional multiferroic physics.",['cond-mat.mtrl-sci'],False,,,,Modelling spin-orbitronics effects at interfaces and chiral molecules,"Orbital Order Triggered Out-of-Plane Ferroelectricity in Magnetic
  Transition Metal di-halide Monolayers"
neg-d2-201,2025-02-03,,2502.01728," Gravitational microlensing is a unique probe of the stellar content in strong
lens galaxies. Flux ratio anomalies from gravitationally lensed supernovae
(glSNe), just like lensed quasars, can be used to constrain the stellar mass
fractions at the image positions. Type Ia supernovae are of particular interest
as knowledge of the intrinsic source brightness helps constrain the amount of
(de)magnification from the macromodel predictions that might be due to
microlensing. In addition, the presence or absence of caustic crossings in the
light curves of glSNe can be used to constrain the mass of the microlenses. We
find that a sample of 50 well-modeled glSNe Ia systems with single epoch
observations at peak intrinsic supernova luminosity should be able to constrain
a stellar mass-to-light ratio to within $\sim 15\%$. A set of systems with
light curve level information providing the location (or absence) of caustic
crossing events can also constrain the mass of the microlenses to within $\sim
50\%$. Much work is needed to make such a measurement in practice, but our
results demonstrate the feasibility of microlensing to place constraints on
astrophysical parameters related to the initial mass function of lensing
galaxies without any prior assumptions on the stellar mass.",['astro-ph.GA'],2503.1718," Aims. We study the behaviour of Cl abundance and its ratios with respect to
O, S and Ar abundances in a sample of more than 200 spectra of Galactic and
extragalactic H ii regions and star-forming galaxies (SFGs) of the local
Universe. Methods. We use the DEep Spectra of Ionised REgions Database
(DESIRED) Extended project (DESIRED-E) that comprises more than 2000 spectra of
H ii regions and SFGs with direct determinations of electron temperature
($T_e$). From this database we select those spectra where it is possible to
determine the Cl$^{2+}$ abundance and whose line ratios meet certain
observational criteria. We calculate the physical conditions and Cl, O, S and
Ar abundances in an homogeneous manner for all the spectra. We compare with
results of photoionisation models to carry out an analysis of which is the most
appropriate $T_e$ indicator for the nebular volume where Cl$^{2+}$ lies,
proposing a scheme that improves the determination of the Cl$^{2+}$ abundance.
We compare the Cl/O ratios obtained using two different ionisation correction
factor (ICF) schemes. We also compare the nebular Cl/O distribution with
stellar determinations. Results. Our analysis indicates that the ICF scheme
proposed by Izotov et al. (2006) better reproduces the observed distributions
of the Cl/O ratio. We find that the log(Cl/O) vs. 12+log(O/H) and log(Cl/Ar)
vs. 12+log(Ar/H) distributions are not correlated in the whole metallicity
range covered by our objects indicating a lockstep evolution of those elements.
In contrast, the log(Cl/S) vs. 12+log(S/H) distribution shows a weak
correlation with a slight negative slope.",['astro-ph.GA'],False,,,,"Stars as cosmic scales: measuring stellar mass with microlensed
  supernovae",Chlorine abundances in star-forming regions of the local Universe
neg-d2-202,2025-02-12,,2502.08618," The Fitzhugh-Nagumo neuronal model is used to explore the influence of the
electric field on thermosensitive neurons' dynamics. This study investigates
how the electric field affects polarization modulation in cell media induced by
changes in ion charge density by adding electrical field as a new variable.
Driven by a voltage source acting as an external stimulus current, different
firing mode responses of the proposed model are analyzed when an external
electrical field is applied. Through computational analysis, the study
evaluates the impact of parameters such as cell radius, stimulus voltage source
amplitude, frequency, and as well as the presence of an external electric
field. The results demonstrate distinct mode transitions of isolated neurons
ranging from spiking to bursting, regular and chaotic oscillations. These
findings suggest that the firing mode is triggered by periodic external
electric fields and cell radius, with the electric field's involvement enhanced
to regulate neuron activity and control the dynamics. External electric fields
and stimuli play a crucial role in neuronal firing dynamics, affecting the
transition between different firing modes. Understanding these effects
contributes to the comprehension of neural processes and the potential
manipulation of neural activity for various applications in neuroscience and
biophysics.",['nlin.CD'],2502.08618," The Fitzhugh-Nagumo neuronal model is used to explore the influence of the
electric field on thermosensitive neurons' dynamics. This study investigates
how the electric field affects polarization modulation in cell media induced by
changes in ion charge density by adding electrical field as a new variable.
Driven by a voltage source acting as an external stimulus current, different
firing mode responses of the proposed model are analyzed when an external
electrical field is applied. Through computational analysis, the study
evaluates the impact of parameters such as cell radius, stimulus voltage source
amplitude, frequency, and as well as the presence of an external electric
field. The results demonstrate distinct mode transitions of isolated neurons
ranging from spiking to bursting, regular and chaotic oscillations. These
findings suggest that the firing mode is triggered by periodic external
electric fields and cell radius, with the electric field's involvement enhanced
to regulate neuron activity and control the dynamics. External electric fields
and stimuli play a crucial role in neuronal firing dynamics, affecting the
transition between different firing modes. Understanding these effects
contributes to the comprehension of neural processes and the potential
manipulation of neural activity for various applications in neuroscience and
biophysics.",['nlin.CD'],False,,,,"Modulation of Neuronal Firing Modes by Electric Fields in a
  Thermosensitive FitzHugh-Nagumo Model","Modulation of Neuronal Firing Modes by Electric Fields in a
  Thermosensitive FitzHugh-Nagumo Model"
neg-d2-203,2025-02-27,,2502.19905," We consider moments of higher powers of quadratic Dirichlet character sums.
In a restricted region, we give their asymptotic behavior by using de la
Bret\`{e}che's multivariable Tauberian theorem. We also give the lower bound of
the exponent of $\log$ factor in the conjecture of Jutila. As an application,
we give a lower bound of a weighted average of shifted moments of quadratic
Dirichlet $L$-functions.",['math.NT'],2502.19905," We consider moments of higher powers of quadratic Dirichlet character sums.
In a restricted region, we give their asymptotic behavior by using de la
Bret\`{e}che's multivariable Tauberian theorem. We also give the lower bound of
the exponent of $\log$ factor in the conjecture of Jutila. As an application,
we give a lower bound of a weighted average of shifted moments of quadratic
Dirichlet $L$-functions.",['math.NT'],False,,,,Moments of quadratic Dirichlet character sums,Moments of quadratic Dirichlet character sums
neg-d2-204,2025-03-14,,2503.11091," Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned
aerial vehicle agent to navigate aerial 3D environments following human
instruction. Compared to ground-based VLN, aerial VLN requires the agent to
decide the next action in both horizontal and vertical directions based on the
first-person view observations. Previous methods struggle to perform well due
to the longer navigation path, more complicated 3D scenes, and the neglect of
the interplay between vertical and horizontal actions. In this paper, we
propose a novel grid-based view selection framework that formulates aerial VLN
action prediction as a grid-based view selection task, incorporating vertical
action prediction in a manner that accounts for the coupling with horizontal
actions, thereby enabling effective altitude adjustments. We further introduce
a grid-based bird's eye view map for aerial space to fuse the visual
information in the navigation history, provide contextual scene information,
and mitigate the impact of obstacles. Finally, a cross-modal transformer is
adopted to explicitly align the long navigation history with the instruction.
We demonstrate the superiority of our method in extensive experiments.",['cs.CV'],2503.17226," Deep neural networks trained with Empirical Risk Minimization (ERM) perform
well when both training and test data come from the same domain, but they often
fail to generalize to out-of-distribution samples. In image classification,
these models may rely on spurious correlations that often exist between labels
and irrelevant features of images, making predictions unreliable when those
features do not exist. We propose a technique to generate training samples with
text-to-image (T2I) diffusion models for addressing the spurious correlation
problem. First, we compute the best describing token for the visual features
pertaining to the causal components of samples by a textual inversion
mechanism. Then, leveraging a language segmentation method and a diffusion
model, we generate new samples by combining the causal component with the
elements from other classes. We also meticulously prune the generated samples
based on the prediction probabilities and attribution scores of the ERM model
to ensure their correct composition for our objective. Finally, we retrain the
ERM model on our augmented dataset. This process reduces the model's reliance
on spurious correlations by learning from carefully crafted samples for in
which this correlation does not exist. Our experiments show that across
different benchmarks, our technique achieves better worst-group accuracy than
the existing state-of-the-art methods.",['cs.CV'],False,,,,"Aerial Vision-and-Language Navigation with Grid-based View Selection and
  Map Construction",Leveraging Text-to-Image Generation for Handling Spurious Correlation
neg-d2-205,2025-02-10,,2502.06223," Understanding the interfaces of layered nanostructures is key to optimizing
their structural and magnetic properties for the desired functionality. In the
present work, the two interfaces of a few nm thick Fe layer in Ag-57Fe-Ag
trilayers are studied with a depth resolution of a fraction of a nanometer
using x-ray standing waves (XSWs) generated by an underlying [W-Si]x10
multilayer (MLT) at an x-ray incident angle around the Bragg peak of the MLT.
Interface selectivity in Ag-57Fe-Ag trilayers was achieved by moving XSW
antinodes across the interfaces by optimizing suitable incident angles and
performing depth-resolved nuclear resonance scattering (NRS) and X-ray
fluorescence (XRF) measurements for magnetic and structural properties. The
combined analysis revealed that the rms roughness of 57Fe-on-Ag and Ag-on-57Fe
interfaces are not equal. The roughness of the 57Fe-on-Ag interface is 10
Angstrom, while that of the Ag-on-57Fe interface is 6 Angstrom. 57Fe isotope
sensitive NRS revealed that hyperfine field (HFF) at both interfaces of
57Fe-on-Ag and Ag-on-57Fe interfaces are distinct, which is consistent with the
difference in interface roughnesses measured as root mean square (RMS)
roughness. Thermal annealing induces 57Fe diffusion into the Ag layer, and
annealing at 325 C transforms the sample into a paramagnetic state. This
behavior is attributed to forming 57Fe nanoparticles within the Ag matrix,
exhibiting a paramagnetic nature. These findings provide deep insights into
interface properties crucial for developing advanced nanostructures and
spintronic devices.",['cond-mat.mtrl-sci'],2502.00337," Exotic nondiffusive heat transfer regimes such as the second sound, where
heat propagates as a damped wave at speeds comparable to those of mechanical
disturbances, often occur at cryogenic temperatures (T) and nanosecond
timescales in semiconductors. First-principles prediction of such rapid, low-T
phonon dynamics requires finely-resolved temporal tracking of large, dense, and
coupled linear phonon dynamical systems arising from the governing linearized
Peierls-Boltzmann equation (LPBE). Here, we uncover a rigorous low-rank
representation of these linear dynamical systems, derived from the spectral
properties of the phonon collision matrix, that accelerates the
first-principles prediction of phonon dynamics by a factor of over a million
without compromising on the computational accuracy. By employing this low-rank
representation of the LPBE, we predict strong amplification of the wave-like
second sound regime upon isotopic enrichment in diamond - a finding that would
have otherwise been computationally intractable using the conventional
brute-force approaches. Our framework enables a rapid and accurate discovery of
the conditions under which wave-like heat flow can be realized in common
semiconductors.",['cond-mat.mtrl-sci'],False,,,,"Investigation of Fe-Ag and Ag-Fe Interfaces in Ag-57Fe-Ag trilayer Using
  Nuclear Resonance Scattering under X-ray Standing Wave Conditions","Efficient calculation of phonon dynamics through a low-rank solution of
  the Boltzmann equation"
neg-d2-206,2025-03-01,,2503.00712," We consider the Survivable Network Design problem (SNDP) in the single-pass
insertion-only streaming model. The input to SNDP is an edge-weighted graph $G
= (V, E)$ and an integer connectivity requirement $r(uv)$ for each $u, v \in
V$. The objective is to find a min-weight subgraph $H \subseteq G$ s.t., for
every $u, v \in V$, $u$ and $v$ are $r(uv)$-edge/vertex-connected. Recent work
by [JKMV24] obtained approximation algorithms for edge-connectivity
augmentation, and via that, also derived algorithms for edge-connectivity SNDP
(EC-SNDP). We consider vertex-connectivity setting (VC-SNDP) and obtain several
results for it as well as improved bounds for EC-SNDP.
  * We provide a general framework for solving connectivity problems in
streaming; this is based on a connection to fault-tolerant spanners. For
VC-SNDP we provide an $O(tk)$-approximation in $\tilde O(k^{1-1/t}n^{1 + 1/t})$
space, where $k$ is the maximum connectivity requirement, assuming an exact
algorithm at the end of the stream. Using a refined LP-based analysis, we
provide an $O(\beta t)$-approximation in polynomial time, where $\beta$ is the
best polytime approximation w.r.t. the optimal fractional solution to a natural
LP relaxation. When applied to EC-SNDP, our framework provides an
$O(t)$-approximation in $\tilde O(k^{1-1/t}n^{1 + 1/t})$ space, improving the
$O(t \log k)$-approximation of [JKMV24]; this also extends to
element-connectivity SNDP.
  * We consider vertex connectivity-augmentation in the link-arrival model. The
input is a $k$-vertex-connected subgraph $G$, and the weighted links $L$ arrive
in the stream; the goal is to store the min-weight set of links s.t. $G \cup L$
is $(k+1)$-vertex-connected. We obtain $O(1)$ approximations in near-linear
space for $k = 1, 2$. Our result for $k=2$ is based on SPQR tree, a novel
application for this well-known representation of $2$-connected graphs.",['cs.DS'],2503.00712," We consider the Survivable Network Design problem (SNDP) in the single-pass
insertion-only streaming model. The input to SNDP is an edge-weighted graph $G
= (V, E)$ and an integer connectivity requirement $r(uv)$ for each $u, v \in
V$. The objective is to find a min-weight subgraph $H \subseteq G$ s.t., for
every $u, v \in V$, $u$ and $v$ are $r(uv)$-edge/vertex-connected. Recent work
by [JKMV24] obtained approximation algorithms for edge-connectivity
augmentation, and via that, also derived algorithms for edge-connectivity SNDP
(EC-SNDP). We consider vertex-connectivity setting (VC-SNDP) and obtain several
results for it as well as improved bounds for EC-SNDP.
  * We provide a general framework for solving connectivity problems in
streaming; this is based on a connection to fault-tolerant spanners. For
VC-SNDP we provide an $O(tk)$-approximation in $\tilde O(k^{1-1/t}n^{1 + 1/t})$
space, where $k$ is the maximum connectivity requirement, assuming an exact
algorithm at the end of the stream. Using a refined LP-based analysis, we
provide an $O(\beta t)$-approximation in polynomial time, where $\beta$ is the
best polytime approximation w.r.t. the optimal fractional solution to a natural
LP relaxation. When applied to EC-SNDP, our framework provides an
$O(t)$-approximation in $\tilde O(k^{1-1/t}n^{1 + 1/t})$ space, improving the
$O(t \log k)$-approximation of [JKMV24]; this also extends to
element-connectivity SNDP.
  * We consider vertex connectivity-augmentation in the link-arrival model. The
input is a $k$-vertex-connected subgraph $G$, and the weighted links $L$ arrive
in the stream; the goal is to store the min-weight set of links s.t. $G \cup L$
is $(k+1)$-vertex-connected. We obtain $O(1)$ approximations in near-linear
space for $k = 1, 2$. Our result for $k=2$ is based on SPQR tree, a novel
application for this well-known representation of $2$-connected graphs.",['cs.DS'],False,,,,Streaming Algorithms for Network Design,Streaming Algorithms for Network Design
neg-d2-207,2025-01-16,,2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],2503.13722," The largest prime p that can be the order of an automorphism of a 2-(35,17,8)
design is p=17, and all 2-(35,17,8) designs with an automorphism of order 17
were classified by Tonchev. The symmetric 2-(35,17,8) designs with
automorphisms of odd prime order $p<17$ were also classified. In this paper we
give the classification of all symmetric 2-(35,17,8) designs that admit an
automorphism of order $p=2$. It is shown that there are exactly $11,642,495$
nonisomorphic such designs. Furthermore, it is shown that the number of
nonisomorphic 3-(36,18,8) designs which have at least one derived 2-$(35,17,8)$
design with an automorphism of order 2, is $1,015,225$.",['math.CO'],False,,,,Coarse tree-width,"Symmetric 2-(35,17,8) designs with an automorphism of order 2"
neg-d2-208,2025-01-10,,2501.0599," The paper discusses the role of WordNet-based semantic classification in the
formalization of constructions, and more specifically in the semantic
annotation of schematic fillers, in the Italian Constructicon. We outline how
the Italian Constructicon project uses Open Multilingual WordNet topics to
represent semantic features and constraints of constructions.",['cs.CL'],2502.04718," Text Style Transfer (TST) is the task of transforming a text to reflect a
particular style while preserving its original content. Evaluating TST outputs
is a multidimensional challenge, requiring the assessment of style transfer
accuracy, content preservation, and naturalness. Using human evaluation is
ideal but costly, same as in other natural language processing (NLP) tasks,
however, automatic metrics for TST have not received as much attention as
metrics for, e.g., machine translation or summarization. In this paper, we
examine both set of existing and novel metrics from broader NLP tasks for TST
evaluation, focusing on two popular subtasks-sentiment transfer and
detoxification-in a multilingual context comprising English, Hindi, and
Bengali. By conducting meta-evaluation through correlation with human
judgments, we demonstrate the effectiveness of these metrics when used
individually and in ensembles. Additionally, we investigate the potential of
Large Language Models (LLMs) as tools for TST evaluation. Our findings
highlight that certain advanced NLP metrics and experimental-hybrid-techniques,
provide better insights than existing TST metrics for delivering more accurate,
consistent, and reproducible TST evaluations.",['cs.CL'],False,,,,"Constraining constructions with WordNet: pros and cons for the semantic
  annotation of fillers in the Italian Constructicon","Evaluating Text Style Transfer Evaluation: Are There Any Reliable
  Metrics?"
neg-d2-209,2025-02-02,,2502.0068," The complexity class $\exists\mathbb R$, standing for the complexity of
deciding the existential first order theory of the reals as real closed field
in the Turing model, has raised considerable interest in recent years. It is
well known that NP $ \subseteq \exists\mathbb R\subseteq$ PSPACE. In their
compendium, Schaefer, Cardinal, and Miltzow give a comprehensive presentation
of results together with a rich collection of open problems. Here, we answer
some of them dealing with structural issues of $\exists\mathbb R$ as a
complexity class. We show analogues of the classical results of Baker, Gill,
and Solovay finding oracles which do and do not separate NP form
$\exists\mathbb R$, of Ladner's theorem showing the existence of problems in
$\exists\mathbb R \setminus$ NP not being complete for $\exists\mathbb R$ (in
case the two classes are different), as well as a characterization of
$\exists\mathbb R$ by means of descriptive complexity.",['cs.CC'],2503.02694," In directed graphs, a cycle can be seen as a structure that allows its
vertices to loop back to themselves, or as a structure that allows pairs of
vertices to reach each other through distinct paths. We extend these concepts
to temporal graph theory, resulting in multiple interesting definitions of a
""temporal cycle"". For each of these, we consider the problems of Cycle
Detection and Acyclic Temporization. For the former, we are given an input
temporal digraph, and we want to decide whether it contains a temporal cycle.
Regarding the latter, for a given input (static) digraph, we want to time the
arcs such that no temporal cycle exists in the resulting temporal digraph.
We're also interested in Acyclic Temporization where we bound the lifetime of
the resulting temporal digraph. Multiple results are presented, including
polynomial and fixed-parameter tractable search algorithms, polynomial-time
reductions from 3-SAT and Not All Equal 3-SAT, and temporizations resulting
from arbitrary vertex orderings which cover (almost) all cases.",['cs.CC'],False,,,,Some structural complexity results for $\exists\mathbb R$,Temporal Cycle Detection and Acyclic Temporization
neg-d2-210,2025-03-04,,2503.02774," This paper addresses the optimization of human-robot collaborative work-cells
before their physical deployment. Most of the times, such environments are
designed based on the experience of the system integrators, often leading to
sub-optimal solutions. Accurate simulators of the robotic cell, accounting for
the presence of the human as well, are available today and can be used in the
pre-deployment. We propose an iterative optimization scheme where a digital
model of the work-cell is updated based on a genetic algorithm. The methodology
focuses on the layout optimization and task allocation, encoding both the
problems simultaneously in the design variables handled by the genetic
algorithm, while the task scheduling problem depends on the result of the
upper-level one. The final solution balances conflicting objectives in the
fitness function and is validated to show the impact of the objectives with
respect to a baseline, which represents possible initial choices selected based
on the human judgment.",['cs.RO'],2502.10057," Soft robotics is advancing the use of flexible materials for adaptable
robotic systems. Membrane-actuated soft robots address the limitations of
traditional soft robots by using pressurized, extensible membranes to achieve
stable, large deformations, yet control and state estimation remain challenging
due to their complex deformation dynamics. This paper presents a novel modeling
approach for liquid-driven ballooning membranes, employing an ellipsoid
approximation to model shape and stretch under planar deformation. Relying
solely on intrinsic feedback from pressure data and controlled liquid volume,
this approach enables accurate membrane state estimation. We demonstrate the
effectiveness of the proposed model for ballooning membrane-based actuators by
experimental validation, obtaining the indentation depth error of
$RMSE_{h_2}=0.80\;$mm, which is $23\%$ of the indentation range and $6.67\%$ of
the unindented actuator height range. For the force estimation, the error range
is obtained to be $RMSE_{F}=0.15\;$N which is $10\%$ of the measured force
range.",['cs.RO'],False,,,,"Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task
  Allocation in Human-Robot Collaborative Assemblies",A Generalized Modeling Approach to Liquid-driven Ballooning Membranes
neg-d2-211,2025-03-06,,2503.04093," Developing tools for estimating heterogeneous treatment effects (HTE) and
individualized treatment effects has been an area of active research in recent
years. While these tools have proven to be useful in many contexts, a concern
when deploying such methods is the degree to which incorporating HTE into a
prediction model provides an advantage over predictive methods which do not
allow for variation in treatment effect across individuals. To address this
concern, we propose a procedure which evaluates the extent to which an HTE
model provides a predictive advantage. Specifically, our procedure targets the
gain in predictive performance from using a flexible predictive model
incorporating HTE versus an alternative model which is similar to the
HTE-utilizing model except that it is constrained to not allow variation in
treatment effect. By drawing upon recent work in using nested cross-validation
techniques for prediction error inference, we generate confidence intervals for
this measure of gain in predictive performance which allows one to directly
calculate the level at which one is confident of a substantial HTE-modeling
gain in prediction -- a quantity which we refer to as the h-value. Our
procedure is generic and can be directly used to assess the benefit of modeling
HTE for any method that incorporates treatment effect variation.",['stat.ME'],2501.13173," Gaussian Process Regression (GPR) is a powerful tool for nonparametric
regression, but its fully Bayesian application in high-dimensional settings is
hindered by two primary challenges: the computational burden (exacerbated by
fully Bayesian inference) and the difficulty of variable selection. This paper
introduces a novel methodology that combines hierarchical global-local
shrinkage priors with normalizing flows to address these challenges. The
hierarchical triple gamma prior offers a principled framework for inducing
sparsity in high-dimensional GPR, effectively excluding irrelevant covariates
while preserving interpretability and flexibility in model size. Normalizing
flows are employed within a variational inference framework to approximate the
posterior distribution of hyperparameters, capturing complex dependencies while
ensuring computational scalability. Simulation studies demonstrate the efficacy
of the proposed approach, outperforming traditional maximum likelihood
estimation and mean-field variational methods, particularly in high-sparsity
and high-dimensional settings. The results highlight the robustness and
flexibility of hierarchical shrinkage priors and the computational efficiency
of normalizing flows for Bayesian GPR. This work provides a scalable and
interpretable solution for high-dimensional regression, with implications for
sparse modeling and posterior approximation in broader Bayesian contexts.",['stat.ME'],False,,,,Evaluating and Testing for Actionable Treatment Effect Heterogeneity,"Normalizing Flows for Gaussian Process Regression under Hierarchical
  Shrinkage Priors"
neg-d2-212,2025-03-09,,2503.06645," This paper investigates the estimation of high-dimensional factor models in
which factor loadings undergo an unknown number of structural changes over
time. Given that a model with multiple changes in factor loadings can be
observationally indistinguishable from one with constant loadings but varying
factor variances, this reduces the high-dimensional structural change problem
to a lower-dimensional one. Due to the presence of multiple breakpoints, the
factor space may expand, potentially causing the pseudo factor covariance
matrix within some regimes to be singular. We define two types of breakpoints:
{\bf a singular change}, where the number of factors in the combined regime
exceeds the minimum number of factors in the two separate regimes, and {\bf a
rotational change}, where the number of factors in the combined regime equals
that in each separate regime. Under a singular change, we derive the properties
of the small eigenvalues and establish the consistency of the QML estimators.
Under a rotational change, unlike in the single-breakpoint case, the pseudo
factor covariance matrix within each regime can be either full rank or
singular, yet the QML estimation error for the breakpoints remains stably
bounded. We further propose an information criterion (IC) to estimate the
number of breakpoints and show that, with probability approaching one, it
accurately identifies the true number of structural changes. Monte Carlo
simulations confirm strong finite-sample performance. Finally, we apply our
method to the FRED-MD dataset, identifying five structural breaks in factor
loadings between 1959 and 2024.",['econ.EM'],2502.12867," Between 1980 and 2000, the U.S. experienced a significant rise in geographic
sorting and educational homogamy, with college graduates increasingly
concentrating in high-skill cities and marrying similarly educated spouses. We
develop and estimate a spatial equilibrium model with local labor, housing, and
marriage markets, incorporating a marriage matching framework with transferable
utility. Using the model, we estimate trends in assortative preferences,
quantify the interplay between marital and geographic sorting, and assess their
combined impact on household inequality. Welfare analyses show that after
accounting for marriage, the college well-being gap grew substantially more
than the college wage gap.",['econ.EM'],False,,,,"Singularity-Based Consistent QML Estimation of Multiple Breakpoints in
  High-Dimensional Factor Models",Assortative Marriage and Geographic Sorting
neg-d2-213,2025-03-12,,2503.09986," Motivated by the remarkable success of artificial intelligence (AI) across
diverse fields, the application of AI to solve scientific problems-often
formulated as partial differential equations (PDEs)-has garnered increasing
attention. While most existing research concentrates on theoretical properties
(such as well-posedness, regularity, and continuity) of the solutions,
alongside direct AI-driven methods for solving PDEs, the challenge of
uncovering symbolic relationships within these equations remains largely
unexplored. In this paper, we propose leveraging large language models (LLMs)
to learn such symbolic relationships. Our results demonstrate that LLMs can
effectively predict the operators involved in PDE solutions by utilizing the
symbolic information in the PDEs. Furthermore, we show that discovering these
symbolic relationships can substantially improve both the efficiency and
accuracy of the finite expression method for finding analytical approximation
of PDE solutions, delivering a fully interpretable solution pipeline. This work
opens new avenues for understanding the symbolic structure of scientific
problems and advancing their solution processes.",['cs.LG'],2501.01344," This paper presents a suite of machine learning models, CRC-ML-Radio Metrics,
designed for modeling RSRP, RSRQ, and RSSI wireless radio metrics in 4G
environments. These models utilize crowdsourced data with local environmental
features to enhance prediction accuracy across both indoor at elevation and
outdoor urban settings. They achieve RMSE performance of 9.76 to 11.69 dB for
RSRP, 2.90 to 3.23 dB for RSRQ, and 9.50 to 10.36 dB for RSSI, evaluated on
over 300,000 data points in the Toronto, Montreal, and Vancouver areas. These
results demonstrate the robustness and adaptability of the models, supporting
precise network planning and quality of service optimization in complex
Canadian urban environments.",['cs.LG'],False,,,,"From Equations to Insights: Unraveling Symbolic Structures in PDEs with
  LLMs","Machine Learning for Modeling Wireless Radio Metrics with Crowdsourced
  Data and Local Environment Features"
neg-d2-214,2025-01-24,,2501.14416," We classify the global dynamics of the five-parameter family of planar
Kolmogorov systems \begin{equation*}
  \begin{split}
  \dot{y}&=y \left( b_0+ b_1 y z + b_2 y + b_3 z\right),
  \dot{z}&=z\left( c_0 + b_1 y z + b_2 y + b_3 z\right),
  \end{split} \end{equation*} which is obtained from the Lotka-Volterra systems
of dimension three. These systems have infinitely many singular points at
inifnity. We give the topological classification of their phase portraits in
the Poincar\'e disc, so we can describe the dynamics of these systems near
infinity. We prove that these systems have 13 topologically distinct global
phase portraits.",['math.DS'],2501.01923," We generalize Hopf's theorem to thermostats: the total thermostat curvature
of a thermostat without conjugate points is non-positive, and vanishes only if
the thermostat curvature is identically zero. We further show that, if the
thermostat curvature is zero, then the flow has no conjugate points, and the
Green bundles collapse almost everywhere. Given a thermostat without conjugate
points, we prove that the Green bundles are transversal everywhere if and only
if it admits a dominated splitting. Finally, we provide an example showing that
Hopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is
also the first example of a thermostat with a dominated splitting which is not
Anosov.",['math.DS'],False,,,,"Planar Kolmogorov systems with infinitely many singular points at
  infinity",Thermostats without conjugate points
neg-d2-215,2025-02-26,,2502.19262," This paper presents two remarkable phenomena associated with the heat
equation with a time delay: namely, the propagation of singularities and
periodicity. These are manifested through a distinctive mode of propagation of
singularities in the solutions. Precisely, the singularities of the solutions
propagate periodically in a bidirectional fashion along the time axis.
Furthermore, this propagation occurs in a stepwise manner. More specifically,
when propagating in the positive time direction, the order of the joint
derivatives of the solution increases by 2 for each period; conversely, when
propagating in the reverse time direction, the order of the joint derivatives
decreases by 2 per period. Additionally, we elucidate the way in which the
initial data and historical values impact such a propagation of singularities.
  The phenomena we have discerned not only corroborate the pronounced
differences between heat equations with and without time delay but also vividly
illustrate the substantial divergence between the heat equation with a time
delay and the wave equation, especially when viewed from the point of view of
singularity propagation.",['math.AP'],2503.09323," In the present paper, we establish a multiplicity result for a following
class of nonlocal Neumann eigenvalue problems involving the fractional
p-Laplacian.
  \begin{align} \begin{cases}
  (-\Delta)^{s}_{p}u + a(x) \abs{u}^{p-2}u =\lambda h(x,u) & \text {in }
\Omega,
  \mathcal{N}_{s,p}u=0 & \text {in } \mathbb{R}^N \setminus \overline{\Omega},
  \end{cases}
  \end{align}
  Precisely, we demonstrate the existence of an open interval for positive
eigenvalues $\lambda$, for which the problem has at least three non-zero
solutions in $W^{s,p}_{\Omega}.$",['math.AP'],False,,,,Periodic propagation of singularities for heat equations with time delay,"Three non-zero solutions of a Neumann eigenvalue problems involving the
  fractional p-Laplacian"
neg-d2-216,2025-01-31,,2502.00276," Paleoclimate records reveal a fuller range of natural climate variability
than modern records and are essential for better understanding the modern
climate change. However, most paleoclimate records are point-based proxies and
lack the temporal resolution needed to analyze spatiotemporal changes in
destructive extremes like tropical cyclones (TCs). Here we show that historical
records by pre-industrial Chinese intellectuals help investigate long-term
variability of TC landfalls in East Asia. Despite inherent limitations, these
records show a landfalling TC climatology resembling modern observations in
spatial-temporal distributions. Comparisons between the pre-industrial records
(1776-1850), modern observations (1946-2020), and climate simulations reveal an
earlier seasonal occurrence of modern TCs. However, the variations of
seasonally aggregated landfall time show pronounced multi-century variations.
The modern changes and multi-decade trends appear moderate compared to
long-term variability in pre-industrial TC records, suggesting that an
overreliance on modern data may lead to an underestimation of the full range of
TC activity potentially arising from natural variability alone. Analyses of
newly available climate data reveal associations between past landfalling TC
activity and the large-scale climate variability of tropical ocean and
extratropical land. These findings demonstrate the value of paleoclimate data
for exploring natural variability in TC activity and inform the development of
effective adaptation strategies for future climate change.",['physics.ao-ph'],2501.04381," In the present work, we collect solar irradiance and atmospheric condition
data from several products, obtained from both numerical models (ERA5 and
NORA3) and satellite observations (CMSAF-SARAH3). We then train simple
supervised Machine Learning (ML) data fusion models, using these products as
predictors and direct in-situ Global Horizontal Irradiance (GHI) measurements
over Norway as ground-truth. We show that combining these products by applying
our trained ML models provides a GHI estimate that is significantly more
accurate than that obtained from any product taken individually. Using the
trained models, we generate a 30-year ML-corrected map of GHI over Norway,
which we release as a new open data product. Our ML-based data fusion
methodology could be applied, after suitable training and input data selection,
to any geographic area on Earth.",['physics.ao-ph'],False,,,,"Chinese Historical Documents Reveal Multi-Century Seasonal Shifts in
  Tropical Cyclone Landfalls","Data fusion of complementary data sources using Machine Learning enables
  higher accuracy Solar Resource Maps"
neg-d2-217,2025-01-22,,2501.12964," In this paper, we conduct the first systematic investigation of twisted
cubics on Gushel-Mukai (GM) fourfolds. We then study the double EPW cube, a
6-dimensional hyperk\""ahler manifold associated with a general GM fourfold $X$,
through the Bridgeland moduli space, and show that it is the maximal rationally
connected (MRC) quotient of the Hilbert scheme of twisted cubics on $X$. We
also prove that a general double EPW cube admits a covering by Lagrangian
subvarieties constructed from the Hilbert schemes of twisted cubics on GM
threefolds, which provides a new example for a conjecture of O'Grady.",['math.AG'],2501.09829," The purpose of this is the study of certain coherent sheaves of meromorphic
forms on reduced complex space and particularly their behavior with respect to
pull back and higher direct image.",['math.AG'],False,,,,Double EPW cubes from twisted cubics on Gushel-Mukai fourfolds,"Sur certains faisceaux de formes m\'eromorphes sur un espace complexe
  r\'eduit"
neg-d2-218,2025-01-18,,2501.10739," Chiasmus, a debated literary device in Biblical texts, has captivated mystics
while sparking ongoing scholarly discussion. In this paper, we introduce the
first computational approach to systematically detect chiasmus within Biblical
passages. Our method leverages neural embeddings to capture lexical and
semantic patterns associated with chiasmus, applied at multiple levels of
textual granularity (half-verses, verses). We also involve expert annotators to
review a subset of the detected patterns. Despite its computational efficiency,
our method achieves robust results, with high inter-annotator agreement and
system precision@k of 0.80 at the verse level and 0.60 at the half-verse level.
We further provide a qualitative analysis of the distribution of detected
chiasmi, along with selected examples that highlight the effectiveness of our
approach.",['cs.CL'],2501.16748," Large Language Models (LLMs) have shown remarkable advancements but also
raise concerns about cultural bias, often reflecting dominant narratives at the
expense of under-represented subcultures. In this study, we evaluate the
capacity of LLMs to recognize and accurately respond to the Little Traditions
within Indian society, encompassing localized cultural practices and
subcultures such as caste, kinship, marriage, and religion. Through a series of
case studies, we assess whether LLMs can balance the interplay between dominant
Great Traditions and localized Little Traditions. We explore various prompting
strategies and further investigate whether using prompts in regional languages
enhances the models cultural sensitivity and response quality. Our findings
reveal that while LLMs demonstrate an ability to articulate cultural nuances,
they often struggle to apply this understanding in practical, context-specific
scenarios. To the best of our knowledge, this is the first study to analyze
LLMs engagement with Indian subcultures, offering critical insights into the
challenges of embedding cultural diversity in AI systems.",['cs.CL'],False,,,,Computational Discovery of Chiasmus in Ancient Religious Text,"Through the Prism of Culture: Evaluating LLMs' Understanding of Indian
  Subcultures and Traditions"
neg-d2-219,2025-01-15,,2501.08719," For a class of sparse optimization problems with the penalty function of
$\|(\cdot)_+\|_0$, we first characterize its local minimizers and then propose
an extrapolated hard thresholding algorithm to solve such problems. We show
that the iterates generated by the proposed algorithm with $\epsilon>0$ (where
$\epsilon$ is the dry friction coefficient) have finite length, without relying
on the Kurdyka-{\L}ojasiewicz inequality. Furthermore, we demonstrate that the
algorithm converges to an $\epsilon$-local minimizer of this problem. For the
special case that $\epsilon=0$, we establish that any accumulation point of the
iterates is a local minimizer of the problem. Additionally, we analyze the
convergence when an error term is present in the algorithm, showing that the
algorithm still converges in the same manner as before, provided that the
errors asymptotically approach zero. Finally, we conduct numerical experiments
to verify the theoretical results of the proposed algorithm.",['math.OC'],2503.06702," We propose and analyze a sequential quadratic programming algorithm for
minimizing a noisy nonlinear smooth function subject to noisy nonlinear smooth
equality constraints. The algorithm uses a step decomposition strategy and, as
a result, is robust to potential rank-deficiency in the constraints, allows for
two different step size strategies, and has an early stopping mechanism. Under
the linear independence constraint qualification, convergence is established to
a neighborhood of a first-order stationary point, where the radius of the
neighborhood is proportional to the noise levels in the objective function and
constraints. Moreover, in the rank-deficient setting, the merit parameter may
converge to zero, and convergence to a neighborhood of an infeasible stationary
point is established. Numerical experiments demonstrate the efficiency and
robustness of the proposed method.",['math.OC'],False,,,,"Extrapolated Hard Thresholding Algorithms with Finite Length for
  Composite $\ell_0$ Penalized Problems","Optimistic Noise-Aware Sequential Quadratic Programming for Equality
  Constrained Optimization with Rank-Deficient Jacobians"
neg-d2-220,2025-02-25,,2502.17927," Alignment techniques enable Large Language Models (LLMs) to generate outputs
that align with human preferences and play a crucial role in their
effectiveness. However, their impact often diminishes when applied to Small
Language Models (SLMs), likely due to the limited capacity of these models.
Instead of directly applying existing alignment techniques to SLMs, we propose
to utilize a well-aligned teacher LLM to guide the alignment process for these
models, thereby facilitating the transfer of the teacher's knowledge of human
preferences to the student model. To achieve this, we first explore a
straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that
employs knowledge distillation with two KL-divergence constraints from the
aligned teacher to the unaligned student. To further enhance the student's
ability to distinguish between preferred and dispreferred responses, we then
propose Advantage-Guided Distillation for Preference Alignment (ADPA), which
leverages an advantage function from the aligned teacher to deliver more
nuanced, distribution-level reward signals for the student's alignment. Our
experimental results show that these two approaches appreciably improve the
alignment of SLMs and narrow the performance gap with larger counterparts.
Among them, ADPA demonstrates superior performance and achieves even greater
effectiveness when integrated with DCKD. Our code is available at
https://github.com/SLIT-AI/ADPA.",['cs.CL'],2502.11405," Despite being pretrained on multilingual corpora, large language models
(LLMs) exhibit suboptimal performance on low-resource languages. Recent
approaches have leveraged multilingual encoders alongside LLMs by introducing
trainable parameters connecting the two models. However, these methods
typically focus on the encoder's output, overlooking valuable information from
other layers. We propose \aname (\mname), a framework that integrates
representations from all encoder layers, coupled with the \attaname mechanism
to enable layer-wise interaction between the LLM and the multilingual encoder.
Extensive experiments on multilingual reasoning tasks, along with analyses of
learned representations, show that our approach consistently outperforms
existing baselines.",['cs.CL'],False,,,,"Advantage-Guided Distillation for Preference Alignment in Small Language
  Models","LayAlign: Enhancing Multilingual Reasoning in Large Language Models via
  Layer-Wise Adaptive Fusion and Alignment Strategy"
neg-d2-221,2025-01-17,,2501.10247," Noisy intermediate-scale quantum processors have produced a quantum
computation revolution in recent times. However, to make further advances new
strategies to overcome the error rate growth are needed. One possible way out
is dividing these devices into many cores. On the other hand, the majorization
criterion efficiently classifies quantum circuits in terms of their complexity,
which can be directly related to their ability of performing non classically
simulatable computations. In this paper, we use this criterion to study the
complexity behavior of a paradigmatic universal family of random circuits
distributed into several cores with different architectures. We find that the
optimal complexity is reached with few interconnects, this giving further hope
to actual implementations in nowadays available devices. A universal behavior
is found irrespective of the architecture and (approximately) of the core size.
We also analyze the complexity properties when scaling processors up by means
of adding cores of the same size. We provide a conjecture to explain the
results.",['quant-ph'],2501.17526," We investigate the charging dynamics of a frequency-modulated quantum battery
(QB) placed within a dissipative cavity environment. Our study focuses on the
interaction of such a battery under both weak and strong coupling regimes,
employing a model in which the quantum battery and charger are represented as
frequency-modulated qubits indirectly coupled through a zero-temperature
environment. It is demonstrated that both the modulation frequency and
amplitude are crucial for optimizing the charging process and the ergotropy of
the quantum battery. Specifically, high-amplitude, low-frequency modulation
significantly enhances charging performance and work extraction in the strong
coupling regime. As an intriguing result, it is deduced that modulation at very
low frequencies leads to the emergence of energy storage and work extraction in
the weak coupling regime. Such a result can never be achieved without
modulation in the weak coupling regime. These results highlight the importance
of adjusting modulation parameters to optimize the performance of quantum
batteries for real-world applications in quantum technologies.",['quant-ph'],False,,,,Optimal multicore quantum computing with few interconnects,Amplified quantum battery via dynamical modulation
neg-d2-222,2025-02-11,,2502.08107," This study advances real-time volumetric cloud rendering in Computer Graphics
(CG) by developing a specialized shader in Unreal Engine (UE), focusing on
realistic cloud modeling and lighting. By leveraging ray-casting-based lighting
algorithms, this work demonstrates the practical application of a dual-layered
procedural noise model, eliminating the need for conventional two-dimensional
(2D) weather textures. The shader allows for procedurally configured skies with
a defined parameter set, offering flexibility for both artistic expression and
realistic simulation. Empirical results reveal that the shader achieves an
average rendering time of 35ms per frame while maintaining high visual accuracy
and scene realism. Visual fidelity assessments indicate a 15% improvement in
cloud realism over traditional 2D techniques, particularly in dynamic lighting
scenarios. This research contributes to CG by bridging technical and aesthetic
elements, enhancing real-time visual storytelling and immersion within gigital
media environments.",['cs.GR'],2502.08107," This study advances real-time volumetric cloud rendering in Computer Graphics
(CG) by developing a specialized shader in Unreal Engine (UE), focusing on
realistic cloud modeling and lighting. By leveraging ray-casting-based lighting
algorithms, this work demonstrates the practical application of a dual-layered
procedural noise model, eliminating the need for conventional two-dimensional
(2D) weather textures. The shader allows for procedurally configured skies with
a defined parameter set, offering flexibility for both artistic expression and
realistic simulation. Empirical results reveal that the shader achieves an
average rendering time of 35ms per frame while maintaining high visual accuracy
and scene realism. Visual fidelity assessments indicate a 15% improvement in
cloud realism over traditional 2D techniques, particularly in dynamic lighting
scenarios. This research contributes to CG by bridging technical and aesthetic
elements, enhancing real-time visual storytelling and immersion within gigital
media environments.",['cs.GR'],False,,,,"Machine Learning-Driven Volumetric Cloud Rendering: Procedural Shader
  Optimization and Dynamic Lighting in Unreal Engine for Realistic Atmospheric
  Simulation","Machine Learning-Driven Volumetric Cloud Rendering: Procedural Shader
  Optimization and Dynamic Lighting in Unreal Engine for Realistic Atmospheric
  Simulation"
neg-d2-223,2025-02-17,,2502.11511," GRB 240529A is a long-duration gamma-ray burst (GRB) whose light curve of
prompt emission is composed of a triple-episode structure, separated by
quiescent gaps of tens to hundreds of seconds. More interestingly, its X-ray
light curve of afterglow exhibits two-plateau emissions, namely, an internal
plateau emission that is smoothly connected with a $\sim t^{-0.1}$ segment and
followed by a $\sim t^{-2}$ power-law decay. The three episodes in the prompt
emission, together with two plateau emissions in X-ray, are unique in the Swift
era. They are very difficult to explain with the standard internal/external
shock model by invoking a black hole central engine. However, it could be
consistent with the prediction of a supramassive magnetar as the central
engine, the physical process of phase transition from magnetar to strange star,
as well as the cooling and spin-down of the strange star. In this paper, we
propose that the first- and second-episode emissions in the prompt $\gamma-$ray
of GRB 240529A are from the jet emission of a massive star collapsing into a
supramassive magnetar and the re-activity of central engine, respectively.
Then, the third-episode emission of prompt is attributed to the phase
transition from a magnetar to a strange star. Finally, the first- and
second-plateau emissions of the X-ray afterglow are powered by the cooling and
spin-down of the strange star, respectively. The observational data of each
component of GRB 240529A are roughly coincident with the estimations of the
above physical picture.",['astro-ph.HE'],2503.15084," The discovery of the kilonova (KN) AT 2017gfo, accompanying the gravitational
wave event GW170817, provides crucial insight into the synthesis of heavy
elements during binary neutron star (BNS) mergers. Following this landmark
event, another KN was detected in association with the second-brightest
gamma-ray burst (GRB) observed to date, GRB 230307A, and subsequently confirmed
by observations of the James Webb Space Telescope (JWST). In this work, we
conduct an end-to-end simulation to analyze the temporal evolution of the KN AT
2023vfi associated with GRB 230307A, and constrain the abundances of superheavy
elements produced. We find that the temporal evolution of AT 2023vfi is similar
to AT 2017gfo in the first week post-burst. Additionally, the
\textit{r}-process nuclide abundances of lanthanide-rich ejecta, derived from
numerical relativity simulations of BNS mergers, can also successfully
interpret the temporal evolution of the KN with the lanthanide-rich ejecta mass
of $0.02 M_\odot$, which is consistent with the mass range of dynamical ejecta
from numerical simulations in literature. Both findings strongly suggest the
hypothesis that GRB 230307A originated from a BNS merger, similar to AT
2017gfo. Based on the first time observation of the KN for JWST, we are able to
constrain the superheavy elements of another KN following AT 2017gfo. The
pre-radioactive-decay abundances of the superheavy nuclides: $^{222}$Rn,
$^{223}$Ra, $^{224}$Ra and $^{225}$Ac, are estimated to be at least on the
order of $1 \times 10^{-5}$. These abundance estimates provide valuable insight
into the synthesis of superheavy elements in BNS mergers, contributing to our
understanding of astrophysical \textit{r}-process nucleosynthesis.",['astro-ph.HE'],False,,,,Signature of strange star as the central engine of GRB 240529A,A constraint on superheavy elements of the GRB-kilonova AT 2023vfi
neg-d2-224,2025-01-20,,2501.11874," The aim of this paper is to investigate the large deviations for a class of
slow-fast mean-field diffusions, which extends some existing results to the
case where the laws of fast process are also involved in the slow component.
Due to the perturbations of fast process and its time marginal law, one cannot
prove the large deviations based on verifying the powerful weak convergence
criterion directly. To overcome this problem, we employ the functional
occupation measure, which combined with the notion of the viable pair and the
controls of feedback form to characterize the limits of controlled sequences
and justify the upper and lower bounds of Laplace principle. As a consequence,
the explicit representation formula of the rate function for large deviations
is also presented.",['math.PR'],2502.0256," The automorphism group of a transitive graph defines a weight function on the
vertices through the Haar modulus. Benjamini, Lyons, Peres, and Schramm
introduced the notion of weighted-amenability for a transitive graph, which is
equivalent to the amenability of its automorphism group. We prove that this
property is equivalent to level-amenability, that is, the property that the
collection of vertices of weights in a given finite set always induces an
amenable graph. We then use this to prove a version of Hutchcroft's conjecture
about $p_h<p_u$, relaxed \`a la Pak-Smirnova-Nagnibeda, where $p_h$ is the
critical probability for the regime where clusters of infinite total weight
arise, and $p_u$ is the uniqueness threshold. Further characterizations are
given in terms of the spectral radius and invariant spanning forests. One
consequence is the continuity of the phase transition at $p_h$ for
weighted-nonamenable graphs.",['math.PR'],False,,,,Large Deviations for Slow-Fast Mean-Field Diffusions,Weighted-amenability and percolation
neg-d2-225,2025-01-11,,2501.06461," This study explores the use of artificial intelligence (AI) as a
complementary tool for grading essay-type questions in higher education,
focusing on its consistency with human grading and potential to reduce biases.
Using 70 handwritten exams from an introductory sociology course, we evaluated
generative pre-trained transformers (GPT) models' performance in transcribing
and scoring students' responses. GPT models were tested under various settings
for both transcription and grading tasks. Results show high similarity between
human and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in
accuracy. For grading, GPT demonstrated strong correlations with the human
grader scores, especially when template answers were provided. However,
discrepancies remained, highlighting GPT's role as a ""second grader"" to flag
inconsistencies for assessment reviewing rather than fully replace human
evaluation. This study contributes to the growing literature on AI in
education, demonstrating its potential to enhance fairness and efficiency in
grading essay-type questions.",['cs.AI'],2501.06461," This study explores the use of artificial intelligence (AI) as a
complementary tool for grading essay-type questions in higher education,
focusing on its consistency with human grading and potential to reduce biases.
Using 70 handwritten exams from an introductory sociology course, we evaluated
generative pre-trained transformers (GPT) models' performance in transcribing
and scoring students' responses. GPT models were tested under various settings
for both transcription and grading tasks. Results show high similarity between
human and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in
accuracy. For grading, GPT demonstrated strong correlations with the human
grader scores, especially when template answers were provided. However,
discrepancies remained, highlighting GPT's role as a ""second grader"" to flag
inconsistencies for assessment reviewing rather than fully replace human
evaluation. This study contributes to the growing literature on AI in
education, demonstrating its potential to enhance fairness and efficiency in
grading essay-type questions.",['cs.AI'],False,,,,"Assessing instructor-AI cooperation for grading essay-type questions in
  an introductory sociology course","Assessing instructor-AI cooperation for grading essay-type questions in
  an introductory sociology course"
neg-d2-226,2025-01-29,,2501.17849," Theories of self-organised active fluid surfaces have emerged as an important
class of minimal models for the shape dynamics of biological membranes, cells
and tissues. Here, we develop and apply a variational approach for active fluid
surfaces to systematically study the nonlinear dynamics and emergent shape
spaces such theories give rise to. To represent dynamic surfaces, we design an
arbitrary Lagrangian-Eulerian parameterizations for deforming surfaces.
Exploiting the symmetries imposed by Onsager relations, we construct a
variational formulation that is based on the entropy production in active
surfaces. The resulting dissipation functional is complemented by Lagrange
multipliers to relax nonlinear geometric constraints, which allows for a direct
computation of steady state solutions of surface shapes and flows. We apply
this framework to study the dynamics of open fluid membranes and closed active
fluid surfaces, and characterize the space of stationary solutions that
corresponding surfaces and flows occupy. Our analysis rationalizes the
interplay of first-order shape transitions of internally and externally forced
fluid membranes, reveals degenerate regions in stationary shape spaces of
mechanochemically active surfaces and identifies a mechanism by which
hydrodynamic screening controls the geometry of active surfaces undergoing cell
division-like shape transformations.",['cond-mat.soft'],2501.17849," Theories of self-organised active fluid surfaces have emerged as an important
class of minimal models for the shape dynamics of biological membranes, cells
and tissues. Here, we develop and apply a variational approach for active fluid
surfaces to systematically study the nonlinear dynamics and emergent shape
spaces such theories give rise to. To represent dynamic surfaces, we design an
arbitrary Lagrangian-Eulerian parameterizations for deforming surfaces.
Exploiting the symmetries imposed by Onsager relations, we construct a
variational formulation that is based on the entropy production in active
surfaces. The resulting dissipation functional is complemented by Lagrange
multipliers to relax nonlinear geometric constraints, which allows for a direct
computation of steady state solutions of surface shapes and flows. We apply
this framework to study the dynamics of open fluid membranes and closed active
fluid surfaces, and characterize the space of stationary solutions that
corresponding surfaces and flows occupy. Our analysis rationalizes the
interplay of first-order shape transitions of internally and externally forced
fluid membranes, reveals degenerate regions in stationary shape spaces of
mechanochemically active surfaces and identifies a mechanism by which
hydrodynamic screening controls the geometry of active surfaces undergoing cell
division-like shape transformations.",['cond-mat.soft'],False,,,,"Self-organised dynamics and emergent shape spaces of active isotropic
  fluid surfaces","Self-organised dynamics and emergent shape spaces of active isotropic
  fluid surfaces"
neg-d2-227,2025-03-03,,2503.0227," Salient object detection (SOD) in RGB-D images is an essential task in
computer vision, enabling applications in scene understanding, robotics, and
augmented reality. However, existing methods struggle to capture global
dependency across modalities, lack comprehensive saliency priors from both RGB
and depth data, and are ineffective in handling low-quality depth maps. To
address these challenges, we propose SSNet, a saliency-prior and state space
model (SSM)-based network for the RGB-D SOD task. Unlike existing convolution-
or transformer-based approaches, SSNet introduces an SSM-based multi-modal
multi-scale decoder module to efficiently capture both intra- and inter-modal
global dependency with linear complexity. Specifically, we propose a
cross-modal selective scan SSM (CM-S6) mechanism, which effectively captures
global dependency between different modalities. Furthermore, we introduce a
saliency enhancement module (SEM) that integrates three saliency priors with
deep features to refine feature representation and improve the localization of
salient objects. To further address the issue of low-quality depth maps, we
propose an adaptive contrast enhancement technique that dynamically refines
depth maps, making them more suitable for the RGB-D SOD task. Extensive
quantitative and qualitative experiments on seven benchmark datasets
demonstrate that SSNet outperforms state-of-the-art methods.",['cs.CV'],2503.15625," Surficial geologic mapping is essential for understanding Earth surface
processes, addressing modern challenges such as climate change and national
security, and supporting common applications in engineering and resource
management. However, traditional mapping methods are labor-intensive, limiting
spatial coverage and introducing potential biases. To address these
limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset
specifically designed for surficial geologic mapping and Earth surface
analysis. EarthScape integrates high-resolution aerial RGB and near-infrared
(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain
features, and hydrologic and infrastructure vector data. The dataset provides
detailed annotations for seven distinct surficial geologic classes encompassing
various geological processes. We present a comprehensive data processing
pipeline using open-sourced raw data and establish baseline benchmarks using
different spatial modalities to demonstrate the utility of EarthScape. As a
living dataset with a vision for expansion, EarthScape bridges the gap between
computer vision and Earth sciences, offering a valuable resource for advancing
research in multimodal learning, geospatial analysis, and geological mapping.
Our code is available at https://github.com/masseygeo/earthscape.",['cs.CV'],False,,,,"SSNet: Saliency Prior and State Space Model-based Network for Salient
  Object Detection in RGB-D Images","EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and
  Earth Surface Analysis"
neg-d2-228,2025-03-01,,2503.00677," General continual learning (GCL) is a broad concept to describe real-world
continual learning (CL) problems, which are often characterized by online data
streams without distinct transitions between tasks, i.e., blurry task
boundaries. Such requirements result in poor initial performance, limited
generalizability, and severe catastrophic forgetting, heavily impacting the
effectiveness of mainstream GCL models trained from scratch. While the use of a
frozen pretrained backbone with appropriate prompt tuning can partially address
these challenges, such prompt-based methods remain suboptimal for CL of
remaining tunable parameters on the fly. In this regard, we propose an
innovative approach named MISA (Mask and Initial Session Adaption) to advance
prompt-based methods in GCL. It includes a forgetting-aware initial session
adaption that employs pretraining data to initialize prompt parameters and
improve generalizability, as well as a non-parametric logit mask of the output
layers to mitigate catastrophic forgetting. Empirical results demonstrate
substantial performance gains of our approach compared to recent competitors,
especially without a replay buffer (e.g., up to 18.39%, 22.06%, and 11.96%
performance lead on CIFAR-100, Tiny-ImageNet, and ImageNet-R, respectively).
Moreover, our approach features the plug-in nature for prompt-based methods,
independence of replay, ease of implementation, and avoidance of CL-relevant
hyperparameters, serving as a strong baseline for GCL research. Our source code
is publicly available at https://github.com/kangzhiq/MISA",['cs.CV'],2502.19896," Existing point cloud completion methods, which typically depend on predefined
synthetic training datasets, encounter significant challenges when applied to
out-of-distribution, real-world scans. To overcome this limitation, we
introduce a zero-shot completion framework, termed GenPC, designed to
reconstruct high-quality real-world scans by leveraging explicit 3D generative
priors. Our key insight is that recent feed-forward 3D generative models,
trained on extensive internet-scale data, have demonstrated the ability to
perform 3D generation from single-view images in a zero-shot setting. To
harness this for completion, we first develop a Depth Prompting module that
links partial point clouds with image-to-3D generative models by leveraging
depth images as a stepping stone. To retain the original partial structure in
the final results, we design the Geometric Preserving Fusion module that aligns
the generated shape with input by adaptively adjusting its pose and scale.
Extensive experiments on widely used benchmarks validate the superiority and
generalizability of our approach, bringing us a step closer to robust
real-world scan completion.",['cs.CV'],False,,,,"Advancing Prompt-Based Methods for Replay-Independent General Continual
  Learning",GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors
neg-d2-229,2025-03-18,,2503.14504," Large language models (LLMs) can handle a wide variety of general tasks with
simple prompts, without the need for task-specific training. Multimodal Large
Language Models (MLLMs), built upon LLMs, have demonstrated impressive
potential in tackling complex tasks involving visual, auditory, and textual
data. However, critical issues related to truthfulness, safety, o1-like
reasoning, and alignment with human preference remain insufficiently addressed.
This gap has spurred the emergence of various alignment algorithms, each
targeting different application scenarios and optimization goals. Recent
studies have shown that alignment algorithms are a powerful approach to
resolving the aforementioned challenges. In this paper, we aim to provide a
comprehensive and systematic review of alignment algorithms for MLLMs.
Specifically, we explore four key aspects: (1) the application scenarios
covered by alignment algorithms, including general image understanding,
multi-image, video, and audio, and extended multimodal applications; (2) the
core factors in constructing alignment datasets, including data sources, model
responses, and preference annotations; (3) the benchmarks used to evaluate
alignment algorithms; and (4) a discussion of potential future directions for
the development of alignment algorithms. This work seeks to help researchers
organize current advancements in the field and inspire better alignment
methods. The project page of this paper is available at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",['cs.CV'],2501.1215," Deferred neural rendering (DNR) is an emerging computer graphics pipeline
designed for high-fidelity rendering and robotic perception. However, DNR
heavily relies on datasets composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the rendering
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
rendering. We first propose a novel view selector for deferred neural rendering
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
rendering. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity rendering results. We conduct detailed experiments and
ablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness
of DNRSelect. The code will be released.",['cs.CV'],False,,,,Aligning Multimodal LLM with Human Preference: A Survey,DNRSelect: Active Best View Selection for Deferred Neural Rendering
neg-d2-230,2025-02-24,,2502.17404," In this note, we show that the $p$-adic periods of motives introduced
recently by Ancona and Fr\u{a}\c{t}il\u{a} (``Andr\'e periods'') reduce to the
classically studied notion in the case of Mixed Tate motives. We also connect
Andr\'e periods with Coleman integration by observing that the Frobenius-fixed
de Rham paths of Besser and Vologodsky come from motivic paths in
characteristic $p$ (unconditionally in the mixed Tate setting, conditionally in
general). We use this to realize special values of $p$-adic multiple
polylogarithms as Andr\'e periods in a concrete way.",['math.AG'],2502.04966," We study the geometry of the Hitchin fibration for $\mathcal{L}$-valued
$G$-Higgs bundles over a smooth projective curve of genus $g$, where $G$ is a
reductive group and $\mathcal{L}$ is a suitably positive line bundle. We show
that the Hitchin fibration admits the structure of a weak Abelian fibration. In
the case when the line bundle $\mathcal{L}$ is a twist of the canonical bundle
of the curve by a (possibly empty) reduced effective divisor, we prove a
cohomological bound and $\delta$-regularity of the weak Abelian fibration.",['math.AG'],False,,,,On Andr\'e periods of mixed Tate motives,Hitchin fibrations are Ng\^{o} fibrations
neg-d2-231,2025-01-09,,2501.05344," Let $X$ be a ruled surface over a nonsingular curve $C$ of genus $g\geq0$.
The main goal of this paper is to construct simple prioritary vector bundles of
any rank $r$ on $X$ and to give effective bounds for the dimension of their
module of global sections.",['math.AG'],2503.16255," We investigate limit linear series on chains of elliptic curves, giving a
simple proof of a conjecture of Farkas stating the existence of curves with a
theta-characteristic with a given number of sections for the expected range of
genera. Using the additional structure afforded by considering limit linear
series on chains of elliptic curves, we find examples of reducible
Brill-Noether loci, admitting at least two components, with and without a
theta-characteristic respectively. This allows us to display reducible Hilbert
schemes for $r\ge 3$ and the largest possible value of $d$, namely $d=g-1$. We
also give examples of Brill-Noether loci with three components. On the positive
side, we provide optimal bounds on the degree under which Brill-Noether loci
are irreducible when $r=2$.",['math.AG'],False,,,,"Higher rank prioritary bundles on ruled surfaces and their global
  sections",Some reducible and irreducible Brill-Noether loci
neg-d2-232,2025-01-15,,2501.09281," In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.",['cs.CV'],2501.12218," Point tracking in videos is a fundamental task with applications in robotics,
video editing, and more. While many vision tasks benefit from pre-trained
feature backbones to improve generalizability, point tracking has primarily
relied on simpler backbones trained from scratch on synthetic data, which may
limit robustness in real-world scenarios. Additionally, point tracking requires
temporal awareness to ensure coherence across frames, but using
temporally-aware features is still underexplored. Most current methods often
employ a two-stage process: an initial coarse prediction followed by a
refinement stage to inject temporal information and correct errors from the
coarse stage. These approach, however, is computationally expensive and
potentially redundant if the feature backbone itself captures sufficient
temporal information.
  In this work, we introduce Chrono, a feature backbone specifically designed
for point tracking with built-in temporal awareness. Leveraging pre-trained
representations from self-supervised learner DINOv2 and enhanced with a
temporal adapter, Chrono effectively captures long-term temporal context,
enabling precise prediction even without the refinement stage. Experimental
results demonstrate that Chrono achieves state-of-the-art performance in a
refiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, among
common feature backbones used in point tracking as well as DINOv2, with
exceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/",['cs.CV'],False,,,,SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection,Exploring Temporally-Aware Features for Point Tracking
neg-d2-233,2025-03-17,,2503.13689," Traditional hypothesis testing methods for differences in binomial
proportions can either be too liberal (Wald test) or overly conservative
(Fisher's exact test), especially in small samples. Regulators favour
conservative approaches for robust type I error control, though excessive
conservatism may significantly reduce statistical power. We offer fundamental
theoretical contributions that extend an approach proposed in 1969, resulting
in the derivation of a family of exact tests designed to maximize a specific
type of power. We establish theoretical guarantees for controlling type I error
despite the discretization of the null parameter space. This theoretical
advancement is supported by a comprehensive series of experiments to
empirically quantify the power advantages compared to traditional hypothesis
tests. The approach determines the rejection region through a binary decision
for each outcome dataset and uses integer programming to find an optimal
decision boundary that maximizes power subject to type I error constraints. Our
analysis provides new theoretical properties and insights into this approach's
comparative advantages. When optimized for average power over all possible
parameter configurations under the alternative, the method exhibits remarkable
robustness, performing optimally or near-optimally across specific alternatives
while maintaining exact type I error control. The method can be further
customized for particular prior beliefs by using a weighted average. The
findings highlight both the method's practical utility and how techniques from
combinatorial optimization can enhance statistical methodology.",['stat.ME'],2503.04093," Developing tools for estimating heterogeneous treatment effects (HTE) and
individualized treatment effects has been an area of active research in recent
years. While these tools have proven to be useful in many contexts, a concern
when deploying such methods is the degree to which incorporating HTE into a
prediction model provides an advantage over predictive methods which do not
allow for variation in treatment effect across individuals. To address this
concern, we propose a procedure which evaluates the extent to which an HTE
model provides a predictive advantage. Specifically, our procedure targets the
gain in predictive performance from using a flexible predictive model
incorporating HTE versus an alternative model which is similar to the
HTE-utilizing model except that it is constrained to not allow variation in
treatment effect. By drawing upon recent work in using nested cross-validation
techniques for prediction error inference, we generate confidence intervals for
this measure of gain in predictive performance which allows one to directly
calculate the level at which one is confident of a substantial HTE-modeling
gain in prediction -- a quantity which we refer to as the h-value. Our
procedure is generic and can be directly used to assess the benefit of modeling
HTE for any method that incorporates treatment effect variation.",['stat.ME'],False,,,,"Exact statistical tests using integer programming: Leveraging an
  overlooked approach for maximizing power for differences between binomial
  proportions",Evaluating and Testing for Actionable Treatment Effect Heterogeneity
neg-d2-234,2025-02-14,,2502.09936," Atomic magnetometers based on Zeeman shift measurement have the potential for
high sensitivity and long-term stability. Like other atomic sensors including
atomic clocks and atom interferometers, the atomic magnetometer could in
principle be augmented with spin squeezing for further sensitivity enhancement.
However, existing atomic magnetometers are not compatible with spin squeezing
because the atoms can hardly be in a pure quantum state during operation. A
natural challenge is the arbitrary direction of the magnetic field. In this
paper, we propose a cold-atom-based magnetometer with spin squeezing that can
measure both the magnitude and the direction of an arbitrary magnetic field.
For experimentally accessible parameters, we show that the technique described
above could achieve a sensitivity nearly three orders of magnitude higher than
that of the best existing magnetometers.",['quant-ph'],2503.16314," Understanding the impact of disturbances in quantum channels is of paramount
importance for the implementation of many quantum technologies, as noise can be
detrimental to quantum correlations. Among the various types of disturbances,
we explore the effects of white and colored noise and experimentally test the
resilience of a quantum ghost spectrometer against these two types of noise,
showing that it is always robust against white noise, whereas colored noise
introduces a huge impact on the process.",['quant-ph'],False,,,,Spin-squeezed vector atomic magnetometry,"An experimental investigation of quantum frequency correlations
  resilience against white and colored noise"
neg-d2-235,2025-01-14,,2501.08178," Quasiperiodicity, a partially synchronous state that precedes the onset of
forced synchronization in hydrodynamic systems, exhibits distinct geometrical
patterns based on the specific route to lock-in. In this study, we explore
these dynamic behaviors using recurrence quantification analysis. Focusing on a
self-excited hydrodynamic system-a low-density jet subjected to external
acoustic forcing at varying frequencies and amplitudes. We generate recurrence
plots from unsteady velocity time traces. These recurrence plots provide
insight into the synchronization dynamics and pathways of the jet under forced
conditions. Further, we show that recurrence quantities are helpful to detect
and distinguish between different routes to lock-in.",['physics.flu-dyn'],2503.09359," We explore the mechanisms and regimes of mixing in yield-stress fluids by
simulating the stirring of an infinite, two-dimensional domain filled with a
Bingham fluid. A cylindrical stirrer moves along a circular path at constant
speed to stir the fluid, with an initially quiescent domain marked by a passive
dye in the lower half, facilitating the analysis of dye interface evolution and
mixing dynamics. We first examine the mixing process in Newtonian fluids,
identifying three key mechanisms: interface stretching and folding around the
stirrer's path, diffusion across streamlines, and dye advection and interface
stretching due to vortex shedding. Introducing yield stress into the system
leads to notable localization effects in mixing, manifesting through three
mechanisms: advection of vortices within a finite distance of the stirrer,
vortex entrapment near the stirrer, and complete suppression of vortex shedding
at high yield stresses. Based on these mechanisms, we classify three distinct
mixing regimes in yield-stress fluids: (i) Regime SE, where shed vortices
escape the central region, (ii) Regime ST, where shed vortices remain trapped
near the stirrer, and (iii) Regime NS, where no vortex shedding occurs. These
regimes are quantitatively distinguished through spectral analysis of energy
oscillations, revealing transitions and the critical Bingham and Reynolds
numbers. The transitions are captured through effective Reynolds numbers,
supporting a hypothesis that mixing regime transitions in yield-stress fluids
share fundamental characteristics with bluff-body flow dynamics. The findings
provide a mechanistic framework for understanding and predicting mixing
behaviors in yield-stress fluids, suggesting that the localization mechanisms
and mixing regimes observed here are archetypal for stirred-tank applications.",['physics.flu-dyn'],False,,,,"Detection and analysis of synchronization routes in an axially forced
  globally unstable jet using recurrence quantification","Yield-Stress Fluid Mixing: Localization Mechanisms and Regime
  Transitions"
neg-d2-236,2025-01-22,,2501.12686," In rechargeable batteries, electron transport properties of inorganics in the
solid-electrolyte interphase (SEI) critically determine the safety, lifespan
and capacity loss of batteries. However, the electron transport properties of
heterogeneous interfaces among different solid inorganics in SEI have not been
studied experimentally or theoretically yet, although such heterogeneous
interfaces exist inevitably. Here, by employing non-equilibrium Green's
function (NEGF) method, we theoretically evaluated the atomic-scale electron
transport properties under bias voltage for LiF/Li2O interfaces and
single-component layers of them, since LiF and Li2O are common stable
inorganics in the SEI. We reveal that heterogeneous interfaces orthogonal to
the external electric-field direction greatly impede electron transport in SEI,
whereas heterogeneous parallel-orientated interfaces enhance it. Structural
disorders induced by densely distributed interfaces can severely interfere with
electron transport. For each component, single-crystal LiF is highly effective
to block electron transport, with a critical thickness of 2.9 nm, much smaller
than that of Li2O (19.0 nm). This study sheds a new light into direct and
quantitative understanding of the electron transport properties of
heterogeneous interfaces in SEI, which holds promise for the advancement of a
new generation of high-performance batteries.",['cond-mat.mtrl-sci'],2501.12686," In rechargeable batteries, electron transport properties of inorganics in the
solid-electrolyte interphase (SEI) critically determine the safety, lifespan
and capacity loss of batteries. However, the electron transport properties of
heterogeneous interfaces among different solid inorganics in SEI have not been
studied experimentally or theoretically yet, although such heterogeneous
interfaces exist inevitably. Here, by employing non-equilibrium Green's
function (NEGF) method, we theoretically evaluated the atomic-scale electron
transport properties under bias voltage for LiF/Li2O interfaces and
single-component layers of them, since LiF and Li2O are common stable
inorganics in the SEI. We reveal that heterogeneous interfaces orthogonal to
the external electric-field direction greatly impede electron transport in SEI,
whereas heterogeneous parallel-orientated interfaces enhance it. Structural
disorders induced by densely distributed interfaces can severely interfere with
electron transport. For each component, single-crystal LiF is highly effective
to block electron transport, with a critical thickness of 2.9 nm, much smaller
than that of Li2O (19.0 nm). This study sheds a new light into direct and
quantitative understanding of the electron transport properties of
heterogeneous interfaces in SEI, which holds promise for the advancement of a
new generation of high-performance batteries.",['cond-mat.mtrl-sci'],False,,,,"Electron transport properties of heterogeneous interfaces in solid
  electrolyte interphase on lithium metal anodes","Electron transport properties of heterogeneous interfaces in solid
  electrolyte interphase on lithium metal anodes"
neg-d2-237,2025-03-06,,2503.04268," In this report, I present an inpainting framework named \textit{ControlFill},
which involves training two distinct prompts: one for generating plausible
objects within a designated mask (\textit{creation}) and another for filling
the region by extending the background (\textit{removal}). During the inference
stage, these learned embeddings guide a diffusion network that operates without
requiring heavy text encoders. By adjusting the relative significance of the
two prompts and employing classifier-free guidance, users can control the
intensity of removal or creation. Furthermore, I introduce a method to
spatially vary the intensity of guidance by assigning different scales to
individual pixels.",['cs.CV'],2501.06312," Foundation models are becoming increasingly popular due to their strong
generalization capabilities resulting from being trained on huge datasets.
These generalization capabilities are attractive in areas such as NIR Iris
Presentation Attack Detection (PAD), in which databases are limited in the
number of subjects and diversity of attack instruments, and there is no
correspondence between the bona fide and attack images because, most of the
time, they do not belong to the same subjects. This work explores an iris PAD
approach based on two foundation models, DinoV2 and VisualOpenClip. The results
show that fine-tuning prediction with a small neural network as head overpasses
the state-of-the-art performance based on deep learning approaches. However,
systems trained from scratch have still reached better results if bona fide and
attack images are available.",['cs.CV'],False,,,,ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning,Towards Iris Presentation Attack Detection with Foundation Models
neg-d2-238,2025-03-23,,2503.18164," We compute the closest convex piecewise linear-quadratic (PLQ) function with
minimal number of pieces to a given univariate piecewise linear-quadratic
function. The Euclidean norm is used to measure the distance between functions.
First, we assume that the number and positions of the breakpoints of the output
function are fixed, and solve a convex optimization problem. Next, we assume
the number of breakpoints is fixed, but not their position, and solve a
nonconvex optimization problem to determine optimal breakpoints placement.
Finally, we propose an algorithm composed of a greedy search preprocessing and
a dichotomic search that solves a logarithmic number of optimization problems
to obtain an approximation of any PLQ function with minimal number of pieces
thereby obtaining in two steps the closest convex function with minimal number
of pieces.
  We illustrate our algorithms with multiple examples, compare our approach
with a previous globally optimal univariate spline approximation algorithm, and
apply our method to simplify vertical alignment curves in road design
optimization. CPLEX, Gurobi, and BARON are used with the YALMIP library in
MATLAB to effectively select the most efficient solver.",['math.OC'],2502.20831," This study introduces a dynamic bus lane (DBL) strategy, referred to as the
dynamic bus priority lane (DBPL) strategy, designed for mixed traffic
environments featuring both manual and automated vehicles. Unlike previous DBL
strategies, this approach accounts for partially connected and autonomous
vehicles (CAVs) capable of autonomous trajectory planning. By leveraging this
capability, the strategy grants certain CAVs Right of Way (ROW) in bus lanes
while utilizing their leading effects in general lanes to guide vehicle
platoons through intersections, thereby indirectly influencing the trajectories
of other vehicles. The ROW allocation is optimized using a mixed-integer linear
programming (MILP) model, aimed at minimizing total vehicle travel time. Since
different CAVs entering the bus lane affect other vehicles travel times, the
model incorporates lane change effects when estimating the states of CAVs,
human-driven vehicles (HDVs), and connected autonomous buses (CABs) as they
approach the stop bar. A dynamic control framework with a rolling horizon
procedure is established to ensure precise execution of the ROW optimization
under varying traffic conditions. Simulation experiments across two scenarios
assess the performance of the proposed DBPL strategy at different CAV market
penetration rates (MPRs).",['math.OC'],False,,,,"Closest univariate convex linear-quadratic function approximation with
  minimal number of Pieces","A Dynamic Bus Lane Strategy for Integrated Management of Human-Driven
  and Autonomous Vehicles"
neg-d2-239,2025-03-09,,2503.06471," Dense point tracking is a challenging task requiring the continuous tracking
of every point in the initial frame throughout a substantial portion of a
video, even in the presence of occlusions. Traditional methods use optical flow
models to directly estimate long-range motion, but they often suffer from
appearance drifting without considering temporal consistency. Recent point
tracking algorithms usually depend on sliding windows for indirect information
propagation from the first frame to the current one, which is slow and less
effective for long-range tracking. To account for temporal consistency and
enable efficient information propagation, we present a lightweight and fast
model with \textbf{S}treaming memory for dense \textbf{PO}int \textbf{T}racking
and online video processing. The \textbf{SPOT} framework features three core
components: a customized memory reading module for feature enhancement, a
sensory memory for short-term motion dynamics modeling, and a visibility-guided
splatting module for accurate information propagation. This combination enables
SPOT to perform dense point tracking with state-of-the-art accuracy on the CVO
benchmark, as well as comparable or superior performance to offline models on
sparse tracking benchmarks such as TAP-Vid and RoboTAP. Notably, SPOT with
10$\times$ smaller parameter numbers operates at least 2$\times$ faster than
previous state-of-the-art models while maintaining the best performance on CVO.
We will release the models and codes at: https://github.com/DQiaole/SPOT.",['cs.CV'],2503.13743," We tackle the problem of monocular 3D object detection across different
sensors, environments, and camera setups. In this paper, we introduce a novel
unsupervised domain adaptation approach, MonoCT, that generates highly accurate
pseudo labels for self-supervision. Inspired by our observation that accurate
depth estimation is critical to mitigating domain shifts, MonoCT introduces a
novel Generalized Depth Enhancement (GDE) module with an ensemble concept to
improve depth estimation accuracy. Moreover, we introduce a novel Pseudo Label
Scoring (PLS) module by exploring inner-model consistency measurement and a
Diversity Maximization (DM) strategy to further generate high-quality pseudo
labels for self-training. Extensive experiments on six benchmarks show that
MonoCT outperforms existing SOTA domain adaptation methods by large margins
(~21% minimum for AP Mod.) and generalizes well to car, traffic camera and
drone views.",['cs.CV'],False,,,,Online Dense Point Tracking with Streaming Memory,"MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent
  Teacher Models"
neg-d2-240,2025-03-11,,2503.08035," LLMs often fail to meet the specialized needs of distinct user groups due to
their one-size-fits-all training paradigm \cite{lucy-etal-2024-one} and there
is limited research on what personalization aspects each group expect. To
address these limitations, we propose a group-aware personalization framework,
Group Preference Alignment (GPA), that identifies context-specific variations
in conversational preferences across user groups and then steers LLMs to
address those preferences. Our approach consists of two steps: (1) Group-Aware
Preference Extraction, where maximally divergent user-group preferences are
extracted from real-world conversation logs and distilled into interpretable
rubrics, and (2) Tailored Response Generation, which leverages these rubrics
through two methods: a) Context-Tuned Inference (GAP-CT), that dynamically
adjusts responses via context-dependent prompt instructions, and b)
Rubric-Finetuning Inference (GPA-FT), which uses the rubrics to generate
contrastive synthetic data for personalization of group-specific models via
alignment. Experiments demonstrate that our framework significantly improves
alignment of the output with respect to user preferences and outperforms
baseline methods, while maintaining robust performance on standard benchmarks.",['cs.CL'],2503.04104," Large Language Models (LLMs) have shown remarkable capabilities across tasks,
yet they often require additional prompting techniques when facing complex
problems. While approaches like self-correction and response selection have
emerged as popular solutions, recent studies have shown these methods perform
poorly when relying on the LLM itself to provide feedback or selection
criteria. We argue this limitation stems from the fact that common LLM
post-training procedures lack explicit supervision for discriminative judgment
tasks. In this paper, we propose Generative Self-Aggregation (GSA), a novel
prompting method that improves answer quality without requiring the model's
discriminative capabilities. GSA first samples multiple diverse responses from
the LLM, then aggregates them to obtain an improved solution. Unlike previous
approaches, our method does not require the LLM to correct errors or compare
response quality; instead, it leverages the model's generative abilities to
synthesize a new response based on the context of multiple samples. While GSA
shares similarities with the self-consistency (SC) approach for response
aggregation, SC requires specific verifiable tokens to enable majority voting.
In contrast, our approach is more general and can be applied to open-ended
tasks. Empirical evaluation demonstrates that GSA effectively improves response
quality across various tasks, including mathematical reasoning, knowledge-based
problems, and open-ended generation tasks such as code synthesis and
conversational responses.",['cs.CL'],False,,,,"Group Preference Alignment: Customized LLM Response Generation from
  In-Situ Conversations",LLMs Can Generate a Better Answer by Aggregating Their Own Responses
neg-d2-241,2025-01-07,,2501.04063," Nowadays, there are many similar services available on the internet, making
Quality of Service (QoS) a key concern for users. Since collecting QoS values
for all services through user invocations is impractical, predicting QoS values
is a more feasible approach. Matrix factorization is considered an effective
prediction method. However, most existing matrix factorization algorithms focus
on capturing global similarities between users and services, overlooking the
local similarities between users and their similar neighbors, as well as the
non-interactive effects between users and services. This paper proposes a
matrix factorization approach based on user information entropy and region
bias, which utilizes a similarity measurement method based on fuzzy information
entropy to identify similar neighbors of users. Simultaneously, it integrates
the region bias between each user and service linearly into matrix
factorization to capture the non-interactive features between users and
services. This method demonstrates improved predictive performance in more
realistic and complex network environments. Additionally, numerous experiments
are conducted on real-world QoS datasets. The experimental results show that
the proposed method outperforms some of the state-of-the-art methods in the
field at matrix densities ranging from 5% to 20%.",['cs.LG'],2503.07917," Clustering of high-dimensional data sets is a growing need in artificial
intelligence, machine learning and pattern recognition. In this paper, we
propose a new clustering method based on a combinatorial-topological approach
applied to regions of space defined by signs of coordinates (hyperoctants). In
high-dimensional spaces, this approach often reduces the size of the dataset
while preserving sufficient topological features. According to a density
criterion, the method builds clusters of data points based on the partitioning
of a graph, whose vertices represent hyperoctants, and whose edges connect
neighboring hyperoctants under the Levenshtein distance. We call this method
HyperOctant Search Clustering. We prove some mathematical properties of the
method. In order to as assess its performance, we choose the application of
topic detection, which is an important task in text mining. Our results suggest
that our method is more stable under variations of the main hyperparameter, and
remarkably, it is not only a clustering method, but also a tool to explore the
dataset from a topological perspective, as it directly provides information
about the number of hyperoctants where there are data points. We also discuss
the possible connections between our clustering method and other research
fields.",['cs.LG'],False,,,,"Fuzzy Information Entropy and Region Biased Matrix Factorization for Web
  Service QoS Prediction","Hyperoctant Search Clustering: A Method for Clustering Data in
  High-Dimensional Hyperspheres"
neg-d2-242,2025-02-16,,2502.11364," While multilingual large language models generally perform adequately, and
sometimes even rival English performance on high-resource languages (HRLs),
they often significantly underperform on low-resource languages (LRLs). Among
several prompting strategies aiming at bridging the gap, multilingual
in-context learning (ICL) has been particularly effective when demonstration in
target languages is unavailable. However, there lacks a systematic
understanding of when and why it works well.
  In this work, we systematically analyze multilingual ICL, using
demonstrations in HRLs to enhance cross-lingual transfer. We show that
demonstrations in mixed HRLs consistently outperform English-only ones across
the board, particularly for tasks written in LRLs. Surprisingly, our ablation
study shows that the presence of irrelevant non-English sentences in the prompt
yields measurable gains, suggesting the effectiveness of multilingual exposure
itself. Our results highlight the potential of strategically leveraging
multilingual resources to bridge the performance gap for underrepresented
languages.",['cs.CL'],2502.12611," The rise of Large Language Models (LLMs) necessitates accurate AI-generated
text detection. However, current approaches largely overlook the influence of
author characteristics. We investigate how sociolinguistic attributes-gender,
CEFR proficiency, academic field, and language environment-impact
state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored
texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous
evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our
results reveal significant biases: CEFR proficiency and language environment
consistently affected detector accuracy, while gender and academic field showed
detector-dependent effects. These findings highlight the crucial need for
socially aware AI text detection to avoid unfairly penalizing specific
demographic groups. We offer novel empirical evidence, a robust statistical
framework, and actionable insights for developing more equitable and reliable
detection systems in real-world, out-of-domain contexts. This work paves the
way for future research on bias mitigation, inclusive evaluation benchmarks,
and socially responsible LLM detectors.",['cs.CL'],False,,,,"Blessing of Multilinguality: A Systematic Analysis of Multilingual
  In-Context Learning","Who Writes What: Unveiling the Impact of Author Roles on AI-generated
  Text Detection"
neg-d2-243,2025-02-15,,2502.10787," Excess mortality, i.e. the difference between expected and observed
mortality, is used to quantify the death toll of mortality shocks, such as
infectious disease-related epidemics and pandemics. However, predictions of
expected mortality are sensitive to model assumptions. Among three
specifications of a Serfling-Poisson regression for seasonal mortality, we
analyse which one yields the most accurate predictions. We compare the
Serfling-Poisson models with: 1) parametric effect for the trend and
seasonality (SP), 2) non-parametric effect for the trend and seasonality
(SP-STSS), also known as modulation model, and 3) non-parametric effect for the
trend and parametric effect for the seasonality (SP-STFS). Forecasting is
achieved with P-splines smoothing. The SP-STFS model resulted in more accurate
historical forecasts of monthly rates from national statistical offices in 25
European countries. An application to the COVID-19 pandemic years illustrates
how excess mortality can be used to evaluate the vulnerability of populations
and aid public health planning.",['stat.AP'],2502.17679," Adverse childhood experiences (ACEs) have been linked to a wide range of
negative health outcomes in adulthood. However, few studies have investigated
what specific combinations of ACEs most substantially impact mental health. In
this article, we provide the protocol for our observational study of the
effects of combinations of ACEs on adult depression. We use data from the 2023
Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We
will evaluate the replicability of our findings by splitting the sample into
two discrete subpopulations of individuals. We employ data turnover for this
analysis, enabling a single team of statisticians and domain experts to
collaboratively evaluate the strength of evidence, and also integrating both
qualitative and quantitative insights from exploratory data analysis. We
outline our analysis plan using this method and conclude with a brief
discussion of several specifics for our study.",['stat.AP'],False,,,,Modelling and short-term forecasting of seasonal mortality,"Protocol For An Observational Study On The Effects Of Combinations Of
  Adverse Childhood Experiences On Adult Depression"
neg-d2-244,2025-02-01,,2502.00496," The nodes are traditionally viewed as fixed points where the probability
density vanishes. However, this work demonstrates that these nodes exhibit
time-dependent oscillation in quantum superposition states. We derive this
effect for a fundamental system: the 1D particle in a box. It is shown that the
probability density in a superposition of two eigenstates evolves with a
time-dependent interference term, introducing an oscillation of the nodes at a
specific frequency equal to the energy difference between the states. This
result suggests a deeper dynamical role for nodes in quantum systems.",['quant-ph'],2503.17147," We consider entangling operations in a single nitrogen-vacancy (NV) center in
diamond where the hyperfine-coupled nuclear spin qubits are addressed with
radio-frequency (rf) pulses conditioned on the state of the central electron
spin. Limiting factors for the gate fidelity are coherent errors due to
off-resonant driving of neighboring transitions in the dense, hyperfine-split
energy spectrum of the defect and non-negligible perpendicular hyperfine tensor
components that narrow the choice of $^{13}\rm C$ nuclear spin qubits. We
address these issues by presenting protocols based on synchronization effects
that allow for a complete suppression of both error sources in state-of-the-art
CNOT gate schemes. This is possible by a suitable choice of parameter sets that
incorporate the error into the scheme instead of avoiding it. These results
contribute to the recent progress toward scalable quantum computation with
defects in solids.",['quant-ph'],False,,,,Time evolution of nodes in quantum superposition states,"Suppression of coherent errors during entangling operations in NV
  centers in diamond"
neg-d2-245,2025-02-20,,2502.14972," We present GalactiKit, a data-driven methodology for estimating the lookback
infall time, stellar mass, halo mass and mass ratio of the disrupted
progenitors of Milky Way-like galaxies at the time of infall. GalactiKit uses
simulation-based inference to extract the information on galaxy formation
processes encoded in the Auriga cosmological MHD simulations of Milky Way-mass
halos to create a model that relates the properties of mergers to those of the
corresponding merger debris at $z=0$. We investigate how well GalactiKit can
reconstruct the merger properties given the dynamical, chemical, and the
combined chemo-dynamical information of debris. For this purpose, three models
were implemented considering the following properties of merger debris: (a)
total energy and angular momentum, (b) iron-to-hydrogen and alpha-to-iron
abundance ratios, and (c) a combination of all of these. We find that the
kinematics of the debris can be used to trace the lookback time at which the
progenitor was first accreted into the main halo. However, chemical information
is necessary for inferring the stellar and halo masses of the progenitors. In
both models (b) and (c), the stellar masses are predicted more accurately than
the halo masses, which could be related to the scatter in the stellar mass-halo
mass relation. Model (c) provides the most accurate predictions for the merger
parameters, which suggests that combining chemical and dynamical data of debris
can significantly improve the reconstruction of the Milky Way's assembly
history.",['astro-ph.GA'],2502.08111," The obscuration of light from a distant galaxy has raised the possibility
that a type of carbon dust existed in the earliest epochs of the Universe --
challenging the idea that stars had not yet evolved enough to make such
material.",['astro-ph.GA'],False,,,,"GalactiKit: reconstructing mergers from $z=0$ debris using
  simulation-based inference in Auriga",Interstellar dust revealed by light from cosmic dawn
neg-d2-246,2025-03-18,,2503.14096," In 3D design, specifying design objectives and visualizing complex shapes
through text alone proves to be a significant challenge. Although advancements
in 3D GenAI have significantly enhanced part assembly and the creation of
high-quality 3D designs, many systems still to dynamically generate and edit
design elements based on the shape parameters. To bridge this gap, we propose
GenPara, an interactive 3D design editing system that leverages
text-conditional shape parameters of part-aware 3D designs and visualizes
design space within the Exploration Map and Design Versioning Tree.
Additionally, among the various shape parameters generated by LLM, the system
extracts and provides design outcomes within the user's regions of interest
based on Bayesian inference. A user study N = 16 revealed that \textit{GenPara}
enhanced the comprehension and management of designers with text-conditional
shape parameters, streamlining design exploration and concretization. This
improvement boosted efficiency and creativity of the 3D design process.",['cs.HC'],2502.10542," Drug overdose deaths, including those due to prescription opioids, represent
a critical public health issue in the United States and worldwide. Artificial
intelligence (AI) approaches have been developed and deployed to help
prescribers assess a patient's risk for overdose-related death, but it is
unknown whether public health experts can leverage similar predictions to make
local resource allocation decisions more effectively. In this work, we
evaluated how AI-based overdose risk assessment could be used to inform local
public health decisions using a working prototype system. Experts from three
health departments, of varying locations and sizes with respect to staff and
population served, were receptive to the potential benefits of algorithmic risk
prediction and of using AI-augmented visualization to connect across data
sources. However, they also expressed concerns about whether the risk
prediction model's formulation and underlying data would match the state of the
overdose epidemic as it evolved in their specific locations. Our findings
extend those of other studies on algorithmic systems in the public sector, and
they present opportunities for future human-AI collaborative tools to support
decision-making in local, time-varying contexts.",['cs.HC'],False,,,,"GenPara: Enhancing the 3D Design Editing Process by Inferring Users'
  Regions of Interest with Text-Conditional Shape Parameters","Static Algorithm, Evolving Epidemic: Understanding the Potential of
  Human-AI Risk Assessment to Support Regional Overdose Prevention"
neg-d2-247,2025-03-04,,2503.03017," We study Perelman's W-entropy functional on finite-dimensional RCD spaces, a
synthetic generalization of spaces with Bakry-\'{E}mery Ricci curvature bounded
from below. We rigorously justify the formula for the time derivative of the
W-entropy and derive its monotonicity and rigidity properties. Additionally, we
establish bounds for solutions of the heat equation, which are of independent
interest.",['math.DG'],2501.05437," We show that the complete Sp(2)-invariant expanding solitons for Bryant's
Laplacian flow on the anti-self-dual bundle of the 4-sphere form a 1-parameter
family, and that they are all asymptotically conical (AC). We determine their
asymptotic cones, and prove that this cone determines the complete expander (up
to scale). Neither the unique Sp(2)-invariant torsion-free G_2-cone nor the
asymptotic cone of the explicit AC Sp(2)-invariant shrinker from
arxiv:2112.09095 occurs as the asymptotic cone of a complete AC Sp(2)-invariant
expander.
  We determine all possible end behaviours of Sp(2)-invariant solitons,
identifying novel forward-complete end solutions for both expanders and
shrinkers with faster-than-Euclidean volume growth. We conjecture that there
exists a 1-parameter family of complete SU(3)-invariant expanders on the
anti-self-dual bundle of the complex projective plane CP^2 with such asymptotic
behaviour.
  We also conjecture that, in contrast to the Sp(2)-invariant case, there exist
complete SU(3)-invariant AC expanders with asymptotic cone matching that of the
explicit AC SU(3)-invariant shrinker from arxiv:2112.09095. The latter
conjecture suggests that Laplacian flow may naturally implement a type of
surgery in which a CP^2 shrinks to a conically singular point, but after which
the flow can be continued smoothly, expanding a topologically different CP^2
from the singularity.",['math.DG'],False,,,,Perelman's entropy and heat kernel bounds on RCD spaces,Sp(2)-invariant expanders and shrinkers in Laplacian flow
neg-d2-248,2025-02-13,,2502.09374," Deep neural networks (DNNs) are increasingly used in safety-critical
applications. Reliable fault analysis and mitigation are essential to ensure
their functionality in harsh environments that contain high radiation levels.
This study analyses the impact of multiple single-bit single-event upsets in
DNNs by performing fault injection at the level of a DNN model. Additionally, a
fault aware training (FAT) methodology is proposed that improves the DNNs'
robustness to faults without any modification to the hardware. Experimental
results show that the FAT methodology improves the tolerance to faults up to a
factor 3.",['cs.LG'],2501.14588," With the development of the digital economy, data is increasingly recognized
as an essential resource for both work and life. However, due to privacy
concerns, data owners tend to maximize the value of data through the
circulation of information rather than direct data transfer. Federated learning
(FL) provides an effective approach to collaborative training models while
preserving privacy. However, as model parameters and training data grow, there
are not only real differences in data resources between different data owners,
but also mismatches between data and computing resources. These challenges lead
to inadequate collaboration among data owners, compute centers, and model
owners, reducing the global utility of the three parties and the effectiveness
of data assetization. In this work, we first propose a framework for
resource-decoupled FL involving three parties. Then, we design a Tripartite
Stackelberg Model and theoretically analyze the Stackelberg-Nash equilibrium
(SNE) for participants to optimize global utility. Next, we propose the
Quality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we
derive and solve the optimal strategies of all parties to achieve SNE using
backward induction. We also design a dynamic optimization mechanism to improve
the optimal strategy profile by evaluating the contribution of data quality
from data owners to the global model during real training. Finally, our
extensive experiments demonstrate that our method effectively encourages the
linkage of the three parties involved, maximizing the global utility and value
of data assets.",['cs.LG'],False,,,,"Mitigating multiple single-event upsets during deep neural network
  inference using fault-aware training",Data Assetization via Resources-decoupled Federated Learning
neg-d2-249,2025-03-20,,2503.16786," For $1\le p,q\le \infty$, the Nikolskii factor for a trigonometric polynomial
$T_{\bf a}$ is defined by $$\mathcal N_{p,q}(T_{\bf a})=\frac{\|T_{\bf
a}\|_{q}}{\|T_{\bf a}\|_{p}},\ \ T_{\bf
a}(x)=a_{1}+\sum\limits^{n}_{k=1}(a_{2k}\sqrt{2}\cos kx+a_{2k+1}\sqrt{2}\sin
kx).$$ We study this average Nikolskii factor for random trigonometric
polynomials with independent $N(0,\sigma^{2})$ coefficients and obtain that the
exact order. For $1\leq p<q<\infty$, the average Nikolskii factor is order
degree to the 0, as compared to the degree $1/p-1/q$ worst case bound. We also
give the generalization to random multivariate trigonometric polynomials.",['math.CA'],2502.13786," Let $G$ be a locally compact abelian group, and let $\widehat{G}$ denote its
dual group, equipped with a Haar measure. A variant of the uncertainty
principle states that for any $S \subset G$ and $\Sigma \subset \widehat{G}$,
there exists a constant $C(S, \Sigma)$ such that for any $f \in L^2(G)$, the
following inequality holds: \[\|f\|_{L^2(G)} \leq C(S, \Sigma) \bigl(
\|f\|_{L^2(G \setminus S)} + \|\widehat{f}\|_{L^2(\widehat{G} \setminus
\Sigma)} \bigr),\] where $\widehat{f}$ denotes the Fourier transform of $f$.
This variant of the uncertainty principle is particularly useful in
applications such as signal processing and control theory.The purpose of this
paper is to show that such estimates can be strengthened when $S$ or $\Sigma$
satisfies a restriction theorem and to provide an estimate for the constant
$C(S, \Sigma)$. This result serves as a quantitative counterpart to a recent
finding by the first and last author. In the setting of finite groups, the
results also extend those of Matolcsi-Sz\""ucs and Donoho-Stark.",['math.CA'],False,,,,Average Nikolskii factors for random trigonometric polynomials,"Uncertainty Principle, annihilating pairs and Fourier restriction"
neg-d2-250,2025-02-24,,2502.16865," We present a multimodal search tool that facilitates retrieval of chemical
reactions, molecular structures, and associated text from scientific
literature. Queries may combine molecular diagrams, textual descriptions, and
reaction data, allowing users to connect different representations of chemical
information. To support this, the indexing process includes chemical diagram
extraction and parsing, extraction of reaction data from text in tabular form,
and cross-modal linking of diagrams and their mentions in text. We describe the
system's architecture, key functionalities, and retrieval process, along with
expert assessments of the system. This demo highlights the workflow and
technical components of the search system.",['cs.IR'],2502.13763," In session-based recommender systems, predictions are based on the user's
preceding behavior in the session. State-of-the-art sequential recommendation
algorithms either use graph neural networks to model sessions in a graph or
leverage the similarity of sessions by exploiting item features. In this paper,
we combine these two approaches and propose a novel method, Graph Convolutional
Network Extension (GCNext), which incorporates item features directly into the
graph representation via graph convolutional networks. GCNext creates a
feature-rich item co-occurrence graph and learns the corresponding item
embeddings in an unsupervised manner. We show on three datasets that
integrating GCNext into sequential recommendation algorithms significantly
boosts the performance of nearest-neighbor methods as well as neural network
models. Our flexible extension is easy to incorporate in state-of-the-art
methods and increases the MRR@20 by up to 12.79%.",['cs.IR'],False,,,,Multimodal Search in Chemical Documents and Reactions,"Unsupervised Graph Embeddings for Session-based Recommendation with Item
  Features"
neg-d2-251,2025-02-22,,2502.16362," Epidemiologic studies often evaluate the association between an exposure and
an event risk. When time-varying, exposure updates usually occur at discrete
visits although changes are in continuous time and survival models require
values to be constantly known. Moreover, exposures are likely measured with
error, and their observation truncated at the event time. We aimed to quantify
in a Cox regression the bias in the association resulting from intermittent
measurements of an error-prone exposure. Using simulations under various
scenarios, we compared five methods: last observation carried-forward (LOCF),
classical two-stage regression-calibration using measurements up to the event
(RC) or also after (PE-RC), multiple imputation (MI) and joint modeling of the
exposure and the event (JM). The LOCF, and to a lesser extent the classical RC,
showed substantial bias in almost all 43 scenarios. The RC bias was avoided
when considering post-event information. The MI performed relatively well, as
did the JM. Illustrations exploring the association of Body Mass Index and
Executive Functioning with dementia risk showed consistent conclusions.
Accounting for measurement error and discrete updates is critical when studying
time-varying exposures. MI and JM techniques may be applied in this context,
while classical RC should be avoided due to the informative truncation.",['stat.AP'],2502.10787," Excess mortality, i.e. the difference between expected and observed
mortality, is used to quantify the death toll of mortality shocks, such as
infectious disease-related epidemics and pandemics. However, predictions of
expected mortality are sensitive to model assumptions. Among three
specifications of a Serfling-Poisson regression for seasonal mortality, we
analyse which one yields the most accurate predictions. We compare the
Serfling-Poisson models with: 1) parametric effect for the trend and
seasonality (SP), 2) non-parametric effect for the trend and seasonality
(SP-STSS), also known as modulation model, and 3) non-parametric effect for the
trend and parametric effect for the seasonality (SP-STFS). Forecasting is
achieved with P-splines smoothing. The SP-STFS model resulted in more accurate
historical forecasts of monthly rates from national statistical offices in 25
European countries. An application to the COVID-19 pandemic years illustrates
how excess mortality can be used to evaluate the vulnerability of populations
and aid public health planning.",['stat.AP'],False,,,,"Including an infrequently measured time-dependent error-prone covariate
  in survival analyses: a simulation-based comparison of methods",Modelling and short-term forecasting of seasonal mortality
neg-d2-252,2025-01-25,,2501.15272," Transporting a heavy payload using multiple aerial robots (MARs) is an
efficient manner to extend the load capacity of a single aerial robot. However,
existing schemes for the multiple aerial robots transportation system (MARTS)
still lack the capability to generate a collision-free and dynamically feasible
trajectory in real-time and further track an agile trajectory especially when
there are no sensors available to measure the states of payload and cable.
Therefore, they are limited to low-agility transportation in simple
environments. To bridge the gap, we propose complete planning and control
schemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of
a cable-suspended payload in complex environments. Flatness maps for the aerial
robot considering the complete kinematical constraint and the dynamical
coupling between each aerial robot and payload are derived. To improve the
responsiveness for the generation of the safe, dynamically feasible, and agile
trajectory in complex environments, a real-time spatio-temporal trajectory
planning scheme is proposed for the MARTS. Besides, we break away from the
reliance on the state measurement for both the payload and cable, as well as
the closed-loop control for the payload, and propose a fully distributed
control scheme to track the agile trajectory that is robust against imprecise
payload mass and non-point mass payload. The proposed schemes are extensively
validated through benchmark comparisons, ablation studies, and simulations.
Finally, extensive real-world experiments are conducted on a MARTS integrated
by three aerial robots with onboard computers and sensors. The result validates
the efficiency and robustness of our proposed schemes for SAAT in complex
environments.",['cs.RO'],2502.15613," Current robotic pick-and-place policies typically require consistent gripper
configurations across training and inference. This constraint imposes high
retraining or fine-tuning costs, especially for imitation learning-based
approaches, when adapting to new end-effectors. To mitigate this issue, we
present a diffusion-based policy with a hybrid learning-optimization framework,
enabling zero-shot adaptation to novel grippers without additional data
collection for retraining policy. During training, the policy learns
manipulation primitives from demonstrations collected using a base gripper. At
inference, a diffusion-based optimization strategy dynamically enforces
kinematic and safety constraints, ensuring that generated trajectories align
with the physical properties of unseen grippers. This is achieved through a
constrained denoising procedure that adapts trajectories to gripper-specific
parameters (e.g., tool-center-point offsets, jaw widths) while preserving
collision avoidance and task feasibility. We validate our method on a Franka
Panda robot across six gripper configurations, including 3D-printed fingertips,
flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a
93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion
policy baselines), supporting tool-center-point variations of 16-23.5 cm and
jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion
enables robust cross-gripper manipulation while maintaining the sample
efficiency of imitation learning, eliminating the need for gripper-specific
retraining. Video and code are available at https://github.com/yaoxt3/GADP.",['cs.RO'],False,,,,"Safe and Agile Transportation of Cable-Suspended Payload via Multiple
  Aerial Robots","Pick-and-place Manipulation Across Grippers Without Retraining: A
  Learning-optimization Diffusion Policy Approach"
neg-d2-253,2025-02-25,,2502.17988," Oscillatory signals from coherently excited phonons are regularly observed in
ultrafast pump-probe experiments on condensed matter samples. Electron-phonon
coupling implies that coherent phonons also modulate the electronic band
structure. These oscillations can be probed with energy and momentum resolution
using time- and angle-resolved photoemission spectroscopy (trARPES) which
reveals the orbital dependence of the electron-phonon coupling for a specific
phonon mode. However, a comprehensive analysis remains challenging when
multiple coherent phonon modes couple to multiple electronic bands. Complex
spectral line shapes due to strong correlations in quantum materials add to
this challenge. In this work, we examine how the frequency domain
representation of trARPES data facilitates a quantitative analysis of coherent
oscillations of the electronic bands. We investigate the frequency domain
representation of the photoemission intensity and \tred{the first moment of the
energy distribution curves}. Both quantities provide complimentary information
and are able to distinguish oscillations of binding energy, linewidth and
intensity.We analyze a representative trARPES dataset of the transition metal
dichalcogenide WTe$_2$ and construct composite spectra which intuitively
illustrate how much each electronic band is affected by a specific phonon mode.
We also show how a linearly chirped probe pulse can generate extrinsic
artifacts that are distinct from the intrinsic coherent phonon signal.",['cond-mat.mtrl-sci'],2501.03387," Controlled modulation of electronic band structure in two-dimensional (2D)
materials via doping is crucial for devices fabrication. For instance doped
graphene has been envisaged for various applications like sensors,
super-capacitors, transistors, p-n junctions, photo-detectors, etc. Many
different techniques have been developed to achieve desired doping in 2D
materials, like chemical doping, electrostatic doping, substrate doping, etc.
Here, we have combined space charge doping with space and angle resolved
photoemission (nano-ARPES), in order to directly observe the Fermi level
modulation on micron-sized flakes of monolayer and bilayer graphene. The doping
level can be tuned in a controlled manner, which allows us to directly observe
the Fermi level tuning. In our experiment we successfully doped the graphene
with p- and n-type carriers (holes/electrons) which are directly observed
through band shift in ARPES measurements. The observed band shift is $\sim$250
meV for bilayer and $\sim$500 meV for monolayer graphene. The results from our
experiment promote the space charge doping technique and nano-ARPES into other
materials such as 2D semiconductors and superconductors, in order to directly
observe the physical phenomena such as band gap transition and phase transition
as function of carrier doping.",['cond-mat.mtrl-sci'],False,,,,"Analysis methodology of coherent oscillations in time- and
  angle-resolved photoemission spectroscopy","Space Charge Doping Induced Band Modulation in Mono- and Bi-layer
  Graphene: a nano-ARPES study"
neg-d2-254,2025-02-14,,2502.10585," Human motion is stochastic and ensuring safe robot navigation in a
pedestrian-rich environment requires proactive decision-making. Past research
relied on incorporating deterministic future states of surrounding pedestrians
which can be overconfident leading to unsafe robot behaviour. The current paper
proposes a predictive uncertainty-aware planner that integrates neural network
based probabilistic trajectory prediction into planning. Our method uses a deep
ensemble based network for probabilistic forecasting of surrounding humans and
integrates the predictive uncertainty as constraints into the planner. We
compare numerous constraint satisfaction methods on the planner and evaluated
its performance on real world pedestrian datasets. Further, offline robot
navigation was carried out on out-of-distribution pedestrian trajectories
inside a narrow corridor",['cs.RO'],2503.12265," In this letter, we demonstrate that previously proposed improved state
parameterizations for soft and continuum robots are specific cases of Clarke
coordinates. By explicitly deriving these improved parameterizations from a
generalized Clarke transformation matrix, we unify various approaches into one
comprehensive mathematical framework. This unified representation provides
clarity regarding their relationships and generalizes them beyond existing
constraints, including arbitrary joint numbers, joint distributions, and
underlying modeling assumptions. This unification consolidates prior insights
and establishes Clarke coordinates as a foundational tool, enabling systematic
knowledge transfer across different subfields within soft and continuum
robotics.",['cs.RO'],False,,,,"Prediction uncertainty-aware planning using deep ensembles and
  trajectory optimisation","Clarke Coordinates Are Generalized Improved State Parametrization for
  Continuum Robots"
neg-d2-255,2025-01-19,,2501.11073," The standard notion of poset probability of a finite poset P involves
calculating, for incomparable $\alpha$, $\beta$ in P, the number of linear
extensions of P for which $\alpha$ precedes $\beta$. The fraction of those
linear extensions among all linear extensions of P is the probability that
$\alpha < \beta$. The question of whether there is always a pair $\alpha,
\beta$ such that this probability lies between 1/3 and 2/3, in any poset P
(that is not a chain) is the famous ""1/3-2/3-conjecture"".
  A general way of counting linear extensions of P for which $\alpha$ precedes
$\beta$ is to count linear extensions of the poset obtained by adding the
relation $(\alpha,\beta)$, and its transitive consequences.
  For chain-products, and more generally for partition posets, lattice-path
methods can be used to count the number of those linear extensions.
  We present an alternative approach to find the pertinent linear extensions.
It relies on finding the ""blocking ideals"" in $J(P)$, where $J(P)$ is the
lattice of order ideals in P. This method works for all finite posets.
  We illustrate this method by using blocking ideals to find explicit formulas
of poset probabilities in cell posets $P_{\lambda}$ of two-row partitions.
Well-known formulae such as the hook-length formula for $f^\lambda$, the number
of standard Young tableaux on a partition $\lambda$, and the corresponding
determinental formula by Jacobi-Trudi-Aitken for $f^{\lambda / \mu}$, the
number of standard Young tableaux on a skew partition $\lambda / \mu$, are used
along the way.
  We also calculate the limit probabilities when the elements $\alpha,\beta$
are fixed cells, but the arm-lengths tend to infinity.",['math.CO'],2501.12211," Rogers-Ramanujan type identities occur in various branches of mathematics and
physics. As a classic and powerful tool to deal with Rogers-Ramanujan type
identities, the theory of Bailey's lemma has been extensively studied and
generalized. In this paper, we found a bilateral Bailey pair that naturally
arises from the q-binomial theorem. By applying the bilateral versions of
Bailey lemmas, Bailey chains and Bailey lattices, we derive a number of
Rogers-Ramanujan type identities, which unify many known identities as special
cases. Further combined with the bilateral Bailey chains due to Berkovich,
McCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we
also obtain identities on Appell-Lerch series and identities of Andrews-Gordon
type. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we
derive identities on Hecke-type series.",['math.CO'],False,,,,"Blocking Ideals: a method for sieving linear extensions of a finite
  poset",Bilateral Bailey pairs and Rogers-Ramanujan type identities
neg-d2-256,2025-01-25,,2501.15334," Palladium diselenide (PdSe$_2$) -- a layered van der Waals material -- is
attracting significant attention for optoelectronics due to the wide tunability
of its band gap from the infrared through the visible range as a function of
the number of layers. However, there continues to be disagreement over the
precise nature and value of the optical band gap of bulk PdSe$_2$, owing to the
rather small value of this gap that complicates experimental measurements and
their interpretation. Here, we design and employ a Wannier-localized
optimally-tuned screened range-separated hybrid (WOT-SRSH) functional to
investigate the electronic bandstructures and optical absorption spectra of
bulk and monolayer PdSe$_2$. In particular, we account carefully for the finite
exciton center-of-mass momentum within a time-dependent WOT-SRSH framework to
calculate the \emph{indirect} optical gap and absorption onset accurately. Our
results agree well with the best available photoconductivity measurements, as
well as with state-of-the-art many-body perturbation theory calculations,
confirming that bulk PdSe$_2$ has an optical gap in the mid-infrared
(upper-bound of 0.44 eV). More generally, this work further bolsters the
utility of the WOT-SRSH approach for predictive modeling of layered
semiconductors.",['cond-mat.mtrl-sci'],2501.12686," In rechargeable batteries, electron transport properties of inorganics in the
solid-electrolyte interphase (SEI) critically determine the safety, lifespan
and capacity loss of batteries. However, the electron transport properties of
heterogeneous interfaces among different solid inorganics in SEI have not been
studied experimentally or theoretically yet, although such heterogeneous
interfaces exist inevitably. Here, by employing non-equilibrium Green's
function (NEGF) method, we theoretically evaluated the atomic-scale electron
transport properties under bias voltage for LiF/Li2O interfaces and
single-component layers of them, since LiF and Li2O are common stable
inorganics in the SEI. We reveal that heterogeneous interfaces orthogonal to
the external electric-field direction greatly impede electron transport in SEI,
whereas heterogeneous parallel-orientated interfaces enhance it. Structural
disorders induced by densely distributed interfaces can severely interfere with
electron transport. For each component, single-crystal LiF is highly effective
to block electron transport, with a critical thickness of 2.9 nm, much smaller
than that of Li2O (19.0 nm). This study sheds a new light into direct and
quantitative understanding of the electron transport properties of
heterogeneous interfaces in SEI, which holds promise for the advancement of a
new generation of high-performance batteries.",['cond-mat.mtrl-sci'],False,,,,"Resolving Contradictory Estimates of Band Gaps of Bulk PdSe$_2$: A
  Wannier-Localized Optimally-Tuned Screened Range-Separated Hybrid Density
  Functional Theory Study","Electron transport properties of heterogeneous interfaces in solid
  electrolyte interphase on lithium metal anodes"
neg-d2-257,2025-02-11,,2502.07773," Terrestrial worlds with $P < 1$ day, known as ultra-short period planets
(USPs), comprise a physically distinct population whose origins may be
attributed to various possible formation channels within multi-planet systems.
However, the conventional 1 day boundary adopted for USPs is an arbitrary
prescription, and it has yet to be evaluated whether this specific cutoff, or
any alternatives, may emerge from the data with minimal assumptions. We
accordingly present a statistical evaluation of the USP classification boundary
for 376 multi-planet systems across Kepler, K2, and TESS. We find that USPs are
smaller in size ($p = 0.004$) and exhibit larger period ratios with their
immediate neighbors ($\mathcal{P} = P_{2}/P_{1}$; $p < 10^{-4}$) when compared
to non-USP short-period ($1 < P/\text{days} < 5$) worlds, and that these
discrepancies rapidly transition towards statistical insignificance ($p >
0.05$) at respective orbital periods of $P_{R} = 0.97^{+0.25}_{-0.19}$ days and
$P_{\mathcal{P}} = 2.09^{+0.16}_{-0.22}$ days (see Figure 3). We verify that
these results are not driven by imprecise planetary parameters, giant
companions, low-mass host stars, or detection biases. Our findings provide
qualitative support for pathways in which proto-USPs are detached from
companions and delivered to $P \lesssim 2$ days via eccentric migration, while
a subset of these objects near $P \sim 1$ day experience subsequent orbital
decay and refractory mass loss to become USPs. These results lend evidence
towards an astrophysical basis for the 1 day USP cutoff and encourage
consideration of an additional 2 day boundary within future investigations of
USP architectures and evolutionary dynamics.",['astro-ph.EP'],2502.01606," High-resolution spectroscopy has provided a wealth of information about the
climate and composition of ultra-hot Jupiters. However, the 3D structure of
their atmospheres makes observations more challenging to interpret,
necessitating 3D forward-modeling studies. In this work, we model
phase-dependent thermal emission spectra of the archetype ultra-hot Jupiter
WASP-76b to understand how the line strengths and Doppler shifts of Fe, CO,
H$_2$O, and OH evolve throughout the orbit. We post-process outputs of the
SPARC/MITgcm global circulation model with the 3D Monte-Carlo radiative
transfer code gCMCRT to simulate emission spectra at 36 orbital phases. We then
cross-correlate the spectra with different templates to obtain CCF and
$K_{\text{p}}$$-$$V_{\text{sys}}$ maps. For each species, our models produce
consistently negative $K_{\text{p}}$ offsets in pre- and post-eclipse, which
are driven by planet rotation. The size of these offsets is similar to the
equatorial rotation velocity of the planet. Furthermore, we demonstrate how the
weak vertical temperature gradient on the nightside of ultra-hot Jupiters mutes
the absorption features of CO and H$_2$O, which significantly hampers their
detectability in pre- and post-transit. We also show that the $K_{\text{p}}$
and $V_{\text{sys}}$ offsets in pre- and post-transit are not always a measure
for the line-of-sight velocities in the atmosphere. This is because the
cross-correlation signal is a blend of dayside emission and nightside
absorption features. Finally, we highlight that the observational uncertainty
in the known orbital velocity of ultra-hot Jupiters can be multiple km/s, which
makes it hard for certain targets to meaningfully report absolute
$K_{\text{p}}$ offsets.",['astro-ph.EP'],False,,,,"Statistical Reevaluation of the USP Classification Boundary: Smaller
  Planets Within 1 Day, Larger Period Ratios Below 2 Days","From pre-transit to post-eclipse: investigating the impact of 3D
  temperature, chemistry, and dynamics on high-resolution emission spectra of
  the ultra-hot Jupiter WASP-76b"
neg-d2-258,2025-02-20,,2502.14616," Transparent object perception is indispensable for numerous robotic tasks.
However, accurately segmenting and estimating the depth of transparent objects
remain challenging due to complex optical properties. Existing methods
primarily delve into only one task using extra inputs or specialized sensors,
neglecting the valuable interactions among tasks and the subsequent refinement
process, leading to suboptimal and blurry predictions. To address these issues,
we propose a monocular framework, which is the first to excel in both
segmentation and depth estimation of transparent objects, with only a
single-image input. Specifically, we devise a novel semantic and geometric
fusion module, effectively integrating the multi-scale information between
tasks. In addition, drawing inspiration from human perception of objects, we
further incorporate an iterative strategy, which progressively refines initial
features for clearer results. Experiments on two challenging synthetic and
real-world datasets demonstrate that our model surpasses state-of-the-art
monocular, stereo, and multi-view methods by a large margin of about
38.8%-46.2% with only a single RGB input. Codes and models are publicly
available at https://github.com/L-J-Yuan/MODEST.",['cs.CV'],2503.00677," General continual learning (GCL) is a broad concept to describe real-world
continual learning (CL) problems, which are often characterized by online data
streams without distinct transitions between tasks, i.e., blurry task
boundaries. Such requirements result in poor initial performance, limited
generalizability, and severe catastrophic forgetting, heavily impacting the
effectiveness of mainstream GCL models trained from scratch. While the use of a
frozen pretrained backbone with appropriate prompt tuning can partially address
these challenges, such prompt-based methods remain suboptimal for CL of
remaining tunable parameters on the fly. In this regard, we propose an
innovative approach named MISA (Mask and Initial Session Adaption) to advance
prompt-based methods in GCL. It includes a forgetting-aware initial session
adaption that employs pretraining data to initialize prompt parameters and
improve generalizability, as well as a non-parametric logit mask of the output
layers to mitigate catastrophic forgetting. Empirical results demonstrate
substantial performance gains of our approach compared to recent competitors,
especially without a replay buffer (e.g., up to 18.39%, 22.06%, and 11.96%
performance lead on CIFAR-100, Tiny-ImageNet, and ImageNet-R, respectively).
Moreover, our approach features the plug-in nature for prompt-based methods,
independence of replay, ease of implementation, and avoidance of CL-relevant
hyperparameters, serving as a strong baseline for GCL research. Our source code
is publicly available at https://github.com/kangzhiq/MISA",['cs.CV'],False,,,,"Monocular Depth Estimation and Segmentation for Transparent Object with
  Iterative Semantic and Geometric Fusion","Advancing Prompt-Based Methods for Replay-Independent General Continual
  Learning"
neg-d2-259,2025-02-14,,2502.10057," Soft robotics is advancing the use of flexible materials for adaptable
robotic systems. Membrane-actuated soft robots address the limitations of
traditional soft robots by using pressurized, extensible membranes to achieve
stable, large deformations, yet control and state estimation remain challenging
due to their complex deformation dynamics. This paper presents a novel modeling
approach for liquid-driven ballooning membranes, employing an ellipsoid
approximation to model shape and stretch under planar deformation. Relying
solely on intrinsic feedback from pressure data and controlled liquid volume,
this approach enables accurate membrane state estimation. We demonstrate the
effectiveness of the proposed model for ballooning membrane-based actuators by
experimental validation, obtaining the indentation depth error of
$RMSE_{h_2}=0.80\;$mm, which is $23\%$ of the indentation range and $6.67\%$ of
the unindented actuator height range. For the force estimation, the error range
is obtained to be $RMSE_{F}=0.15\;$N which is $10\%$ of the measured force
range.",['cs.RO'],2502.04012," This chapter is about the fundamentals of fabrication, control, and
human-robot interaction of a new type of collaborative robotic manipulators,
called malleable robots, which are based on adjustable architectures of varying
stiffness for achieving high dexterity with lower mobility arms. Collaborative
robots, or cobots, commonly integrate six or more degrees of freedom (DOF) in a
serial arm in order to allow positioning in constrained spaces and adaptability
across tasks. Increasing the dexterity of robotic arms has been indeed
traditionally accomplished by increasing the number of degrees of freedom of
the system; however, once a robotic task has been established (e.g., a
pick-and-place operation), the motion of the end-effector can be normally
achieved using less than 6-DOF (i.e., lower mobility). The aim of malleable
robots is to close the technological gap that separates current cobots from
achieving flexible, accessible manufacturing automation with a reduced number
of actuators.",['cs.RO'],False,,,,A Generalized Modeling Approach to Liquid-driven Ballooning Membranes,Malleable Robots
neg-d2-260,2025-01-25,,2501.15137," Matrix operations are of great significance in quantum computing, which
manipulate quantum states in information processing. This paper presents
quantum algorithms for several important matrix operations. By leveraging
multi-qubit Toffoli gates and basic single-qubit operations, these algorithms
efficiently carry out matrix row addition, row swapping, trace calculation and
transpose. By using the ancillary measurement techniques to eliminate redundant
information, these algorithms achieve streamlined and efficient computations,
and demonstrate excellent performance with the running time increasing
logarithmically as the matrix dimension grows, ensuring scalability. The
success probability depends on the matrix dimensions for the trace calculation,
and on the matrix elements for row addition. Interestingly, the success
probability is a constant for matrix row swapping and transpose, highlighting
the reliability and efficiency.",['quant-ph'],2501.06846," As is well known, unital Pauli maps can be eternally non-CP-divisible. In
contrast, here we show that in the case of non-unital maps, eternal
non-Markovianity in the non-unital part is ruled out. In the unital case, the
eternal non-Markovianity can be obtained by a convex combination of two
dephasing semigroups, but not all three of them. We study these results and the
ramifications arising from them.",['quant-ph'],False,,,,"Quantum Algorithms for Matrix Operations Based on Unitary
  Transformations and Ancillary State Measurements",On the eternal non-Markovianity of qubit maps
neg-d2-261,2025-03-24,,2503.18588," We present a new form of CP violation (CPV) that can be realised in Two-Higgs
Doublet Models (2HDMs) and was studied recently in [1]. By examining the vacuum
manifold of a generic (convex) 2HDM potential, we identify scenarios that
exhibit Mixed Spontaneous and Explicit CP Violation (MCPV), in which at least
two non-degenerate CP-violating local minima coexist. We illustrate how this
identification is achieved at the tree level by determining the magnitude and
phase of a novel complex parameter, which we call $r_{\rm CP}$. Since explicit
CP Violation vanishes in 2HDMs where SM Higgs alignment is enforced through
global continuous symmetries, we investigate how to maximise CPV in such
scenarios by introducing soft or explicit breaking of the relevant symmetries.
In doing so, we derive upper bounds on key CP-violating parameters that
characterise misalignment with the SM, subject to constraints from the
non-observation of the electron electric dipole moment. Finally, we delineate
the region of the CP-violating parameter space in such constrained 2HDMs that
can be further tested at the CERN Large Hadron Collider.",['hep-ph'],2502.00986," One method for deriving a factorization for QCD processes is to use
successive integration over fields in the functional integral. In this
approach, we separate the fields into two categories: dynamical fields with
momenta above a relevant cutoff, and background fields with momenta below the
cutoff. The dynamical fields are then integrated out in the background of the
low-momentum background fields. This strategy works well at tree level,
allowing us to quickly derive QCD factorization formulas at leading order.
However, to extend the approach to higher loops, it is necessary to rigorously
define the functional integral over dynamical fields in an arbitrary background
field. This framework was carefully developed for the calculation of the
effective action in a background field at the two-loop level in the classic
paper by Abbott [1]. Building on this work, I specify the renormalized
background-field Lagrangian and define the notion of the quantum average of an
operator in a background field, consistent with the ``separation of scales''
scheme mentioned earlier. As examples, I discuss the evolution of the twist-2
gluon light-ray operator and the one-loop gluon propagator in a background
field near the light cone.",['hep-ph'],False,,,,Mixed CP Violation and Natural Alignment in 2HDMs,Background-field method and QCD factorization
neg-d2-262,2025-01-31,,2501.1923," Cathodoluminescence spectroscopy has recently emerged as a novel platform for
nanoscale control of nonclassical features of light. Here, we propose a
theoretical model for cathodoluminescence from a multi-level quantum emitter.
Employing a master equation approach and treating the electron-beam excitation
as an incoherent broadband field source, we show that quantum interference can
arise between the different relaxation pathways. The induced-interference can
significantly modify the time-dependent spectra resulting in the enhancement or
suppression of cathodoluminescence. We find that the excitation rate, initial
state of the emitter, and excited level spacing play a crucial role in
determining the influence of interference. Our findings shed light on
electron-beam-induced quantum interference in cathodoluminescence and provides
a theoretical basis for exploring quantum optical phenomena in electron-driven
multi-level systems.",['quant-ph'],2502.00496," The nodes are traditionally viewed as fixed points where the probability
density vanishes. However, this work demonstrates that these nodes exhibit
time-dependent oscillation in quantum superposition states. We derive this
effect for a fundamental system: the 1D particle in a box. It is shown that the
probability density in a superposition of two eigenstates evolves with a
time-dependent interference term, introducing an oscillation of the nodes at a
specific frequency equal to the energy difference between the states. This
result suggests a deeper dynamical role for nodes in quantum systems.",['quant-ph'],False,,,,"Electron-beam-induced quantum interference effects in a multi-level
  quantum emitter",Time evolution of nodes in quantum superposition states
neg-d2-263,2025-02-13,,2502.09707," We assess the impact of CaII 3934,3969 and NaI 5891,5897 absorption arising
in the interstellar medium (ISM) on the SDSS-IV MaNGA Stellar Library (MaStar)
and produce corrected spectroscopy for 80% of the 24,162-star catalog. We model
the absorption strength of these transitions as a function of stellar distance,
Galactic latitude, and dust reddening based upon high-spectral resolution
studies. With this model, we identify 6342 MaStar stars that have negligible
ISM absorption ($W^\mathrm{ISM}$(CaII K) $<0.07$ Ang and $W^\mathrm{ISM}$(NaI
5891) $<0.05$ Ang). For 12,110 of the remaining stars, we replace their NaI D
profile (and their CaII profile for effective temperatures $T_{\rm eff}>9000$
K) with a coadded spectrum of low-ISM stars with similar $T_{\rm eff}$, surface
gravity, and metallicity. For 738 additional stars with $T_{\rm eff}>9000$ K,
we replace these spectral regions with a matching ATLAS9-based BOSZ model. This
results in a mean reduction in $W$(CaII K) ($W$(NaI D)) of $0.4-0.7$ Ang
($0.6-1.1$ Ang) for hot stars ($T_{\rm eff}>7610$ K), and a mean reduction in
$W$(NaI D) of $0.1-0.2$ Ang for cooler stars. We show that interstellar
absorption in simple stellar population (SSP) model spectra constructed from
the original library artificially enhances $W$(CaII K) by $\gtrsim20\%$ at
young ages ($<400$ Myr); dramatically enhances the strength of stellar NaI D in
starbursting systems (by ${\gtrsim}50\%$); and enhances stellar NaI D in older
stellar populations (${\gtrsim}10$ Gyr) by ${\gtrsim}10\%$. We provide SSP
spectra constructed from the cleaned library, and discuss the implications of
these effects for stellar population synthesis analyses constraining stellar
age, [Na/Fe] abundance, and the initial mass function.",['astro-ph.GA'],2501.04089," We present deep optical observations of the stellar halo of NGC 300, an
LMC-mass galaxy, acquired with the DEEP sub-component of the DECam Local Volume
Exploration survey (DELVE) using the 4 m Blanco Telescope. Our resolved star
analysis reveals a large, low surface brightness stellar stream
($M_{V}\sim-8.5$; [Fe/H] $= -1.4\pm0.15$) extending more than 40 kpc north from
the galaxy's center. We also find other halo structures, including potentially
an additional stream wrap to the south, which may be associated with the main
stream. The morphology and derived low metallicities of the streams and shells
discovered surrounding NGC 300 are highly suggestive of a past accretion event.
Assuming a single progenitor, the accreted system is approximately Fornax-like
in luminosity, with an inferred mass ratio to NGC 300 of approximately $1:15$.
We also present the discovery of a metal-poor globular cluster
($R_{\rm{proj}}=23.3$~kpc; $M_{V}=-8.99\pm0.16$; [Fe/H] $\approx-1.6\pm0.6$) in
the halo of NGC 300, the furthest identified globular cluster associated with
NGC 300. The stellar structures around NGC 300 represent the richest features
observed in a Magellanic Cloud analog to date, strongly supporting the idea
that accretion and subsequent disruption is an important mechanism in the
assembly of dwarf galaxy stellar halos.",['astro-ph.GA'],False,,,,"SDSS-IV MaStar: Quantification and Abatement of Interstellar Absorption
  in the Largest Empirical Stellar Spectral Library","Streams, Shells, and Substructures in the Accretion-Built Stellar Halo
  of NGC 300"
neg-d2-264,2025-01-29,,2501.1767," Diffusion models (DMs) have emerged as promising approaches for sequential
recommendation due to their strong ability to model data distributions and
generate high-quality items. Existing work typically adds noise to the next
item and progressively denoises it guided by the user's interaction sequence,
generating items that closely align with user interests. However, we identify
two key issues in this paradigm. First, the sequences are often heterogeneous
in length and content, exhibiting noise due to stochastic user behaviors. Using
such sequences as guidance may hinder DMs from accurately understanding user
interests. Second, DMs are prone to data bias and tend to generate only the
popular items that dominate the training dataset, thus failing to meet the
personalized needs of different users. To address these issues, we propose
Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation
(DiQDiff), which aims to extract robust guidance to understand user interests
and generate distinguished items for personalized user interests within DMs. To
extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)
to quantize sequences into semantic vectors (e.g., collaborative signals and
category interests) using a codebook, which can enrich the guidance to better
understand user interests. To generate distinguished items, DiQDiff
personalizes the generation through Contrastive Discrepancy Maximization (CDM),
which maximizes the distance between denoising trajectories using contrastive
loss to prevent biased generation for different users. Extensive experiments
are conducted to compare DiQDiff with multiple baseline models across four
widely-used datasets. The superior recommendation performance of DiQDiff
against leading approaches demonstrates its effectiveness in sequential
recommendation tasks.",['cs.IR'],2502.16256," The cold-start problem remains a significant challenge in recommendation
systems based on generative models. Current methods primarily focus on
enriching embeddings or inputs by gathering more data, often overlooking the
effectiveness of how existing training knowledge is utilized. This inefficiency
can lead to missed opportunities for improving cold-start recommendations. To
address this, we propose the use of epistemic uncertainty, which reflects a
lack of certainty about the optimal model, as a tool to measure and enhance the
efficiency with which a recommendation system leverages available knowledge. By
considering epistemic uncertainty as a reducible component of overall
uncertainty, we introduce a new approach to refine model performance. The
effectiveness of this approach is validated through extensive offline
experiments on publicly available datasets, demonstrating its superior
performance and robustness in tackling the cold-start problem.",['cs.IR'],False,,,,"Distinguished Quantized Guidance for Diffusion-based Sequence
  Recommendation",Exploiting Epistemic Uncertainty in Cold-Start Recommendation Systems
neg-d2-265,2025-02-24,,2502.17386," Modelling of plasma dynamics is fundamental to ensure appropriate diverter
and core performance, and is desirable for both interpreting the current
generation of experiments and informing the next generation devices like ITER
\cite{Loarte2007Chapter4P,Eich2013ScalingOT}. Yet the computational expense of
many plasma simulations makes them unsuitable for real-time applications or
iterative design workflows. Neural operator surrogate models of JOREK
\cite{Hoelzl_2021} and STORM \cite{Walkden2016-ys} are evaluated, investigating
their capability to replicate plasma dynamics accurately whilst reducing
computational cost. It is found that the accuracy of the surrogate models will
degrade for long term predictions, and that physics considerations are
important in assessing the performance of the surrogates. Surrogates trained on
one dataset can be effectively fine tuned with only a few simulations from a
target domain. This is particularly effective where the source domain is a low
fidelity physics model and the target domain is a high fidelity model, with an
order of magnitude improvement in performance for a small dataset and a short
rollout.",['physics.plasm-ph'],2502.19061," We introduce a new approach to measure the magnetic pitch angle profile in
tokamak plasmas with Doppler backscattering (DBS), a technique traditionally
used for measuring flows and density fluctuations. The DBS signal is maximised
when its probe beam's wavevector is perpendicular to the magnetic field at the
cutoff location, independent of the density fluctuations. Hence, if one could
isolate this effect, DBS would then yield information about the magnetic pitch
angle. By varying the toroidal launch angle, the DBS beam reaches cutoff with
different angles with respect to the magnetic field, but with other properties
remaining similar. Hence, the toroidal launch angle which gives maximum
backscattered power is thus that which is matched to the pitch angle at the
cutoff location, enabling inference of the magnetic pitch angle. We performed
systematic scans of the DBS toroidal launch angle for repeated DIII-D tokamak
discharges. Experimental DBS data from this scan were analysed and combined
with Gaussian beam-tracing simulations using the Scotty code. The pitch-angle
inferred from DBS is consistent with that from magnetics-only and
motional-Stark-effect-constrained (MSE) equilibrium reconstruction in the edge.
In the core, the pitch angles from DBS and magnetics-only reconstructions
differ by one to two degrees, while simultaneous MSE measurements were not
available. The uncertainty in these measurements was under a degree; we show
that this uncertainty is primarily due to the error in toroidal steering, the
number of toroidally separated measurements, and shot-to-shot repeatability. We
find that the error of pitch-angle measurements can be reduced by optimising
the poloidal launch angle and initial beam properties.",['physics.plasm-ph'],False,,,,"Data efficiency and long-term prediction capabilities for neural
  operator surrogate models of edge plasma simulations","Conceptual study on using Doppler backscattering to measure magnetic
  pitch angle in tokamak plasmas"
neg-d2-266,2025-03-12,,2503.09179," This paper concerns the problem of reachability of a given state for a
multiagent control system in $\mathbb{R}^d$. In such a system, at every time
each agent can choose his/her velocity which depends both on his/her position
and on the position of the whole crowd of agents (modeled by a probability
measure on $ \mathbb{R}^d$). The main contribution of the paper is to study the
above reachability problem with a given rate of attainability through a
Lyapunov method adapted to the Wasserstein space of probability measures. As a
byproduct we obtain a new comparison result for viscosity solutions of Hamilton
Jacobi equations in the Wasserstein space.",['math.OC'],2501.02254," In this paper, we consider a class of structured nonsmooth fractional
minimization, where the first part of the objective is the ratio of a
nonnegative nonsmooth nonconvex function to a nonnegative nonsmooth convex
function, while the second part is the difference of a smooth nonconvex
function and a nonsmooth convex function. This model problem has many important
applications, for example, the scale-invariant sparse signal recovery in signal
processing. However, the existing methods for fractional programs are not
suitable for solving this problem due to its special structure. We first
present a novel nonfractional min-max reformulation for the original fractional
program and show the connections between their global (local) optimal solutions
and stationary points. Based on the reformulation, we propose an alternating
maximization proximal descent algorithm and show its subsequential convergence
towards a critical point of the original fractional program under a mild
assumption. By further assuming the Kurdyka-{\L}ojasiewicz (KL) property of an
auxiliary function, we also establish the convergence of the entire solution
sequence generated by the proposed algorithm. Finally, some numerical
experiments on the scale-invariant sparse signal recovery are conducted to
demonstrate the efficiency of the proposed method.",['math.OC'],False,,,,Reachability for multiagent control systems via Lyapunov functions,"A min-max reformulation and proximal algorithms for a class of
  structured nonsmooth fractional optimization problems"
neg-d2-267,2025-01-24,,2501.14588," With the development of the digital economy, data is increasingly recognized
as an essential resource for both work and life. However, due to privacy
concerns, data owners tend to maximize the value of data through the
circulation of information rather than direct data transfer. Federated learning
(FL) provides an effective approach to collaborative training models while
preserving privacy. However, as model parameters and training data grow, there
are not only real differences in data resources between different data owners,
but also mismatches between data and computing resources. These challenges lead
to inadequate collaboration among data owners, compute centers, and model
owners, reducing the global utility of the three parties and the effectiveness
of data assetization. In this work, we first propose a framework for
resource-decoupled FL involving three parties. Then, we design a Tripartite
Stackelberg Model and theoretically analyze the Stackelberg-Nash equilibrium
(SNE) for participants to optimize global utility. Next, we propose the
Quality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we
derive and solve the optimal strategies of all parties to achieve SNE using
backward induction. We also design a dynamic optimization mechanism to improve
the optimal strategy profile by evaluating the contribution of data quality
from data owners to the global model during real training. Finally, our
extensive experiments demonstrate that our method effectively encourages the
linkage of the three parties involved, maximizing the global utility and value
of data assets.",['cs.LG'],2502.00716," Graph-structured datasets often suffer from class imbalance, which
complicates node classification tasks. In this work, we address this issue by
first providing an upper bound on population risk for imbalanced transductive
node classification. We then propose a simple and novel algorithm,
Uncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels
assigned to unlabeled nodes to mitigate the adverse effects of imbalance on
classification accuracy. Furthermore, the UPL algorithm enhances the accuracy
of pseudo-labeling by reducing training noise of pseudo-labels through a novel
uncertainty-aware approach. We comprehensively evaluate the UPL algorithm
across various benchmark datasets, demonstrating its superior performance
compared to existing state-of-the-art methods.",['cs.LG'],False,,,,Data Assetization via Resources-decoupled Federated Learning,"UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node
  Classification"
neg-d2-268,2025-01-28,,2501.17046," Extended very-high-energy $\gamma$-ray emission from middle-aged pulsars as
revealed recently by several groundbased $\gamma$-ray experiments has strong
implication on the transport of high-energy particles in the interstellar
medium surrounding those pulsars. The $\gamma$-ray emission is widely believed
to be produced by high-energy electrons and positrons accelerated by the pulsar
wind nebulae when scattering off the interstellar radiation field via the
inverse Compton process. Multiwavelength counterparts of the $\gamma$-ray halos
are expected to be present, which are, however, not observed yet. In this work,
we report for the first time the detection of extended X-ray emission from
$\sim 0.2^{\circ}$ radius region of PSR B0656+14 with eROSITA. The spectrum of
the emission can be described by a power-law function with an index of
$\sim3.7$. The fluxes decrease with radius faster than the prediction of the
particle diffusion and synchrotron radiation in a uniform magnetic field,
suggesting the existence of a radial gradient of the magnetic field strength as
$\sim r^{-1}$. The magnetic field strength in the X-ray emitting region is
constrained to be $4-10~\mu$G.",['astro-ph.HE'],2503.15084," The discovery of the kilonova (KN) AT 2017gfo, accompanying the gravitational
wave event GW170817, provides crucial insight into the synthesis of heavy
elements during binary neutron star (BNS) mergers. Following this landmark
event, another KN was detected in association with the second-brightest
gamma-ray burst (GRB) observed to date, GRB 230307A, and subsequently confirmed
by observations of the James Webb Space Telescope (JWST). In this work, we
conduct an end-to-end simulation to analyze the temporal evolution of the KN AT
2023vfi associated with GRB 230307A, and constrain the abundances of superheavy
elements produced. We find that the temporal evolution of AT 2023vfi is similar
to AT 2017gfo in the first week post-burst. Additionally, the
\textit{r}-process nuclide abundances of lanthanide-rich ejecta, derived from
numerical relativity simulations of BNS mergers, can also successfully
interpret the temporal evolution of the KN with the lanthanide-rich ejecta mass
of $0.02 M_\odot$, which is consistent with the mass range of dynamical ejecta
from numerical simulations in literature. Both findings strongly suggest the
hypothesis that GRB 230307A originated from a BNS merger, similar to AT
2017gfo. Based on the first time observation of the KN for JWST, we are able to
constrain the superheavy elements of another KN following AT 2017gfo. The
pre-radioactive-decay abundances of the superheavy nuclides: $^{222}$Rn,
$^{223}$Ra, $^{224}$Ra and $^{225}$Ac, are estimated to be at least on the
order of $1 \times 10^{-5}$. These abundance estimates provide valuable insight
into the synthesis of superheavy elements in BNS mergers, contributing to our
understanding of astrophysical \textit{r}-process nucleosynthesis.",['astro-ph.HE'],False,,,,"Detection of extended X-ray emission surrounding PSR B0656+14 with
  eROSITA",A constraint on superheavy elements of the GRB-kilonova AT 2023vfi
neg-d2-269,2025-03-11,,2503.0882," Recently, Pan and Yu showed that Lascoux polynomials can be defined in terms
of certain collections of diagrams consisting of unit cells arranged in the
first quadrant. Starting from certain initial diagrams, one forms a finite set
of diagrams by applying two types of moves: Kohnert and ghost moves. Both moves
cause at most one cell to move to a lower row with ghost moves leaving a new
""ghost cell"" in its place. Each diagram formed in this way defines a monomial
in the associated Lascoux polynomial. Restricting attention to diagrams formed
by applying sequences of only Kohnert moves in the definition of Lascoux
polynomials, one obtains the family of key polynomials. Recent articles have
considered a poset structure on the collections of diagrams formed when one
uses only Kohnert moves. In general, these posets are not ""well-behaved,"" not
usually having desirable poset properties. Here, as an intermediate step to
studying the analogous posets associated with Lascoux polynomials, we consider
the posets formed by restricting attention to those diagrams formed by using
only ghost moves. Unlike in the case of Kohnert posets, we show that such
""ghost Kohnert posets"" are always ranked join semi-lattices. In addition, we
establish a necessary condition for when ghost Kohnert posets are bounded and,
consequently, lattices.",['math.CO'],2502.15647," We introduce two new families of permutation group polynomials over finite
fields of arbitrary characteristic, which are special types of bivariate local
permutation polynomials. For each family, we explicitly construct their
companions. Furthermore, we precisely determine the total number of permutation
group polynomials equivalent to the proposed families. Moreover, we resolve the
problem of enumerating permutation group polynomials that are equivalent to
$e$-Klenian polynomials over finite fields for $e\geq 1$, a problem previously
noted as nontrivial by Gutierrez and Urroz (2023).",['math.CO'],False,,,,Ghost Kohnert posets,"Bivariate local permutation polynomials, their companions, and related
  enumeration results"
neg-d2-270,2025-01-10,,2501.06068," We report on a few interrelations between bi-Hermitian metrics and locally
conformally K\""ahler metrics on complex surfaces.",['math.DG'],2502.11914," We address the long-standing problem of the existence of a Riemannian metric
on $S^2\times T^2$ with strictly positive biorthogonal curvature ($
K_{\text{biort}}(\sigma) > 0 $), but in a weaker framework, by introducing an
affine connection with antisymmetric closed torsion, naturally encoded in the
cohomology of $S^2 \times T^2$ ($H^3(S^2 \times T^2; \mathbb{R}) \cong
\mathbb{R}^2$). This torsion, parametrized by non-trivial cohomology classes,
overcomes topological constraints imposed by the zero Euler characteristic,
ensuring $ K_{\text{biort}}(\sigma) > 0$ globally.",['math.DG'],False,,,,"Bi-Hermitian and locally conformally K\""ahler surfaces","Positive biorthogonal curvature on $S^2 \times T^2$ via affine
  connection"
neg-d2-271,2025-02-01,,2502.00485," Using molecular dynamics simulations of a binary Lennard-Jones model of
glass-forming liquids, we examine how the decay of the normalized
neighbor-persistence function $C_{\rm B}(t)$, which decays from unity at short
times to zero at long times as particles lose the neighbors that were present
in their original first coordination shell, compares with those of other, more
conventionally utilized relaxation metrics. In the strongly-non-Arrhenius
temperature regime below the onset temperature $T_{\rm A}$, we find that
$C_{\rm B}(t)$ can be described using the same stretched-exponential functional
form that is often utilized to fit the self-intermediate scattering function
$S(q, t)$ of glass-forming liquids in this regime. The ratio of the bond
lifetime $\tau_{\rm bond}$ associated with the terminal decay of $C_{\rm B}(t)$
to the $\alpha$-relaxation time $\tau_\alpha$ varies appreciably and
non-monotonically with $T$, peaking at $\tau_{\rm bond}/\tau_\alpha \simeq 45$
at $T \simeq T_{\rm x}$, where $T_{\rm x}$ is a crossover temperature
separating the high- and low-temperature regimes of glass-formation. In
contrast, $\tau_{\rm bond}$ remains on the order of the overlap time $\tau_{\rm
ov}$ (the time interval over which a typical particle moves by half its
diameter), and the peak time $\tau_\chi$ for the susceptibility $\chi_{\rm
B}(t)$ associated with the spatial heterogeneity of $C_{\rm B}(t)$ remains on
the order of $\tau_{\rm imm}$ (the characteristic lifetime of immobile-particle
clusters), even as each of these quantities varies by roughly $5$ orders of
magnitude over our studied range of $T$. Thus, we show that $C_{\rm B}(t)$ and
$\chi_{\rm B}(t)$ provide semi-quantitative spatially-averaged measures of the
slow heterogeneous dynamics associated with the persistence of
immobile-particle clusters.",['cond-mat.soft'],2501.17849," Theories of self-organised active fluid surfaces have emerged as an important
class of minimal models for the shape dynamics of biological membranes, cells
and tissues. Here, we develop and apply a variational approach for active fluid
surfaces to systematically study the nonlinear dynamics and emergent shape
spaces such theories give rise to. To represent dynamic surfaces, we design an
arbitrary Lagrangian-Eulerian parameterizations for deforming surfaces.
Exploiting the symmetries imposed by Onsager relations, we construct a
variational formulation that is based on the entropy production in active
surfaces. The resulting dissipation functional is complemented by Lagrange
multipliers to relax nonlinear geometric constraints, which allows for a direct
computation of steady state solutions of surface shapes and flows. We apply
this framework to study the dynamics of open fluid membranes and closed active
fluid surfaces, and characterize the space of stationary solutions that
corresponding surfaces and flows occupy. Our analysis rationalizes the
interplay of first-order shape transitions of internally and externally forced
fluid membranes, reveals degenerate regions in stationary shape spaces of
mechanochemically active surfaces and identifies a mechanism by which
hydrodynamic screening controls the geometry of active surfaces undergoing cell
division-like shape transformations.",['cond-mat.soft'],False,,,,"Quantitative relations between nearest-neighbor persistence and slow
  heterogeneous dynamics in supercooled liquids","Self-organised dynamics and emergent shape spaces of active isotropic
  fluid surfaces"
neg-d2-272,2025-01-28,,2501.16949," Motivated by the work of Aldroubi et al., we investigate the stability of the
source term of the discrete dynamical system indexing over a non-uniform
discrete set arising from spectral pairs in infinite-dimensional separable
Hilbert spaces. Extending results due to Aldroubi et al., firstly, we give a
necessary and sufficient condition for the recovery of the source term in
finitely many iterations. Afterwards, we derive a necessary condition for the
stability of the source term in finitely many iterations when it belongs to the
closed subspace of an infinite-dimensional separable Hilbert space. Finally, we
give a necessary and sufficient condition for the recovery of the source term
in infinitely many iterations.",['math.FA'],2501.10101," In this paper, we establish sharp bounds for a family of Kantorovich-type
neural network operators within the general frameworks of Sobolev-Orlicz and
Orlicz spaces. We establish both strong (in terms of the Luxemburg norm) and
weak (in terms of the modular functional) estimates, using different
approaches. The strong estimates are derived for spaces generated by
$\varphi$-functions that are $N$-functions or satisfy the
$\Delta^\prime$-condition. Such estimates also lead to convergence results with
respect to the Luxemburg norm in several instances of Orlicz spaces, including
the exponential case. Meanwhile, the weak estimates are achieved under less
restrictive assumptions on the involved $\varphi$-function. To obtain these
results, we introduce some new tools and techniques in Orlicz spaces. Central
to our approach is the Orlicz Minkowski inequality, which allows us to obtain
unified strong estimates for the operators. We also present a weak (modular)
version of this inequality holding under weaker conditions. Additionally, we
introduce a novel notion of discrete absolute $\varphi$-moments of hybrid type,
and we employ the Hardy-Littlewood maximal operator within Orlicz spaces for
the asymptotic analysis. Furthermore, we introduce the new space
$\mathcal{W}^{1,\varphi}(I)$, which is embedded in the Sobolev-Orlicz space
$W^{1,\varphi}(I)$ and modularly dense in $L^\varphi(I)$. This allows to
achieve asymptotic estimates for a wider class of $\varphi$-functions,
including those that do not meet the $\Delta_2$-condition. For the extension to
the whole Orlicz-setting, we generalize a Sobolev-Orlicz density result given
by H. Musielak using Steklov functions, providing a modular counterpart.
Finally, we explore the relationships between weak and strong Orlicz Lipschitz
classes, providing qualitative results for the rate of convergence of the
operators.",['math.FA'],False,,,,Frames for source recovery from non-uniform dynamical samples,"Strong and weak sharp bounds for Neural Network Operators in
  Sobolev-Orlicz spaces and their quantitative extensions to Orlicz spaces"
neg-d2-273,2025-03-21,,2503.1755," Clinical calculators are widely used, and large language models (LLMs) make
it possible to engage them using natural language. We demonstrate a
purpose-built chatbot that leverages software implementations of verifiable
clinical calculators via LLM tools and metadata about these calculators via
retrieval augmented generation (RAG). We compare the chatbot's response
accuracy to an unassisted off-the-shelf LLM on four natural language
conversation workloads. Our chatbot achieves 100% accuracy on queries
interrogating calculator metadata content and shows a significant increase in
clinical calculation accuracy vs. the off-the-shelf LLM when prompted with
complete sentences (86.4% vs. 61.8%) or with medical shorthand (79.2% vs.
62.0%). It eliminates calculation errors when prompted with complete sentences
(0% vs. 16.8%) and greatly reduces them when prompted with medical shorthand
(2.4% vs. 18%). While our chatbot is not ready for clinical use, these results
show progress in minimizing incorrect calculation results.",['q-bio.QM'],2501.06548," Repeated waves of emerging variants during the SARS-CoV-2 pandemics have
highlighted the urge of collecting longitudinal genomic data and developing
statistical methods based on time series analyses for detecting new threatening
lineages and estimating their fitness early in time. Most models study the
evolution of the prevalence of particular lineages over time and require a
prior classification of sequences into lineages. Such process is prone to
induce delays and bias. More recently, few authors studied the evolution of the
prevalence of mutations over time with alternative clustering approaches,
avoiding specific lineage classification. Most of the aforementioned methods
are however either non parametric or unsuited to pooled data characterizing,
for instance, wastewater samples. In this context, we propose an alternative
unsupervised method for clustering mutations according to their frequency
trajectory over time and estimating group fitness from time series of pooled
mutation prevalence data. Our model is a mixture of observed count data and
latent group assignment and we use the expectation-maximization algorithm for
model selection and parameter estimation. The application of our method to time
series of SARS-CoV-2 sequencing data collected from wastewater treatment plants
in France from October 2020 to April 2021 shows its ability to agnostically
group mutations according to their probability of belonging to B.1.160, Alpha,
Beta, B.1.177 variants with selection coefficient estimates per group in
coherence with the viral dynamics in France reported by Nextstrain. Moreover,
our method detected the Alpha variant as threatening as early as supervised
methods (which track specific mutations over time) with the noticeable
difference that, since unsupervised, it does not require any prior information
on the set of mutations.",['q-bio.QM'],False,,,,"An LLM-Powered Clinical Calculator Chatbot Backed by Verifiable Clinical
  Calculators and their Metadata","Unsupervised detection and fitness estimation of emerging SARS-CoV-2
  variants. Application to wastewater samples (ANRS0160)"
neg-d2-274,2025-01-15,,2501.08709," Extended dynamic mode decomposition (EDMD) is a popular data-driven method to
predict the action of the Koopman operator, i.e., the evolution of an
observable function along the flow of a dynamical system. In this paper, we
leverage a recently-introduced kernel EDMD method for control systems for
data-driven model predictive control. Building upon pointwise error bounds
proportional in the state, we rigorously show practical asymptotic stability of
the origin w.r.t. the MPC closed loop without stabilizing terminal conditions.
The key novelty is that we avoid restrictive invariance conditions. Last, we
verify our findings by numerical simulations.",['math.OC'],2502.13483," This paper presents PyJobShop, an open-source Python library for solving
scheduling problems with constraint programming. PyJobShop provides an
easy-to-use modeling interface that supports a wide variety of scheduling
problems, including well-known variants such as the flexible job shop problem
and the resource-constrained project scheduling problem. PyJobShop integrates
two state-of-the-art constraint programming solvers: Google's OR-Tools CP-SAT
and IBM ILOG's CP Optimizer. We leverage PyJobShop to conduct large-scale
numerical experiments on more than 9,000 benchmark instances from the machine
scheduling and project scheduling literature, comparing the performance of
OR-Tools and CP Optimizer. While CP Optimizer performs better on permutation
scheduling and large-scale problems, OR-Tools is highly competitive on job shop
scheduling and project scheduling problems--while also being fully open-source.
By providing an accessible and tested implementation of constraint programming
for scheduling, we hope that PyJobShop will enable researchers and
practitioners to use constraint programming for real-world scheduling problems.",['math.OC'],False,,,,"Kernel EDMD for data-driven nonlinear Koopman MPC with stability
  guarantees","PyJobShop: Solving scheduling problems with constraint programming in
  Python"
neg-d2-275,2025-01-22,,2501.13337," Many real-world problems, such as airfoil design, involve optimizing a
black-box expensive objective function over complex structured input space
(e.g., discrete space or non-Euclidean space). By mapping the complex
structured input space into a latent space of dozens of variables, a two-stage
procedure labeled as generative model based optimization (GMO) in this paper,
shows promise in solving such problems. However, the latent dimension of GMO is
hard to determine, which may trigger the conflicting issue between desirable
solution accuracy and convergence rate. To address the above issue, we propose
a multi-form GMO approach, namely generative multi-form optimization (GMFoO),
which conducts optimization over multiple latent spaces simultaneously to
complement each other. More specifically, we devise a generative model which
promotes positive correlation between latent spaces to facilitate effective
knowledge transfer in GMFoO. And further, by using Bayesian optimization (BO)
as the optimizer, we propose two strategies to exchange information between
these latent spaces continuously. Experimental results are presented on airfoil
and corbel design problems and an area maximization problem as well to
demonstrate that our proposed GMFoO converges to better designs on a limited
computational budget.",['cs.CE'],2501.182," The characterization of an interior permanent magnet synchronous machine
(IPMSM) requires numerical analysis of the nonlinear magnetic motor model in
different load conditions. To obtain the case-specific best machine behavior, a
strategy for the determination of stator input current amplitude and angle is
employed for all possible load torques given a limited terminal current
amplitude and DC bus voltage. Various losses are calculated using state of the
art loss models. The electromagnetic performance of the electric machine is
stored in lookup tables. These can then be used for the drive cycle analysis of
the electric drive train in the design and optimization stages.
  To avoid the use of a dedicated mesh generator in the numerical analysis,
volumetric spline-based models are suggested.With this approach, the mesh can
be generated directly from the Computer Aided Design (CAD) geometry. This
enables an automatic adaption of the grid following a geometry perturbation.
With this the approximated solution is kept consistent over the different
iterations of an overlying optimization, improving its convergence behavior.",['cs.CE'],False,,,,Generative Multi-Form Bayesian Optimization,"Characterization of Permanent Magnet Synchronous Machines based on
  semi-analytic model reduction for drive cycle analysis"
neg-d2-276,2025-02-10,,2502.06928," We derive a formula for the half-BPS interface entropy between any pair of
${\cal N}=(4,4)$ theories on the same conformal manifold. This generalizes the
diastasis formula derived in arXiv:1311.2202 for ${\cal N}=(2,2)$ theories,
which is restricted to the conformal submanifolds generated by either chiral or
twisted chiral multiples of ${\cal N}=(2,2)$ supersymmetry. To derive the
${\cal N}=(4,4)$ formula, we use the fact that the conformal manifold of ${\cal
N}=(4,4)$ theories is symmetric and quaternionic-K\""ahler and that its isotropy
group contains the $SU(2) \otimes SU(2)$ external automorphism of the ${\cal
N}=(4,4)$ superconformal algebra. As an application of the formula, we prove a
supersymmetric non-renormalization theorem, which explains the observation in
arXiv:1005.4433 that the interface entropy for half-BPS Janus solutions in type
IIB supergravity on ${\it AdS}_3 \times S^3 \times T^4$ coincides with the
corresponding quantity in their free conformal field limits.",['hep-th'],2502.06928," We derive a formula for the half-BPS interface entropy between any pair of
${\cal N}=(4,4)$ theories on the same conformal manifold. This generalizes the
diastasis formula derived in arXiv:1311.2202 for ${\cal N}=(2,2)$ theories,
which is restricted to the conformal submanifolds generated by either chiral or
twisted chiral multiples of ${\cal N}=(2,2)$ supersymmetry. To derive the
${\cal N}=(4,4)$ formula, we use the fact that the conformal manifold of ${\cal
N}=(4,4)$ theories is symmetric and quaternionic-K\""ahler and that its isotropy
group contains the $SU(2) \otimes SU(2)$ external automorphism of the ${\cal
N}=(4,4)$ superconformal algebra. As an application of the formula, we prove a
supersymmetric non-renormalization theorem, which explains the observation in
arXiv:1005.4433 that the interface entropy for half-BPS Janus solutions in type
IIB supergravity on ${\it AdS}_3 \times S^3 \times T^4$ coincides with the
corresponding quantity in their free conformal field limits.",['hep-th'],False,,,,"Nonrenormalization Theorem for ${\cal N}=(4,4)$ Interface Entropy","Nonrenormalization Theorem for ${\cal N}=(4,4)$ Interface Entropy"
neg-d2-277,2025-03-04,,2503.02493," The Compton Spectrometer and Imager (COSI) is a Compton telescope designed to
survey the 0.2-5 MeV sky, consisting of a compact array of cross-strip
germanium detectors. As part of its development, in 2016 COSI had a successful
46 day flight on board NASA's Super Pressure Balloon platform. This was a
precursor to the COSI Small Explorer (COSI-SMEX) satellite mission that will
launch in 2027 into a equatorial low Earth (530 km) orbit. The observation of
MeV gamma-rays is dominated by background radiation, especially due to the
activation of the detector materials induced by cosmic-ray interactions. Thus,
background simulation and identification are crucial for the data analysis.
Because the COSI-SMEX detectors will be similar to the ones used for the
balloon flight, the balloon measurements provide an important tool for testing
and cross-checking our background simulations for the upcoming space mission.
In this work we perform Monte Carlo simulations of the background emission from
the 2016 COSI balloon flight. Including a phenomenological shape correction, we
obtain an agreement with the data at the 10-20% level for energies between
0.1-1.6 MeV, and we successfully reproduce most of the activation lines induced
by cosmic ray interactions.",['astro-ph.IM'],2503.02493," The Compton Spectrometer and Imager (COSI) is a Compton telescope designed to
survey the 0.2-5 MeV sky, consisting of a compact array of cross-strip
germanium detectors. As part of its development, in 2016 COSI had a successful
46 day flight on board NASA's Super Pressure Balloon platform. This was a
precursor to the COSI Small Explorer (COSI-SMEX) satellite mission that will
launch in 2027 into a equatorial low Earth (530 km) orbit. The observation of
MeV gamma-rays is dominated by background radiation, especially due to the
activation of the detector materials induced by cosmic-ray interactions. Thus,
background simulation and identification are crucial for the data analysis.
Because the COSI-SMEX detectors will be similar to the ones used for the
balloon flight, the balloon measurements provide an important tool for testing
and cross-checking our background simulations for the upcoming space mission.
In this work we perform Monte Carlo simulations of the background emission from
the 2016 COSI balloon flight. Including a phenomenological shape correction, we
obtain an agreement with the data at the 10-20% level for energies between
0.1-1.6 MeV, and we successfully reproduce most of the activation lines induced
by cosmic ray interactions.",['astro-ph.IM'],False,,,,Bottom-up Background Simulations of the 2016 COSI Balloon Flight,Bottom-up Background Simulations of the 2016 COSI Balloon Flight
neg-d2-278,2025-01-26,,2501.15575," Within the NRQCD framework, we calculate the exclusive production of double
heavy quarkonium(double charmonium and double bottomonium) at future super $Z$
factory and at the CEPC/FCC-ee. The color-octet(CO) channels in the
$\gamma^*/Z^*$-propagated process are considered along with the
color-singlet(CS) channels. We found that the contributions of CO states to the
total cross section are significant or dominant for many processes within
energy region at $Z$ factory and at the CEPC/FCC-ee. The experimental
measurements will help us to verify the CO mechanism. Among these CO channels,
the gluon fragmentation into $^3S_1^{8}$ states is most important. Thus, the
comparison between the theoretical results and future data will give a strong
constraint to the matrix elements
$\langle\mathcal{O}\left(^3S_1^{[8]}\right)\rangle$. Additionally, we consider
the relativistic corrections to both the CS and CO channels which decrease the
cross sections significantly. Specially, the $K$ factors are about $0.5$ for
most charmonium channels. We get estimates of the events for double heavy
quarkonium production. The final events of $J/\psi+\eta_c$, $J/\psi+J/\psi$,
$\Upsilon+\eta_b$, $\Upsilon+\Upsilon$ production would be (22, 570, 71, 61)
and (206, 5343, 665, 576) at the CEPC (2-year) and at the FCC-ee (4-year) for
the $Z$ factory mode, respectively.",['hep-ph'],2501.15649," The evidence of a Stochastic Gravitational Wave Background (SGWB) in the nHz
frequency range is posed to open a new window on the Universe. A preferred
explanation relies on a supercooled first order phase transition at the 100 MeV
- GeV scale. In this article, we address the feasibility going from the
particle physics model to the production of the gravitational waves. We take a
minimal approach for the dark sector model introducing the fewest ingredients
required, namely a new U(1) gauge group and a dark scalar that dynamically
breaks the symmetry. Supercooling poses challenges in the analysis that put
under question the feasibility of this explanation: we address them, going
beyond previous studies by carefully considering the effects of a vacuum
domination phase and explicitly tracking the phase transition from its onset to
its completion. We find that the proposed model can successfully give origin to
the observed PTA SGWB signal. The strong supercooling imposes a correlation
between the new gauge coupling and the scalar quartic one, leading to a
significant hierarchy between the (heavier) gauge boson and the dark scalar.
Ultimately, information on phase transitions from SGWB observations could
provide a direct probe of the microphysics of the Early Universe and be used to
guide future searches of dark sector in laboratories.",['hep-ph'],False,,,,"Double heavy quarkonia production with color-octet channels at Z factory
  and at the CEPC/FCC-ee",Supercooled Dark Scalar Phase Transitions explanation of NANOGrav data
neg-d2-279,2025-01-01,,2501.00881," The evolution of agentic systems represents a significant milestone in
artificial intelligence and modern software systems, driven by the demand for
vertical intelligence tailored to diverse industries. These systems enhance
business outcomes through adaptability, learning, and interaction with dynamic
environments. At the forefront of this revolution are Large Language Model
(LLM) agents, which serve as the cognitive backbone of these intelligent
systems. In response to the need for consistency and scalability, this work
attempts to define a level of standardization for Vertical AI agent design
patterns by identifying core building blocks and proposing a \textbf{Cognitive
Skills } Module, which incorporates domain-specific, purpose-built inference
capabilities. Building on these foundational concepts, this paper offers a
comprehensive introduction to agentic systems, detailing their core components,
operational patterns, and implementation strategies. It further explores
practical use cases and examples across various industries, highlighting the
transformative potential of LLM agents in driving industry-specific
applications.",['cs.MA'],2503.07702," Collaboration is a fundamental and essential characteristic of many complex
systems, ranging from ant colonies to human societies. Each component within a
complex system interacts with others, even at a distance, to accomplish a given
task. A network of collaboration can be defined to study the collective
behavior of such systems within the framework of complex networks. The nodes in
these networks may represent simple organisms or more sophisticated intelligent
agents, such as humans. In this study, we utilize intelligent agents (nodes)
trained through reinforcement learning techniques to establish connections with
their neighbors, ultimately leading to the emergence of a large-scale
communication cluster. Notably, there is no centralized administrator; instead,
agents must adjust their connections based on information obtained from local
observations. The connection strategy is formulated using a physical
Hamiltonian, thereby categorizing this intelligent system under the paradigm of
""Physics-Guided Machine Learning"".",['cs.MA'],False,,,,"Agentic Systems: A Guide to Transforming Industries with Vertical AI
  Agents","A Reliable Self-Organized Distributed Complex Network for Communication
  of Smart Agents"
neg-d2-280,2025-02-07,,2502.05336," This paper introduces Monotone Delta, an order-theoretic measure designed to
enhance the reliability assessment of survey-based instruments in human-machine
interactions. Traditional reliability measures, such as Cronbach's Alpha and
McDonald's Omega, often yield misleading estimates due to their sensitivity to
redundancy, multidimensional constructs, and assumptions of normality and
uncorrelated errors. These limitations can compromise decision-making in
human-centric evaluations, where survey instruments inform adaptive interfaces,
cognitive workload assessments, and human-AI trust models. Monotone Delta
addresses these issues by quantifying internal consistency through the
minimization of ordinal contradictions and alignment with a unidimensional
latent order using weighted tournaments. Unlike traditional approaches, it
operates without parametric or model-based assumptions. We conducted
theoretical analyses and experimental evaluations on four challenging
scenarios: tau-equivalence, redundancy, multidimensionality, and non-normal
distributions, and proved that Monotone Delta provides more stable reliability
assessments compared to existing methods. The Monotone Delta is a valuable
alternative for evaluating questionnaire-based assessments in psychology, human
factors, healthcare, and interactive system design, enabling organizations to
optimize survey instruments, reduce costly redundancies, and enhance confidence
in human-system interactions.",['stat.OT'],2502.05336," This paper introduces Monotone Delta, an order-theoretic measure designed to
enhance the reliability assessment of survey-based instruments in human-machine
interactions. Traditional reliability measures, such as Cronbach's Alpha and
McDonald's Omega, often yield misleading estimates due to their sensitivity to
redundancy, multidimensional constructs, and assumptions of normality and
uncorrelated errors. These limitations can compromise decision-making in
human-centric evaluations, where survey instruments inform adaptive interfaces,
cognitive workload assessments, and human-AI trust models. Monotone Delta
addresses these issues by quantifying internal consistency through the
minimization of ordinal contradictions and alignment with a unidimensional
latent order using weighted tournaments. Unlike traditional approaches, it
operates without parametric or model-based assumptions. We conducted
theoretical analyses and experimental evaluations on four challenging
scenarios: tau-equivalence, redundancy, multidimensionality, and non-normal
distributions, and proved that Monotone Delta provides more stable reliability
assessments compared to existing methods. The Monotone Delta is a valuable
alternative for evaluating questionnaire-based assessments in psychology, human
factors, healthcare, and interactive system design, enabling organizations to
optimize survey instruments, reduce costly redundancies, and enhance confidence
in human-system interactions.",['stat.OT'],False,,,,"Leveraging Order-Theoretic Tournament Graphs for Assessing Internal
  Consistency in Survey-Based Instruments Across Diverse Scenarios","Leveraging Order-Theoretic Tournament Graphs for Assessing Internal
  Consistency in Survey-Based Instruments Across Diverse Scenarios"
neg-d2-281,2025-02-17,,2502.11914," We address the long-standing problem of the existence of a Riemannian metric
on $S^2\times T^2$ with strictly positive biorthogonal curvature ($
K_{\text{biort}}(\sigma) > 0 $), but in a weaker framework, by introducing an
affine connection with antisymmetric closed torsion, naturally encoded in the
cohomology of $S^2 \times T^2$ ($H^3(S^2 \times T^2; \mathbb{R}) \cong
\mathbb{R}^2$). This torsion, parametrized by non-trivial cohomology classes,
overcomes topological constraints imposed by the zero Euler characteristic,
ensuring $ K_{\text{biort}}(\sigma) > 0$ globally.",['math.DG'],2501.17511," On a complete Riemannian manifold $M$, we study the spectral flow of a family
of Callias operators on $M$. We derive a codimension zero formula when
dimension of $M$ is odd and a codimension one formula when dimension of $M$ is
even. These can be seen as analogues of Gromov--Lawson's relative index theorem
and classical Callias index theorem, respectively. Secondly, we introduce an
intrinsic definition of K-cowaist on odd-dimensional manifolds, making use of
the odd Chern character of a smooth map from $M$ to a unitary group. It behaves
just like the usual K-cowaist on even-dimensional manifolds. We then apply the
notion of odd K-cowaist and the tool of spectral flow to investigate problems
related to positive scalar curvature on spin manifolds. In particular, we prove
infinite odd K-cowaist to be an obstruction to the existence of PSC metrics. We
obtain quantitative scalar curvature estimates on complete non-compact
manifolds and scalar-mean curvature estimates on compact manifolds with
boundary. They extend several previous results optimally, which unfolds a major
advantage of our method via spectral flow and odd K-cowaist.",['math.DG'],False,,,,"Positive biorthogonal curvature on $S^2 \times T^2$ via affine
  connection","Spectral flow of Callias operators, odd K-cowaist, and positive scalar
  curvature"
neg-d2-282,2025-03-07,,2503.05988," In recent years, machine learning (ML) methods have become increasingly
popular in wireless communication systems for several applications. A critical
bottleneck for designing ML systems for wireless communications is the
availability of realistic wireless channel datasets, which are extremely
resource intensive to produce. To this end, the generation of realistic
wireless channels plays a key role in the subsequent design of effective ML
algorithms for wireless communication systems. Generative models have been
proposed to synthesize channel matrices, but outputs produced by such methods
may not correspond to geometrically viable channels and do not provide any
insight into the scenario of interest. In this work, we aim to address both
these issues by integrating a parametric, physics-based geometric channel
(PBGC) modeling framework with generative methods. To address limitations with
gradient flow through the PBGC model, a linearized reformulation is presented,
which ensures smooth gradient flow during generative model training, while also
capturing insights about the underlying physical environment. We evaluate our
model against prior baselines by comparing the generated samples in terms of
the 2-Wasserstein distance and through the utility of generated data when used
for downstream compression tasks.",['eess.SP'],2502.18636," In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.",['eess.SP'],False,,,,Physics-Informed Generative Approaches for Wireless Channel Modeling,"Transfer Learning Assisted Fast Design Migration Over Technology Nodes:
  A Study on Transformer Matching Network"
neg-d2-283,2025-02-10,,2502.06966," Recent discoveries of transiting giant exoplanets around M dwarfs (GEMS)
present an opportunity to investigate their atmospheric compositions and
explore how such massive planets can form around low-mass stars contrary to
canonical formation models. Here, we present the first transmission spectra of
TOI-5205b, a short-period ($P=1.63~\mathrm{days}$) Jupiter-like planet
($M_p=1.08~\mathrm{M_J}$ and $R_p=0.94~\mathrm{R_J}$) orbiting an M4 dwarf. We
obtained three transits using the PRISM mode of the JWST Near Infrared
Spectrograph (NIRSpec) spanning $0.6-5.3$ um. Our data reveal significant
stellar contamination that is evident in the light curves as spot-crossing
events and in the transmission spectra as a larger transit depth at bluer
wavelengths. Atmospheric retrievals demonstrate that stellar contamination from
unocculted star spots is the dominant component of the transmission spectrum at
wavelengths $\lambda\lesssim3.0$ um, which reduced the sensitivity to the
presence of clouds or hazes in our models. The degree of stellar contamination
also prevented the definitive detection of any $\mathrm{H_2O}$, which has
primary absorption features at these shorter wavelengths. The broad wavelength
coverage of NIRSpec PRISM enabled a robust detection of $\mathrm{CH_4}$ and
$\mathrm{H_2S}$, which have detectable molecular features between $3.0-5.0$ um.
Our gridded and Bayesian retrievals consistently favored an atmosphere with
both sub-solar metallicity ($\log\mathrm{[M/H]}\sim-2$ for a clear atmosphere)
and super-solar C/O ratio ($\log\mathrm{[C/O]}\sim3$ for a clear or cloudy
atmosphere). This contrasts with estimates from planetary interior models that
predict a bulk metallicity of 10--20%, which is $\sim100\times$ the atmospheric
metallicity, and suggests that the planetary interior for TOI-5205b is
decoupled from its atmosphere and not well mixed.",['astro-ph.EP'],2501.0752," We present the results of combined hydrodynamic and particle tracking
post-processing modeling to study the transport of small dust in a
protoplanetary disk containing an embedded embryo in 3D. We use a suite of
FARGO3D hydrodynamic simulations of disks containing a planetary embryo varying
in mass up to 300 $M_\oplus$ on a fixed orbit in both high and low viscosity
disks. We then simulate solid particles through the disk as a post-processing
step using a Monte Carlo integration, allowing us to track the trajectories of
individual particles as they travel throughout the disk. We find that gas
advection onto the planet can carry small, well-coupled solids across the gap
opened in the disk by the embedded planet for planetary masses above the pebble
isolation mass. This mixing between the inner and outer disk can occur in both
directions, with solids in the inner disk mixing to the outer disk as well.
Additionally, in low viscosity disks, multiple pile-ups in the outer disk may
preserve isotopic heterogeneities, possibly providing an outermost tertiary
isotopic reservoir. Throughout Jupiter's growth, the extent of mixing between
isotopic reservoirs varied depending on dust size, gas turbulence, and the
Jovian embryo mass.",['astro-ph.EP'],False,,,,"GEMS JWST: Transmission spectroscopy of TOI-5205b reveals significant
  stellar contamination and a metal-poor atmosphere","Three-dimensional transport of solids in a protoplanetary disk
  containing a growing giant planet"
neg-d2-284,2025-03-08,,2503.06091," We give an explicit construction of the generating set of a colored operad
that implements theta theory in the mathematical model of Minimalism in
generative linguistics, in the form of a coloring algorithm for syntactic
objects. We show that the coproduct operation on workspaces allows for a
recursive implementation of the theta criterion. We also show that this
filtering by coloring rules on structures freely formed by Merge is equivalent
to a process of structure formation by a colored version of Merge: the form of
the generators of the colored operad then implies the dichotomy is semantics
between External and Internal Merge, where Internal Merge only moves to
non-theta positions.",['cs.CL'],2501.01028," As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.",['cs.CL'],False,,,,Theta Theory: operads and coloring,KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
neg-d2-285,2025-01-29,,2501.17655," 3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene
reconstruction using 3D Gaussians. However, neither the centers nor surfaces of
the Gaussians are accurately aligned to the object surface, complicating their
direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically
produces floater artifacts, increasing the number of Gaussians and storage
requirements. To address these issues, we present FeatureGS, which incorporates
an additional geometric loss term based on an eigenvalue-derived 3D shape
feature into the optimization process of 3DGS. The goal is to improve geometric
accuracy and enhance properties of planar surfaces with reduced structural
entropy in local 3D neighborhoods.We present four alternative formulations for
the geometric loss term based on 'planarity' of Gaussians, as well as
'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We
provide quantitative and qualitative evaluations on 15 scenes of the DTU
benchmark dataset focusing on following key aspects: Geometric accuracy and
artifact-reduction, measured by the Chamfer distance, and memory efficiency,
evaluated by the total number of Gaussians. Additionally, rendering quality is
monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement
in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses
floater artifacts, while maintaining comparable photometric rendering quality.
The geometric loss with 'planarity' from Gaussians provides the highest
geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces
floater artifacts and number of Gaussians the most. This makes FeatureGS a
strong method for geometrically accurate, artifact-reduced and memory-efficient
3D scene reconstruction, enabling the direct use of Gaussian centers for
geometric representation.",['cs.CV'],2502.12723," This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.",['cs.CV'],False,,,,"FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for
  Geometrically Accurate and Artifact-Reduced Reconstruction","myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  Dataset"
neg-d2-286,2025-02-17,,2502.11503," For an elliptic CW-complex $X$, we denote its group of self-homotopy
equivalences as $\E(X)$, and its subgroup consisting of elements inducing the
identity on the homology groups as $\E_{*}(X)$. This paper aims to investigate
how the homology groups of $X$ influence the finiteness of $\E(X)$ and
$\E_{*}(X)$.",['math.AT'],2502.12228," In this note, we show that there does not exist a $C_2$-ring spectrum whose
underlying ring spectrum is $\mathrm{MSpin}^c$ and whose $C_2$-fixed point
spectrum is $\mathrm{MSpin}$.",['math.AT'],False,,,,"Impact of the Homology Groups on the Finiteness of the group of
  Self-Homotopy Equivalences of an Elliptic Space",On the nonexistence of a Green functor with values MSpin${}^c$ and MSpin
neg-d2-287,2025-01-20,,2501.11334," It is known that there are many notions of largeness in a semigroup that own
rich combinatorial properties. In this paper, we focus on partition and almost
disjoint properties of these notions. One of the most remarkable results with
respect to this topic is that in an infinite very weakly cancellative semigroup
of size \kappa, every central set can be split into \kappa disjoint central
subsets. Moreover, if \kappa contains \lambda almost disjoint subsets, then
every central set contains a family of \lambda almost disjoint central subsets.
And many other combinatorial notions are found successively to have analogous
properties, among these are thick sets, piecewise syndetic sets, J-sets and
C-sets. In this paper, we mainly study four other notions: IP sets,
combinatorially rich sets, Cp-sets and PP-rich sets. Where the latter two are
known in (N, +), related to the polynomial extension of the central sets
theorem. We lift them up to commutative cancellative semigroups and obtain an
uncountable version of the polynomial extension of the central sets theorem
incidentally. And we finally find that the infinite partition and almost
disjoint properties hold for Cp-sets in commutative cancellative semigroups and
for other three notions in (N, +).",['math.CO'],2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],False,,,,On partition and almost disjoint properties of combinatorial notions,Coarse tree-width
neg-d2-288,2025-02-24,,2502.17408," Reconfigurable holographic surfaces (RHS) have emerged as a transformative
material technology, enabling dynamic control of electromagnetic waves to
generate versatile holographic beam patterns. This paper addresses the problem
of joint hybrid holographic beamforming and user scheduling under per-user
minimum quality-of-service (QoS) constraints, a critical challenge in
resource-constrained networks. However, such a problem results in mixed-integer
non-convex optimization, making it difficult to identify feasible solutions
efficiently. To overcome this challenge, we propose a novel iterative
optimization framework that jointly solves the problem to maximize the
RHS-assisted network sum-rate, efficiently managing holographic beamforming
patterns, dynamically scheduling users, and ensuring the minimum QoS
requirements for each scheduled user. The proposed framework relies on
zero-forcing digital beamforming, gradient-ascent-based holographic beamformer
optimization, and a greedy user selection principle. Our extensive simulation
results validate the effectiveness of the proposed scheme, demonstrating their
superior performance compared to the benchmark algorithms in terms of sum-rate
performance, while meeting the minimum per-user QoS constraints",['eess.SP'],2502.08368," A Single Ensemble Empirical Mode Decomposition (SEEMD) is proposed for
locating the damage in rolling element bearings. The SEEMD does not require a
number of ensembles from the addition or subtraction of noise every time while
processing the signals. The SEEMD requires just a single sifting process of a
modified raw signal to reduce the computation time significantly. The other
advantage of the SEEMD method is its success in dealing with non-Gaussian or
non-stationary perturbing signals. In SEEMD, initially, a fractional Gaussian
noise (FGN) is added to the raw signal to emphasize on high frequencies of the
signal. Then, a convoluted white Gaussian noise is multiplied to the resulting
signal which changes the spectral content of the signal which helps in
extraction of the weak periodic signal. Finally, the obtained signal is
decomposed by using a single sifting process. The proposed methodology is
applied to the raw signals obtained from the mining industry. These signals are
difficult to analyze since cyclic impulsive components are obscured by noise
and other interference. Based on the results, the proposed method can
effectively detect the fault where the signal of interest (SOI) has been
extracted with good quality.",['eess.SP'],False,,,,"Joint Holographic Beamforming and User Scheduling with Individual QoS
  Constraints","Local damage detection in rolling element bearings based on a Single
  Ensemble Empirical Mode Decomposition"
neg-d2-289,2025-01-23,,2501.14124," Identifying the intrinsic coordinates or modes of the dynamical systems is
essential to understand, analyze, and characterize the underlying dynamical
behaviors of complex systems. For nonlinear dynamical systems, this presents a
critical challenge as the linear modal transformation, which is universal for
linear systems, does not apply to nonlinear dynamical systems. As natural
extensions to linear normal modes,the nonlinear normal modes (NNMs) framework
provides a comprehensive representation of nonlinear dynamics. Theoretically,
NNMs may either be computed numerically or analytically from the closed-form
models or equations of dynamical systems, or experimentally identified from
controllable input-output tests, both of which, however, are typically unknown
or unavailable practically. In this study, we present a physics-integrated
Normalizing Flows deep learning-based data-driven approach which identifies the
NNMs and the nonlinear modal transformation function of NNMs using measured
response data only. Specifically, we leverage the unique features of the
Normalizing Flows model: 1) the independent latent spaces, naturally spanned by
the Normalizing Flows, are exploited to facilitate nonlinear modal
decomposition; 2) the invertible transformation through the Normalizing Flows,
enabling efficient and accurate nonlinear transformation between original and
modal coordinates transformation. Therefore, our framework leverages the
independency feature and invertibility of Normalizing Flows to create a model
that captures the dynamics of unknown nonlinear dynamical systems.",['nlin.CD'],2502.08618," The Fitzhugh-Nagumo neuronal model is used to explore the influence of the
electric field on thermosensitive neurons' dynamics. This study investigates
how the electric field affects polarization modulation in cell media induced by
changes in ion charge density by adding electrical field as a new variable.
Driven by a voltage source acting as an external stimulus current, different
firing mode responses of the proposed model are analyzed when an external
electrical field is applied. Through computational analysis, the study
evaluates the impact of parameters such as cell radius, stimulus voltage source
amplitude, frequency, and as well as the presence of an external electric
field. The results demonstrate distinct mode transitions of isolated neurons
ranging from spiking to bursting, regular and chaotic oscillations. These
findings suggest that the firing mode is triggered by periodic external
electric fields and cell radius, with the electric field's involvement enhanced
to regulate neuron activity and control the dynamics. External electric fields
and stimuli play a crucial role in neuronal firing dynamics, affecting the
transition between different firing modes. Understanding these effects
contributes to the comprehension of neural processes and the potential
manipulation of neural activity for various applications in neuroscience and
biophysics.",['nlin.CD'],False,,,,"Data-driven nonlinear modal identification of nonlinear dynamical
  systems with physics-constrained Normalizing Flows","Modulation of Neuronal Firing Modes by Electric Fields in a
  Thermosensitive FitzHugh-Nagumo Model"
neg-d2-290,2025-02-08,,2502.0553," User identification procedures, essential to the information security of
systems, enable system-user interactions by exchanging data through
communication links and interfaces to validate and confirm user authenticity.
However, human errors can introduce vulnerabilities that may disrupt the
intended identification workflow and thus impact system behavior. Therefore,
ensuring the integrity of these procedures requires accounting for such
erroneous behaviors. We follow a formal, human-centric approach to analyze user
identification procedures by modeling them as security ceremonies and apply
proven techniques for automatically analyzing such ceremonies. The approach
relies on mutation rules to model potential human errors that deviate from
expected interactions during the identification process, and is implemented as
the X-Men tool, an extension of the Tamarin prover, which automatically
generates models with human mutations and implements matching mutations to
other ceremony participants for analysis. As a proof-of-concept, we consider a
real-life pilot study involving an AI-driven, virtual receptionist kiosk for
authenticating visitors.",['cs.CR'],2501.0861," As the Internet rapidly expands, the increasing complexity and diversity of
network activities pose significant challenges to effective network governance
and security regulation. Network traffic, which serves as a crucial data
carrier of network activities, has become indispensable in this process.
Network traffic detection aims to monitor, analyze, and evaluate the data flows
transmitted across the network to ensure network security and optimize
performance. However, existing network traffic detection methods generally
suffer from several limitations: 1) a narrow focus on characterizing traffic
features from a single perspective; 2) insufficient exploration of
discriminative features for different traffic; 3) poor generalization to
different traffic scenarios. To address these issues, we propose a multi-view
correlation-aware framework named FlowID for network traffic detection. FlowID
captures multi-view traffic features via temporal and interaction awareness,
while a hypergraph encoder further explores higher-order relationships between
flows. To overcome the challenges of data imbalance and label scarcity, we
design a dual-contrastive proxy task, enhancing the framework's ability to
differentiate between various traffic flows through traffic-to-traffic and
group-to-group contrast. Extensive experiments on five real-world datasets
demonstrate that FlowID significantly outperforms existing methods in accuracy,
robustness, and generalization across diverse network scenarios, particularly
in detecting malicious traffic.",['cs.CR'],False,,,,"User Identification Procedures with Human Mutations: Formal Analysis and
  Pilot Study (Extended Version)","Multi-view Correlation-aware Network Traffic Detection on Flow
  Hypergraph"
neg-d2-291,2025-02-08,,2502.05457," Content-based video retrieval is one of the most challenging tasks in
surveillance systems. In this study, Latent Dirichlet Allocation (LDA) topic
model is used to annotate surveillance videos in an unsupervised manner. In
scene understanding methods, some of the learned patterns are ambiguous and
represents a mixture of atomic actions. To address the ambiguity issue in the
proposed method, feature vectors, and the primary model are processed to obtain
a secondary model which describes the scene with primitive patterns that lack
any ambiguity. Experiments show performance improvement in the retrieval task
compared to other topic model-based methods. In terms of false positive and
true positive responses, the proposed method achieves at least 80\% and 124\%
improvement respectively. Four search strategies are proposed, and users can
define and search for a variety of activities using the proposed query
formulation which is based on topic models. In addition, the lightweight
database in our method occupies much fewer storage which in turn speeds up the
search procedure compared to the methods which are based on low-level features.",['cs.CV'],2503.13743," We tackle the problem of monocular 3D object detection across different
sensors, environments, and camera setups. In this paper, we introduce a novel
unsupervised domain adaptation approach, MonoCT, that generates highly accurate
pseudo labels for self-supervision. Inspired by our observation that accurate
depth estimation is critical to mitigating domain shifts, MonoCT introduces a
novel Generalized Depth Enhancement (GDE) module with an ensemble concept to
improve depth estimation accuracy. Moreover, we introduce a novel Pseudo Label
Scoring (PLS) module by exploring inner-model consistency measurement and a
Diversity Maximization (DM) strategy to further generate high-quality pseudo
labels for self-training. Extensive experiments on six benchmarks show that
MonoCT outperforms existing SOTA domain adaptation methods by large margins
(~21% minimum for AP Mod.) and generalizes well to car, traffic camera and
drone views.",['cs.CV'],False,,,,"Content-based Video Retrieval in Traffic Videos using Latent Dirichlet
  Allocation Topic Model","MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent
  Teacher Models"
neg-d2-292,2025-03-10,,2503.08005," 3D object reconstruction from single-view image is a fundamental task in
computer vision with wide-ranging applications. Recent advancements in Large
Reconstruction Models (LRMs) have shown great promise in leveraging multi-view
images generated by 2D diffusion models to extract 3D content. However,
challenges remain as 2D diffusion models often struggle to produce dense images
with strong multi-view consistency, and LRMs tend to amplify these
inconsistencies during the 3D reconstruction process. Addressing these issues
is critical for achieving high-quality and efficient 3D reconstruction. In this
paper, we present CDI3D, a feed-forward framework designed for efficient,
high-quality image-to-3D generation with view interpolation. To tackle the
aforementioned challenges, we propose to integrate 2D diffusion-based view
interpolation into the LRM pipeline to enhance the quality and consistency of
the generated mesh. Specifically, our approach introduces a Dense View
Interpolation (DVI) module, which synthesizes interpolated images between main
views generated by the 2D diffusion model, effectively densifying the input
views with better multi-view consistency. We also design a tilt camera pose
trajectory to capture views with different elevations and perspectives.
Subsequently, we employ a tri-plane-based mesh reconstruction strategy to
extract robust tokens from these interpolated and original views, enabling the
generation of high-quality 3D meshes with superior texture and geometry.
Extensive experiments demonstrate that our method significantly outperforms
previous state-of-the-art approaches across various benchmarks, producing 3D
content with enhanced texture fidelity and geometric accuracy.",['cs.CV'],2503.08668," Vector Quantization (VQ) has emerged as a prominent weight compression
technique, showcasing substantially lower quantization errors than uniform
quantization across diverse models, particularly in extreme compression
scenarios. However, its efficacy during fine-tuning is limited by the
constraint of the compression format, where weight vectors assigned to the same
codeword are restricted to updates in the same direction. Consequently, many
quantized weights are compelled to move in directions contrary to their local
gradient information. To mitigate this issue, we introduce a novel VQ paradigm,
Sign-Splitting VQ (SSVQ), which decouples the sign bit of weights from the
codebook. Our approach involves extracting the sign bits of uncompressed
weights and performing clustering and compression on all-positive weights. We
then introduce latent variables for the sign bit and jointly optimize both the
signs and the codebook. Additionally, we implement a progressive freezing
strategy for the learnable sign to ensure training stability. Extensive
experiments on various modern models and tasks demonstrate that SSVQ achieves a
significantly superior compression-accuracy trade-off compared to conventional
VQ. Furthermore, we validate our algorithm on a hardware accelerator, showing
that SSVQ achieves a 3$\times$ speedup over the 8-bit compressed model by
reducing memory access.",['cs.CV'],False,,,,CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction,"SSVQ: Unleashing the Potential of Vector Quantization with
  Sign-Splitting"
neg-d2-293,2025-03-20,,2503.16194," Autoregressive models have shown remarkable success in image generation by
adapting sequential prediction techniques from language modeling. However,
applying these approaches to images requires discretizing continuous pixel data
through vector quantization methods like VQ-VAE. To alleviate the quantization
errors that existed in VQ-VAE, recent works tend to use larger codebooks.
However, this will accordingly expand vocabulary size, complicating the
autoregressive modeling task. This paper aims to find a way to enjoy the
benefits of large codebooks without making autoregressive modeling more
difficult. Through empirical investigation, we discover that tokens with
similar codeword representations produce similar effects on the final generated
image, revealing significant redundancy in large codebooks. Based on this
insight, we propose to predict tokens from coarse to fine (CTF), realized by
assigning the same coarse label for similar tokens. Our framework consists of
two stages: (1) an autoregressive model that sequentially predicts coarse
labels for each token in the sequence, and (2) an auxiliary model that
simultaneously predicts fine-grained labels for all tokens conditioned on their
coarse labels. Experiments on ImageNet demonstrate our method's superior
performance, achieving an average improvement of 59 points in Inception Score
compared to baselines. Notably, despite adding an inference step, our approach
achieves faster sampling speeds.",['cs.CV'],2502.06445," This paper introduces an open-source benchmark for evaluating Vision-Language
Models (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video
environments. We present a curated dataset containing 1,477 manually annotated
frames spanning diverse domains, including code editors, news broadcasts,
YouTube videos, and advertisements. Three state of the art VLMs - Claude-3,
Gemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as
EasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),
Character Error Rate (CER), and Accuracy. Our results highlight the strengths
and limitations of VLMs in video-based OCR tasks, demonstrating their potential
to outperform conventional OCR models in many scenarios. However, challenges
such as hallucinations, content security policies, and sensitivity to occluded
or stylized text remain. The dataset and benchmarking framework are publicly
available to foster further research.",['cs.CV'],False,,,,"Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction","Benchmarking Vision-Language Models on Optical Character Recognition in
  Dynamic Video Environments"
neg-d2-294,2025-02-03,,2502.01606," High-resolution spectroscopy has provided a wealth of information about the
climate and composition of ultra-hot Jupiters. However, the 3D structure of
their atmospheres makes observations more challenging to interpret,
necessitating 3D forward-modeling studies. In this work, we model
phase-dependent thermal emission spectra of the archetype ultra-hot Jupiter
WASP-76b to understand how the line strengths and Doppler shifts of Fe, CO,
H$_2$O, and OH evolve throughout the orbit. We post-process outputs of the
SPARC/MITgcm global circulation model with the 3D Monte-Carlo radiative
transfer code gCMCRT to simulate emission spectra at 36 orbital phases. We then
cross-correlate the spectra with different templates to obtain CCF and
$K_{\text{p}}$$-$$V_{\text{sys}}$ maps. For each species, our models produce
consistently negative $K_{\text{p}}$ offsets in pre- and post-eclipse, which
are driven by planet rotation. The size of these offsets is similar to the
equatorial rotation velocity of the planet. Furthermore, we demonstrate how the
weak vertical temperature gradient on the nightside of ultra-hot Jupiters mutes
the absorption features of CO and H$_2$O, which significantly hampers their
detectability in pre- and post-transit. We also show that the $K_{\text{p}}$
and $V_{\text{sys}}$ offsets in pre- and post-transit are not always a measure
for the line-of-sight velocities in the atmosphere. This is because the
cross-correlation signal is a blend of dayside emission and nightside
absorption features. Finally, we highlight that the observational uncertainty
in the known orbital velocity of ultra-hot Jupiters can be multiple km/s, which
makes it hard for certain targets to meaningfully report absolute
$K_{\text{p}}$ offsets.",['astro-ph.EP'],2503.10441," We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density
sub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of
observational data (2009-2022), our study incorporates 7 new ground-based
photometric transit observations, three sectors of Transiting Exoplanet Survey
Satellite (TESS) data, and 23 previously published light curves. A total of 46
light curves were analyzed using various analytical models, such as linear,
orbital decay, apsidal precession, and sinusoidal models to investigate the
presence of additional planets. The stellar tidal quality factor ($Q_\star'
\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay
model an unlikely explanation. The apsidal precession model with a $\chi_r^2$
of 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession
rate of 0.0045 rad/epoch. Frequency analysis using the Generalized Lomb-Scargle
(GLS) periodogram identified a significant periodic signal at 0.00415
cycles/day (FAP = 5.1$\times$10$^{-6}$ %), suggesting the influence of an
additional planetary companion. The sinusoidal model provides the lowest
reduced chi-squared value ($\chi_r^2$) of 3.2. Sinusoidal fitting of the timing
residuals estimated this companion to have a mass of approximately 0.02 $M_J$ ,
assuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.
Additionally, the Applegate mechanism, with an amplitude much smaller than the
observed TTV amplitude of 156 s, confirms that stellar activity is not
responsible for the observed variations.",['astro-ph.EP'],False,,,,"From pre-transit to post-eclipse: investigating the impact of 3D
  temperature, chemistry, and dynamics on high-resolution emission spectra of
  the ultra-hot Jupiter WASP-76b",Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b
neg-d2-295,2025-02-03,,2502.01173," Railway transportation contributes to the objectives of decarbonization but
also generates negative externalities, including noise. Energy noise indicators
used to characterize population exposure do not adequately reflect the
repetitive nature of railway noise peaks. The GENIFER pilot study aims to test
a protocol designed to characterize railway noise events according to the
instantaneous perceived annoyance when the train is passing, in order to
improve understanding of the influence of acoustic factors on annoyance. The
first phase of the survey was carried out in 2023 among 62 residents of a pilot
site. An electronic device was used to collect around 5,000 ratings, ranging
from 1 to 10, assessing the instantaneous annoyance induced by railway noise at
passing trains. The site instrumentation included sixteen sound level meters
and two video recording systems, enabling annoyance ratings to be associated
with the acoustic characteristics of railway noise events. A questionnaire
aimed at identifying co-determinants of long-term annoyance was also
administered to participants. Feedback on the field implementation of this
survey and initial results concerning acoustic measurements, instantaneous
annoyance ratings and questionnaire responses will be presented.",['physics.class-ph'],2502.01173," Railway transportation contributes to the objectives of decarbonization but
also generates negative externalities, including noise. Energy noise indicators
used to characterize population exposure do not adequately reflect the
repetitive nature of railway noise peaks. The GENIFER pilot study aims to test
a protocol designed to characterize railway noise events according to the
instantaneous perceived annoyance when the train is passing, in order to
improve understanding of the influence of acoustic factors on annoyance. The
first phase of the survey was carried out in 2023 among 62 residents of a pilot
site. An electronic device was used to collect around 5,000 ratings, ranging
from 1 to 10, assessing the instantaneous annoyance induced by railway noise at
passing trains. The site instrumentation included sixteen sound level meters
and two video recording systems, enabling annoyance ratings to be associated
with the acoustic characteristics of railway noise events. A questionnaire
aimed at identifying co-determinants of long-term annoyance was also
administered to participants. Feedback on the field implementation of this
survey and initial results concerning acoustic measurements, instantaneous
annoyance ratings and questionnaire responses will be presented.",['physics.class-ph'],False,,,,"Improving knowledge of the acoustic factors involved in railway noise
  annoyance: first results of a pilot field survey of sixty-two local residents","Improving knowledge of the acoustic factors involved in railway noise
  annoyance: first results of a pilot field survey of sixty-two local residents"
neg-d2-296,2025-02-11,,2502.0738," Simulation has been pivotal in recent robotics milestones and is poised to
play a prominent role in the field's future. However, recent robotic advances
often rely on expensive and high-maintenance platforms, limiting access to
broader robotics audiences. This work introduces Wheeled Lab, a framework for
the low-cost, open-source wheeled platforms that are already widely established
in education and research. Through integration with Isaac Lab, Wheeled Lab
introduces modern techniques in Sim2Real, such as domain randomization, sensor
simulation, and end-to-end learning, to new user communities. To kickstart
education and demonstrate the framework's capabilities, we develop three
state-of-the-art policies for small-scale RC cars: controlled drifting,
elevation traversal, and visual navigation, each trained in simulation and
deployed in the real world. By bridging the gap between advanced Sim2Real
methods and affordable, available robotics, Wheeled Lab aims to democratize
access to cutting-edge tools, fostering innovation and education in a broader
robotics context. The full stack, from hardware to software, is low cost and
open-source.",['cs.RO'],2503.10141," Collision-free flight in cluttered environments is a critical capability for
autonomous quadrotors. Traditional methods often rely on detailed 3D map
construction, trajectory generation, and tracking. However, this cascade
pipeline can introduce accumulated errors and computational delays, limiting
flight agility and safety. In this paper, we propose a novel method for
enabling collision-free flight in cluttered environments without explicitly
constructing 3D maps or generating and tracking collision-free trajectories.
Instead, we leverage Model Predictive Control (MPC) to directly produce safe
actions from sparse waypoints and point clouds from a depth camera. These
sparse waypoints are dynamically adjusted online based on nearby obstacles
detected from point clouds. To achieve this, we introduce a dual KD-Tree
mechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle for
avoidance, while the Edge KD-Tree provides a robust initial guess for the MPC
solver, preventing it from getting stuck in local minima during obstacle
avoidance. We validate our approach through extensive simulations and
real-world experiments. The results show that our approach significantly
outperforms the mapping-based methods and is also superior to imitation
learning-based methods, demonstrating reliable obstacle avoidance at up to 12
m/s in simulations and 6 m/s in real-world tests. Our method provides a simple
and robust alternative to existing methods.",['cs.RO'],False,,,,"Demonstrating Wheeled Lab: Modern Sim2Real for Low-cost, Open-source
  Wheeled Robotics","Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered
  Environments"
neg-d2-297,2025-02-17,,2502.11733," Large language models (LLMs) have risen to prominence as 'chatbots' for users
to interact via natural language. However, their abilities to capture
common-sense knowledge make them seem promising as language-based planners of
situated or embodied action as well. We have implemented a simple text-based
environment -- similar to others that have before been used for
reinforcement-learning of agents -- that simulates, very abstractly, a
household setting. We use this environment and the detailed error-tracking
capabilities we implemented for targeted benchmarking of LLMs on the problem of
practical reasoning: Going from goals and observations to actions. Our findings
show that environmental complexity and game restrictions hamper performance,
and concise action planning is demanding for current LLMs.",['cs.CL'],2502.15401," In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.",['cs.CL'],False,,,,"Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking
  Practical Reasoning and Situation Modelling in a Text-Simulated Situated
  Environment","Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs
  Complex Reasoning"
neg-d2-298,2025-02-19,,2502.13872," Recurrent neural networks are frequently studied in terms of their
information-processing capabilities. The structural properties of these
networks are seldom considered, beyond those emerging from the connectivity
tuning necessary for network training. However, real biological networks have
non-contingent architectures that have been shaped by evolution over eons,
constrained partly by information-processing criteria, but more generally by
fitness maximization requirements. Here we examine the topological properties
of existing biological networks, focusing in particular on gene regulatory
networks in bacteria. We identify structural features, both local and global,
that dictate the ability of recurrent networks to store information on the fly
and process complex time-dependent inputs.",['q-bio.MN'],2502.13872," Recurrent neural networks are frequently studied in terms of their
information-processing capabilities. The structural properties of these
networks are seldom considered, beyond those emerging from the connectivity
tuning necessary for network training. However, real biological networks have
non-contingent architectures that have been shaped by evolution over eons,
constrained partly by information-processing criteria, but more generally by
fitness maximization requirements. Here we examine the topological properties
of existing biological networks, focusing in particular on gene regulatory
networks in bacteria. We identify structural features, both local and global,
that dictate the ability of recurrent networks to store information on the fly
and process complex time-dependent inputs.",['q-bio.MN'],False,,,,Structural determinants of soft memory in recurrent biological networks,Structural determinants of soft memory in recurrent biological networks
neg-d2-299,2025-02-09,,2502.06145," Recent character image animation methods based on diffusion models, such as
Animate Anyone, have made significant progress in generating consistent and
generalizable character animations. However, these approaches fail to produce
reasonable associations between characters and their environments. To address
this limitation, we introduce Animate Anyone 2, aiming to animate characters
with environment affordance. Beyond extracting motion signals from source
video, we additionally capture environmental representations as conditional
inputs. The environment is formulated as the region with the exclusion of
characters and our model generates characters to populate these regions while
maintaining coherence with the environmental context. We propose a
shape-agnostic mask strategy that more effectively characterizes the
relationship between character and environment. Furthermore, to enhance the
fidelity of object interactions, we leverage an object guider to extract
features of interacting objects and employ spatial blending for feature
injection. We also introduce a pose modulation strategy that enables the model
to handle more diverse motion patterns. Experimental results demonstrate the
superior performance of the proposed method.",['cs.CV'],2503.01257," Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on
mobile devices. However, due to the manufacturing constraints of compact
devices and the inherent physical principles of imaging, dToF depth maps are
sparse and noisy. In this paper, we propose a novel video depth completion
method, called SVDC, by fusing the sparse dToF data with the corresponding RGB
guidance. Our method employs a multi-frame fusion scheme to mitigate the
spatial ambiguity resulting from the sparse dToF imaging. Misalignment between
consecutive frames during multi-frame fusion could cause blending between
object edges and the background, which results in a loss of detail. To address
this, we introduce an adaptive frequency selective fusion (AFSF) module, which
automatically selects convolution kernel sizes to fuse multi-frame features.
Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to
enhance features and generates an attention map as fusion weights. The AFSF
ensures edge detail recovery while suppressing high-frequency noise in smooth
regions. To further enhance temporal consistency, We propose a cross-window
consistency loss to ensure consistent predictions across different windows,
effectively reducing flickering. Our proposed SVDC achieves optimal accuracy
and consistency on the TartanAir and Dynamic Replica datasets. Code is
available at https://github.com/Lan1eve/SVDC.",['cs.CV'],False,,,,"Animate Anyone 2: High-Fidelity Character Image Animation with
  Environment Affordance","SVDC: Consistent Direct Time-of-Flight Video Depth Completion with
  Frequency Selective Fusion"
neg-d2-300,2025-01-24,,2501.15069," We investigate magnomechanically induced transparency (MMIT) in a microwave
3D copper cavity with two YIG spheres under varying interaction parameters.
Numerical simulations show that the steady-state magnon number increases with
stronger coupling between cavity photons and magnons, and is sensitive to both
bias and drive magnetic fields. Pronounced peaks in the magnon population near
resonant fields highlight the importance of the bias field in energy transfer.
The transparency windows are tunable, with up to quadruple windows depending on
the coupling and magnon-phonon interactions, as seen in the transmission
spectrum. Dispersion analysis reveals normal and anomalous regions, enabling
slow and fast light propagation modulated by coupling strength. Phase and group
delay variations, influenced by the drive field, further validate the
tunability of transparency windows. This study demonstrates the potential of
MMIT for precise control with out any additional non-linearity over
light-matter interactions, with applications in quantum information processing
and optical communications.",['physics.optics'],2501.04248," Nonreciprocal optical devices are key components in photonic integrated
circuits for light reflection blocking and routing. Most reported silicon
integrated nonreciprocal optical devices to date were unit devices. To allow
complex signal routing between multi-ports in photonic networks, multi-port
magneto-optical (MO) nonreciprocal photonic devices are desired. In this study,
we report experimental demonstration of a silicon integrated 5*5 multiport
nonreciprocal photonic device based on magneto-optical waveguides. By
introducing different nonreciprocal phase shift effect to planar photonic
waveguides, the device focuses light to different ports for both forward and
backward propagation. The device shows designable nonreciprocal transmission
between 5*5 ports, achieving 16 dB isolation ratio and -18 dB crosstalk.",['physics.optics'],False,,,,"Magnetic Field induced control and Multiple Magnomechanically Induced
  Transparency in Single Cavity","Nonreciprocal Optical Routing in Multi-port Magneto-Optical Devices on
  Silicon"
neg-d2-301,2025-03-20,,2503.1618," Chalcogenide perovskites have emerged as a promising class of materials for
the next generation of optoelectronic applications, with BaZrS$_\text{3}$
attracting significant attention due to its wide bandgap, earth-abundant
composition, and thermal and chemical stability. However, previous studies have
consistently reported weak and ambiguous photoluminescence (PL), regardless of
synthesis method, raising questions about the intrinsic optoelectronic quality
of this compound. In this work, we demonstrate strong, band-to-band-dominated
PL at room temperature in high-quality BaZrS$_\text{3}$ single crystals.
Despite the narrow, single-component PL emission band, time-resolved PL
measurements reveal a carrier lifetime of $1.0\pm0.2$ ns. To understand the
origin of the strong PL and short carrier lifetime, we perform multiwavelength
excitation and polarization-dependent Raman measurements, supported by
first-principles lattice dynamics calculations. We identify all 23
theoretically predicted Raman-active modes and their symmetries, providing a
comprehensive reference for future studies. Our results indicate that
phonon-assisted carrier decay and strong electron-phonon coupling contribute to
the short carrier lifetimes, as evidenced by Raman spectroscopy and DFT
calculations. Further studies on compositional variations or partial
cation/anion substitutions could mitigate electron-phonon coupling and enhance
carrier lifetimes. By establishing a detailed reference for the intrinsic
vibrational and optoelectronic properties of BaZrS$_\text{3}$, this work paves
the way for further advancements in chalcogenide perovskites for energy and
optoelectronic technologies.",['cond-mat.mtrl-sci'],2503.09531," The Quasi-harmonic Approximation (QHA) is a widely used method for
calculating the temperature dependence of lattice parameters and the thermal
expansion coefficients from first principles. However, applying QHA to
anisotropic systems typically requires several dozens or even hundreds of
phonon band structure calculations, leading to high computational costs. The
Zero Static Internal Stress Approximation (ZSISA) QHA method partly addresses
such caveat, but the computational load of its implementation remains high, so
that its volumetric-only counterpart v-ZSISA-QHA is preferred. In this work, we
present an efficient implementation of the ZSISA-QHA, enabling its application
across a wide range of crystal structures under varying temperature (T) and
pressure (P) conditions. By incorporating second-order derivatives of the
vibrational free energy with respect to lattice degrees of freedom, we
significantly reduce the number of required phonon band structure calculations
for the determination of all lattice parameters and angles. For hexagonal,
trigonal, and tetragonal systems, only six phonon band structure calculations
are needed, while 10, 15, and 28 calculations suffice for orthorhombic,
monoclinic, and triclinic systems, respectively. This method is tested for a
variety of non-cubic materials, from uniaxial ones like ZnO and CaCO3 to
monoclinic or triclinic materials such as ZrO2, HfO2, and Al2SiO5,
demonstrating a significant reduction in computational effort while maintaining
accuracy in modeling anisotropic thermal expansion, unlike the v-ZSISA-QHA. The
method is also applied to the first-principles calculation of
temperature-dependent elastic constants, with only up to six more phonon band
structure calculations, depending on the crystallographic system.",['cond-mat.mtrl-sci'],False,,,,"BaZrS$_\text{3}$ Lights Up: The Interplay of Electrons, Photons, and
  Phonons in Strongly Luminescent Single Crystals","Anisotropic temperature-dependent lattice parameters and elastic
  constants from first principles"
neg-d2-302,2025-01-30,,2501.18746," Estimating dynamic discrete choice models with large state spaces poses
computational difficulties. This paper develops a novel model-adaptive approach
to solve the linear system of fixed point equations of the policy valuation
operator. We propose a model-adaptive sieve space, constructed by iteratively
augmenting the space with the residual from the previous iteration. We show
both theoretically and numerically that model-adaptive sieves dramatically
improve performance. In particular, the approximation error decays at a
superlinear rate in the sieve dimension, unlike a linear rate achieved using
conventional methods. Our method works for both conditional choice probability
estimators and full-solution estimators with policy iteration. We apply the
method to analyze consumer demand for laundry detergent using Kantar's
Worldpanel Take Home data. On average, our method is 51.5% faster than the
conventional methods in solving the dynamic programming problem, making the
Bayesian MCMC estimator computationally feasible. The results confirm the
computational efficiency of our method in practice.",['econ.EM'],2502.05353," Sample selection is pervasive in applied economic studies. This paper
develops semiparametric selection models that achieve point identification
without relying on exclusion restrictions, an assumption long believed
necessary for identification in semiparametric selection models. Our
identification conditions require at least one continuously distributed
covariate and certain nonlinearity in the selection process. We propose a
two-step plug-in estimator that is root-n-consistent, asymptotically normal,
and computationally straightforward (readily available in statistical
software), allowing for heteroskedasticity. Our approach provides a middle
ground between Lee (2009)'s nonparametric bounds and Honor\'e and Hu (2020)'s
linear selection bounds, while ensuring point identification. Simulation
evidence confirms its excellent finite-sample performance. We apply our method
to estimate the racial and gender wage disparity using data from the US Current
Population Survey. Our estimates tend to lie outside the Honor\'e and Hu
bounds.",['econ.EM'],False,,,,"Model-Adaptive Approach to Dynamic Discrete Choice Models with Large
  State Spaces","Point-Identifying Semiparametric Sample Selection Models with No
  Excluded Variable"
neg-d2-303,2025-02-25,,2502.18401," We conduct a theoretical study of the $ D_{s1}(2460) \to D_s \pi^+ \pi^- $
decay from the perspective that the $D_{s1}$ is a molecular state, built mostly
from the $D^* K$ and $D_s^* \eta$ components. The $D^*$ and $D_s^*$ mesons are
allowed to decay into two pseudoscalars, with one of them merging with the
other pseudoscalar that forms the $D_{s1}$ state, ultimately leading to the
$\pi^+ \pi^- D_s$ final state. This results in a triangle diagram mechanism
where all theoretical ingredients are well-known, leading to a free parameter
framework. We evaluate the mass distributions of particle pairs and find good
agreement with the experimental distributions of a recent LHCb experiment,
providing strong support to the molecular picture of the $D_{s1}(2460)$ state.
We also discuss the role played by the scalar mesons $f_0(500)$ and $f_0(980)$,
at odds with the interpretation of the experimental analysis.",['hep-ph'],2502.18931," We investigate axion emission from singlet proton Cooper pairs in neutron
stars, a process that dominates axion emission in young neutron stars in the
KSVZ model. By re-deriving its emissivity, we confirm consistency with most
existing literature, except for a recent study that exhibits a different
dependence on the effective mass. This discrepancy results in more than an
order-of-magnitude deviation in emissivity, significantly impacting constraints
on the KSVZ axion from the cooling observations of the Cassiopeia A neutron
star. Furthermore, we examine uncertainties arising from neutron-star equations
of state and their role in the discrepancy, finding that the large deviation
persists regardless of the choice of equations of state.",['hep-ph'],False,,,,"The $D_{s1}(2460) \to D_s \pi^+ \pi^- $ decay from a $D_{s1}$ molecular
  perspective",Axion Emission from Proton Cooper Pairs in Neutron Stars
neg-d2-304,2025-02-07,,2502.04996," The Tilloy-Di\'osi (TD) prescription allows to turn any Markovian spontaneous
collapse model that can be formally regarded as a continuous measurement
process of the mass density into a hybrid classical-quantum theory in which the
gravitational Newtonian field enters as a classical field. We study the
application of a similar idea to the Poissonian Spontaneous Localization (PSL)
model. As in the TD case, the Newtonian classical field is again recovered upon
averaging, and additional decoherence appears due to the gravitational
back-reaction. We study general features of this model and investigate the
dynamics of a single particle and a rigid spherical body. With respect to the
TD models, the PSL model presents some notable differences such as the absence
of long-range decoherence due to the gravitational back-reaction noise and the
absence of negative mass measurements.",['quant-ph'],2502.04996," The Tilloy-Di\'osi (TD) prescription allows to turn any Markovian spontaneous
collapse model that can be formally regarded as a continuous measurement
process of the mass density into a hybrid classical-quantum theory in which the
gravitational Newtonian field enters as a classical field. We study the
application of a similar idea to the Poissonian Spontaneous Localization (PSL)
model. As in the TD case, the Newtonian classical field is again recovered upon
averaging, and additional decoherence appears due to the gravitational
back-reaction. We study general features of this model and investigate the
dynamics of a single particle and a rigid spherical body. With respect to the
TD models, the PSL model presents some notable differences such as the absence
of long-range decoherence due to the gravitational back-reaction noise and the
absence of negative mass measurements.",['quant-ph'],False,,,,Newtonian Gravity from a Poissonian Spontaneous Collapse Model,Newtonian Gravity from a Poissonian Spontaneous Collapse Model
neg-d2-305,2025-03-08,,2503.06317," Object detection in videos plays a crucial role in advancing applications
such as public safety and anomaly detection. Existing methods have explored
different techniques, including CNN, deep learning, and Transformers, for
object detection and video classification. However, detecting tiny objects,
e.g., guns, in videos remains challenging due to their small scale and varying
appearances in complex scenes. Moreover, existing video analysis models for
classification or detection often perform poorly in real-world gun detection
scenarios due to limited labeled video datasets for training. Thus, developing
efficient methods for effectively capturing tiny object features and designing
models capable of accurate gun detection in real-world videos is imperative. To
address these challenges, we make three original contributions in this paper.
First, we conduct an empirical study of several existing video classification
and object detection methods to identify guns in videos. Our extensive analysis
shows that these methods may not accurately detect guns in videos. Second, we
propose a novel two-stage gun detection method. In stage 1, we train an
image-augmented model to effectively classify ``Gun'' videos. To make the
detection more precise and efficient, stage 2 employs an object detection model
to locate the exact region of the gun within video frames for videos classified
as ``Gun'' by stage 1. Third, our experimental results demonstrate that the
proposed domain-specific method achieves significant performance improvements
and enhances efficiency compared with existing techniques. We also discuss
challenges and future research directions in gun detection tasks in computer
vision.",['cs.CV'],2501.09281," In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.",['cs.CV'],False,,,,Accurate and Efficient Two-Stage Gun Detection in Video,SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection
neg-d2-306,2025-02-25,,2502.18149," This paper initiates a systematic study for key properties of Artinian
Gorenstein \(K\)-algebras having binomial Macaulay dual generators. In
codimension 3, we demonstrate that all such algebras satisfy the strong
Lefschetz property, can be constructed as a doubling of an appropriate
0-dimensional scheme in \(\mathbb{P}^2\), and we provide an explicit
characterization of when they form a complete intersection. For arbitrary
codimension, we establish sufficient conditions under which the weak Lefschetz
property holds and show that these conditions are optimal.",['math.AC'],2501.10005," Let R be a commutative ring with identity and M be an R-module. The purpose
of this paper is to introduce and investigate the dual notion of morphic
modules over a commutative ring.",['math.AC'],False,,,,Artinian Gorenstein algebras with binomial Macaulay dual generator,The dual notion of morphic modules over commutative rings
neg-d2-307,2025-01-13,,2501.07073," In three dimensions, the parabolic-elliptic Keller-Segel system exhibits a
rich variety of singularity formations. Notably, it admits an explicit
self-similar blow-up solution whose radial stability, conjectured more than two
decades ago in [Brenner-Constantin-Kadanoff-Schenkel-Venkataramani, 1999], was
recently confirmed by [Glogi\'c-Sch\""orkhuber, 2024]. This paper aims to extend
the radial stability to the nonradial setting, building on the
finite-codimensional stability analysis in our previous work [Li-Zhou, 2024].
The main input is the mode stability of the linearized operator, whose nonlocal
nature presents challenges for the spectral analysis. Besides a quantitative
perturbative analysis for the high spherical classes, we adapted in the first
spherical class the wave operator method of [Li-Wei-Zhang, 2020] for the fluid
stability to localize the operator and remove the known unstable mode
simultaneously. Our method provides localization beyond the partial mass
variable and is independent of the explicit formula of the profile, so it
potentially sheds light on other linear nonlocal problems.",['math.AP'],2502.05876," \begin{equation*}
  \left\{
  \begin{array}{l}
  u'' + \lambda h(x,\alpha) e^u = 0, \quad x \in (-1,1), \\[1ex]
  u(-1) = u(1) = 0,
  \end{array}
  \right. \end{equation*} where $\lambda>0$, $0<\alpha<1$, $h(x,\alpha)=0$ for
$|x|<\alpha$, and $h(x,\alpha)=1$ for $\alpha \le |x| \le 1$. We compute the
Morse index of positive even solutions, and then we prove the existence of an
unbounded connected set of positive non-even solutions emanating from a
symmetry-breaking bifurcation point.",['math.AP'],False,,,,"Nonradial stability of self-similar blowup to Keller-Segel equation in
  three dimensions","Morse index and symmetry-breaking bifurcation of positive solutions to
  the one-dimensional Liouville type equation with a step function weight"
neg-d2-308,2025-02-26,,2502.18899," The spreading behaviour of cohesive sand powder is modelled by Discrete
Element Method, and the spreadability and the mechanical jamming are focused.
The empty patches and total particle volume of the spread layer are examined,
followed by the analysis of the geometry force and jamming structure. The
results show that several empty patches with different size and shapes could be
observed within the spread layer along the spreading direction even when the
gap height increases to 3.0D90. Large particles are more difficult to be spread
onto the base due to jamming, although their size is smaller than the gap
height. Size segregation of particles occurs before particles entering the gap
between the blade and base. There are almost no particles on the smooth base
when the gap height is small, due to the full-slip flow of particles. The
difference of the spread layer and spreadability between the cases with rough
and smooth base is reduced by the increase of the gap height. An interesting
correlation between jamming effect and local defects (empty spaces) in the
powder layer is identified. The resistance to particle rolling is important for
the mechanical jamming reported in this work. The jammed particles with a
larger size ratio tend to be more stable.",['physics.flu-dyn'],2501.05331," Aerodynamic loads play a central role in many fluid dynamics applications,
and we present a method for identifying the structures (or modes) in a flow
that make dominant contributions to the time-varying aerodynamic loads in a
flow. The method results from the combination of the force partitioning method
(Menon and Mittal, J. Fluid Mech., 907:A37, 2021) and modal decomposition
techniques such as Reynolds decomposition, triple decomposition, and proper
orthogonal decomposition, and is applied here to three distinct flows -
two-dimensional flows past a circular cylinder and an airfoil, and the
three-dimensional flow over a revolving rectangular wing. We show that the
force partitioning method applied to modal decomposition of velocity fields
results in complex, and difficult to interpret inter-modal interactions. We
therefore propose and apply modal decomposition directly to the $Q$-field
associated with these flows. The variable $Q$ is a non-linear observable that
is typically used to identify vortices in a flow, and we find that the direct
decomposition of $Q$ leads to results that are more amenable to interpretation.
We also demonstrate that this modal force partitioning can be extended to
provide insights into the far-field aeroacoustic loading noise of these flows.",['physics.flu-dyn'],False,,,,"Investigation on the Spreading Behaviour of Sand Powder Used in Binder
  Jet 3D Printing","Modal Force Partitioning -- A Method for Determining the Aerodynamic
  Loads for Decomposed Flow Modes with Application to Aeroacoustic Noise"
neg-d2-309,2025-02-10,,2502.06311," We develop an analog classical simulation algorithm of noiseless quantum
dynamics. By formulating the Schr\""{o}dinger equation into a linear system of
real-valued ordinary differential equations (ODEs), the probability amplitudes
of a complex state vector can be encoded in the continuous physical variables
of an analog computer. Our algorithm reveals the full dynamics of complex
probability amplitudes. Such real-time simulation is impossible in quantum
simulation approaches without collapsing the state vector, and it is relatively
computationally expensive for digital classical computers. For a real symmetric
time-independent Hamiltonian, the ODEs may be solved by a simple analog
mechanical device such as a one-dimensional spring-mass system. Since the
underlying dynamics of quantum computers is governed by the Schr\""{o}dinger
equation, our findings imply that analog computers can also perform quantum
algorithms. We illustrate how to simulate the Schr\""{o}dinger equation in such
a paradigm, with an application to quantum approximate optimization algorithm.
This may pave the way to emulate quantum algorithms with physical computing
devices, including analog, continuous-time circuits.",['quant-ph'],2502.04425," We present a quantum solver for partial differential equations based on a
flexible matrix product operator representation. Utilizing mid-circuit
measurements and a state-dependent norm correction, this scheme overcomes the
restriction of unitary operators. Hence, it allows for the direct
implementation of a broad class of differential equations governing the
dynamics of classical and quantum systems. The capabilities of the framework
are demonstrated for an example system governed by Euler equations with
absorbing boundaries.",['quant-ph'],False,,,,Analog classical simulation of closed quantum systems,Tensor-Programmable Quantum Circuits for Solving Differential Equations
neg-d2-310,2025-02-06,,2502.03996," The efficient and independent operation of power-over-fiber (PoF) and
distributed acoustic sensing (DAS) has been demonstrated using standard
single-mode fiber (SSMF). A transmission optical power efficiency (OPTE) of
6.67% was achieved over an 11.8 km fiber link, supporting both power delivery
and distributed optical fiber sensing (DOFS). To minimize cross-talk, the
system separates the power and sensing channels by a 40 THz bandwidth. In the
experiment, the power and sensing light wavelengths are 1064 nm (continuous)
and 1550 nm (pulsed), respectively. As the transmitted optical power increased
from 0 W to 2.13 W, the DAS system successfully localized vibration sources and
reconstructed phase information, confirming its ability to operate under high
optical power. The reported scheme verifies the possibility of constructing the
sensing-energy hybrid network based on conventional optical fiber with the
advantages of flexibility and low cost.",['physics.optics'],2501.14564," Diffractive optical elements that divide an input beam into a set of replicas
are used in many optical applications ranging from image processing to
communications. Their design requires time-consuming optimization processes,
which, for a given number of generated beams, are to be separately treated for
one-dimensional and two-dimensional cases because the corresponding optimal
efficiencies may be different. After generalizing their Fourier treatment, we
prove that, once a particular divider has been designed, its transmission
function can be used to generate numberless other dividers through affine
transforms that preserve the efficiency of the original element without
requiring any further optimization.",['physics.optics'],False,,,,"Power-over-fiber and distributed acoustic sensing hybridization in
  single fiber channel",Affine diffractive beam dividers
neg-d2-311,2025-02-10,,2502.06933," Non-ideal MHD effects are thought to be a crucial component of the
star-formation process. Numerically, several complications render the study of
non-ideal MHD effects in 3D simulations extremely challenging and hinder our
efforts of exploring a large parameter space. We aim to overcome such
challenges by proposing a novel, physically-motivated empirical approximation
to model non-ideal MHD effects. We perform a number of 2D axisymmetric 3-fluid
non-ideal MHD simulations of collapsing prestellar cores and clouds with
non-equilibrium chemistry and leverage upon previously-published results. We
utilize these simulations to develop a multivariate interpolating function to
predict the ionization fraction in each region of the cloud depending on the
local physical conditions. We subsequently use analytically-derived, simplified
expressions to calculate the resistivities of the cloud in each grid cell.
Therefore, in our new approach the resistivities are calculated without the use
of a chemical network. We benchmark our method against additional 2D
axisymmetric non-ideal MHD simulations with random initial conditions and a 3D
non-ideal MHD simulation with non-equilibrium chemistry. We find excellent
quantitative and qualitative agreement between our approach and the ""full""
non-ideal MHD simulations both in terms of the spatial structure of the
simulated clouds and regarding their time evolution. We achieve a factor of
100-1000 increase in computational speed. Given that we ignore the contribution
of grains, our approximation is valid up to number densities of 10^6 cm^(-3)
and is therefore suitable for pc-scale simulations of molecular clouds. The
tabulated data required for integrating our method in hydrodynamical codes,
along with a fortran implementation of the interpolating function are publicly
available at https://github.com/manosagian/Non-Ideal-MHD-Approximate-Code.",['astro-ph.GA'],2501.03985," We investigate AGN feedback from an intermediate-mass black hole at the
center of a dwarf spheroidal galaxy, by performing isolated galaxy simulations
using a modified version of the GADGET-3 code. We consider Leo II (PGC 34176)
in the Local Group as our simulation reference model. Beginning with black hole
seeds ranging from $10^3$ to $10^6$ M$_{\odot}$, our simulations focus on
comparing stellar-only feedback with AGN+stellar/SN feedback over 13.7 Gyr of
galactic evolution. Our results indicate that a low-mass AGN in a dwarf galaxy
influences the star formation history under specific physical conditions. While
AGN feedback is generally negative on star formation, instances of positive
feedback were also identified. Despite measurable effects on the evolution of
the dwarf host galaxy, black hole seeds exhibited only marginal growth. We
tested several physical scenarios as modified models in our simulations,
primarily concerning the dynamics of the central black holes, which may wander
within dwarf galaxies rather than being centrally located. However, none of
these adjustments significantly impacted the growth of the black hole seeds.
This suggests that intermediate-mass black holes may struggle to achieve higher
masses in isolated environments, with mergers and interactions likely playing
crucial roles in their growth. Nevertheless, AGN feedback exhibited
non-negligible effects in our simulated dwarf spheroidal galaxies, despite the
assumed dominant role of stellar feedback in the low-mass regime.",['astro-ph.GA'],False,,,,"A fast and robust recipe for modeling non-ideal MHD effects in
  star-formation simulations","Exploring the evolution of a dwarf spheroidal galaxy with SPH
  simulations: II. AGN feedback"
neg-d2-312,2025-01-10,,2501.06312," Foundation models are becoming increasingly popular due to their strong
generalization capabilities resulting from being trained on huge datasets.
These generalization capabilities are attractive in areas such as NIR Iris
Presentation Attack Detection (PAD), in which databases are limited in the
number of subjects and diversity of attack instruments, and there is no
correspondence between the bona fide and attack images because, most of the
time, they do not belong to the same subjects. This work explores an iris PAD
approach based on two foundation models, DinoV2 and VisualOpenClip. The results
show that fine-tuning prediction with a small neural network as head overpasses
the state-of-the-art performance based on deep learning approaches. However,
systems trained from scratch have still reached better results if bona fide and
attack images are available.",['cs.CV'],2503.18282," Multi-object tracking, player identification, and pose estimation are
fundamental components of sports analytics, essential for analyzing player
movements, performance, and tactical strategies. However, existing datasets and
methodologies primarily target mainstream team sports such as soccer and
conventional 5-on-5 basketball, often overlooking scenarios involving
fixed-camera setups commonly used at amateur levels, less mainstream sports, or
datasets that explicitly incorporate pose annotations. In this paper, we
propose the TrackID3x3 dataset, the first publicly available comprehensive
dataset specifically designed for multi-player tracking, player identification,
and pose estimation in 3x3 basketball scenarios. The dataset comprises three
distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera
footage), capturing diverse full-court camera perspectives and environments. We
also introduce the Track-ID task, a simplified variant of the game state
reconstruction task that excludes field detection and focuses exclusively on
fixed-camera scenarios. To evaluate performance, we propose a baseline
algorithm called Track-ID algorithm, tailored to assess tracking and
identification quality. Furthermore, our benchmark experiments, utilizing
recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose
estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results
and highlight remaining challenges. Our dataset and evaluation benchmarks
provide a solid foundation for advancing automated analytics in 3x3 basketball.
Dataset and code will be available at
https://github.com/open-starlab/TrackID3x3.",['cs.CV'],False,,,,Towards Iris Presentation Attack Detection with Foundation Models,"TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with
  Identification and Pose Estimation in 3x3 Basketball Full-court Videos"
neg-d2-313,2025-02-02,,2502.00689," IoT systems face significant challenges in adapting to user needs, which are
often under-specified and evolve with changing environmental contexts. To
address these complexities, users should be able to explore possibilities,
while IoT systems must learn and support users in the process of providing
proper services, e.g., to serve novel experiences. The IoT-Together paradigm
aims to meet this demand through the Mixed-Initiative Interaction (MII)
paradigm that facilitates a collaborative synergy between users and IoT
systems, enabling the co-creation of intelligent and adaptive solutions that
are precisely aligned with user-defined goals. This work advances IoT-Together
by integrating Large Language Models (LLMs) into its architecture. Our approach
enables intelligent goal interpretation through a multi-pass dialogue framework
and dynamic service generation at runtime according to user needs. To
demonstrate the efficacy of our methodology, we design and implement the system
in the context of a smart city tourism case study. We evaluate the system's
performance using agent-based simulation and user studies. Results indicate
efficient and accurate service identification and high adaptation quality. The
empirical evidence indicates that the integration of Large Language Models
(LLMs) into IoT architectures can significantly enhance the architectural
adaptability of the system while ensuring real-world usability.",['cs.SE'],2503.1722," Infrastructure as Code (IaC) enables scalable and automated IT infrastructure
management but is prone to errors that can lead to security vulnerabilities,
outages, and data loss. While prior research has focused on detecting IaC
issues, Automated Program Repair (APR) remains underexplored, largely due to
the lack of suitable specifications. In this work, we propose InfraFix, the
first technology-agnostic framework for repairing IaC scripts. Unlike prior
approaches, InfraFix allows APR techniques to be guided by diverse information
sources.
  Additionally, we introduce a novel approach for generating repair scenarios,
enabling large-scale evaluation of APR techniques for IaC. We implement and
evaluate InfraFix using an SMT-based repair module and a state inference module
that uses system calls, demonstrating its effectiveness across 254,755 repair
scenarios with a success rate of 95.5%. Our work provides a foundation for
advancing APR in IaC by enabling researchers to experiment with new state
inference and repair techniques using InfraFix and to evaluate their approaches
at scale with our repair scenario generation method.",['cs.SE'],False,,,,"Leveraging LLMs for Dynamic IoT Systems Generation through
  Mixed-Initiative Interaction",InfraFix: Technology-Agnostic Repair of Infrastructure as Code
neg-d2-314,2025-03-07,,2503.05433," The IRAC camera on the Spitzer Space Telescope observed 2175 Near Earth
Objects (NEOs) during its Warm Mission phase, primarily in three large surveys,
and also in a small number of a dedicated projects. In this paper we present
the final reprocessing of the NEO data and determine fluxes at 3.6 microns
(where available) and 4.5 microns. The observing windows range from minutes to
nearly ten hours, which means that for 39 NEOs we observe a complete
lightcurve, and for these objects we present period and amplitude estimates and
derive minimum cohesive strengths for the objects with well-determined periods.
For an additional 128 objects we detect a significant fraction of a complete
lightcurve, and present estimated lower limits to their rotation periods. This
paper presents the final and definitive Spitzer/IRAC NEO flux catalog.",['astro-ph.EP'],2503.01599," Of the ~25 directly imaged planets to date, all are younger than 500Myr and
all but 6 are younger than 100Myr. Eps Ind A (HD209100, HIP108870) is a K5V
star of roughly solar age (recently derived as 3.7-5.7Gyr and
3.5$^{+0.8}_{-1.3}$Gyr). A long-term radial velocity trend as well as an
astrometric acceleration led to claims of a giant planet orbiting the nearby
star (3.6384$\pm$0.0013pc). Here we report JWST coronagraphic images that
reveal a giant exoplanet which is consistent with these radial and astrometric
measurements, but inconsistent with the previously claimed planet properties.
The new planet has temperature ~275K, and is remarkably bright at 10.65um and
15.50um. Non-detections between 3.5-5um indicate an unknown opacity source in
the atmosphere, possibly suggesting a high metallicity, high carbon-to-oxygen
ratio planet. The best-fit temperature of the planet is consistent with
theoretical thermal evolution models, which are previously untested at this
temperature range. The data indicates that this is likely the only giant planet
in the system and we therefore refer to it as ``b"", despite it having
significantly different orbital properties than the previously claimed planet
``b"".",['astro-ph.EP'],False,,,,"Infrared Fluxes and Light Curves of Near-Earth Objects: The full Spitzer
  Sample",A temperate super-Jupiter imaged with JWST in the mid-infrared
neg-d2-315,2025-01-13,,2501.07234," The integration of haptics within Augmented Reality may help to deliver an
enriched experience, while facilitating the performance of specific actions
(e.g. repositioning or resizin ) that are still dependent on the user's skills.
This paper gathers the description of a flexible architecture designed to
deploy haptically-enabled AR applications. The haptic feedback may be generated
through a variety of devices (e.g., wearable, graspable, or mid-air ones), and
the architecture facilitates handling the specificity of each. For this reason,
it is discussed how to generate a haptic representation of a 3D digital object
depending on the application and the target device. Additionally, it is
included an analysis of practical, relevant issues that arise when setting up a
system to work with specific devices like Head-Mounted Displays (e.g.,
HoloLens) and mid-air haptic devices (e.g., Ultrahaptics UHK), such as the
alignment between the real world and the virtual one. The architecture
applicability is demonstrated through the implementation of two applications:
Form Inspector and Simon Game, built for HoloLens and iOS mobile phones for
visualization and for UHK for mid-air haptics delivery. These applications have
been used by nine users to explore the efficiency, meaningfulness, and
usefulness of mid-air haptics for form perception, object resizing, and push
interaction tasks. Results show that, although mobile interaction is preferred
when this option is available, haptics turn out to be more meaningful in
identifying shapes when compared to what users initially expect and in
contributing to the execution of resizing tasks. Moreover, this preliminary
user study reveals that users may be expecting a tailored interface metaphor,
not necessarily inspired in natural interaction.",['cs.HC'],2503.07797," News reading helps individuals stay informed about events and developments in
society. Local residents and new immigrants often approach the same news
differently, prompting the question of how technology, such as LLM-powered
chatbots, can best enhance a reader-oriented news experience. The current paper
presents an empirical study involving 144 participants from three groups in
Virginia, United States: local residents born and raised there (N=48), Chinese
immigrants (N=48), and Vietnamese immigrants (N=48). All participants read
local housing news with the assistance of the Copilot chatbot. We collected
data on each participant's Q&A interactions with the chatbot, along with their
takeaways from news reading. While engaging with the news content, participants
in both immigrant groups asked the chatbot fewer analytical questions than the
local group. They also demonstrated a greater tendency to rely on the chatbot
when formulating practical takeaways. These findings offer insights into
technology design that aims to serve diverse news readers.",['cs.HC'],False,,,,"Enhancing Interaction with Augmented Reality through Mid-Air Haptic
  Feedback: Architecture Design and User Feedback","The News Says, the Bot Says: How Immigrants and Locals Differ in
  Chatbot-Facilitated News Reading"
neg-d2-316,2025-03-04,,2503.02694," In directed graphs, a cycle can be seen as a structure that allows its
vertices to loop back to themselves, or as a structure that allows pairs of
vertices to reach each other through distinct paths. We extend these concepts
to temporal graph theory, resulting in multiple interesting definitions of a
""temporal cycle"". For each of these, we consider the problems of Cycle
Detection and Acyclic Temporization. For the former, we are given an input
temporal digraph, and we want to decide whether it contains a temporal cycle.
Regarding the latter, for a given input (static) digraph, we want to time the
arcs such that no temporal cycle exists in the resulting temporal digraph.
We're also interested in Acyclic Temporization where we bound the lifetime of
the resulting temporal digraph. Multiple results are presented, including
polynomial and fixed-parameter tractable search algorithms, polynomial-time
reductions from 3-SAT and Not All Equal 3-SAT, and temporizations resulting
from arbitrary vertex orderings which cover (almost) all cases.",['cs.CC'],2502.0068," The complexity class $\exists\mathbb R$, standing for the complexity of
deciding the existential first order theory of the reals as real closed field
in the Turing model, has raised considerable interest in recent years. It is
well known that NP $ \subseteq \exists\mathbb R\subseteq$ PSPACE. In their
compendium, Schaefer, Cardinal, and Miltzow give a comprehensive presentation
of results together with a rich collection of open problems. Here, we answer
some of them dealing with structural issues of $\exists\mathbb R$ as a
complexity class. We show analogues of the classical results of Baker, Gill,
and Solovay finding oracles which do and do not separate NP form
$\exists\mathbb R$, of Ladner's theorem showing the existence of problems in
$\exists\mathbb R \setminus$ NP not being complete for $\exists\mathbb R$ (in
case the two classes are different), as well as a characterization of
$\exists\mathbb R$ by means of descriptive complexity.",['cs.CC'],False,,,,Temporal Cycle Detection and Acyclic Temporization,Some structural complexity results for $\exists\mathbb R$
neg-d2-317,2025-01-07,,2501.04249," Despite the remarkable advancements and widespread applications of deep
neural networks, their ability to perform reasoning tasks remains limited,
particularly in domains requiring structured, abstract thought. In this paper,
we investigate the linguistic reasoning capabilities of state-of-the-art large
language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from
International Linguistics Olympiad (IOL) problems. This dataset encompasses
diverse problems testing syntax, morphology, phonology, and semantics, all
carefully designed to be self-contained and independent of external knowledge.
These tasks challenge models to engage in metacognitive linguistic reasoning,
requiring the deduction of linguistic rules and patterns from minimal examples.
Through extensive benchmarking of leading LLMs, we find that even the most
advanced models struggle to handle the intricacies of linguistic complexity,
particularly in areas demanding compositional generalization and rule
abstraction. Our analysis highlights both the strengths and persistent
limitations of current models in linguistic problem-solving, offering valuable
insights into their reasoning capabilities. By introducing IOLBENCH, we aim to
foster further research into developing models capable of human-like reasoning,
with broader implications for the fields of computational linguistics and
artificial intelligence.",['cs.CL'],2502.2062," Large language models (LLMs) can exhibit advanced reasoning yet still
generate incorrect answers. We hypothesize that such errors frequently stem
from spurious beliefs, propositions the model internally considers true but are
incorrect. To address this, we propose a method to rectify the belief space by
suppressing these spurious beliefs while simultaneously enhancing true ones,
thereby enabling more reliable inferences. Our approach first identifies the
beliefs that lead to incorrect or correct answers by prompting the model to
generate textual explanations, using our Forward-Backward Beam Search (FBBS).
We then apply unlearning to suppress the identified spurious beliefs and
enhance the true ones, effectively rectifying the model's belief space.
Empirical results on multiple QA datasets and LLMs show that our method
corrects previously misanswered questions without harming overall model
performance. Furthermore, our approach yields improved generalization on unseen
data, suggesting that rectifying a model's belief space is a promising
direction for mitigating errors and enhancing overall reliability.",['cs.CL'],False,,,,IOLBENCH: Benchmarking LLMs on Linguistic Reasoning,Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning
neg-d2-318,2025-02-27,,2502.20009," Determination of sample size is critical, however not easy to do. Sample size
defined as the number of observations in a sample should be big enough to have
a high likelihood of detecting a true difference between groups. Practical
procedure for determining sample size, using G*power and previous dental
articles, is shown in this study. Examples involving independent t-test, paired
t-test, one-way analysis of variance(ANOVA), and one-way repeated-measures(RM)
ANOVA are used. The purpose of this study is to enable researchers with
non-statistical backgrounds to use in practice freely available statistical
software G*power to determine sample size and power.",['stat.ME'],2502.15112," Categorical response data are ubiquitous in complex survey applications, yet
few methods model the dependence across different outcome categories when the
response is ordinal. Likewise, few methods exist for the common combination of
a longitudinal design and categorical data. By modeling individual survey
responses at the unit-level, it is possible to capture both ordering
information in ordinal responses and any longitudinal correlation. However,
accounting for a complex survey design becomes more challenging in the
unit-level setting. We propose a Bayesian hierarchical, unit-level, model-based
approach for categorical data that is able to capture ordering among response
categories, can incorporate longitudinal dependence, and accounts for the
survey design. To handle computational scalability, we develop efficient Gibbs
samplers with appropriate data augmentation as well as variational Bayes
algorithms. Using public-use microdata from the Household Pulse Survey, we
provide an analysis of an ordinal response that asks about the frequency of
anxiety symptoms at the beginning of the COVID-19 pandemic. We compare both
design-based and model-based estimators and demonstrate superior performance
for the proposed approaches.",['stat.ME'],False,,,,Estimating sample size in dental research,"Bayesian Unit-level Modeling of Categorical Survey Data with a
  Longitudinal Design"
neg-d2-319,2025-03-20,,2503.15871," In this work, we tackle action-scene hallucination in Video Large Language
Models (Video-LLMs), where models incorrectly predict actions based on the
scene context or scenes based on observed actions. We observe that existing
Video-LLMs often suffer from action-scene hallucination due to two main
factors. First, existing Video-LLMs intermingle spatial and temporal features
by applying an attention operation across all tokens. Second, they use the
standard Rotary Position Embedding (RoPE), which causes the text tokens to
overemphasize certain types of tokens depending on their sequential orders. To
address these issues, we introduce MASH-VLM, Mitigating Action-Scene
Hallucination in Video-LLMs through disentangled spatial-temporal
representations. Our approach includes two key innovations: (1) DST-attention,
a novel attention mechanism that disentangles the spatial and temporal tokens
within the LLM by using masked attention to restrict direct interactions
between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the
dimensionality of the positional IDs, allowing the spatial and temporal tokens
to maintain balanced positions relative to the text tokens. To evaluate the
action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark
with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that
MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as
on existing video understanding benchmarks.",['cs.CV'],2502.09598," The continuous operation of Earth-orbiting satellites generates vast and
ever-growing archives of Remote Sensing (RS) images. Natural language presents
an intuitive interface for accessing, querying, and interpreting the data from
such archives. However, existing Vision-Language Models (VLMs) are
predominantly trained on web-scraped, noisy image-text data, exhibiting limited
exposure to the specialized domain of RS. This deficiency results in poor
performance on RS-specific tasks, as commonly used datasets often lack
detailed, scientifically accurate textual descriptions and instead emphasize
solely on attributes like date and location. To bridge this critical gap, we
introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and
multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated
RS image-text pairs, representing a diverse range of RS modalities associated
to different spatial resolutions. Unlike existing vision-language datasets in
RS, GAIA specifically focuses on capturing a diverse range of RS applications,
providing unique information about environmental changes, natural disasters,
and various other dynamic phenomena. The dataset provides a spatially and
temporally balanced distribution, spanning across the globe, covering the last
25 years with a balanced temporal distribution of observations. GAIA's
construction involved a two-stage process: (1) targeted web-scraping of images
and accompanying text from reputable RS-related sources, and (2) generation of
five high-quality, scientifically grounded synthetic captions for each image
using carefully crafted prompts that leverage the advanced vision-language
capabilities of GPT-4o. Our extensive experiments, including fine-tuning of
CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance
on RS image classification, cross-modal retrieval and image captioning tasks.",['cs.CV'],False,,,,"MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through
  Disentangled Spatial-Temporal Representations","GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for
  Remote Sensing Image Analysis"
neg-d2-320,2025-03-19,,2503.15299," This work presents a framework for assessing whether large language models
(LLMs) encode more factual knowledge in their parameters than what they express
in their outputs. While a few studies hint at this possibility, none has
clearly defined or demonstrated this phenomenon. We first propose a formal
definition of knowledge, quantifying it for a given question as the fraction of
correct-incorrect answer pairs where the correct one is ranked higher. This
gives rise to external and internal knowledge, depending on the information
used to score individual answer candidates: either the model's observable
token-level probabilities or its intermediate computations. Hidden knowledge
arises when internal knowledge exceeds external knowledge. We then present a
case study, applying this framework to three popular open-weights LLMs in a
closed-book QA setup. Our results indicate that: (1) LLMs consistently encode
more factual knowledge internally than what they express externally, with an
average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply
hidden that a model can internally know an answer perfectly, yet fail to
generate it even once, despite large-scale repeated sampling of 1,000 answers.
This reveals fundamental limitations in the generation capabilities of LLMs,
which (3) put a practical constraint on scaling test-time compute via repeated
answer sampling in closed-book QA: significant performance improvements remain
inaccessible because some answers are practically never sampled, yet if they
were, we would be guaranteed to rank them first.",['cs.CL'],2501.05468," Systematic literature reviews and meta-analyses are essential for
synthesizing research insights, but they remain time-intensive and
labor-intensive due to the iterative processes of screening, evaluation, and
data extraction. This paper introduces and evaluates LatteReview, a
Python-based framework that leverages large language models (LLMs) and
multi-agent systems to automate key elements of the systematic review process.
Designed to streamline workflows while maintaining rigor, LatteReview utilizes
modular agents for tasks such as title and abstract screening, relevance
scoring, and structured data extraction. These agents operate within
orchestrated workflows, supporting sequential and parallel review rounds,
dynamic decision-making, and iterative refinement based on user feedback.
LatteReview's architecture integrates LLM providers, enabling compatibility
with both cloud-based and locally hosted models. The framework supports
features such as Retrieval-Augmented Generation (RAG) for incorporating
external context, multimodal reviews, Pydantic-based validation for structured
inputs and outputs, and asynchronous programming for handling large-scale
datasets. The framework is available on the GitHub repository, with detailed
documentation and an installable package.",['cs.CL'],False,,,,Inside-Out: Hidden Factual Knowledge in LLMs,"LatteReview: A Multi-Agent Framework for Systematic Review Automation
  Using Large Language Models"
neg-d2-321,2025-02-13,,2502.09708," Prethermalization phenomena in driven systems are generally understood via a
local Floquet Hamiltonian obtained from a high-frequency expansion. Remarkably,
recently it has been shown that a driven Kitaev spin liquid with fractionalized
excitations can realize a quasi-stationary state that is not captured by this
paradigm. Instead distinct types of fractionalized excitations are
characterized by vastly different temperatures-a phenomenon dubbed
""fractionalized prethermalization"". In our work, we analyze fractionalized
prethermalization in a driven one-dimensional Hubbard model at strong coupling
which hosts spin-charge fractionalization. At intermediate frequencies
quasi-steady states emerge which are characterized by a low spin and high
charge temperature with lifetimes set by two competing processes: the lifetime
of the quasiparticles determined by Fermi's Golden rule and the exponential
lifetime of the Floquet prethermal plateau. We classify drives into three
categories, each giving rise to distinct (fractional) prethermalization
dynamics. Resorting to a time-dependent variant of the Schrieffer-Wolff
transformation, we systematically analyze how these drive categories are linked
to the underlying driven Hubbard model, thereby providing a general
understanding of the emergent thermalization dynamics. We discuss routes
towards an experimental realization of this phenomenon in quantum simulation
platforms.",['cond-mat.str-el'],2502.08389," We numerically study the Kitaev honeycomb model with the additional XX Ising
interaction between the nearest and the next nearest neighbors
(Kitaev-Ising-$J_1$-$J_2$ model), by using the density matrix renormalization
group (DMRG) method. Such additional interaction correspond to the nearest and
diagonal interactions on the square lattice. Phase diagram of the bare Kitaev
model consist of low entangled commensurate magnetic phases and entagled Kitaev
spin liquid. Anisotropic Ising interaction allows the entangled incommensurate
magnetic phases in the phase diagram, which previously was predicted only for
more complex type of interactions. We study the scaling law of the entanglement
entropy and the bond dimension of the matrix product state with the size of the
system. In addition, we propose an optimization algorithm to prevent DMRG from
getting stuck in the low-entangled phases.",['cond-mat.str-el'],False,,,,Fractionalized Prethermalization in the One-Dimensional Hubbard Model,"Kitaev-Ising-$J_1$-$J_2$ model: a density matrix renormalization group
  study"
neg-d2-322,2025-03-06,,2503.04359," Current advanced long-context language models offer great potential for
real-world software engineering applications. However, progress in this
critical domain remains hampered by a fundamental limitation: the absence of a
rigorous evaluation framework for long code understanding. To gap this
obstacle, we propose a long code understanding benchmark LONGCODEU from four
aspects (8 tasks) to evaluate LCLMs' long code understanding ability required
for practical applications, including code unit perception, intra-code unit
understanding, inter-code unit relation understanding, and long code
documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6
general models and 3 code models). Our experimental results reveal key
limitations in current LCLMs' capabilities for long code understanding.
Particularly, the performance of LCLMs drops dramatically when the long code
length is greater than 32K, falling far short of their claimed 128K-1M context
windows. In the four aspects, inter-code unit relation understanding is the
most challenging for LCLMs. Our study provides valuable insights for optimizing
LCLMs and driving advancements in software engineering.",['cs.SE'],2502.10249," Changes are inherent in software development, often increasing developers'
perception of instability. Understanding the relationship between human factors
and Software Engineering processes is crucial to mitigating and preventing
issues. One such factor is burnout, a recognized disease that impacts
productivity, turnover, and, most importantly, developers' well-being.
Investigating the link between instability and burnout can help organizations
implement strategies to improve developers' work conditions and performance.
  This study aims to identify and describe the relationship between perceived
instability and burnout among software developers. A cross-sectional survey was
conducted with 411 respondents, using convenience sampling and self-selection.
In addition to analyzing variable relationships, confirmatory factor analysis
was applied.
  Key findings include: (1) A significant positive relationship between burnout
(exhaustion and cynicism) and team, technological, and task instability; (2) A
weak negative relationship between efficacy and technological/team instability,
with no correlation to task instability; (3) Exhaustion was the most frequently
reported burnout symptom, while task instability was the most perceived type of
instability.
  These results are valuable for both industry and academia, providing insights
to reduce burnout and instability among software engineers. Future research can
further explore the impact of instability, offering new perspectives on
monitoring and mitigating its effects in software development.",['cs.SE'],False,,,,"LONGCODEU: Benchmarking Long-Context Language Models on Long Code
  Understanding","Understanding the relationships between the perceptions of burnout and
  instability in Software Engineering"
neg-d2-323,2025-02-16,,2502.1113," In osteochondral tissue engineering (OCTE), simultaneously regenerating
subchondral bone and cartilage tissue presents a significant challenge.
Multiphasic scaffolds were created and manufactured using 3D printing to
address this issue. Excellent interfacial mechanical properties and
biocompatibility enhance the growth and chondrogenic differentiation of bone
marrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is
mimicked by incorporating varying concentrations of graphene oxide (GO) (0%,
1%, and 2% w/v) into a bioink composed of alginate (Alg) and gelatin (Gel).
Based on evaluations of mechanical and biocompatibility properties, 1% GO is
selected for further studies. Subsequently, the GO concentration is kept
constant while varying the platelet-rich plasma (PRP) dosage in the multiphasic
scaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w/v) are integrated into
the Alg-Gel bioink to simulate cartilage tissues. Results indicate that
3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical
properties, with no significant differences observed. However, BM-MSCs exposed
to 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,
real-time PCR and Alcian blue staining confirm increased chondrogenic
expression and glycosaminoglycans (GAGs) synthesis. This work highlights the
promising potential of 3D-printed multiphasic frameworks in the development of
OCTE.",['q-bio.TO'],2502.03661," While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.",['q-bio.TO'],False,,,,"Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for
  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering","Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues"
neg-d2-324,2025-03-11,,2503.08331," Modern telescopes provide breathtaking images of nebulae, clouds and galaxies
shaped by gravity-driven interactions between complex bodies. While such
structures are prevalent on an astrophysical scale, they are rarely observed at
the human scale. In this letter, we report the observations of the complex
orbits, collision, and coalescence of droplets on a soap film, forming
structures such as bridges and spiral arms, reminiscent of their astrophysical
counterparts. These dynamics emerge from attractive forces caused by
gravito-capillary-driven distortions of the supporting soap film. Long orbits
and intricate coalescence mechanisms are enabled by the small dissipation in
the soap film and the fluidic nature of the droplets and supporting film,
respectively. The existence of stable droplets within the soap film featuring a
universal radius, as well as the attractive potentials, are explained through a
careful comparison of experimental data with models computing the distortions
of the supporting soap film. This work opens perspectives to",['physics.flu-dyn'],2501.01137," Since the geometry structure of ultra-high-pressure (UHP) water-jet nozzle is
a critical factor to enhance its hydrodynamic performance, it is critical to
obtain a suitable geometry for a UHP water jet nozzle. In this study, a
CFD-based optimization loop for UHP nozzle structure has been developed by
integrating an approximate model to optimize nozzle structure for increasing
the radial peak wall shear stress. In order to improve the optimization
accuracy of the sparrow search algorithm (SSA), an enhanced version called the
Logistic-Tent chaotic sparrow search algorithm (LTC-SSA) is proposed. The
LTC-SSA algorithm utilizes the Logistic-Tent Chaotic (LTC) map, which is
designed by combining the Logistic and Tent maps. This new approach aims to
overcome the shortcoming of ""premature convergence"" for the SSA algorithm by
increasing the diversity of the sparrow population. In addition, to improve the
prediction accuracy of peak wall shear stress, a data prediction method based
on LTC-SSA-support vector machine (SVM) is proposed. Herein, LTC-SSA algorithm
is used to train the penalty coefficient C and parameter gamma g of SVM model.
In order to build LTC-SSA-SVM model, optimal Latin hypercube design (Opt LHD)
is used to design the sampling nozzle structures, and the peak wall shear
stress (objective function) of these nozzle structures are calculated by CFD
method. For the purpose of this article, this optimization framework has been
employed to optimize original nozzle structure. The results show that the
optimization framework developed in this study can be used to optimize nozzle
structure with significantly improved its hydrodynamic performance.",['physics.flu-dyn'],False,,,,"Orbiting, colliding and merging droplets on a soap film: toward
  gravitational analogues","Computational fluid dynamics-based structure optimization of
  ultra-high-pressure water-jet nozzle using approximation method"
neg-d2-325,2025-01-02,,2501.01402," Label noise refers to incorrect labels in a dataset caused by human errors or
collection defects, which is common in real-world applications and can
significantly reduce the accuracy of models. This report explores how to
estimate noise transition matrices and construct deep learning classifiers that
are robust against label noise. In cases where the transition matrix is known,
we apply forward correction and importance reweighting methods to correct the
impact of label noise using the transition matrix. When the transition matrix
is unknown or inaccurate, we use the anchor point assumption and T-Revision
series methods to estimate or correct the noise matrix. In this study, we
further improved the T-Revision method by developing T-Revision-Alpha and
T-Revision-Softmax to enhance stability and robustness. Additionally, we
designed and implemented two baseline classifiers, a Multi-Layer Perceptron
(MLP) and ResNet-18, based on the cross-entropy loss function. We compared the
performance of these methods on predicting clean labels and estimating
transition matrices using the FashionMINIST dataset with known noise transition
matrices. For the CIFAR-10 dataset, where the noise transition matrix is
unknown, we estimated the noise matrix and evaluated the ability of the methods
to predict clean labels.",['cs.LG'],2502.05376," Post-training quantization (PTQ) is a promising approach to reducing the
storage and computational requirements of large language models (LLMs) without
additional training cost. Recent PTQ studies have primarily focused on
quantizing only weights to sub-8-bits while maintaining activations at 8-bits
or higher. Accurate sub-8-bit quantization for both weights and activations
without relying on quantization-aware training remains a significant challenge.
We propose a novel quantization method called block clustered quantization
(BCQ) wherein each operand tensor is decomposed into blocks (a block is a group
of contiguous scalars), blocks are clustered based on their statistics, and a
dedicated optimal quantization codebook is designed for each cluster. As a
specific embodiment of this approach, we propose a PTQ algorithm called
Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block
clustering and codebook design to greedily minimize the quantization mean
squared error. When weight and activation scalars are encoded to W4A4 format
(with 0.5-bits of overhead for storing scaling factors and codebook selectors),
we advance the current state-of-the-art by demonstrating <1% loss in inference
accuracy across several LLMs and downstream tasks.",['cs.LG'],False,,,,"Best Transition Matrix Esitimation or Best Label Noise Robustness
  Classifier? Two Possible Methods to Enhance the Performance of T-revision",BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference
neg-d2-326,2025-01-13,,2501.075," This work exploits a framework whereby a graph (in the mathematical sense)
serves to connect a classical system to a state space that we call
`quantum-like' (QL). The QL states comprise arbitrary superpositions of states
in a tensor product basis. The graph plays a special dual role by directing
design of the classical system and defining the state space. We study a
specific example of a large, dynamical classical system -- a system of coupled
phase oscillators -- that maps, via a graph, to the QL state space. We
investigate how mixedness of the state diminishes or increases as the
underlying classical system synchronizes or de-synchronizes respectively. This
shows the interplay between the nonlinear dynamics of the variables of the
classical system and the QL state space. We prove that maps from one time point
to another in the state space are linear maps. In the limit of a strongly
phase-locked classical network -- that is, where couplings between phase
oscillators are very large -- the state space evolves according to unitary
dynamics, whereas in the cases of weaker synchronization, the classical
variables act as a hidden environment that promotes decoherence of
superpositions.",['quant-ph'],2502.09124," We propose the scheme realizing the two-level control over the unitary
operators $U_k$ creating the required quantum state of the system $S$. These
operators are controlled by the superposition state of the auxiliary subsystem
$R$ which is governed by two control centers. The
  first-level control center (main control) creates the equal-probability pure
state of $R$ with certain distribution of phase factors that, in turn, govern
the power of the second-level control center $C$ that applies the special
$V$-operators to the same subsystem $R$ changing its state and thus controlling
the applicability of $U_k$. In addition, the above phases are responsible for
the entanglement in the subsystem $R$. We find the direct relation between this
entanglement and the number of operators $U_k$ that can be controlled by $C$.
The simple example of a two-level control system governing the creation of
entangled state of the two-qubit system $S$ is presented.",['quant-ph'],False,,,,"Dynamics in an emergent quantum-like state space generated by a
  nonlinear classical network","Two-level control over quantum state creation via entangled
  equal-probability state"
neg-d2-327,2025-03-03,,2503.02," When materials are deformed at extreme strain rates, greater than $10^6
\text{ s}^{-1}$, a counterintuitive mechanical response is seen where the
strength and hardness of pure metals increases with increasing temperature. The
anti-thermal hardening is due to defects in the material becoming pinned by
phonons in the crystal lattice. However, here, using optically driven
microballistic impact testing to measure the dynamic strength and hardness, we
show that when the composition is systematically varied away from high purity,
the mechanical response of metals transitions from ballistic transport of
dislocations back to thermally activated pinning of dislocations, even at the
highest strain rates. This boundary from ""hotter-is-stronger"" to
""hotter-is-softer"" is observed and mapped for nickel, titanium, and gold. The
ability to tune between deformation mechanisms with very different temperature
dependencies speaks to new directions for alloy design in extreme conditions.",['cond-mat.mtrl-sci'],2501.05025," We investigate the magnetoelectric properties of the monolayer NiX$_{2}$ (X =
Br, I) through first-principles calculations. Our calculations predict that the
NiBr$_{2}$ monolayer exhibits a cycloidal magnetic ground state. For the
NiI$_{2}$ monolayer, a proper-screw helical magnetic ground state with
modulation vector \(\boldsymbol{Q} = (q, 0, 0)\) is adopted, approximated based
on experimental observations. The electric polarization in NiBr$_{2}$ shows a
linear dependence on the spin-orbit coupling strength \(\lambda_{\text{SOC}}\),
which can be adequately described by the generalized Katsura-Nagaosa-Balatsky
(gKNB) model, considering contributions from up to the third nearest-neighbor
spin pairs. In contrast, the electric polarization in NiI$_{2}$ exhibits a
distinct dependence on \(q\) and \(\lambda_{\text{SOC}}\), which cannot be
fully explained by the gKNB mechanism alone. To address this, the \(p\)-\(d\)
hybridization mechanism is extended to NiI$_{2}$ to explain the observed
behavior. The respective contributions from the \(p\)-\(d\) hybridization and
the gKNB mechanism in NiI$_{2}$ are then quantitatively evaluated. Overall, our
work elucidates the microscopic mechanisms underlying multiferroicity in
NiBr$_{2}$ and NiI$_{2}$ monolayers, with the conclusions readily applicable to
their bulk forms.",['cond-mat.mtrl-sci'],False,,,,"At extreme strain rates, pure metals thermally harden while alloys
  thermally soften","Microscopic origin of magnetoferroelectricity in monolayer NiBr$_{2}$
  and NiI$_{2}$"
neg-d2-328,2025-01-11,,2501.06544," In the mean-field regime, a gas of quantum particles with Boltzmann
statistics can be described by the Hartree-Fock equation. This dynamics becomes
trivial if the initial distribution of particle is invariant by translation.
However, the first correction is given on time of order $O(N)$ by the quantum
Lenard--Balescu equation. In the first part of the present article, we justify
this equation until time of order $O((\log N)^{1-\delta})$ (for any
$\delta\in(0,1)$).
  A similar phenomenon exists in the classical setting (with a similar validity
time obtained by Duerinckx \cite{Duerinckx}). In a second time, we prove the
convergence for dimension $d\geq 2$ of the solutions of the quantum
Lenard--Balescu equation to the solutions of its classical counterpart in the
semi-classical limit. This problem can be interpreted as a grazing collision
limit: the quantum Lenard--Balescu equation looks like a cut-off Boltzmann
equation, when the classical one looks like the Landau equation.",['math.AP'],2503.03209," Isolated skyrmion solutions to the 2D Landau-Lifshitz equation with the
Dzyaloshinskii-Moriya interaction, Zeeman interaction, and easy-plane
anisotropy are considered. In a wide range of parameters illustrating the
various interaction strengths, we construct exact solutions and examine their
monotonicity, exponential decay, and stability using a careful mathematical
analysis. We also estimate the distance between the constructed solutions and
the harmonic maps by exploiting the structure of the linearized equation and by
proving a resolvent estimate for the linearized operator that is uniform in
extra implicit potentials.",['math.AP'],False,,,,Around the Quantum Lenard-Balescu equation,"Global perturbation of isolated equivariant chiral skyrmions from the
  harmonic maps"
neg-d2-329,2025-01-10,,2501.05733," The application of Multi-modal Large Language Models (MLLMs) in Autonomous
Driving (AD) faces significant challenges due to their limited training on
traffic-specific data and the absence of dedicated benchmarks for
spatiotemporal understanding. This study addresses these issues by proposing
TB-Bench, a comprehensive benchmark designed to evaluate MLLMs on understanding
traffic behaviors across eight perception tasks from ego-centric views. We also
introduce vision-language instruction tuning datasets, TB-100k and TB-250k,
along with simple yet effective baselines for the tasks. Through extensive
experiments, we show that existing MLLMs underperform in these tasks, with even
a powerful model like GPT-4o achieving less than 35% accuracy on average. In
contrast, when fine-tuned with TB-100k or TB-250k, our baseline models achieve
average accuracy up to 85%, significantly enhancing performance on the tasks.
Additionally, we demonstrate performance transfer by co-training TB-100k with
another traffic dataset, leading to improved performance on the latter.
Overall, this study represents a step forward by introducing a comprehensive
benchmark, high-quality datasets, and baselines, thus supporting the gradual
integration of MLLMs into the perception, prediction, and planning stages of
AD.",['cs.CV'],2501.19083," Diffusion models have become a popular choice for human motion synthesis due
to their powerful generative capabilities. However, their high computational
complexity and large sampling steps pose challenges for real-time applications.
Fortunately, the Consistency Model (CM) provides a solution to greatly reduce
the number of sampling steps from hundreds to a few, typically fewer than four,
significantly accelerating the synthesis of diffusion models. However, applying
CM to text-conditioned human motion synthesis in latent space yields
unsatisfactory generation results. In this paper, we introduce
\textbf{MotionPCM}, a phased consistency model-based approach designed to
improve the quality and efficiency for real-time motion synthesis in latent
space. Experimental results on the HumanML3D dataset show that our model
achieves real-time inference at over 30 frames per second in a single sampling
step while outperforming the previous state-of-the-art with a 38.9\%
improvement in FID. The code will be available for reproduction.",['cs.CV'],False,,,,"TB-Bench: Training and Testing Multi-Modal AI for Understanding
  Spatio-Temporal Traffic Behaviors from Dashcam Images/Videos",MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model
neg-d2-330,2025-02-04,,2502.02745," We consider the fourth-order nonlinear elliptic problem: \begin{equation*}
\begin{array}{ll}
  \Delta(a(x)\Delta u) = a(x) \left\vert u \right\vert^{p-2-\epsilon} u \
\text{ in } \ \Omega,
  \hspace{0.6cm} u = 0 \ \text{ on } \ \partial \Omega,
  \hspace{0.6cm} \Delta u = 0 \ \text{ on } \ \partial \Omega,
  \end{array}\end{equation*} where $\Omega$ is a smooth, bounded domain in
$\mathbb{R}^N$ with $N \geq 5$. Here, $p := \frac{2N}{N-4}$ is the Sobolev
critical exponent for the embedding $H^2 \cap H_0^1(\Omega) \hookrightarrow
L^p(\Omega)$, and $a \in C^2(\overline{\Omega})$ is a strictly positive
function on $\overline{\Omega}$.
  We establish sufficient conditions on the function $a$ and the domain
$\Omega$ for this problem to admit both positive and sign-changing solutions
with an explicit asymptotic profile. These solutions concentrate and blow up at
a point on the boundary $\partial \Omega$ as $\epsilon \to 0$. The proofs of
the main results rely on the Lyapunov-Schmidt finite-dimensional reduction
method.",['math.AP'],2503.09088," This paper studies the derivation and well-posedness of a class of high -
order water wave equations, the fifth - order Benjamin - Bona - Mahony (BBM)
equation. Low - order models have limitations in describing strong nonlinear
and high - frequency dispersion effects. Thus, it is proposed to improve the
modeling accuracy of water wave dynamics on long - time scales through high -
order correction models. By making small - parameter corrections to the
$abcd-$system, then performing approximate estimations, the fifth - order BBM
equation is finally derived.For local well - posedness, the equation is first
transformed into an equivalent integral equation form. With the help of
multilinear estimates and the contraction mapping principle, it is proved that
when $s\geq1$, for a given initial value $\eta_{0}\in H^{s}(\mathbb{R})$, the
equation has a local solution $\eta \in C([0, T];H^{s})$, and the solution
depends continuously on the initial value. Meanwhile, the maximum existence
time of the solution and its growth restriction are given.For global well -
posedness, when $s\geq2$, through energy estimates and local theory, combined
with conservation laws, it is proved that the initial - value problem of the
equation is globally well - posed in $H^{s}(\mathbb{R})$. When $1\leq s<2$, the
initial value is decomposed into a rough small part and a smooth part, and
evolution equations are established respectively. It is proved that the
corresponding integral equation is locally well - posed in $H^{2}$ and the
solution can be extended, thus concluding that the initial - value problem of
the equation is globally well - posed in $H^{s}$.",['math.AP'],False,,,,"Concentration on the Boundary and Sign-Changing Solutions for a Slightly
  Subcritical Biharmonic Problem","Derivation and Well-Posedness Analysis of the Higher-Order
  Benjamin-Bona-Mahony Equation"
neg-d2-331,2025-03-08,,2503.06363," Imaging thermal sources naturally yields Gaussian states at the receiver,
raising the question of whether Gaussian measurements can perform optimally in
quantum imaging. In this work, we establish no-go theorems on the performance
of Gaussian measurements when imaging thermal sources with mean photon number
per temporal mode $\epsilon \rightarrow 0$ or when solving two sources with the
separation $L \rightarrow 0$. Our results show that non-Gaussian measurements
can outperform any Gaussian measurement by a factor of $\epsilon$ (or $L^2$) in
terms of the estimation variance, for both interferometric and single-lens
imaging. We also present several examples to illustrate the no-go results.",['quant-ph'],2503.15216," In this work, we investigate the non-Markovian dynamical evolution of a
${\Lambda}$-type atom interacting with a semi-infinite one-dimensional photonic
waveguide via two atomic transitions. The waveguide terminates at a perfect
mirror, which reflects the light and introduces boundary effects. We derive
exact analytical expressions and show that, under suitable conditions, the
instantaneous and retarded decay rates reach equilibrium, leading to the
formation of an atom-photon bound state that suppresses dissipation.
Consequently, the atom retains a long-lived population in the asymptotic time
limit. Furthermore, we analyze the output field intensity and demonstrate that
blocking one of the coupling channels forces the atomic system to emit photons
of a single frequency. Finally, we extend the model to a two-atom system and
examine the disentanglement dynamics of the two spatially separated atoms.
These findings elucidate the dynamic process of spontaneous emission involving
multi-frequency photons from multi-level atoms and provide insights into the
complex interference between different decay pathways.",['quant-ph'],False,,,,Limitations of Gaussian measurements in quantum imaging,"Non-Markovian dynamics with ${\Lambda}$-type atomic systems in a single
  end photonic waveguide"
neg-d2-332,2025-02-12,,2502.08949," Self-supervised graph representation learning has driven significant
advancements in domains such as social network analysis, molecular design, and
electronics design automation (EDA). However, prior works in EDA have mainly
focused on the representation of gate-level digital circuits, failing to
capture analog and mixed-signal circuits. To address this gap, we introduce
DICE: Device-level Integrated Circuits Encoder, the first self-supervised
pretrained graph neural network (GNN) model for any circuit expressed at the
device level. DICE is a message-passing neural network (MPNN) trained through
graph contrastive learning, and its pretraining process is simulation-free,
incorporating two novel data augmentation techniques. Experimental results
demonstrate that DICE achieves substantial performance gains across three
downstream tasks, underscoring its effectiveness for both analog and digital
circuits.",['cs.LG'],2502.06663," Modern large language models (LLMs) driven by scaling laws, achieve
intelligence emergency in large model sizes. Recently, the increasing concerns
about cloud costs, latency, and privacy make it an urgent requirement to
develop compact edge language models. Distinguished from direct pretraining
that bounded by the scaling law, this work proposes the pruning-aware
pretraining, focusing on retaining performance of much larger optimized models.
It features following characteristics: 1) Data-scalable: we introduce minimal
parameter groups in LLM and continuously optimize structural pruning, extending
post-training pruning methods like LLM-Pruner and SparseGPT into the
pretraining phase. 2) Architecture-agnostic: the LLM architecture is
auto-designed using saliency-driven pruning, which is the first time to exceed
SoTA human-designed LLMs in modern pretraining. We reveal that it achieves
top-quality edge language models, termed EfficientLLM, by scaling up LLM
compression and extending its boundary. EfficientLLM significantly outperforms
SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM,
Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first
attempt, EfficientLLM bridges the performance gap between traditional LLM
compression and direct pretraining methods, and we will fully open source at
https://github.com/Xingrun-Xing2/EfficientLLM.",['cs.LG'],False,,,,"Self-Supervised Graph Contrastive Pretraining for Device-level
  Integrated Circuits","EfficientLLM: Scalable Pruning-Aware Pretraining for
  Architecture-Agnostic Edge Language Models"
neg-d2-333,2025-01-10,,2501.06008," In this paper, we study the problem of partitioning a graph into connected
and colored components called blocks. Using bivariate generating functions and
combinatorial techniques, we determine the expected number of blocks when the
vertices of a graph $G$, for $G$ in certain families of graphs, are colored
uniformly and independently. Special emphasis is placed on graphs of the form
$G \times P_n$, where $P_n$ is the path graph on $n$ vertices. This case serves
as a generalization of the problem of enumerating the number of tilings of an
$m \times n$ grid using colored polyominoes.",['math.CO'],2501.01634," There has been much work on the following question: given n how large can a
subset of {1,...,n} be that has no arithmetic progressions of length 3. We call
such sets 3-free. Most of the work has been asymptotic. In this paper we sketch
applications of large 3-free sets, review the literature of how to construct
large 3-free sets, and present empirical studies on how large such sets
actually are. The two main questions considered are (1) How large can a 3-free
set be when n is small, and (2) How do the methods in the literature compare to
each other? In particular, when do the ones that are asymptotically better
actually yield larger sets? (This paper overlaps with our previous paper with
the title { Finding Large 3-Free Sets I: the Small n Case}.)",['math.CO'],False,,,,Enumeration of Colored Tilings on Graphs via Generating Functions,"Finding Large Sets Without Arithmetic Progressions of Length Three: An
  Empirical View and Survey II"
neg-d2-334,2025-01-02,,2501.01351," We provide a simple proof for of the central limit theorem for the number of
vertices in the giant for super-critical stochastic block model using the
breadth-first walk of Konarovskyi, Limic and the author (2024). Our approach
follows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the
classic central limit theorem for the Erd\H{o}s-R\'{e}nyi model obtained by
Stepanov (1970).",['math.PR'],2503.04166," We study the composition of bivariate L\'evy process with bivariate inverse
subordinator. The explicit expressions for its dispersion and auto correlation
matrices are obtained. Also, the time-changed two parameter L\'evy processes
with rectangular increments are studied. We introduce some time-changed
variants of the Poisson random field in plane with and without drift, and
derive the associated fractional differential equations for their
distributions. Later, we consider some time-changed L\'evy processes where the
time-changing components are two parameter Poisson random fields with drifts.
Moreover, two parameter coordinatewise semigroup operators associated with some
of the introduced processes are discussed.",['math.PR'],False,,,,A central limit theorem for the giant in a stochastic block model,On Two Parameter Time-Changed Poisson Random Fields with Drifts
neg-d2-335,2025-02-20,,2502.1468," We study nonlinear n-term approximation of harmonic functions on the unit
ball in $R^d$ from linear combinations of shifts of the Newtonian kernel
(fundamental solution of the Laplace equation) in BMO. A sharp Jackson estimate
is established that naturally involves certain Besov spaces. The method for
obtaining this result is based on the construction of highly localized frames
for Besov spaces and VMO on the sphere whose elements are linear combinations
of a fixed number of shifts of the Newtonian kernel.",['math.CA'],2501.17118," For each $f\!:\!\mathbb{R}\to\mathbb{C}$ that is Henstock--Kurzweil
integrable on the real line, or is a distribution in the completion of the
space of Henstock--Kurzweil integrable functions in the Alexiewicz norm, it is
shown that the Fourier transform is the second distributional derivative of a
H\""older continuous function. The space of such Fourier transforms is
isometrically isomorphic to the completion of the Henstock--Kurzweil integrable
functions. There is an exchange theorem, inversion in norm and convolution
results. Sufficient conditions are given for an $L^1$ function to have a
Fourier transform that is of bounded variation. Pointwise inversion of the
Fourier transform is proved for functions in $L^p$ spaces for $1<p<\infty$. The
exchange theorem is used to evaluate an integral that does not appear in
published tables.",['math.CA'],False,,,,"Nonlinear approximation of harmonic functions from shifts of the
  Newtonian kernel in BMO","The Fourier transform with Henstock--Kurzweil and continuous primitive
  integrals"
neg-d2-336,2025-03-21,,2503.17186," This paper presents searches for the direct pair production of charged
light-flavour sleptons, each decaying into a stable neutralino and an
associated Standard Model lepton. The analyses focus on the challenging
""corridor"" region, where the mass difference, $\Delta m$, between the slepton
($\tilde{e}$ or $\tilde{\mu}$) and the lightest neutralino
($\tilde{\chi}^{0}_{1}$) is less or similar to the mass of the $W$ boson,
$m(W)$, with the aim to close a persistent gap in sensitivity to models with
$\Delta m \lesssim m(W)$. Events are required to contain a high-energy jet,
significant missing transverse momentum, and two same-flavour opposite-sign
leptons ($e$ or $\mu$). The analysis uses $pp$ collision data at $\sqrt{s} =
13$ TeV recorded by the ATLAS detector, corresponding to an integrated
luminosity of 140 fb$^{-1}$. Several kinematic selections are applied,
including a set of boosted decision trees. These are each optimised for
different $\Delta m$ to provide expected sensitivity for the first time across
the full $\Delta m$ corridor. The results are generally consistent with the
Standard Model, with the most significant deviations observed with a local
significance of 2.0 $\sigma$ in the selectron search, and 2.4 $\sigma$ in the
smuon search. While these deviations weaken the observed exclusion reach in
some parts of the signal parameter space, the previously present sensitivity
gap to this corridor is largely reduced. Constraints at the 95% confidence
level are set on simplified models of selectron and smuon pair production,
where selectrons (smuons) with masses up to 300 (350) GeV can be excluded for
$\Delta m$ between 2 GeV and 100 GeV.",['hep-ex'],2503.11383," Using $e^+e^-$ annihilation data corresponding to a total integrated
luminosity of 7.33 $\rm fb^{-1}$ collected at center-of-mass energies between
4.128 and 4.226~GeV with the BESIII detector, we provide the first amplitude
analysis and absolute branching fraction measurement of the hadronic decay
$D_{s}^{+} \to K_{S}^{0}K_{L}^{0}\pi^{+}$. The branching fraction of $D_{s}^{+}
\to K_{S}^{0}K_{L}^{0}\pi^{+}$ is determined to be $(1.86\pm0.06_{\rm
stat}\pm0.03_{\rm syst})\%$.
  Combining the $\mathcal{B}(D_{s}^{+} \to \phi(\to K_{S}^0K_{L}^0) \pi^+)$
obtained in this work and the world average of $\mathcal{B}(D_{s}^{+} \to
\phi(\to K^+K^-) \pi^+)$, we measure the relative branching fraction
$\mathcal{B}(\phi \to K_S^0K_L^0)/\mathcal{B}(\phi \to K^+K^-)$=($0.597 \pm
0.023_{\rm stat} \pm 0.018_{\rm syst} \pm 0.016_{\rm PDG}$), which deviates
from the PDG value by more than 3$\sigma$. Furthermore, the asymmetry of the
branching fractions of $D^+_s\to K_{S}^0K^{*}(892)^{+}$ and $D^+_s\to
K_{L}^0K^{*}(892)^{+}$, $\frac{\mathcal{B}(D_{s}^{+} \to
K_{S}^0K^{*}(892)^{+})-\mathcal{B}(D_{s}^{+} \to
K_{L}^0K^{*}(892)^{+})}{\mathcal{B}(D_{s}^{+} \to
K_{S}^0K^{*}(892)^{+})+\mathcal{B}(D_{s}^{+} \to K_{L}^0K^{*}(892)^{+})}$, is
determined to be $(-13.4\pm5.0_{\rm stat}\pm3.4_{\rm syst})\%$.",['hep-ex'],False,,,,"Searches for direct slepton production in the compressed-mass corridor
  in $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector","Study of $\phi\to K\bar{K}$ and $K_{S}^{0}-K_{L}^{0}$ asymmetry in the
  amplitude analysis of $D_{s}^{+} \to K_{S}^{0}K_{L}^{0}\pi^{+}$ decay"
neg-d2-337,2025-01-21,,2501.12211," Rogers-Ramanujan type identities occur in various branches of mathematics and
physics. As a classic and powerful tool to deal with Rogers-Ramanujan type
identities, the theory of Bailey's lemma has been extensively studied and
generalized. In this paper, we found a bilateral Bailey pair that naturally
arises from the q-binomial theorem. By applying the bilateral versions of
Bailey lemmas, Bailey chains and Bailey lattices, we derive a number of
Rogers-Ramanujan type identities, which unify many known identities as special
cases. Further combined with the bilateral Bailey chains due to Berkovich,
McCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we
also obtain identities on Appell-Lerch series and identities of Andrews-Gordon
type. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we
derive identities on Hecke-type series.",['math.CO'],2502.12495," Let $\phi$ be a collineation of $\mathrm{PG}\left(2, q^{3}\right)$ of order 3
which fixes a plane of order $q$ pointwise. The points of $\mathrm{PG}\left(2,
q^{3}\right)$ can be partitioned into three types with respect to orbits of
$\phi$ : fixed points; points $P$ with $P, P^{\phi}, P^{\phi^{2}}$ distinct and
collinear; and points $P$ with $P, P^{\phi}, P^{\phi^{2}}$ not collinear. Under
field reduction, the collineation $\phi$ corresponds to a projectivity $\sigma$
of $\operatorname{PG}(8, q)$ of order 3 . With respect to the field reduction
and the orbits of $\sigma$, the points of $\mathrm{PG}(8, q)$ can be
partitioned into six types. This article looks at the projectivity $\sigma$ in
detail, and classifies and counts the fixed points, fixed lines and fixed
planes. The motivation is to give a description of the lines of the Figueroa
projective plane in the $\mathrm{PG}(8, q)$ field reduction setting.",['math.CO'],False,,,,Bilateral Bailey pairs and Rogers-Ramanujan type identities,"The planar projectivity of PG(2, $q^3$) of order 3 under field reduction"
neg-d2-338,2025-02-18,,2502.12867," Between 1980 and 2000, the U.S. experienced a significant rise in geographic
sorting and educational homogamy, with college graduates increasingly
concentrating in high-skill cities and marrying similarly educated spouses. We
develop and estimate a spatial equilibrium model with local labor, housing, and
marriage markets, incorporating a marriage matching framework with transferable
utility. Using the model, we estimate trends in assortative preferences,
quantify the interplay between marital and geographic sorting, and assess their
combined impact on household inequality. Welfare analyses show that after
accounting for marriage, the college well-being gap grew substantially more
than the college wage gap.",['econ.EM'],2502.05353," Sample selection is pervasive in applied economic studies. This paper
develops semiparametric selection models that achieve point identification
without relying on exclusion restrictions, an assumption long believed
necessary for identification in semiparametric selection models. Our
identification conditions require at least one continuously distributed
covariate and certain nonlinearity in the selection process. We propose a
two-step plug-in estimator that is root-n-consistent, asymptotically normal,
and computationally straightforward (readily available in statistical
software), allowing for heteroskedasticity. Our approach provides a middle
ground between Lee (2009)'s nonparametric bounds and Honor\'e and Hu (2020)'s
linear selection bounds, while ensuring point identification. Simulation
evidence confirms its excellent finite-sample performance. We apply our method
to estimate the racial and gender wage disparity using data from the US Current
Population Survey. Our estimates tend to lie outside the Honor\'e and Hu
bounds.",['econ.EM'],False,,,,Assortative Marriage and Geographic Sorting,"Point-Identifying Semiparametric Sample Selection Models with No
  Excluded Variable"
neg-d2-339,2025-01-16,,2501.09888," Self-Admitted Technical Debt (SATD), cases where developers intentionally
acknowledge suboptimal solutions in code through comments, poses a significant
challenge to software maintainability. Left unresolved, SATD can degrade code
quality and increase maintenance costs. While Large Language Models (LLMs) have
shown promise in tasks like code generation and program repair, their potential
in automated SATD repayment remains underexplored.
  In this paper, we identify three key challenges in training and evaluating
LLMs for SATD repayment: (1) dataset representativeness and scalability, (2)
removal of irrelevant SATD repayments, and (3) limitations of existing
evaluation metrics. To address the first two dataset-related challenges, we
adopt a language-independent SATD tracing tool and design a 10-step filtering
pipeline to extract SATD repayments from repositories, resulting two
large-scale datasets: 58,722 items for Python and 97,347 items for Java. To
improve evaluation, we introduce two diff-based metrics, BLEU-diff and
CrystalBLEU-diff, which measure code changes rather than whole code.
Additionally, we propose another new metric, LEMOD, which is both interpretable
and informative. Using our new benchmarks and evaluation metrics, we evaluate
two types of automated SATD repayment methods: fine-tuning smaller models, and
prompt engineering with five large-scale models. Our results reveal that
fine-tuned small models achieve comparable Exact Match (EM) scores to
prompt-based approaches but underperform on BLEU-based metrics and LEMOD.
Notably, Gemma-2-9B leads in EM, addressing 10.1% of Python and 8.1% of Java
SATDs, while Llama-3.1-70B-Instruct and GPT-4o-mini excel on BLEU-diff,
CrystalBLEU-diff, and LEMOD metrics. Our work contributes a robust benchmark,
improved evaluation metrics, and a comprehensive evaluation of LLMs, advancing
research on automated SATD repayment.",['cs.SE'],2503.03455," Despite advancements in MLOps and AutoML, ML development still remains
challenging for data scientists. First, there is poor support for and limited
control over optimizing and evolving ML models. Second, there is lack of
efficient mechanisms for continuous evolution of ML models which would leverage
the knowledge gained in previous optimizations of the same or different models.
We propose an experiment-driven MLOps approach which tackles these problems.
Our approach relies on the concept of an experiment, which embodies a fully
controllable optimization process. It introduces full traceability and
repeatability to the optimization process, allows humans to be in full control
of it, and enables continuous improvement of the ML system. Importantly, it
also establishes knowledge, which is carried over and built across a series of
experiments and allows for improving the efficiency of experimentation over
time. We demonstrate our approach through its realization and application in
the ExtremeXP1 project (Horizon Europe).",['cs.SE'],False,,,,"Understanding the Effectiveness of LLMs in Automated Self-Admitted
  Technical Debt Repayment",Towards Continuous Experiment-driven MLOps
neg-d2-340,2025-03-10,,2503.07917," Clustering of high-dimensional data sets is a growing need in artificial
intelligence, machine learning and pattern recognition. In this paper, we
propose a new clustering method based on a combinatorial-topological approach
applied to regions of space defined by signs of coordinates (hyperoctants). In
high-dimensional spaces, this approach often reduces the size of the dataset
while preserving sufficient topological features. According to a density
criterion, the method builds clusters of data points based on the partitioning
of a graph, whose vertices represent hyperoctants, and whose edges connect
neighboring hyperoctants under the Levenshtein distance. We call this method
HyperOctant Search Clustering. We prove some mathematical properties of the
method. In order to as assess its performance, we choose the application of
topic detection, which is an important task in text mining. Our results suggest
that our method is more stable under variations of the main hyperparameter, and
remarkably, it is not only a clustering method, but also a tool to explore the
dataset from a topological perspective, as it directly provides information
about the number of hyperoctants where there are data points. We also discuss
the possible connections between our clustering method and other research
fields.",['cs.LG'],2503.01048," Personalizing large language models (LLMs) is essential for delivering
tailored interactions that improve user experience. Many existing
personalization methods require fine-tuning LLMs for each user, rendering them
prohibitively expensive for widespread adoption. Although retrieval-based
approaches offer a more compute-efficient alternative, they still depend on
large, high-quality datasets that are not consistently available for all users.
To address this challenge, we propose CHAMELEON, a scalable and efficient
personalization approach that uses (1) self-generated personal preference data
and (2) representation editing to enable quick and cost-effective
personalization. Our experiments on various tasks, including those from the
LaMP personalization benchmark, show that CHAMELEON efficiently adapts models
to personal preferences, improving instruction-tuned models and outperforms two
personalization baselines by an average of 40% across two model architectures.",['cs.LG'],False,,,,"Hyperoctant Search Clustering: A Method for Clustering Data in
  High-Dimensional Hyperspheres",Personalize Your LLM: Fake it then Align it
neg-d2-341,2025-02-17,,2502.11532," Text-to-image diffusion models have shown remarkable capabilities of
generating high-quality images closely aligned with textual inputs. However,
the effectiveness of text guidance heavily relies on the CLIP text encoder,
which is trained to pay more attention to general content but struggles to
capture semantics in specific domains like styles. As a result, generation
models tend to fail on prompts like ""a photo of a cat in Pokemon style"" in
terms of simply producing images depicting ""a photo of a cat"". To fill this
gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that
enables the CLIP model to learn the meaning of category and style in a
complement manner. With specially designed fine-tuning tasks on minimal data
and a modified cross-attention mechanism, Control-CLIP can precisely guide the
diffusion model to a specific domain. Moreover, the parameters of the diffusion
model remain unchanged at all, preserving the original generation performance
and diversity. Experiments across multiple domains confirm the effectiveness of
our approach, particularly highlighting its robust plug-and-play capability in
generating content with various specific styles.",['cs.CV'],2501.09281," In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.",['cs.CV'],False,,,,"Control-CLIP: Decoupling Category and Style Guidance in CLIP for
  Specific-Domain Generation",SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection
neg-d2-342,2025-02-21,,2502.15961," Planning paths that maximize information gain for robotic platforms has
wide-ranging applications and significant potential impact. To effectively
adapt to real-time data collection, informative path planning must be computed
online and be responsive to new observations. In this work, we present
IA-TIGRIS, an incremental and adaptive sampling-based informative path planner
that can be run efficiently with onboard computation. Our approach leverages
past planning efforts through incremental refinement while continuously
adapting to updated world beliefs. We additionally present detailed
implementation and optimization insights to facilitate real-world deployment,
along with an array of reward functions tailored to specific missions and
behaviors. Extensive simulation results demonstrate IA-TIGRIS generates
higher-quality paths compared to baseline methods. We validate our planner on
two distinct hardware platforms: a hexarotor UAV and a fixed-wing UAV, each
having unique motion models and configuration spaces. Our results show up to a
41% improvement in information gain compared to baseline methods, suggesting
significant potential for deployment in real-world applications.",['cs.RO'],2502.10585," Human motion is stochastic and ensuring safe robot navigation in a
pedestrian-rich environment requires proactive decision-making. Past research
relied on incorporating deterministic future states of surrounding pedestrians
which can be overconfident leading to unsafe robot behaviour. The current paper
proposes a predictive uncertainty-aware planner that integrates neural network
based probabilistic trajectory prediction into planning. Our method uses a deep
ensemble based network for probabilistic forecasting of surrounding humans and
integrates the predictive uncertainty as constraints into the planner. We
compare numerous constraint satisfaction methods on the planner and evaluated
its performance on real world pedestrian datasets. Further, offline robot
navigation was carried out on out-of-distribution pedestrian trajectories
inside a narrow corridor",['cs.RO'],False,,,,"IA-TIGRIS: An Incremental and Adaptive Sampling-Based Planner for Online
  Informative Path Planning","Prediction uncertainty-aware planning using deep ensembles and
  trajectory optimisation"
neg-d2-343,2025-02-10,,2502.06197," Large Language Models (LLMs) have been widely used to support ideation in the
writing process. However, whether generating ideas with the help of LLMs leads
to idea fixation or idea expansion is unclear. This study examines how
different timings of LLM usage - either at the beginning or after independent
ideation - affect people's perceptions and ideation outcomes in a writing task.
In a controlled experiment with 60 participants, we found that using LLMs from
the beginning reduced the number of original ideas and lowered creative
self-efficacy and self-credit, mediated by changes in autonomy and ownership.
We discuss the challenges and opportunities associated with using LLMs to
assist in idea generation. We propose delaying the use of LLMs to support
ideation while considering users' self-efficacy, autonomy, and ownership of the
ideation outcomes.",['cs.HC'],2501.07748," The vertical ground reaction force (vGRF) and its characteristic weight
acceptance and push-off peaks measured during walking are important for gait
and biomechanical analysis. Current wearable vGRF estimation methods suffer
from drifting errors or low generalization performances, limiting their
practical application. This paper proposes a novel method for reliably
estimating vGRF and its characteristic peaks using data collected from the
smart insole, including inertial measurement unit data and the newly introduced
center of the pressed sensor data. These data were fused with machine learning
algorithms including artificial neural networks, random forest regression, and
bi-directional long-short-term memory. The proposed method outperformed the
state-of-the-art methods with the root mean squared error, normalized root mean
squared error, and correlation coefficient of 0.024 body weight (BW), 1.79% BW,
and 0.997 in intra-participant testing, and 0.044 BW, 3.22% BW, and 0.991 in
inter-participant testing, respectively. The difference between the reference
and estimated weight acceptance and push-off peak values are 0.022 BW and 0.017
BW with a delay of 1.4% and 1.8% of the gait cycle for the intra-participant
testing and 0.044 BW and 0.025 BW with a delay of 1.5% and 2.3% of the gait
cycle for the inter-participant testing. The results indicate that the proposed
vGRF estimation method has the potential to achieve accurate vGRF measurement
during walking in free living environments.",['cs.HC'],False,,,,"Timing Matters: How Using LLMs at Different Timings Influences Writers'
  Perceptions and Ideation Outcomes in AI-Assisted Ideation","Reliable Vertical Ground Reaction Force Estimation with Smart Insole
  During Walking"
neg-d2-344,2025-01-13,,2501.07459," Given the importance of datasets for sensing-communication integration
research, a novel simulation platform for constructing communication and
multi-modal sensory dataset is developed. The developed platform integrates
three high-precision software, i.e., AirSim, WaveFarer, and Wireless InSite,
and further achieves in-depth integration and precise alignment of them. Based
on the developed platform, a new synthetic intelligent multi-modal
sensing-communication dataset for Synesthesia of Machines (SoM), named
SynthSoM, is proposed. The SynthSoM dataset contains various air-ground
multi-link cooperative scenarios with comprehensive conditions, including
multiple weather conditions, times of the day, intelligent agent densities,
frequency bands, and antenna types. The SynthSoM dataset encompasses multiple
data modalities, including radio-frequency (RF) channel large-scale and
small-scale fading data, RF millimeter wave (mmWave) radar sensory data, and
non-RF sensory data, e.g., RGB images, depth maps, and light detection and
ranging (LiDAR) point clouds. The quality of SynthSoM dataset is validated via
statistics-based qualitative inspection and evaluation metrics through machine
learning (ML) via real-world measurements. The SynthSoM dataset is open-sourced
and provides consistent data for cross-comparing SoM-related algorithms.",['eess.SP'],2501.0582," Modular Arrays (MAs) are a promising architecture to enable multi-user
communications in next-generation multiple-input multiple-output (MIMO) systems
based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an
improved spatial resolution with characteristic interference patterns
associated to grating lobes. In this work, we analyze whether MAs can
outperform conventional collocated deployments, in terms of achievable sum-rate
and served users in a multi-user downlink set-up. First, we provide a rigorous
analytical characterization of the inter-user interference for modular gMIMO
systems operating in the near field. Then, we leverage these results to
optimize the user selection and precoding mechanisms, designing two algorithms
that largely outperform existing alternatives in the literature, with different
algorithmic complexities. Results show that the proposed algorithms yield over
70% improvements in achievable sum-spectral efficiencies compared to the state
of the art. We also illustrate how MAs allow to serve a larger number of users
thanks to their improved spatial resolution, compared to the collocated
counterpart.",['eess.SP'],False,,,,"SynthSoM: A synthetic intelligent multi-modal sensing-communication
  dataset for Synesthesia of Machines (SoM)",User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays
neg-d2-345,2025-03-13,,2503.10115," The purpose of partial multi-label feature selection is to select the most
representative feature subset, where the data comes from partial multi-label
datasets that have label ambiguity issues. For label disambiguation, previous
methods mainly focus on utilizing the information inside the labels and the
relationship between the labels and features. However, the information existing
in the feature space is rarely considered, especially in partial multi-label
scenarios where the noises is considered to be concentrated in the label space
while the feature information is correct. This paper proposes a method based on
latent space alignment, which uses the information mined in feature space to
disambiguate in latent space through the structural consistency between labels
and features. In addition, previous methods overestimate the consistency of
features and labels in the latent space after convergence. We comprehensively
consider the similarity of latent space projections to feature space and label
space, and propose new feature selection term. This method also significantly
improves the positive label identification ability of the selected features.
Comprehensive experiments demonstrate the superiority of the proposed method.",['cs.LG'],2501.16186," This paper studies the uplink and downlink power allocation for interactive
augmented reality (AR) services, where live video captured by an AR device is
uploaded to the network edge and then the augmented video is subsequently
downloaded. By modeling the AR transmission process as a tandem queuing system,
we derive an upper bound for the probabilistic quality of service (QoS)
requirement concerning end-to-end latency and reliability. The resource
allocation with the QoS constraints results in a functional optimization
problem. To address it, we design a deep neural network to learn the power
allocation policy, leveraging the structure of optimal power allocation to
enhance learning performance. Simulation results demonstrate that the proposed
method effectively reduces transmit powers while meeting the QoS requirement.",['cs.LG'],False,,,,"Reconsidering Feature Structure Information and Latent Space Alignment
  in Partial Multi-label Feature Selection",Learn to Optimize Resource Allocation under QoS Constraint of AR
neg-d2-346,2025-01-03,,2501.02157," As large language models (LLMs) evolve, their ability to deliver personalized
and context-aware responses offers transformative potential for improving user
experiences. Existing personalization approaches, however, often rely solely on
user history to augment the prompt, limiting their effectiveness in generating
tailored outputs, especially in cold-start scenarios with sparse data. To
address these limitations, we propose Personalized Graph-based
Retrieval-Augmented Generation (PGraphRAG), a framework that leverages
user-centric knowledge graphs to enrich personalization. By directly
integrating structured user knowledge into the retrieval process and augmenting
prompts with user-relevant context, PGraphRAG enhances contextual understanding
and output quality. We also introduce the Personalized Graph-based Benchmark
for Text Generation, designed to evaluate personalized text generation tasks in
real-world settings where user history is sparse or unavailable. Experimental
results show that PGraphRAG significantly outperforms state-of-the-art
personalization methods across diverse tasks, demonstrating the unique
advantages of graph-based retrieval for personalization.",['cs.CL'],2502.13646," In-context learning (ICL) has demonstrated significant potential in enhancing
the capabilities of large language models (LLMs) during inference. It's
well-established that ICL heavily relies on selecting effective demonstrations
to generate outputs that better align with the expected results. As for
demonstration selection, previous approaches have typically relied on intuitive
metrics to evaluate the effectiveness of demonstrations, which often results in
limited robustness and poor cross-model generalization capabilities. To tackle
these challenges, we propose a novel method, \textbf{D}emonstration
\textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration
validation perspective into this field. By introducing the demonstration
validation mechanism, our method effectively identifies demonstrations that are
both effective and highly generalizable. \textbf{D.Va} surpasses all existing
demonstration selection techniques across both natural language understanding
(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate
the robustness and generalizability of our approach across various language
models with different retrieval models.",['cs.CL'],False,,,,Personalized Graph-Based Retrieval for Large Language Models,D.Va: Validate Your Demonstration First Before You Use It
neg-d2-347,2025-03-22,,2503.17689," In this paper, we discuss the existence of ghost star models in the
Einstein-Maxwell framework. In order to explore these objects, we put forward
the idea of Zeldovich and Novikov by keeping in mind that the energy density of
such models lie in the negative range in some regions of the spacetime
geometry. We proceed by taking into account a static sphere and develop the
field equations for a charged anisotropic fluid configuration. The two
generating functions are then considered and we rewrite the field equations in
terms of the mass and these physical quantities. Afterwards, we formulate two
different models using the conformally flatness condition along with the
considered generating functions. Further, we adopt the vanishing complexity
constraint as well as null active gravitational mass to find two more
solutions. The energy density for all developed models is also graphically
shown. We conclude that the ghost stars exist in the presence of charge as the
energy density for all the resulting solutions lie in the negative region for a
particular range of the radial coordinate.",['gr-qc'],2502.01759," A generalization of the Majumdar-Papapetrou multi-black hole spacetime was
recently constructed by Teo and Wan [1], describing charged and spinning
(extremal) balanced black holes in asymptotically flat spacetime. We explore
the dynamics of null geodesics on this geometry, focusing on the two-center
solution. Using the topological charge formalism, we show that various light
ring arrangements arise from different choices of individual angular momenta:
light rings with opposite topological charges can merge and annihilate each
other, resulting in configurations with a total of 4, 6, or 8 light rings.
Using backward ray-tracing, we obtained the shadow and lensing of these
spacetimes. The former, in particular, closely resembles those for the
double-Kerr metric.",['gr-qc'],False,,,,"Possible Existence of Ghost Stars in the context of Electromagnetic
  Field","Spinning generalizations of Majumdar-Papapetrou multi-black hole
  spacetimes: light rings, lensing and shadows"
neg-d2-348,2025-02-10,,2502.07235," The Sand Atlas is a publicly accessible repository dedicated to the
collection, processing, and sharing of high-resolution 3D models of sand-sized
particles. This dataset offers valuable insights into the morphology of a wide
variety of natural and synthetic sand-sized particles from different regions,
with varying mineralogy and history. The primary goal of The Sand Atlas is to
support researchers, educators, and industry professionals by providing
detailed, easily accessible and uniformly produced surface meshes and level-set
data. The underlying code that converts volumetric data to meshes is also
available via the sand-atlas python package. This platform encourages community
participation, inviting contributors to share their own data and enrich the
collective understanding of granular materials.",['cond-mat.soft'],2501.10947," Amorphous solids are dynamically inhomogeneous due to in lack of
translational symmetry and hence exhibit vibrational properties different from
crystalline solids with anomalous low frequency vibrational density of states
(VDOS) and related low temperature thermal properties. However, an
interpretation of their origin from basic physical laws is still needed
compared with rapidly progressed particle level investigations. In this work,
we start with the quasi-equilibrium condition, which requires elastic potential
energy to be homogeneously distributed even in an inhomogeneous elastic solid
over long time observation. Analytical result shows that the anomalous low
frequency VDOS behavior $D(\omega) \propto \omega^4$ can be obtained when the
quasi-equilibrium condition is satisfied on an inhomogeneous elastic system.
Under high frequency after a crossover depending on the length scale of
inhomogeneity, the power law of VDOS is changed to square $D(\omega) \propto
\omega^2$ which is Debye's law for crystalline solids. These features agree
with recent particle level investigations. Our work suggest that the universal
low frequency anomaly of amorphous solids can be considered as a result of
homogeneous thermalization.",['cond-mat.soft'],False,,,,The Sand Atlas,"The anomalous density of states and quasi-localized vibration through
  homogeneous thermalization of an inhomogeneous elastic system"
neg-d2-349,2025-01-15,,2501.0869," It is known that an inverse monoid $M$ is E-unitary if and only if the
following diagram is an extension: $E(M) \to M \to M/\sigma$, where $E(M)$ is
the semilattice of idempotents and $M/\sigma$ is the minimal group quotient.
F-inverse monoids are another fundamental class of inverse semigroup and all
F-inverse monoids are E-unitary. Thus given that F-inverse monoids have an
associated extension it is natural to ask if these extensions satisfy any
special properties. Indeed we show that $M$ is F-inverse if and only if the
aforementioned extension is weakly Schreier. This latter result allows us to
make use of relaxed factor systems to provide a new characterization of
F-inverse monoids. We end by restricting to the Clifford case and find a new
characterization of these with much in common with Artin gluings of frames.",['math.RA'],2501.0869," It is known that an inverse monoid $M$ is E-unitary if and only if the
following diagram is an extension: $E(M) \to M \to M/\sigma$, where $E(M)$ is
the semilattice of idempotents and $M/\sigma$ is the minimal group quotient.
F-inverse monoids are another fundamental class of inverse semigroup and all
F-inverse monoids are E-unitary. Thus given that F-inverse monoids have an
associated extension it is natural to ask if these extensions satisfy any
special properties. Indeed we show that $M$ is F-inverse if and only if the
aforementioned extension is weakly Schreier. This latter result allows us to
make use of relaxed factor systems to provide a new characterization of
F-inverse monoids. We end by restricting to the Clifford case and find a new
characterization of these with much in common with Artin gluings of frames.",['math.RA'],False,,,,F-Inverse Monoids as Weakly Schreier Extensions,F-Inverse Monoids as Weakly Schreier Extensions
neg-d2-350,2025-02-13,,2502.09882," The interplay among heat, spin, and charge is the central focus in spin
caloritronic research. While the longitudinal heat-to-spin conversion via the
spin Seebeck effect has been intensively studied, the transverse heat-to-spin
conversion via the spin Nernst effect (SNE) has not been equally explored. One
major challenge is the minuscule signals generated by the SNE, which are often
mixed with the background noises. In this work, we overcome this difficulty by
studying the thin films of Ni70Cu30 alloy with not only a sizable spin Hall
angle but also a large Seebeck coefficient. We observe in the Ni70Cu30 alloy a
large spin Nernst effect with an estimated spin Nernst angle ranging from -28%
to -72%. In comparison, the spin Nernst angle for Pt is -8.2%. Our ab initio
calculation reveals that the large spin Nernst conductivity in Ni70Cu30 is
caused by the Fermi energy shift to the steepest slope of the spin Hall
conductivity curve due to electron doping from 30% Cu. Our study provides
critical directions in searching for materials with a large spin Nernst effect.",['cond-mat.mtrl-sci'],2503.13187," Magnetoelectric composites integrate the coupling between magnetic and
piezoelectric materials to create new functionalities for potential
technological applications. This coupling is typically achieved through the
exchange of magnetic, electric, or elastic energy across the interfaces between
the different constituent materials. Tailoring the strength of the
magnetoelectric effect is primarily accomplished by selecting suitable
materials for each constituent and by optimizing geometrical and
microstructural designs. Various composite architectures, such as (0-3), (2-2),
(1-3) and core-shell connectivities, have been studied to enhance
magnetoelectric coupling and other required physical properties in composites.
This review examines the latest advancements in magnetoelectric materials,
focusing on the impact of different interphase connectivity types on their
properties and performance. Before exploring magnetic-electric coupling, a
brief overview of the historical background of multiferroic magnetoelectric
composites is provided. Fundamental concepts underlying the magnetoelectric
effect, piezoelectricity, and the magnetostrictive effect are explained,
including their origins and examples of these materials' properties. So far,
three types of magnetoelectric composite connectivities have been investigated
experimentally: particulate composites (0-3), laminated and thin films (2-2),
sticks embedded in matrix, core-shell particles, and coaxial fibers. An outlook
on the prospects and scientific challenges in the field of multiferroic
magnetoelectric composites is given at the end of this review.",['cond-mat.mtrl-sci'],False,,,,Large Spin Nernst Effect in Ni70Cu30 Alloy,"Current Advances in Magnetoelectric Composites with Various Interphase
  Connectivity Types"
neg-d2-351,2025-02-04,,2502.02512," Recently, there has been an increasing interest in 6G technology for
integrated sensing and communications, where positioning stands out as a key
application. In the realm of 6G, cell-free massive multiple-input
multiple-output (MIMO) systems, featuring distributed base stations equipped
with a large number of antennas, present an abundant source of angle-of-arrival
(AOA) information that could be exploited for positioning applications. In this
paper we leverage this AOA information at the base stations using the multiple
signal classification (MUSIC) algorithm, in conjunction with received signal
strength (RSS) for positioning through Gaussian process regression (GPR). An
AOA fingerprint database is constructed by capturing the angle data from
multiple locations across the network area and is combined with RSS data from
the same locations to form a hybrid fingerprint which is then used to train a
GPR model employing a squared exponential kernel. The trained regression model
is subsequently utilized to estimate the location of a user equipment.
Simulations demonstrate that the GPR model with hybrid input achieves better
positioning accuracy than traditional GPR models utilizing RSS-only and
AOA-only inputs.",['eess.SP'],2502.11653," Learning-based algorithms have gained great popularity in communications
since they often outperform even carefully engineered solutions by learning
from training samples. In this paper, we show that the selection of appropriate
training examples can be important for the performance of such learning-based
algorithms. In particular, we consider non-linear 1-bit precoding for massive
multi-user MIMO systems using the C2PO algorithm. While previous works have
already shown the advantages of learning critical coefficients of this
algorithm, we demonstrate that straightforward selection of training samples
that follow the channel model distribution does not necessarily lead to the
best result. Instead, we provide a strategy to generate training data based on
the specific properties of the algorithm, which significantly improves its
error floor performance.",['eess.SP'],False,,,,Hybrid Fingerprint-based Positioning in Cell-Free Massive MIMO Systems,"Training Channel Selection for Learning-based 1-bit Precoding in Massive
  MU-MIMO"
neg-d2-352,2025-02-27,,2502.19774," The validity range of the widely used traditional effective range expansion
can be severely limited by the presence of a left-hand cut near the
two-particle threshold. Such a left-hand cut emerges in two-particle scattering
processes involving either a light particle exchange in the $t$-channel or a
particle exchange with a mass slightly heavier than the mass difference of the
two particles in the $u$-channel, which occurs in a wide range of physical
systems. We propose a new parameterization for the low-energy scattering
amplitude that incorporates these left-hand cuts arising from particle exchange
diagrams. This parameterization extends the convergence radius of the effective
range expansion beyond the branch point of the left-hand cut and is applicable
to a broad range of systems. The parameterization enables the extraction of
coupling strengths between the exchange particle and the scattering particles,
and reveals amplitude zeros resulting from the interplay between short- and
long-range interactions. We demonstrate the effectiveness of this new
parameterization through its application to $DD^*$ scattering with meson masses
obtained in a lattice QCD calculation.",['hep-ph'],2502.19045," The investigation of remnants associated with the QCD chiral critical point
is a primary objective in high-energy ion collision experiments. Numerous
studies indicate that a scaling relation between higher-order factorial moments
of hadron multiplicity distributions and the second factorial moment may serve
as a diagnostic tool for identifying the QCD critical point. However, we
demonstrate that this scaling behavior is not exclusive to critical phenomena
but rather arises as a general consequence of the phase-space partitioning
procedure employed in the analysis. This finding is examined in the context of
recent intermittency analyses conducted by the STAR experiment at RHIC.",['hep-ph'],False,,,,"Effective range expansion with the left-hand cut and its application to
  the $T_{cc}(3875)$",Scaling for count-in-cell and factorial moment analysis
neg-d2-353,2025-01-16,,2501.09591," Measuring inter-dataset similarity is an important task in machine learning
and data mining with various use cases and applications. Existing methods for
measuring inter-dataset similarity are computationally expensive, limited, or
sensitive to different entities and non-trivial choices for parameters. They
also lack a holistic perspective on the entire dataset. In this paper, we
propose two novel metrics for measuring inter-dataset similarity. We discuss
the mathematical foundation and the theoretical basis of our proposed metrics.
We demonstrate the effectiveness of the proposed metrics by investigating two
applications in the evaluation of synthetic data and in the evaluation of
feature selection methods. The theoretical and empirical studies conducted in
this paper illustrate the effectiveness of the proposed metrics.",['cs.LG'],2501.03654," Deep learning (DL) models have gained prominence in domains such as computer
vision and natural language processing but remain underutilized for regression
tasks involving tabular data. In these cases, traditional machine learning (ML)
models often outperform DL models. In this study, we propose and evaluate
various data augmentation (DA) techniques to improve the performance of DL
models for tabular data regression tasks. We compare the performance gain of
Neural Networks by different DA strategies ranging from a naive method of
duplicating existing observations and adding noise to a more sophisticated DA
strategy that preserves the underlying statistical relationship in the data.
Our analysis demonstrates that the advanced DA method significantly improves DL
model performance across multiple datasets and regression tasks, resulting in
an average performance increase of over 10\% compared to baseline models
without augmentation. The efficacy of these DA strategies was rigorously
validated across 30 distinct datasets, with multiple iterations and evaluations
using three different automated deep learning (AutoDL) frameworks: AutoKeras,
H2O, and AutoGluon. This study demonstrates that by leveraging advanced DA
techniques, DL models can realize their full potential in regression tasks,
thereby contributing to broader adoption and enhanced performance in practical
applications.",['cs.LG'],False,,,,"Metrics for Inter-Dataset Similarity with Example Applications in
  Synthetic Data and Feature Selection Evaluation -- Extended Version","Data Augmentation for Deep Learning Regression Tasks by Machine Learning
  Models"
neg-d2-354,2025-02-17,,2502.12495," Let $\phi$ be a collineation of $\mathrm{PG}\left(2, q^{3}\right)$ of order 3
which fixes a plane of order $q$ pointwise. The points of $\mathrm{PG}\left(2,
q^{3}\right)$ can be partitioned into three types with respect to orbits of
$\phi$ : fixed points; points $P$ with $P, P^{\phi}, P^{\phi^{2}}$ distinct and
collinear; and points $P$ with $P, P^{\phi}, P^{\phi^{2}}$ not collinear. Under
field reduction, the collineation $\phi$ corresponds to a projectivity $\sigma$
of $\operatorname{PG}(8, q)$ of order 3 . With respect to the field reduction
and the orbits of $\sigma$, the points of $\mathrm{PG}(8, q)$ can be
partitioned into six types. This article looks at the projectivity $\sigma$ in
detail, and classifies and counts the fixed points, fixed lines and fixed
planes. The motivation is to give a description of the lines of the Figueroa
projective plane in the $\mathrm{PG}(8, q)$ field reduction setting.",['math.CO'],2503.06377," In 2023, Greaves et~al.\ constructed several sets of 57 equiangular lines in
dimension 18. Using the concept of switching root introduced by Cao et~al.\ in
2021, these sets of equiangular lines are embedded in a lattice of rank 19
spanned by norm 3 vectors together with a switching root. We characterize this
lattice as an overlattice of the root lattice $A_9\oplus A_9\oplus A_1$, and
show that there are at least $246896$ sets of 57 equiangular lines in dimension
$18$ arising in this way, up to isometry. Additionally, we prove that all of
these sets of equiangular lines are strongly maximal. Here, a set of
equiangular lines is said to be strongly maximal if there is no set of
equiangular lines properly containing it even if the dimension of the
underlying space is increased. Among these sets, there are ones with only six
distinct Seidel eigenvalues.",['math.CO'],False,,,,"The planar projectivity of PG(2, $q^3$) of order 3 under field reduction","Sets of equiangular lines in dimension $18$ constructed from $A_9 \oplus
  A_9 \oplus A_1$"
neg-d2-355,2025-01-27,,2501.15885," Wireless charging pads are common, yet their functionality is mainly
restricted to charging. Existing gesture recognition techniques, such as those
based on machine vision and WiFi, have drawbacks like high costs and poor
precision. This paper presents a new human machine interaction solution using
multicoil wireless charging pads. The proposed approach leverages the pads
existing modules without additional wearable sensors. It determines gestures by
monitoring current and power changes in different coils. The data processing
includes noise removal, sorting, highpass filtering, and slicing. A Bayesian
network and particle filtering are employed for motion tracking. Through
experiments, this solution proves to have wide applications, high recognition
accuracy, and low cost. It can effectively identify diverse gestures,
increasing the value of wireless charging pads. It outperforms traditional
methods, with a 0.73 improvement in recognition accuracy and better
environmental adaptability.",['cs.HC'],2503.14096," In 3D design, specifying design objectives and visualizing complex shapes
through text alone proves to be a significant challenge. Although advancements
in 3D GenAI have significantly enhanced part assembly and the creation of
high-quality 3D designs, many systems still to dynamically generate and edit
design elements based on the shape parameters. To bridge this gap, we propose
GenPara, an interactive 3D design editing system that leverages
text-conditional shape parameters of part-aware 3D designs and visualizes
design space within the Exploration Map and Design Versioning Tree.
Additionally, among the various shape parameters generated by LLM, the system
extracts and provides design outcomes within the user's regions of interest
based on Bayesian inference. A user study N = 16 revealed that \textit{GenPara}
enhanced the comprehension and management of designers with text-conditional
shape parameters, streamlining design exploration and concretization. This
improvement boosted efficiency and creativity of the 3D design process.",['cs.HC'],False,,,,"A Low-Cost, High-Precision Human-Machine Interaction Solution Based on
  Multi-Coil Wireless Charging Pads","GenPara: Enhancing the 3D Design Editing Process by Inferring Users'
  Regions of Interest with Text-Conditional Shape Parameters"
neg-d2-356,2025-03-05,,2503.0339," Let C represent an irreducible algebraic space curve defined by the real
polynomials fi(x1, x2, x3) for i = 1, 2. It is a recognized fact that a
birational relationship invariably exists between the points on C and those on
an associated irreducible plane curve, denoted as Cp. In this work, we leverage
this established relationship to delineate the asymptotic behavior of C by
examining the asymptotes of Cp. Building on this foundation, we introduce a
novel and practical algorithm designed to efficiently compute the asymptotes of
C, given that the asymptotes of Cp have been ascertained.",['math.AG'],2502.04966," We study the geometry of the Hitchin fibration for $\mathcal{L}$-valued
$G$-Higgs bundles over a smooth projective curve of genus $g$, where $G$ is a
reductive group and $\mathcal{L}$ is a suitably positive line bundle. We show
that the Hitchin fibration admits the structure of a weak Abelian fibration. In
the case when the line bundle $\mathcal{L}$ is a twist of the canonical bundle
of the curve by a (possibly empty) reduced effective divisor, we prove a
cohomological bound and $\delta$-regularity of the weak Abelian fibration.",['math.AG'],False,,,,"Infinity Branches and Asymptotic Analysis of Algebraic Space Curves: New
  Techniques and Applications",Hitchin fibrations are Ng\^{o} fibrations
neg-d2-357,2025-01-14,,2501.08168," While autonomous driving technology has made remarkable strides, data-driven
approaches still struggle with complex scenarios due to their limited reasoning
capabilities. Meanwhile, knowledge-driven autonomous driving systems have
evolved considerably with the popularization of visual language models. In this
paper, we propose LeapVAD, a novel method based on cognitive perception and
dual-process thinking. Our approach implements a human-attentional mechanism to
identify and focus on critical traffic elements that influence driving
decisions. By characterizing these objects through comprehensive attributes -
including appearance, motion patterns, and associated risks - LeapVAD achieves
more effective environmental representation and streamlines the decision-making
process. Furthermore, LeapVAD incorporates an innovative dual-process
decision-making module miming the human-driving learning process. The system
consists of an Analytic Process (System-II) that accumulates driving experience
through logical reasoning and a Heuristic Process (System-I) that refines this
knowledge via fine-tuning and few-shot learning. LeapVAD also includes
reflective mechanisms and a growing memory bank, enabling it to learn from past
mistakes and continuously improve its performance in a closed-loop environment.
To enhance efficiency, we develop a scene encoder network that generates
compact scene representations for rapid retrieval of relevant driving
experiences. Extensive evaluations conducted on two leading autonomous driving
simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior
performance compared to camera-only approaches despite limited training data.
Comprehensive ablation studies further emphasize its effectiveness in
continuous learning and domain adaptation. Project page:
https://pjlab-adg.github.io/LeapVAD/.",['cs.AI'],2501.08168," While autonomous driving technology has made remarkable strides, data-driven
approaches still struggle with complex scenarios due to their limited reasoning
capabilities. Meanwhile, knowledge-driven autonomous driving systems have
evolved considerably with the popularization of visual language models. In this
paper, we propose LeapVAD, a novel method based on cognitive perception and
dual-process thinking. Our approach implements a human-attentional mechanism to
identify and focus on critical traffic elements that influence driving
decisions. By characterizing these objects through comprehensive attributes -
including appearance, motion patterns, and associated risks - LeapVAD achieves
more effective environmental representation and streamlines the decision-making
process. Furthermore, LeapVAD incorporates an innovative dual-process
decision-making module miming the human-driving learning process. The system
consists of an Analytic Process (System-II) that accumulates driving experience
through logical reasoning and a Heuristic Process (System-I) that refines this
knowledge via fine-tuning and few-shot learning. LeapVAD also includes
reflective mechanisms and a growing memory bank, enabling it to learn from past
mistakes and continuously improve its performance in a closed-loop environment.
To enhance efficiency, we develop a scene encoder network that generates
compact scene representations for rapid retrieval of relevant driving
experiences. Extensive evaluations conducted on two leading autonomous driving
simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior
performance compared to camera-only approaches despite limited training data.
Comprehensive ablation studies further emphasize its effectiveness in
continuous learning and domain adaptation. Project page:
https://pjlab-adg.github.io/LeapVAD/.",['cs.AI'],False,,,,"LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and
  Dual-Process Thinking","LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and
  Dual-Process Thinking"
neg-d2-358,2025-01-10,,2501.0618," Electricity price forecasting is a critical tool for the efficient operation
of power systems and for supporting informed decision-making by market
participants. This paper explores a novel methodology aimed at improving the
accuracy of electricity price forecasts by incorporating probabilistic inputs
of fundamental variables. Traditional approaches often rely on point forecasts
of exogenous variables such as load, solar, and wind generation. Our method
proposes the integration of quantile forecasts of these fundamental variables,
providing a new set of exogenous variables that account for a more
comprehensive representation of uncertainty. We conducted empirical tests on
the German electricity market using recent data to evaluate the effectiveness
of this approach. The findings indicate that incorporating probabilistic
forecasts of load and renewable energy source generation significantly improves
the accuracy of point forecasts of electricity prices. Furthermore, the results
clearly show that the highest improvement in forecast accuracy can be achieved
with full probabilistic forecast information. This highlights the importance of
probabilistic forecasting in research and practice, particularly that the
current state-of-the-art in reporting load, wind and solar forecast is
insufficient.",['stat.AP'],2503.06308," Computable phenotypes are used to characterize patients and identify outcomes
in studies conducted using healthcare claims and electronic health record data.
Chart review studies establish reference labels against which computable
phenotypes are compared to understand their measurement characteristics, the
quantity of interest, for instance the positive predictive value. We describe a
method to adaptively evaluate a quantity of interest over sequential samples of
charts, with the goal to minimize the number of charts reviewed. With the help
of a simultaneous confidence band, we stop the reviewing once the confidence
band meets a pre-specified stopping threshold. The contribution of this article
is threefold. First, we tested the use of an adaptive approach called Neyman's
sampling of charts versus random or stratified random sampling. Second, we
propose frequentist confidence bands and Bayesian credible intervals to
sequentially evaluate the quantity of interest. Third, we propose a tool to
predict the stopping time (defined as the number of charts reviewed) at which
the chart review would be complete. We observe that Bayesian credible intervals
proved to be tighter than its frequentist confidence band counterparts.
Moreover, we observe that simple random sampling is often performing similarly
to Neyman's sampling.",['stat.AP'],False,,,,"Probabilistic Forecasts of Load, Solar and Wind for Electricity Price
  Forecasting",Adaptive multi-wave sampling for efficient chart validation
neg-d2-359,2025-02-12,,2502.08836," The phenomenon of reflection is quite common in digital images, posing
significant challenges for various applications such as computer vision,
photography, and image processing. Traditional methods for reflection removal
often struggle to achieve clean results while maintaining high fidelity and
robustness, particularly in real-world scenarios. Over the past few decades,
numerous deep learning-based approaches for reflection removal have emerged,
yielding impressive results. In this survey, we conduct a comprehensive review
of the current literature by focusing on key venues such as ICCV, ECCV, CVPR,
NeurIPS, etc., as these conferences and journals have been central to advances
in the field. Our review follows a structured paper selection process, and we
critically assess both single-stage and two-stage deep learning methods for
reflection removal. The contribution of this survey is three-fold: first, we
provide a comprehensive summary of the most recent work on single-image
reflection removal; second, we outline task hypotheses, current deep learning
techniques, publicly available datasets, and relevant evaluation metrics; and
third, we identify key challenges and opportunities in deep learning-based
reflection removal, highlighting the potential of this rapidly evolving
research area.",['cs.CV'],2501.1687," Understanding emotional signals in older adults is crucial for designing
virtual assistants that support their well-being. However, existing affective
computing models often face significant limitations: (1) limited availability
of datasets representing older adults, especially in non-English-speaking
populations, and (2) poor generalization of models trained on younger or
homogeneous demographics. To address these gaps, this study evaluates
state-of-the-art affective computing models -- including facial expression
recognition, text sentiment analysis, and smile detection -- using videos of
older adults interacting with either a person or a virtual avatar. As part of
this effort, we introduce a novel dataset featuring Spanish-speaking older
adults engaged in human-to-human video interviews. Through three comprehensive
analyses, we investigate (1) the alignment between human-annotated labels and
automatic model outputs, (2) the relationships between model outputs across
different modalities, and (3) individual variations in emotional signals. Using
both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover
limited agreement between human annotations and model predictions, weak
consistency across modalities, and significant variability among individuals.
These findings highlight the shortcomings of generalized emotion perception
models and emphasize the need of incorporating personal variability and
cultural nuances into future systems.",['cs.CV'],False,,,,Survey on Single-Image Reflection Removal using Deep Learning Techniques,"Experimenting with Affective Computing Models in Video Interviews with
  Spanish-speaking Older Adults"
neg-d2-360,2025-02-08,,2502.05496," Outlier detection tasks aim at discovering potential issues or opportunities
and are widely used in cybersecurity, financial security, industrial
inspection, etc. To date, thousands of outlier detection algorithms have been
proposed. Clearly, in real-world scenarios, such a large number of algorithms
is unnecessary. In other words, a large number of outlier detection algorithms
are redundant. We believe the root cause of this redundancy lies in the current
highly customized (i.e., non-generic) optimization strategies. Specifically,
when researchers seek to improve the performance of existing outlier detection
algorithms, they have to design separate optimized versions tailored to the
principles of each algorithm, leading to an ever-growing number of outlier
detection algorithms. To address this issue, in this paper, we introduce the
explosion from physics into the outlier detection task and propose a generic
optimization strategy based on feature explosion, called OSD (Optimization
Strategy for outlier Detection algorithms). In the future, when improving the
performance of existing outlier detection algorithms, it will be sufficient to
invoke the OSD plugin without the need to design customized optimized versions
for them. We compared the performances of 14 outlier detection algorithms on 24
datasets before and after invoking the OSD plugin. The experimental results
show that the performances of all outlier detection algorithms are improved on
almost all datasets. In terms of average accuracy, OSD make these outlier
detection algorithms improve by 15% (AUC), 63.7% (AP).",['cs.LG'],2503.11965," We introduce a novel framework for learning in neural networks by decomposing
each neuron's weight vector into two distinct parts, $W_1$ and $W_2$, thereby
modeling contrastive information directly at the neuron level. Traditional
gradient descent stores both positive (target) and negative (non-target)
feature information in a single weight vector, often obscuring fine-grained
distinctions. Our approach, by contrast, maintains separate updates for target
and non-target features, ultimately forming a single effective weight $W = W_1
- W_2$ that is more robust to noise and class imbalance. Experimental results
on both regression (California Housing, Wine Quality) and classification
(MNIST, Fashion-MNIST, CIFAR-10) tasks suggest that this decomposition enhances
generalization and resists overfitting, especially when training data are
sparse or noisy. Crucially, the inference complexity remains the same as in the
standard $WX + \text{bias}$ setup, offering a practical solution for improved
learning without additional inference-time overhead.",['cs.LG'],False,,,,"Feature Explosion: a generic optimization strategy for outlier detection
  algorithms",Revisiting Gradient Descent: A Dual-Weight Method for Improved Learning
neg-d2-361,2025-01-20,,2501.11793," The treatment of intracranial aneurysms (IA) relies on angiography guidance
using biplane views. However, accurate flow estimation and device sizing for
treatment are often compromised by vessel overlap and foreshortening, which can
obscure critical details. This study introduces an epipolar reconstruction
approach to enhance 3D rendering of the internal carotid artery (ICA) and
aneurysm dome using routinely acquired biplane angiographic data. Our method
aims to improve procedural guidance by overcoming the limitations of
traditional two-dimensional imaging techniques. This study employed three 3D
geometries of ICA aneurysms to simulate virtual angiograms. These angiograms
were generated using a computational fluid dynamics (CFD) solver, followed by
the simulation of biplane angiography using a cone-beam geometry.
Self-calibration was accomplished by matching contrast media position as a
function of time between biplane views. Feature-matching was used to
triangulate and reconstruct vascular structures in 3D. The projection data was
utilized to refine the 3D estimation, including elimination of erroneous
structures and ellipse-fitting. The accuracy of the reconstructions was
evaluated using the Dice-Sorensen coefficient, comparing the 3D reconstructions
to the original models. The proposed epipolar reconstruction method generalized
well across the three tested aneurysm models, with respective Dice-Sorensen
coefficients of 0.745, 0.759, and 0.654. Errors were primarily due to partial
vessel overlap. The average reconstruction time for all three volumes was
approximately 10 seconds. The proposed epipolar reconstruction method enhanced
3D visualization, addressing challenges such as projection-induced vessel
foreshortening. This method provides a solution to the complexity of IA
visualization, with the potential to provide more accurate analysis and device
sizing for treatment.",['physics.med-ph'],2501.07348," Full Waveform Inversion (FWI) is a promising technique for achieving
high-resolution imaging in medical ultrasound. Traditional FWI methods suffer
from issues related to computational efficiency, dependence on initial models,
and the inability to quantify uncertainty. This study introduces the Stein
Variational Gradient Descent (SVGD) algorithm into FWI, aiming to improve
inversion performance and enhance uncertainty quantification. By deriving the
posterior gradient, the study explores the integration of SVGD with FWI and
demonstrates its ability to approximate complex priors. In-silico experiments
with synthetic data and real-world breast tissue data highlight the advantages
of the SVGD-based framework over conventional FWI. SVGD-based FWI improves
inversion quality, provides more reliable uncertainty quantification, and
offers a tighter bound for the prior distribution. These findings show that
probabilistic inversion is a promising tool for addressing the limitations of
traditional FWI methods in ultrasonic imaging of medical tissues.",['physics.med-ph'],False,,,,"Self-Calibrated Epipolar Reconstruction for Assessment of Aneurysms in
  the Internal Carotid Artery Using In-Silico Biplane Angiograms","Ultrasonic Medical Tissue Imaging Using Probabilistic Inversion:
  Leveraging Variational Inference for Speed Reconstruction and Uncertainty
  Quantification"
neg-d2-362,2025-02-14,,2502.10664," Nonequilibrium multi-carrier thermal transport is essential for both
scientific research and technological applications in electronic, spintronic,
and energy conversion devices. This article reviews the fundamentals of phonon,
electron, spin, and ion transport driven by temperature gradients in
solid-state and soft condensed matters, and the microscopic interactions
between energy/charge carriers that can be leveraged for manipulating
electrical and thermal transport in energy conversion devices, such as
electron-phonon coupling, spin-phonon interaction, and ion-solvent
interactions, etc. In coupled electron-phonon transport, we discuss the basics
of electron-phonon interactions and their effects on phonon dynamics,
thermalization, and nonequilibrium thermal transport. For the phonon-spin
interaction, nonequilibrium transport formulation is introduced first, followed
by the physics of spin thermoelectric effect and strategies to manipulate them.
Contributions to thermal conductivity from magnons as heat carriers are also
reviewed. For coupled transport of heat and ions/molecules, we highlight the
importance of local molecular configurations that determine the magnitude of
the electrochemical gradient, which is the key to improving the efficiency of
low-grade heat energy conversion.",['cond-mat.mtrl-sci'],2501.12455," In this work we report on a self-assembled growth of a Ge quantum dot lattice
in a single 600-nm-thick Ge+Al2O3 layer during magnetron sputtering deposition
of a Ge+Al2O3 mixture at an elevated substrate temperature. The self-assembly
results in the formation of a well-ordered threedimensional body-centered
tetragonal quantum dot lattice within the whole deposited volume. The quantum
dots formed are very small in size less than 4.0 nm, have a narrow size
distribution and a large packing density. The parameters of the quantum dot
lattice can be tuned by changing the deposition parameters. The self-ordering
of the quantum dots is explained by diffusionmediated nucleation and
surface-morphology effects and simulated by a kinetic Monte Carlo model.",['cond-mat.mtrl-sci'],False,,,,"Multi-Carrier Thermal Transport in Electronic and Energy Conversion
  Devices",Self-assembling of Ge quantum dots in an alumina matrix
neg-d2-363,2025-03-20,,2503.16355," In recent years, the study of secondary anisotropies in the Cosmic Microwave
Background has become a fundamental instrument to test our understanding of
Cosmology and Astrophysics. Using a set of lightcones produced with the ``Dark
Energy and Massive Neutrino Universe'' $N$-body simulations we study how
different dark energy models and neutrino masses impact the properties of the
Sunyaev-Zel'dovich (SZ) effects, focusing on the signal arising from galaxy
clusters and groups. We analyse the distribution of values, Compton-$y$
parameter for the thermal SZ effect and $\Delta T/T$ for the kinematic SZ
effect, and study their angular power spectra. We find that the distribution of
logarithmic Compton parameter can be fitted with a skewed Gaussian, with a mean
that, at fixed dark energy model, decreases linearly with an approximate slope
of $10 f_\nu$. Regarding the power spectrum of the thermal SZ effect, we find
that an increase in $\sum {m_\nu}$ is observed as a power-law scaling with
respect to $\sigma_8^{\mathrm{cb}}$, with exponents ranging from 7.2 to 8.2. We
also find that four cosmological models, one with $\sum {m_\nu} = 0.16$ eV and
three with $\sum {m_\nu} = 0.32$ eV, fit equally well the Planck data for the
Compton-$y$. For all the \texttt{DEMNUni} models we forecast the cumulative
signal-to-noise for thermal SZ observations with the LAT instrument of Simons
Observatory; furthermore, we compute a tailored $\chi_\mathrm{SNR}^2$ estimator
to infer if they can be distinguished from the reference $\Lambda$CDM. We also
provide estimates for the power spectrum of the cluster component of the
kinematic SZ effect, in all the different cosmological scenarios.",['astro-ph.CO'],2501.18333," Deep learning and convolutional neural networks in particular are powerful
and promising tools for cosmological analysis of large-scale structure surveys.
They are already providing similar performance to classical analysis methods
using fixed summary statistics, are showing potential to break key degeneracies
by better probe combination and will likely improve rapidly in the coming years
as progress is made in the physical modelling through both software and
hardware improvement. One key issue remains: unlike classical analysis, a
convolutional neural network's decision process is hidden from the user as the
network optimises millions of parameters with no direct physical meaning. This
prevents a clear understanding of the potential limitations and biases of the
analysis, making it hard to rely on as a main analysis method. In this work, we
explore the behaviour of such a convolutional neural network through a novel
method. Instead of trying to analyse a network a posteriori, i.e. after
training has been completed, we study the impact on the constraining power of
training the network and predicting parameters with degraded data where we
removed part of the information. This allows us to gain an understanding of
which parts and features of a large-scale structure survey are most important
in the network's prediction process. We find that the network's prediction
process relies on a mix of both Gaussian and non-Gaussian information, and
seems to put an emphasis on structures whose scales are at the limit between
linear and non-linear regimes.",['astro-ph.CO'],False,,,,"DEMNUni: the Sunyaev-Zel'dovich effect in the presence of massive
  neutrinos and dynamical dark energy","Interpretability of deep-learning methods applied to large-scale
  structure surveys"
neg-d2-364,2025-02-21,,2502.15401," In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.",['cs.CL'],2502.12611," The rise of Large Language Models (LLMs) necessitates accurate AI-generated
text detection. However, current approaches largely overlook the influence of
author characteristics. We investigate how sociolinguistic attributes-gender,
CEFR proficiency, academic field, and language environment-impact
state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored
texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous
evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our
results reveal significant biases: CEFR proficiency and language environment
consistently affected detector accuracy, while gender and academic field showed
detector-dependent effects. These findings highlight the crucial need for
socially aware AI text detection to avoid unfairly penalizing specific
demographic groups. We offer novel empirical evidence, a robust statistical
framework, and actionable insights for developing more equitable and reliable
detection systems in real-world, out-of-domain contexts. This work paves the
way for future research on bias mitigation, inclusive evaluation benchmarks,
and socially responsible LLM detectors.",['cs.CL'],False,,,,"Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs
  Complex Reasoning","Who Writes What: Unveiling the Impact of Author Roles on AI-generated
  Text Detection"
neg-d2-365,2025-03-19,,2503.15737," Named Entity Recognition (NER) is a fundamental task in Natural Language
Processing (NLP) that plays a crucial role in information extraction, question
answering, and knowledge-based systems. Traditional deep learning-based NER
models often struggle with domain-specific generalization and suffer from data
sparsity issues. In this work, we introduce Knowledge Graph distilled for Named
Entity Recognition (KoGNER), a novel approach that integrates Knowledge Graph
(KG) distillation into NER models to enhance entity recognition performance.
Our framework leverages structured knowledge representations from KGs to enrich
contextual embeddings, thereby improving entity classification and reducing
ambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge
Distillation, where external knowledge sources are distilled into a lightweight
representation for seamless integration with NER models, and (2) Entity-Aware
Augmentation, which integrates contextual embeddings that have been enriched
with knowledge graph information directly into GNN, thereby improving the
model's ability to understand and represent entity relationships. Experimental
results on benchmark datasets demonstrate that KoGNER achieves state-of-the-art
performance, outperforming finetuned NER models and LLMs by a significant
margin. These findings suggest that leveraging knowledge graphs as auxiliary
information can significantly improve NER accuracy, making KoGNER a promising
direction for future research in knowledge-aware NLP.",['cs.CL'],2502.14734," We propose the Sentence Smith framework that enables controlled and specified
manipulation of text meaning. It consists of three main steps: 1. Parsing a
sentence into a semantic graph, 2. Applying human-designed semantic
manipulation rules, and 3. Generating text from the manipulated graph. A final
filtering step (4.) ensures the validity of the applied transformation. To
demonstrate the utility of Sentence Smith in an application study, we use it to
generate hard negative pairs that challenge text embedding models. Since the
controllable generation makes it possible to clearly isolate different types of
semantic shifts, we can gain deeper insights into the specific strengths and
weaknesses of widely used text embedding models, also addressing an issue in
current benchmarking where linguistic phenomena remain opaque. Human validation
confirms that the generations produced by Sentence Smith are highly accurate.",['cs.CL'],False,,,,"KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical
  Named Entity Recognition","Sentence Smith: Formally Controllable Text Transformation and its
  Application to Evaluation of Text Embedding Models"
neg-d2-366,2025-03-22,,2503.17898," We provide a concise framework for generalized ensemble theory through a
matrix-based approach. By introducing an observation matrix, any discrete
probability distribution, including those for non-equilibrium steady states,
can be expressed as a generalized Boltzmann distribution, with observables and
conjugate variables as the basis and coordinates in a linear space. In this
framework, we identify the minimal sufficient statistics required for inferring
the Boltzmann distribution. Furthermore, we show that the Hadamard and
Vandermonde matrices are suitable observation matrices for spin systems and
random walks. In master equation systems, the probability flux observation
matrix facilitates the identification of detailed balance violations. Our
findings provide a new approach to developing generalized ensemble theory for
non-equilibrium steady-state systems.",['cond-mat.stat-mech'],2503.17898," We provide a concise framework for generalized ensemble theory through a
matrix-based approach. By introducing an observation matrix, any discrete
probability distribution, including those for non-equilibrium steady states,
can be expressed as a generalized Boltzmann distribution, with observables and
conjugate variables as the basis and coordinates in a linear space. In this
framework, we identify the minimal sufficient statistics required for inferring
the Boltzmann distribution. Furthermore, we show that the Hadamard and
Vandermonde matrices are suitable observation matrices for spin systems and
random walks. In master equation systems, the probability flux observation
matrix facilitates the identification of detailed balance violations. Our
findings provide a new approach to developing generalized ensemble theory for
non-equilibrium steady-state systems.",['cond-mat.stat-mech'],False,,,,Matrix approach to generalized ensemble theory,Matrix approach to generalized ensemble theory
neg-d2-367,2025-02-06,,2502.03891," When a retrieval system receives a query it has encountered before, previous
relevance feedback, such as clicks or explicit judgments can help to improve
retrieval results. However, the content of a previously relevant document may
have changed, or the document might not be available anymore. Despite this
evolved corpus, we counterfactually use these previously relevant documents as
relevance signals. In this paper we proposed approaches to rewrite user queries
and compare them against a system that directly uses the previous qrels for the
ranking. We expand queries with terms extracted from the previously relevant
documents or derive so-called keyqueries that rank the previously relevant
documents to the top of the current corpus. Our evaluation in the CLEF LongEval
scenario shows that rewriting queries with historical relevance feedback
improves the retrieval effectiveness and even outperforms computationally
expensive transformer-based approaches.",['cs.IR'],2502.16865," We present a multimodal search tool that facilitates retrieval of chemical
reactions, molecular structures, and associated text from scientific
literature. Queries may combine molecular diagrams, textual descriptions, and
reaction data, allowing users to connect different representations of chemical
information. To support this, the indexing process includes chemical diagram
extraction and parsing, extraction of reaction data from text in tabular form,
and cross-modal linking of diagrams and their mentions in text. We describe the
system's architecture, key functionalities, and retrieval process, along with
expert assessments of the system. This demo highlights the workflow and
technical components of the search system.",['cs.IR'],False,,,,Counterfactual Query Rewriting to Use Historical Relevance Feedback,Multimodal Search in Chemical Documents and Reactions
neg-d2-368,2025-01-09,,2501.04984," The so-called KP-mKP hierarchy, which was introduced recently via
pseudo-differential operators with two derivations, can be reduced to the
Kadomtsev-Petviashvili (KP), the modified KP (mKP) and the two-component BKP
hierarchies. In this note, we continue to study reductions properties of the
KP-mKP hierarchy, including its $(n,m)$-reduction and its reduction to a
certain extended $r$-reduced KP hierarchy (the $r$-th Gelfand-Dickey together
with its wave function). As a byproduct, we show that the Hirota equations of
the extended $r$-reduced KP hierarchy follow from those of the mKP hierarchy,
which confirms a conjecture of Alexandrov on the open KdV hierarchy in [ J.
High Energy Phys. 2015 ].",['nlin.SI'],2501.04984," The so-called KP-mKP hierarchy, which was introduced recently via
pseudo-differential operators with two derivations, can be reduced to the
Kadomtsev-Petviashvili (KP), the modified KP (mKP) and the two-component BKP
hierarchies. In this note, we continue to study reductions properties of the
KP-mKP hierarchy, including its $(n,m)$-reduction and its reduction to a
certain extended $r$-reduced KP hierarchy (the $r$-th Gelfand-Dickey together
with its wave function). As a byproduct, we show that the Hirota equations of
the extended $r$-reduced KP hierarchy follow from those of the mKP hierarchy,
which confirms a conjecture of Alexandrov on the open KdV hierarchy in [ J.
High Energy Phys. 2015 ].",['nlin.SI'],False,,,,Reduction properties of the KP-mKP hierarchy,Reduction properties of the KP-mKP hierarchy
neg-d2-369,2025-02-20,,2502.14734," We propose the Sentence Smith framework that enables controlled and specified
manipulation of text meaning. It consists of three main steps: 1. Parsing a
sentence into a semantic graph, 2. Applying human-designed semantic
manipulation rules, and 3. Generating text from the manipulated graph. A final
filtering step (4.) ensures the validity of the applied transformation. To
demonstrate the utility of Sentence Smith in an application study, we use it to
generate hard negative pairs that challenge text embedding models. Since the
controllable generation makes it possible to clearly isolate different types of
semantic shifts, we can gain deeper insights into the specific strengths and
weaknesses of widely used text embedding models, also addressing an issue in
current benchmarking where linguistic phenomena remain opaque. Human validation
confirms that the generations produced by Sentence Smith are highly accurate.",['cs.CL'],2502.11364," While multilingual large language models generally perform adequately, and
sometimes even rival English performance on high-resource languages (HRLs),
they often significantly underperform on low-resource languages (LRLs). Among
several prompting strategies aiming at bridging the gap, multilingual
in-context learning (ICL) has been particularly effective when demonstration in
target languages is unavailable. However, there lacks a systematic
understanding of when and why it works well.
  In this work, we systematically analyze multilingual ICL, using
demonstrations in HRLs to enhance cross-lingual transfer. We show that
demonstrations in mixed HRLs consistently outperform English-only ones across
the board, particularly for tasks written in LRLs. Surprisingly, our ablation
study shows that the presence of irrelevant non-English sentences in the prompt
yields measurable gains, suggesting the effectiveness of multilingual exposure
itself. Our results highlight the potential of strategically leveraging
multilingual resources to bridge the performance gap for underrepresented
languages.",['cs.CL'],False,,,,"Sentence Smith: Formally Controllable Text Transformation and its
  Application to Evaluation of Text Embedding Models","Blessing of Multilinguality: A Systematic Analysis of Multilingual
  In-Context Learning"
neg-d2-370,2025-02-27,,2502.19814," A system of inhomogeneous second-order difference equations with linear parts
given by noncommutative matrix coefficients are considered. Closed form of its
solution is derived by means of newly defined delayed matrix sine/cosine using
the Z-transform and determining function.",['math.DS'],2503.03457," For a minimal Anosov $\mathbb R^{\kappa}$-action on a closed manifold, we
study the measure of maximal entropy constructed by Carrasco and
Rodriguez-Hertz in \cite{CarHer} and show that it fits into the theory of
Ruelle-Taylor resonances introduced by Guedes Bonthonneau, Guillarmou, Hilgert,
and Weich in \cite{GBGHW}. More precisely, we show that the topological entropy
corresponds to the first Ruelle-Taylor resonance for the action on a certain
bundle of forms and that the measure of maximal entropy can be retrieved as the
distributional product of the corresponding resonant and co-resonant states. As
a consequence, we prove a Bowen-type formula for the measure of maximal entropy
and a counting result on the number of periodic torii.",['math.DS'],False,,,,Explicit solution of second-order delayed discrete equations,Measure of maximal entropy for minimal Anosov actions
neg-d2-371,2025-01-03,,2501.02148," Quantum machine learning for classical data is currently perceived to have a
scalability problem due to (i) a bottleneck at the point of loading data into
quantum states, (ii) the lack of clarity around good optimization strategies,
and (iii) barren plateaus that occur when the model parameters are randomly
initialized. In this work, we propose techniques to address all of these
issues. First, we present a quantum classifier that encodes both the input and
the output as binary strings which results in a model that has no restrictions
on expressivity over the encoded data but requires fast classical compression
of typical high-dimensional datasets to only the most predictive degrees of
freedom. Second, we show that if one parameter is updated at a time, quantum
models can be trained without using a classical optimizer in a way that
guarantees convergence to a local minimum, something not possible for classical
deep learning models. Third, we propose a parameter initialization strategy
called sub-net initialization to avoid barren plateaus where smaller models,
trained on more compactly encoded data with fewer qubits, are used to
initialize models that utilize more qubits. Along with theoretical arguments on
efficacy, we demonstrate the combined performance of these methods on subsets
of the MNIST dataset for models with an all-to-all connected architecture that
use up to 16 qubits in simulation. This allows us to conclude that the loss
function consistently decreases as the capability of the model, measured by the
number of parameters and qubits, increases, and this behavior is maintained for
datasets of varying complexity. Together, these techniques offer a coherent
framework for scalable quantum machine learning.",['quant-ph'],2501.17407," In quantum mechanics time is generally treated as a parameter rather than an
observable. For instance wave functions are treated as extending in space, but
not in time. But from relativity we expect time and space should be treated on
the same basis. What are the effects if time is an observable? Are these
effects observable with current technology?
  In earlier work we showed we should see effects in various high energy
scattering processes. We here extend that work to include bound states. The
critical advantage of working with bound states is that the predictions are
significantly more definite, taking the predictions from testable to
falsifiable.
  We estimate the time dispersion for hydrogen as $.177$ attoseconds, possibly
below the current threshold for detection. But the time dispersion should scale
as the $3/2$ power of the principle quantum number $n$. Rydberg atoms can have
$n$ of order $100$, implying a boost by a factor of $1000$. This takes the the
time dispersion to $177$ attoseconds, well within reach of current technology.
  There are a wide variety of experimental targets: any time-dependent
processes should show effects. Falsification will be technically challenging
(due to the short time scales) but immediate and unambiguous. Confirmation
would have significant implications for attosecond physics, quantum computing
and communications, quantum gravity, and the measurement problem. And would
suggest practical uses in these areas as well as circuit design, high speed
biochemistry, cryptography, fusion research, and any area involving change at
attosecond time scales.",['quant-ph'],False,,,,"Bit-bit encoding, optimizer-free training and sub-net initialization:
  techniques for scalable quantum machine learning",Time dispersion in bound states
neg-d2-372,2025-01-28,,2501.1687," Understanding emotional signals in older adults is crucial for designing
virtual assistants that support their well-being. However, existing affective
computing models often face significant limitations: (1) limited availability
of datasets representing older adults, especially in non-English-speaking
populations, and (2) poor generalization of models trained on younger or
homogeneous demographics. To address these gaps, this study evaluates
state-of-the-art affective computing models -- including facial expression
recognition, text sentiment analysis, and smile detection -- using videos of
older adults interacting with either a person or a virtual avatar. As part of
this effort, we introduce a novel dataset featuring Spanish-speaking older
adults engaged in human-to-human video interviews. Through three comprehensive
analyses, we investigate (1) the alignment between human-annotated labels and
automatic model outputs, (2) the relationships between model outputs across
different modalities, and (3) individual variations in emotional signals. Using
both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover
limited agreement between human annotations and model predictions, weak
consistency across modalities, and significant variability among individuals.
These findings highlight the shortcomings of generalized emotion perception
models and emphasize the need of incorporating personal variability and
cultural nuances into future systems.",['cs.CV'],2501.09281," In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.",['cs.CV'],False,,,,"Experimenting with Affective Computing Models in Video Interviews with
  Spanish-speaking Older Adults",SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection
neg-d2-373,2025-03-10,,2503.07077," Traditional rule-based decision-making methods with interpretable advantage,
such as finite state machine, suffer from the jitter or deadlock(JoD) problems
in extremely dynamic scenarios. To realize agent swarm confrontation, decision
conflicts causing many JoD problems are a key issue to be solved. Here, we
propose a novel decision-making framework that integrates probabilistic finite
state machine, deep convolutional networks, and reinforcement learning to
implement interpretable intelligence into agents. Our framework overcomes state
machine instability and JoD problems, ensuring reliable and adaptable decisions
in swarm confrontation. The proposed approach demonstrates effective
performance via enhanced human-like cooperation and competitive strategies in
the rigorous evaluation of real experiments, outperforming other methods.",['cs.AI'],2502.07709," Open-ended learning agents must efficiently prioritize goals in vast
possibility spaces, focusing on those that maximize learning progress (LP).
When such autotelic exploration is achieved by LLM agents trained with online
RL in high-dimensional and evolving goal spaces, a key challenge for LP
prediction is modeling one's own competence, a form of metacognitive
monitoring. Traditional approaches either require extensive sampling or rely on
brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive
framework that lets LLM agents learn to predict their competence and LP online.
By capturing semantic relationships between goals, MAGELLAN enables
sample-efficient LP estimation and dynamic adaptation to evolving goal spaces
through generalization. In an interactive learning environment, we show that
MAGELLAN improves LP prediction efficiency and goal prioritization, being the
only method allowing the agent to fully master a large and evolving goal space.
These results demonstrate how augmenting LLM agents with a metacognitive
ability for LP predictions can effectively scale curriculum learning to
open-ended goal spaces.",['cs.AI'],False,,,,Rule-Based Conflict-Free Decision Framework in Swarm Confrontation,"MAGELLAN: Metacognitive predictions of learning progress guide autotelic
  LLM agents in large goal spaces"
neg-d2-374,2025-03-01,,2503.00559," Brain tissue accommodates non-linear deformations and exhibits time-dependent
mechanical behaviour. The latter is one of the most pronounced features of
brain tissue, manifesting itself primarily through viscoelastic effects such as
stress relaxation. To investigate its viscoelastic behaviour, we performed
ramp-and-hold relaxation tests in torsion on freshly slaughtered cylindrical
ovine brain samples ($25\,\,\text{mm}$ diameter and $\sim 10\,\,\text{mm}$
height). The tests were conducted using a commercial rheometer at varying twist
rates of $\{40,240,400\}\,\,\text{rad}\,\,\text{m}^{-1}\,\,\text{s}^{-1}$, with
the twist remaining fixed at $\sim 88\,\,\text{rad}\,\,\text{m}^{-1}$, which
generated two independent datasets for torque and normal force. The complete
set of viscoelastic material parameters was estimated via a simultaneous fit to
the analytical expressions for the torque and normal force predicted by the
modified quasi-linear viscoelastic model. The model's predictions were further
validated through finite element simulations in FEniCS. Our results show that
the modified quasi-linear viscoelastic model - recently reappraised and largely
unexploited - accurately fits the experimental data. Moreover, the estimated
material parameters are in line with those obtained in previous studies on
brain samples under torsion. When coupled with bespoke finite element models,
these material parameters could enhance our understanding of the forces and
deformations involved in traumatic brain injury and contribute to the design of
improved headgear for sports such as boxing and motorsports. On the other hand,
our novel testing protocol offers new insights into the mechanical behaviour of
soft tissues other than the brain.",['cond-mat.soft'],2502.00485," Using molecular dynamics simulations of a binary Lennard-Jones model of
glass-forming liquids, we examine how the decay of the normalized
neighbor-persistence function $C_{\rm B}(t)$, which decays from unity at short
times to zero at long times as particles lose the neighbors that were present
in their original first coordination shell, compares with those of other, more
conventionally utilized relaxation metrics. In the strongly-non-Arrhenius
temperature regime below the onset temperature $T_{\rm A}$, we find that
$C_{\rm B}(t)$ can be described using the same stretched-exponential functional
form that is often utilized to fit the self-intermediate scattering function
$S(q, t)$ of glass-forming liquids in this regime. The ratio of the bond
lifetime $\tau_{\rm bond}$ associated with the terminal decay of $C_{\rm B}(t)$
to the $\alpha$-relaxation time $\tau_\alpha$ varies appreciably and
non-monotonically with $T$, peaking at $\tau_{\rm bond}/\tau_\alpha \simeq 45$
at $T \simeq T_{\rm x}$, where $T_{\rm x}$ is a crossover temperature
separating the high- and low-temperature regimes of glass-formation. In
contrast, $\tau_{\rm bond}$ remains on the order of the overlap time $\tau_{\rm
ov}$ (the time interval over which a typical particle moves by half its
diameter), and the peak time $\tau_\chi$ for the susceptibility $\chi_{\rm
B}(t)$ associated with the spatial heterogeneity of $C_{\rm B}(t)$ remains on
the order of $\tau_{\rm imm}$ (the characteristic lifetime of immobile-particle
clusters), even as each of these quantities varies by roughly $5$ orders of
magnitude over our studied range of $T$. Thus, we show that $C_{\rm B}(t)$ and
$\chi_{\rm B}(t)$ provide semi-quantitative spatially-averaged measures of the
slow heterogeneous dynamics associated with the persistence of
immobile-particle clusters.",['cond-mat.soft'],False,,,,"Modelling the non-linear viscoelastic behaviour of brain tissue in
  torsion","Quantitative relations between nearest-neighbor persistence and slow
  heterogeneous dynamics in supercooled liquids"
neg-d2-375,2025-01-31,,2502.00227," This paper presents a single-life reinforcement learning (SLRL) approach to
adaptively select the dimension of the Krylov subspace during the generalized
minimal residual (GMRES) iteration. GMRES is an iterative algorithm for solving
large and sparse linear systems of equations in the form of \(Ax = b\) which
are mainly derived from partial differential equations (PDEs). The proposed
framework uses RL to adjust the Krylov subspace dimension (m) in the GMRES (m)
algorithm. This research demonstrates that altering the dimension of the Krylov
subspace in an online setup using SLRL can accelerate the convergence of the
GMRES algorithm by more than an order of magnitude. A comparison of different
matrix sizes and sparsity levels is performed to demonstrate the effectiveness
of adaptive Krylov subspace exploration using single-life RL (AK-SLRL). We
compare AK-SLRL with constant-restart GMRES by applying the highest restart
value used in AK-SLRL to the GMRES method. The results show that using an
adjustable restart parameter with single-life soft-actor critic (SLSAC) and an
experience replay buffer sized to half the matrix dimension converges
significantly faster than the constant restart GMRES with higher values. Higher
values of the restart parameter are equivalent to a higher number of Arnoldi
iterations to construct an orthonormal basis for the Krylov subspace $ K_m(A,
r_0) $. This process includes constructing $m$ orthonormal vectors and updating
the Hessenberg matrix $H$. Therefore, lower values of $m$ result in reduced
computation needed in GMRES minimization to solve the least-squares problem in
the smaller Hessenberg matrix. The robustness of the result is validated
through a wide range of matrix dimensions and sparsity. This paper contributes
to the series of RL combinations with numerical solvers to achieve accelerated
scientific computing.",['cs.CE'],2501.1113," This study proposes a new full-field approach for modeling grain boundary
pinning by second phase particles in two-dimensional polycrystals. These
particles are of great importance during thermomechanical treatments, as they
produce deviations from the microstructural evolution that the alloy produces
in the absence of particles. This phenomenon, well-known as Smith-Zener
pinning, is widely used by metallurgists to control the grain size during the
metal forming process of many alloys. Predictive tools are then needed to
accurately model this phenomenon. This article introduces a new methodology for
the simulation of microstructural evolutions subjected to the presence of
second phase particles. The methodology employs a Lagrangian 2D front-tracking
methodology, while the particles are modeled using discretized circular shapes
or pinning nodes. The evolution of the particles can be considered and modeled
using a constant velocity of particle shrinking. This approach has the
advantages of improving the limited description made of the phenomenon in
vertex approaches, to be usable for a wide range of second-phase particle sizes
and to improve calculation times compared to front-capturing type approaches.",['cs.CE'],False,,,,"AK-SLRL: Adaptive Krylov Subspace Exploration Using Single-Life
  Reinforcement Learning for Sparse Linear System","Efficient and accurate simulation of the Smith-Zener pinning mechanism
  during grain growth using a front-tracking numerical framework"
neg-d2-376,2025-02-07,,2502.04966," We study the geometry of the Hitchin fibration for $\mathcal{L}$-valued
$G$-Higgs bundles over a smooth projective curve of genus $g$, where $G$ is a
reductive group and $\mathcal{L}$ is a suitably positive line bundle. We show
that the Hitchin fibration admits the structure of a weak Abelian fibration. In
the case when the line bundle $\mathcal{L}$ is a twist of the canonical bundle
of the curve by a (possibly empty) reduced effective divisor, we prove a
cohomological bound and $\delta$-regularity of the weak Abelian fibration.",['math.AG'],2503.16255," We investigate limit linear series on chains of elliptic curves, giving a
simple proof of a conjecture of Farkas stating the existence of curves with a
theta-characteristic with a given number of sections for the expected range of
genera. Using the additional structure afforded by considering limit linear
series on chains of elliptic curves, we find examples of reducible
Brill-Noether loci, admitting at least two components, with and without a
theta-characteristic respectively. This allows us to display reducible Hilbert
schemes for $r\ge 3$ and the largest possible value of $d$, namely $d=g-1$. We
also give examples of Brill-Noether loci with three components. On the positive
side, we provide optimal bounds on the degree under which Brill-Noether loci
are irreducible when $r=2$.",['math.AG'],False,,,,Hitchin fibrations are Ng\^{o} fibrations,Some reducible and irreducible Brill-Noether loci
neg-d2-377,2025-01-27,,2501.1621," Despite extensive research and development of tools and technologies for
misinformation tracking and detection, we often find ourselves largely on the
losing side of the battle against misinformation. In an era where
misinformation poses a substantial threat to public discourse, trust in
information sources, and societal and political stability, it is imperative
that we regularly revisit and reorient our work strategies. While we have made
significant strides in understanding how and why misinformation spreads, we
must now broaden our focus and explore how technology can help realise new
approaches to address this complex challenge more efficiently.",['cs.SI'],2503.09281," Accurate graph annotation typically requires substantial labeled data, which
is often challenging and resource-intensive to obtain. In this paper, we
present Crowdsourced Homophily Ties Based Graph Annotation via Large Language
Model (CSA-LLM), a novel approach that combines the strengths of crowdsourced
annotations with the capabilities of large language models (LLMs) to enhance
the graph annotation process. CSA-LLM harnesses the structural context of graph
data by integrating information from 1-hop and 2-hop neighbors. By emphasizing
homophily ties - key connections that signify similarity within the graph -
CSA-LLM significantly improves the accuracy of annotations. Experimental
results demonstrate that this method enhances the performance of Graph Neural
Networks (GNNs) by delivering more precise and reliable annotations.",['cs.SI'],False,,,,New Frontiers in Fighting Misinformation,"Crowdsourced Homophily Ties Based Graph Annotation Via Large Language
  Model"
neg-d2-378,2025-01-21,,2501.12234," For many tasks, multi-robot teams often provide greater efficiency,
robustness, and resiliency. However, multi-robot collaboration in real-world
scenarios poses a number of major challenges, especially when dynamic robots
must balance competing objectives like formation control and obstacle avoidance
in the presence of stochastic dynamics and sensor uncertainty. In this paper,
we propose a distributed, multi-agent receding-horizon feedback motion planning
approach using Probably Approximately Correct Nonlinear Model Predictive
Control (PAC-NMPC) that is able to reason about both model and measurement
uncertainty to achieve robust multi-agent formation control while navigating
cluttered obstacle fields and avoiding inter-robot collisions. Our approach
relies not only on the underlying PAC-NMPC algorithm but also on a terminal
cost-function derived from gyroscopic obstacle avoidance. Through numerical
simulation, we show that our distributed approach performs on par with a
centralized formulation, that it offers improved performance in the case of
significant measurement noise, and that it can scale to more complex dynamical
systems.",['cs.RO'],2501.15272," Transporting a heavy payload using multiple aerial robots (MARs) is an
efficient manner to extend the load capacity of a single aerial robot. However,
existing schemes for the multiple aerial robots transportation system (MARTS)
still lack the capability to generate a collision-free and dynamically feasible
trajectory in real-time and further track an agile trajectory especially when
there are no sensors available to measure the states of payload and cable.
Therefore, they are limited to low-agility transportation in simple
environments. To bridge the gap, we propose complete planning and control
schemes for the MARTS, achieving safe and agile aerial transportation (SAAT) of
a cable-suspended payload in complex environments. Flatness maps for the aerial
robot considering the complete kinematical constraint and the dynamical
coupling between each aerial robot and payload are derived. To improve the
responsiveness for the generation of the safe, dynamically feasible, and agile
trajectory in complex environments, a real-time spatio-temporal trajectory
planning scheme is proposed for the MARTS. Besides, we break away from the
reliance on the state measurement for both the payload and cable, as well as
the closed-loop control for the payload, and propose a fully distributed
control scheme to track the agile trajectory that is robust against imprecise
payload mass and non-point mass payload. The proposed schemes are extensively
validated through benchmark comparisons, ablation studies, and simulations.
Finally, extensive real-world experiments are conducted on a MARTS integrated
by three aerial robots with onboard computers and sensors. The result validates
the efficiency and robustness of our proposed schemes for SAAT in complex
environments.",['cs.RO'],False,,,,"Multi-Agent Feedback Motion Planning using Probably Approximately
  Correct Nonlinear Model Predictive Control","Safe and Agile Transportation of Cable-Suspended Payload via Multiple
  Aerial Robots"
neg-d2-379,2025-01-27,,2501.16014," Diffusion magnetic resonance imaging (dMRI) often suffers from low spatial
and angular resolution due to inherent limitations in imaging hardware and
system noise, adversely affecting the accurate estimation of microstructural
parameters with fine anatomical details. Deep learning-based super-resolution
techniques have shown promise in enhancing dMRI resolution without increasing
acquisition time. However, most existing methods are confined to either spatial
or angular super-resolution, limiting their effectiveness in capturing detailed
microstructural features. Furthermore, traditional pixel-wise loss functions
struggle to recover intricate image details essential for high-resolution
reconstruction. To address these challenges, we propose SARL-dMRI, a novel
Spatial-Angular Representation Learning framework for high-fidelity, continuous
super-resolution in dMRI. SARL-dMRI explores implicit neural representations
and spherical harmonics to model continuous spatial and angular
representations, simultaneously enhancing both spatial and angular resolution
while improving microstructural parameter estimation accuracy. To further
preserve image fidelity, a data-fidelity module and wavelet-based frequency
loss are introduced, ensuring the super-resolved images remain consistent with
the original input and retain fine details. Extensive experiments demonstrate
that, compared to five other state-of-the-art methods, our method significantly
enhances dMRI data resolution, improves the accuracy of microstructural
parameter estimation, and provides better generalization capabilities. It
maintains stable performance even under a 45$\times$ downsampling factor.",['eess.IV'],2501.16014," Diffusion magnetic resonance imaging (dMRI) often suffers from low spatial
and angular resolution due to inherent limitations in imaging hardware and
system noise, adversely affecting the accurate estimation of microstructural
parameters with fine anatomical details. Deep learning-based super-resolution
techniques have shown promise in enhancing dMRI resolution without increasing
acquisition time. However, most existing methods are confined to either spatial
or angular super-resolution, limiting their effectiveness in capturing detailed
microstructural features. Furthermore, traditional pixel-wise loss functions
struggle to recover intricate image details essential for high-resolution
reconstruction. To address these challenges, we propose SARL-dMRI, a novel
Spatial-Angular Representation Learning framework for high-fidelity, continuous
super-resolution in dMRI. SARL-dMRI explores implicit neural representations
and spherical harmonics to model continuous spatial and angular
representations, simultaneously enhancing both spatial and angular resolution
while improving microstructural parameter estimation accuracy. To further
preserve image fidelity, a data-fidelity module and wavelet-based frequency
loss are introduced, ensuring the super-resolved images remain consistent with
the original input and retain fine details. Extensive experiments demonstrate
that, compared to five other state-of-the-art methods, our method significantly
enhances dMRI data resolution, improves the accuracy of microstructural
parameter estimation, and provides better generalization capabilities. It
maintains stable performance even under a 45$\times$ downsampling factor.",['eess.IV'],False,,,,"Spatial-Angular Representation Learning for High-Fidelity Continuous
  Super-Resolution in Diffusion MRI","Spatial-Angular Representation Learning for High-Fidelity Continuous
  Super-Resolution in Diffusion MRI"
neg-d2-380,2025-01-14,,2501.08052," We present the strongest limits to date on the mixing angle, $\theta$, with
which a new scalar particle, $S$, mixes with the Higgs field in the mass range
$100$ $MeV<m_S<155$ MeV. This result uses the MicroBooNE liquid argon time
projection chamber to search for decays of these Higgs-portal scalar particles
through the $S\rightarrow e^+e^-$ channel with the decays of kaons in the NuMI
neutrino beam acting as the source of the scalar particles. The analysis uses
an exposure of $7.01\times 10^{20}$ protons on target of NuMI beam data
including a period when the beam focusing system was configured to focus
positively charged hadrons and a separate period when negatively charged
hadrons were focused. The analysis searches for scalar particles produced from
kaons decaying in flight in the beam's decay volume and at rest in the target
and absorber. At $m_S=125$ MeV ($m_S=150$ MeV$)$ we set a limit of
$\theta<2.65\times 10^{-4}$ ($\theta<1.72\times 10^{-4}$) at the 95$\%$
confidence level.",['hep-ex'],2503.17186," This paper presents searches for the direct pair production of charged
light-flavour sleptons, each decaying into a stable neutralino and an
associated Standard Model lepton. The analyses focus on the challenging
""corridor"" region, where the mass difference, $\Delta m$, between the slepton
($\tilde{e}$ or $\tilde{\mu}$) and the lightest neutralino
($\tilde{\chi}^{0}_{1}$) is less or similar to the mass of the $W$ boson,
$m(W)$, with the aim to close a persistent gap in sensitivity to models with
$\Delta m \lesssim m(W)$. Events are required to contain a high-energy jet,
significant missing transverse momentum, and two same-flavour opposite-sign
leptons ($e$ or $\mu$). The analysis uses $pp$ collision data at $\sqrt{s} =
13$ TeV recorded by the ATLAS detector, corresponding to an integrated
luminosity of 140 fb$^{-1}$. Several kinematic selections are applied,
including a set of boosted decision trees. These are each optimised for
different $\Delta m$ to provide expected sensitivity for the first time across
the full $\Delta m$ corridor. The results are generally consistent with the
Standard Model, with the most significant deviations observed with a local
significance of 2.0 $\sigma$ in the selectron search, and 2.4 $\sigma$ in the
smuon search. While these deviations weaken the observed exclusion reach in
some parts of the signal parameter space, the previously present sensitivity
gap to this corridor is largely reduced. Constraints at the 95% confidence
level are set on simplified models of selectron and smuon pair production,
where selectrons (smuons) with masses up to 300 (350) GeV can be excluded for
$\Delta m$ between 2 GeV and 100 GeV.",['hep-ex'],False,,,,"Search for the production of Higgs-portal scalar bosons in the NuMI beam
  using the MicroBooNE detector","Searches for direct slepton production in the compressed-mass corridor
  in $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector"
neg-d2-381,2025-01-02,,2501.0131," Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range
of time scales. Traditionally, this variability can often be modeled well as a
stochastic process. However, in certain cases, jetted AGN variability displays
regular patterns, enabling us to conduct investigations aimed at understanding
its origins. Additionally, a novel type of variability has emerged in jetted
AGN lightcurves, specifically, the observation of a long-term trend
characterized by a linear increase of the flux with time in blazars such as PG
1553+113, which is among the objects most likely to display periodic behavior.
In this paper, we present the results of a systematic search for long-term
trends, spanning $\approx$10\, years, utilizing 12 years of Fermi-LAT
observations. The study is focused on detecting the presence of linear or
quadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has
identified 40 jetted AGN that exhibit long-term trends, each with distinct
properties, which we also characterize in this study. These long-term trends
may originate from the dynamics of a supermassive black hole binary system, or
they could be the result of intrinsic phenomena within the jet itself. Our
findings can help in addressing questions pertaining to the astrophysical
origins of variability and periodicity within jetted AGN.",['astro-ph.HE'],2501.10108," This paper investigates the effects of saturated thermal conduction (TC) and
thermal-driven winds (TDWs) on magnetized advection-dominated accretion onto a
rotating black hole (BH). We incorporate dissipative processes in the
magnetized accretion flow and expect the accretion disk to be threaded by
predominantly toroidal and turbulent magnetic fields. We solve the
magnetohydrodynamics equations and construct a self-consistent steady model of
the magnetized accretion flow surrounding a rotating BH, which includes TC and
TDWs. We seek global accretion solutions spanning from the BH horizon to a
large distance and analyze the solution's characteristics as a function of
dissipation parameters. Accretion solutions with multiple critical points may
exhibit shock waves if they meet the standing shock criteria. We found steady,
global transonic, and shocked accretion solutions around the rotating BH. In
particular, the wind parameter ($m$) and the saturated conduction parameter
($\Phi_{\rm s}$) significantly influence the dynamical behavior of shocks. The
shock location moves away from the BH horizon as $\Phi_{\rm s}$ and $m$
increase, assuming fixed conditions at the disk's outer edge. Our formalism
explains the declining phase of BH outbursts, characterized by a monotonic
decrease in QPO frequency as the burst decays. Based on our findings, we
conclude that the combined effect of $\Phi_{\rm s}$ and $m$ parameters
substantially alters the steady shock specific energy vs angular momentum
parameter space and also modifies the corresponding post-shock luminosity vs
QPO frequency parameter space. We propose, based on our theoretical model, that
the $\Phi_{\rm s}$ and $m$ parameters may significantly influence the evolution
of the BH outbursts.",['astro-ph.HE'],False,,,,"Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active
  Galactic Nuclei","Thermal Conduction and Thermal-Driven Winds in Magnetized Viscous
  Accretion Disk Dynamics"
neg-d2-382,2025-03-15,,2503.1214," We study the decay properties of non-negative solutions to the
one-dimensional defocusing damped wave equation in the Fujita subcritical case
under a specific initial condition. Specifically, we assume that the initial
data are positive, satisfy a condition ensuring the positiveness of solutions,
and exhibit polynomial decay at infinity.
  To show the decay properties of the solution, we construct suitable
supersolutions composed of an explicit function satisfying an ordinary
differential inequality and the solution of the linear damped wave equation.
Our estimates correspond to the optimal ones inferred from the analysis of the
heat equation.",['math.AP'],2501.06544," In the mean-field regime, a gas of quantum particles with Boltzmann
statistics can be described by the Hartree-Fock equation. This dynamics becomes
trivial if the initial distribution of particle is invariant by translation.
However, the first correction is given on time of order $O(N)$ by the quantum
Lenard--Balescu equation. In the first part of the present article, we justify
this equation until time of order $O((\log N)^{1-\delta})$ (for any
$\delta\in(0,1)$).
  A similar phenomenon exists in the classical setting (with a similar validity
time obtained by Duerinckx \cite{Duerinckx}). In a second time, we prove the
convergence for dimension $d\geq 2$ of the solutions of the quantum
Lenard--Balescu equation to the solutions of its classical counterpart in the
semi-classical limit. This problem can be interpreted as a grazing collision
limit: the quantum Lenard--Balescu equation looks like a cut-off Boltzmann
equation, when the classical one looks like the Landau equation.",['math.AP'],False,,,,"Decay estimate for subcritical semilinear damped wave equations with
  slowly decreasing data",Around the Quantum Lenard-Balescu equation
neg-d2-383,2025-03-06,,2503.04306," We present multiband observations and analysis of EP240801a, a low-energy,
extremely soft gamma-ray burst (GRB) discovered on August 1, 2024 by the
Einstein Probe (EP) satellite, with a weak contemporaneous signal also detected
by Fermi/GBM. Optical spectroscopy of the afterglow, obtained by GTC and Keck,
identified the redshift of $z = 1.6734$. EP240801a exhibits a burst duration of
148 s in X-rays and 22.3 s in gamma-rays, with X-rays leading by 80.61 s.
Spectral lag analysis indicates the gamma-ray signal arrived 8.3 s earlier than
the X-rays. Joint spectral fitting of EP/WXT and Fermi/GBM data yields an
isotropic energy $E_{\gamma,\rm{iso}} = (5.57^{+0.54}_{-0.50})\times
10^{51}\,\rm{erg}$, a peak energy $E_{\rm{peak}} =
14.90^{+7.08}_{-4.71}\,\rm{keV}$, a fluence ratio $\rm
S(25-50\,\rm{keV})/S(50-100\,\rm{keV}) = 1.67^{+0.74}_{-0.46}$, classifying
EP240801a as an X-ray flash (XRF). The host-galaxy continuum spectrum, inferred
using Prospector, was used to correct its contribution for the observed
outburst optical data. Unusual early $R$-band behavior and EP/FXT observations
suggest multiple components in the afterglow. Three models are considered:
two-component jet model, forward-reverse shock model and forward-shock model
with energy injection. Both three provide reasonable explanations. The
two-component jet model and the energy injection model imply a relatively small
initial energy and velocity of the jet in the line of sight, while the
forward-reverse shock model remains typical. Under the two-component jet model,
EP240801a may resemble GRB 221009A (BOAT) if the bright narrow beam is viewed
on-axis. Therefore, EP240801a can be interpreted as an off-beam (narrow) jet or
an intrinsically weak GRB jet. Our findings provide crucial clues for
uncovering the origin of XRFs.",['astro-ph.HE'],2502.10591," Binary black holes (BBH) are expected to form and merge in active galactic
nuclei (AGN), deep in the potential well of a supermassive black hole (SMBH),
from populations that exist in a nuclear star cluster (NSC). Here we
investigate the gravitational wave (GW) signature of a BBH lensed by a nearby
SMBH. For a fiducial GW150914-like BBH orbiting close to a $10^{8}M_{\odot}$
SMBH located at $z=0.1$, the lensed GW signal varies in a predictable manner in
and out of the LISA detectability band and across frequencies. The occurrence
of such signatures has the potential to confound LISA global fit models if they
are not modelled. Detection of these sources provide an independent measure of
AGN inclination angles, along with detecting warping of the inner disk, and
measuring the SMBH spin.",['astro-ph.HE'],False,,,,"EP240801a/XRF 240801B: An X-ray Flash Detected by the Einstein Probe and
  Implications of its Multiband Afterglow",Evolution of LISA Observables for Binary Black Holes Lensed by an SMBH
neg-d2-384,2025-02-03,,2502.01975," We define a new class of regular inclusions, the pseudo-Cartan inclusions. We
show this class coincides with the class of regular inclusions having a Cartan
envelope and also with the class of regular inclusions with the faithful unique
pseudo-expectation property. We describe the twisted groupoid associated with
the Cartan envelope of a pseudo-Cartan inclusion. These results significantly
extend previous results obtained for the unital setting.
  We explore properties of pseudo-Cartan inclusions and the relationship
between a pseudo-Cartan inclusion and its Cartan envelope. For example, if $D
\subseteq C$ is a pseudo-Cartan inclusion with Cartan envelope $B \subseteq A$,
then $C$ is simple if and only if $A$ is simple. We show how to construct
pseudo-Cartan inclusions from a given Cartan inclusion, that the inductive
limit of pseudo-Cartan inclusions with suitable connecting maps is a
pseudo-Cartan inclusion, and the minimal tensor product of pseudo-Cartan
inclusions is a pseudo-Cartan inclusion. Further, we describe the Cartan
envelope of pseudo-Cartan inclusions arising from these constructions. We give
some applications and conclude with a few open questions.",['math.OA'],2502.01975," We define a new class of regular inclusions, the pseudo-Cartan inclusions. We
show this class coincides with the class of regular inclusions having a Cartan
envelope and also with the class of regular inclusions with the faithful unique
pseudo-expectation property. We describe the twisted groupoid associated with
the Cartan envelope of a pseudo-Cartan inclusion. These results significantly
extend previous results obtained for the unital setting.
  We explore properties of pseudo-Cartan inclusions and the relationship
between a pseudo-Cartan inclusion and its Cartan envelope. For example, if $D
\subseteq C$ is a pseudo-Cartan inclusion with Cartan envelope $B \subseteq A$,
then $C$ is simple if and only if $A$ is simple. We show how to construct
pseudo-Cartan inclusions from a given Cartan inclusion, that the inductive
limit of pseudo-Cartan inclusions with suitable connecting maps is a
pseudo-Cartan inclusion, and the minimal tensor product of pseudo-Cartan
inclusions is a pseudo-Cartan inclusion. Further, we describe the Cartan
envelope of pseudo-Cartan inclusions arising from these constructions. We give
some applications and conclude with a few open questions.",['math.OA'],False,,,,Pseudo-Cartan Inclusions,Pseudo-Cartan Inclusions
neg-d2-385,2025-01-21,,2502.03475," Euler angle representation in biomechanical analysis allows straightforward
description of joints rotations. However, application of Euler angles could be
limited due to singularity called gimbal lock. Quaternions offer an alternative
way to describe rotations but they have been mostly avoided in biomechanics as
they are complex and not inherently intuitive, specifically in dynamic models
actuated by muscles. This study introduces a mathematical framework for
describing muscle actions in dynamic quaternion-based musculoskeletal
simulations. The proposed method estimates muscle torques in quaternion-based
musculoskeletal model. Its application is shown on three-dimensional
double-pendulum system actuated by muscle elements. Furthermore, transformation
of muscle moment arms obtained from muscle paths based on Euler angles into
quaternions description is presented. The proposed method is advantageous for
dynamic modeling of musculoskeletal models with complex kinematics and large
range of motion like the shoulder joint.",['physics.med-ph'],2502.1091," Photoacoustic (PA) waves are strongly distorted and attenuated in skull bone.
To study these effects on PA imaging, we designed and 3D-printed
tissue-mimicking phantoms of human skull. We present a comparison of results in
phantom and ex vivo skull.",['physics.med-ph'],False,,,,"Calculation of a force effect from muscle action to a quaternion-based
  musculoskeletal model",3D printed human skull phantoms for transcranial photoacoustic imaging
neg-d2-386,2025-02-03,,2502.07801," In recent years, the large electric field enhancement and tight spatial
confinement supported by the so-called epsilon near-zero (ENZ) mode has
attracted significant attention for the realization of efficient nonlinear
optical devices. Here, we experimentally demonstrate ENZ photonic gap antennas
(PGAs), which consist of a dielectric pillar within which a thin slab of indium
tin oxide (ITO) material is embedded. In ENZ PGAs, hybrid dielectric-ENZ modes
emerge from strong coupling between the dielectric antenna modes and the ENZ
bulk plasmon resonance. These hybrid modes efficiently couple to free space and
allow for large enhancements of the incident electric field over nearly an
octave bandwidth, without the stringent lateral nanofabrication requirements of
conventional plasmonic or dielectric nanoantennas. To understand the modal
features, we probe the linear response of single ENZ PGAs with dark field
scattering and interpret the results in terms of a simple coupled oscillator
framework. Third harmonic generation (THG) is used to probe the ITO local
fields and large enhancements are observed in the THG efficiency over a broad
spectral range. Surprisingly, sharp peaks emerge on top of the nonlinear
response, which were not predicted by full wave calculations. These peaks are
attributed to the ENZ material's nonlocal response, which once included using a
hydrodynamic model for the ITO permittivity improves the agreement of our
calculations for both the linear and nonlinear response. This proof of concept
demonstrates the potential of ENZ PGAs, which we have previously shown can
support electric field enhancements of up to 100--200X, and the importance of
including nonlocal effects when describing the response of thin ENZ layers.
Importantly, inclusion of the ITO nonlocality leads to increases in the
predicted field enhancement, as compared to the local calculation.",['physics.optics'],2501.07657," Narrow linewidth lasers are indispensable for coherent optical systems,
including communications, metrology, and sensing. Although compact
semiconductor lasers with narrow linewidths and low noise have been
demonstrated, their spectral purity typically relies on hybrid or heterogeneous
external cavity feedback. Here, we present a theoretical and experimental
demonstration of a heterogeneous free optical injection locking (HF OIL)
semiconductor laser. By integrating a topological interface state extended
(TISE) laser with a micro ring resonator (MRR) on an AlGaInAs multiple quantum
well platform,we achieve monolithic photon injection and phase locking, thereby
reducing the optical linewidth. We fabricated and characterized a 1550 nm
sidewall HF OIL laser, achieving stable single mode operation over a broad
current range (65 to 300 mA) and a side mode suppression ratio (SMSR) over 50
dB. Under injection locking, the devices Voigt fitted linewidth narrowed from
over 1.7 MHz (free running) to 4.2 kHz, representing a three order of magnitude
improvement over conventional distributed feedback lasers. The intrinsic
linewidth of 1.4 kHz is measured by correlated delayed self-heterodyne
frequency noise power spectrum density (FN PSD) method. Moreover, the HF OIL
laser demonstrated high phase stability and the ability to transition from a
random phased to a phase locked state. These results underscore the potential
of HF-OIL lasers in advancing coherent optical communications and phase
encoders in quantum key distribution (QKD) systems.",['physics.optics'],False,,,,"Field-enhancement and nonlocal effects in epsilon-near-zero photonic gap
  antennas","Heterogeneous-free narrow linewidth semiconductor laser with optical
  injection locking"
neg-d2-387,2025-02-19,,2502.13786," Let $G$ be a locally compact abelian group, and let $\widehat{G}$ denote its
dual group, equipped with a Haar measure. A variant of the uncertainty
principle states that for any $S \subset G$ and $\Sigma \subset \widehat{G}$,
there exists a constant $C(S, \Sigma)$ such that for any $f \in L^2(G)$, the
following inequality holds: \[\|f\|_{L^2(G)} \leq C(S, \Sigma) \bigl(
\|f\|_{L^2(G \setminus S)} + \|\widehat{f}\|_{L^2(\widehat{G} \setminus
\Sigma)} \bigr),\] where $\widehat{f}$ denotes the Fourier transform of $f$.
This variant of the uncertainty principle is particularly useful in
applications such as signal processing and control theory.The purpose of this
paper is to show that such estimates can be strengthened when $S$ or $\Sigma$
satisfies a restriction theorem and to provide an estimate for the constant
$C(S, \Sigma)$. This result serves as a quantitative counterpart to a recent
finding by the first and last author. In the setting of finite groups, the
results also extend those of Matolcsi-Sz\""ucs and Donoho-Stark.",['math.CA'],2502.13786," Let $G$ be a locally compact abelian group, and let $\widehat{G}$ denote its
dual group, equipped with a Haar measure. A variant of the uncertainty
principle states that for any $S \subset G$ and $\Sigma \subset \widehat{G}$,
there exists a constant $C(S, \Sigma)$ such that for any $f \in L^2(G)$, the
following inequality holds: \[\|f\|_{L^2(G)} \leq C(S, \Sigma) \bigl(
\|f\|_{L^2(G \setminus S)} + \|\widehat{f}\|_{L^2(\widehat{G} \setminus
\Sigma)} \bigr),\] where $\widehat{f}$ denotes the Fourier transform of $f$.
This variant of the uncertainty principle is particularly useful in
applications such as signal processing and control theory.The purpose of this
paper is to show that such estimates can be strengthened when $S$ or $\Sigma$
satisfies a restriction theorem and to provide an estimate for the constant
$C(S, \Sigma)$. This result serves as a quantitative counterpart to a recent
finding by the first and last author. In the setting of finite groups, the
results also extend those of Matolcsi-Sz\""ucs and Donoho-Stark.",['math.CA'],False,,,,"Uncertainty Principle, annihilating pairs and Fourier restriction","Uncertainty Principle, annihilating pairs and Fourier restriction"
neg-d2-388,2025-02-16,,2502.11008," Counterfactual reasoning is widely recognized as one of the most challenging
and intricate aspects of causality in artificial intelligence. In this paper,
we evaluate the performance of large language models (LLMs) in counterfactual
reasoning. In contrast to previous studies that primarily focus on commonsense
causal reasoning, where LLMs often rely on prior knowledge for inference, we
specifically assess their ability to perform counterfactual inference using a
set of formal rules. To support this evaluation, we introduce a new benchmark
dataset, CounterBench, comprising 1K counterfactual reasoning questions. The
dataset is designed with varying levels of difficulty, diverse causal graph
structures, distinct types of counterfactual questions, and multiple
nonsensical name variants. Our experiments demonstrate that counterfactual
reasoning poses a significant challenge for LLMs, with most models performing
at levels comparable to random guessing. To enhance LLM's counterfactual
reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides
LLMs through iterative reasoning and backtracking to systematically explore
counterfactual solutions. Experimental results show that our method
significantly improves LLM performance on counterfactual reasoning tasks and
consistently enhances performance across different LLMs.Our dataset is
available at https://huggingface.co/datasets/CounterBench/CounterBench.",['cs.CL'],2502.19953," Regular updates are essential for maintaining up-to-date knowledge in large
language models (LLMs). Consequently, various model editing methods have been
developed to update specific knowledge within LLMs. However, training-based
approaches often struggle to effectively incorporate new knowledge while
preserving unrelated general knowledge. To address this challenge, we propose a
novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes
the geometric relationships of parameter updates from fine-tuning to
differentiate between neurons associated with new knowledge updates and those
related to general knowledge perturbations. By employing a direction-aware
knowledge identification method, we avoid updating neurons with directions
approximately orthogonal to existing knowledge, thus preserving the model's
generalization ability. For the remaining neurons, we integrate both old and
new knowledge for aligned directions and apply a ""forget-then-learn"" editing
strategy for opposite directions. Additionally, we introduce an
importance-guided task vector fusion technique that filters out redundant
information and provides adaptive neuron-level weighting, further enhancing
model editing performance. Extensive experiments on two publicly available
datasets demonstrate the superiority of GeoEdit over existing state-of-the-art
methods.",['cs.CL'],False,,,,"CounterBench: A Benchmark for Counterfactuals Reasoning in Large
  Language Models",GeoEdit: Geometric Knowledge Editing for Large Language Models
neg-d2-389,2025-01-22,,2501.12853," Spectrum maps reflect the utilization and distribution of spectrum resources
in the electromagnetic environment, serving as an effective approach to support
spectrum management. However, the construction of spectrum maps in urban
environments is challenging because of high-density connection and complex
terrain. Moreover, the existing spectrum map construction methods are typically
applied to a fixed frequency, which cannot cover the entire frequency band. To
address the aforementioned challenges, a UNet-based data-and-semantic
dual-driven method is proposed by introducing the semantic knowledge of binary
city maps and binary sampling location maps to enhance the accuracy of spectrum
map construction in complex urban environments with dense communications.
Moreover, a joint frequency-space reasoning model is exploited to capture the
correlation of spectrum data in terms of space and frequency, enabling the
realization of complete spectrum map construction without sampling all
frequencies of spectrum data. The simulation results demonstrate that the
proposed method can infer the spectrum utilization status of missing
frequencies and improve the completeness of the spectrum map construction.
Furthermore, the accuracy of spectrum map construction achieved by the proposed
data-and-semantic dual-driven method outperforms the benchmark schemes,
especially in scenarios with low sampling density.",['cs.LG'],2503.07917," Clustering of high-dimensional data sets is a growing need in artificial
intelligence, machine learning and pattern recognition. In this paper, we
propose a new clustering method based on a combinatorial-topological approach
applied to regions of space defined by signs of coordinates (hyperoctants). In
high-dimensional spaces, this approach often reduces the size of the dataset
while preserving sufficient topological features. According to a density
criterion, the method builds clusters of data points based on the partitioning
of a graph, whose vertices represent hyperoctants, and whose edges connect
neighboring hyperoctants under the Levenshtein distance. We call this method
HyperOctant Search Clustering. We prove some mathematical properties of the
method. In order to as assess its performance, we choose the application of
topic detection, which is an important task in text mining. Our results suggest
that our method is more stable under variations of the main hyperparameter, and
remarkably, it is not only a clustering method, but also a tool to explore the
dataset from a topological perspective, as it directly provides information
about the number of hyperoctants where there are data points. We also discuss
the possible connections between our clustering method and other research
fields.",['cs.LG'],False,,,,"Data-and-Semantic Dual-Driven Spectrum Map Construction for 6G Spectrum
  Management","Hyperoctant Search Clustering: A Method for Clustering Data in
  High-Dimensional Hyperspheres"
neg-d2-390,2025-02-10,,2502.07104," The use of high-dimensional regression techniques from machine learning has
significantly improved the quantitative accuracy of interatomic potentials.
Atomic simulations can now plausibly target quantitative predictions in a
variety of settings, which has brought renewed interest in robust means to
quantify uncertainties on simulation results. In many practical settings,
encompassing both classical and a large class of machine learning potentials,
the dominant form of uncertainty is currently not due to lack of training data
but to misspecification, namely the inability of any one choice of model
parameters to exactly match all ab initio training data. However, Bayesian
inference, the most common formal tool used to quantify uncertainty, is known
to ignore misspecification and thus significantly underestimates parameter
uncertainties. Here, we employ a recent misspecification-aware regression
technique to quantify parameter uncertainties, which is then propagated to a
broad range of phase and defect properties in tungsten via brute force
resampling or implicit differentiation. The propagated misspecification
uncertainties robustly envelope errors to direct \textit{ab initio} calculation
of material properties outside of the training dataset, an essential
requirement for any quantitative multi-scale modeling scheme. Finally, we
demonstrate application to recent foundational machine learning interatomic
potentials, accurately predicting and bounding errors in MACE-MPA-0 energy
predictions across the diverse materials project database. Perspectives for the
approach in multiscale simulation workflows are discussed.",['cond-mat.mtrl-sci'],2503.09531," The Quasi-harmonic Approximation (QHA) is a widely used method for
calculating the temperature dependence of lattice parameters and the thermal
expansion coefficients from first principles. However, applying QHA to
anisotropic systems typically requires several dozens or even hundreds of
phonon band structure calculations, leading to high computational costs. The
Zero Static Internal Stress Approximation (ZSISA) QHA method partly addresses
such caveat, but the computational load of its implementation remains high, so
that its volumetric-only counterpart v-ZSISA-QHA is preferred. In this work, we
present an efficient implementation of the ZSISA-QHA, enabling its application
across a wide range of crystal structures under varying temperature (T) and
pressure (P) conditions. By incorporating second-order derivatives of the
vibrational free energy with respect to lattice degrees of freedom, we
significantly reduce the number of required phonon band structure calculations
for the determination of all lattice parameters and angles. For hexagonal,
trigonal, and tetragonal systems, only six phonon band structure calculations
are needed, while 10, 15, and 28 calculations suffice for orthorhombic,
monoclinic, and triclinic systems, respectively. This method is tested for a
variety of non-cubic materials, from uniaxial ones like ZnO and CaCO3 to
monoclinic or triclinic materials such as ZrO2, HfO2, and Al2SiO5,
demonstrating a significant reduction in computational effort while maintaining
accuracy in modeling anisotropic thermal expansion, unlike the v-ZSISA-QHA. The
method is also applied to the first-principles calculation of
temperature-dependent elastic constants, with only up to six more phonon band
structure calculations, depending on the crystallographic system.",['cond-mat.mtrl-sci'],False,,,,"Uncertainty Quantification for Misspecified Machine Learned Interatomic
  Potentials","Anisotropic temperature-dependent lattice parameters and elastic
  constants from first principles"
neg-d2-391,2025-02-14,,2502.10529," In this paper, we study a Dirac boundary value problem where the operator is
considered with a derivative of order $\alpha \in (0, 1]$, known as the
$F^{\alpha}$-derivative. We prove some spectral properties of eigenvalues and
eigenfunctions and present numerical examples to demonstrate the practical
implications of our approach.",['math.SP'],2502.10529," In this paper, we study a Dirac boundary value problem where the operator is
considered with a derivative of order $\alpha \in (0, 1]$, known as the
$F^{\alpha}$-derivative. We prove some spectral properties of eigenvalues and
eigenfunctions and present numerical examples to demonstrate the practical
implications of our approach.",['math.SP'],False,,,,"A Fractal Dirac Eigenvalue Problem: Spectral Properties and Numerical
  Examples","A Fractal Dirac Eigenvalue Problem: Spectral Properties and Numerical
  Examples"
neg-d2-392,2025-03-11,,2503.08132," In this study, we perform a detailed investigation into the interplay between
disorder-induced electron localization and long-range hopping amplitudes within
the Selective Long-Range Tight-Binding Model (SLRTB). Through numerical
simulations, we analyze the electronic properties of the system, with a focus
on the participation ratio (PR), entanglement entropy (EE), energy spectrum,
and the ratio of level spacings ($r_n$). Our results reveal a marked
distinction between negative and positive long-range hopping amplitudes,
manifesting in different electronic behaviors and transitions. Notably, we
carry out a finite-size scaling analysis, identifying the critical point and
exponents that characterize the system's behavior near the transition. The
investigation highlights the role of gapless regions in shaping the system's
PR, $r_n$, and EE, and the influence of disorder on these properties. The SLRTB
model proves to be an effective framework for understanding the effects of
disorder and long-range hopping on electron dynamics, offering valuable
insights into localization and delocalization phenomena.",['cond-mat.str-el'],2501.09553," We study the hybridization between plasmons, phonons, and electronic sound in
ionic crystals using the Debye model, where the ionic background is modeled as
a homogeneous, isotropic, elastic medium. We explicitly obtain the energies and
the damping of the hybrid plasmon-sound modes in the hydrodynamic regime and
calculate the corresponding dynamic structure factor. We show that the direct
Coulomb interaction between the ions is essential to obtain a collective sound
mode with linear dispersion.",['cond-mat.str-el'],False,,,,"Probing Electron Localization and Delocalization in the Selective
  Long-Range Tight-Binding Model",Plasmon-sound hybridization in ionic crystals
neg-d2-393,2025-01-22,,2501.13107," We propose Inner Loop Feedback (ILF), a novel approach to accelerate
diffusion models' inference. ILF trains a lightweight module to predict future
features in the denoising process by leveraging the outputs from a chosen
diffusion backbone block at a given time step. This approach exploits two key
intuitions; (1) the outputs of a given block at adjacent time steps are
similar, and (2) performing partial computations for a step imposes a lower
burden on the model than skipping the step entirely. Our method is highly
flexible, since we find that the feedback module itself can simply be a block
from the diffusion backbone, with all settings copied. Its influence on the
diffusion forward can be tempered with a learnable scaling factor from zero
initialization. We train this module using distillation losses; however, unlike
some prior work where a full diffusion backbone serves as the student, our
model freezes the backbone, training only the feedback module. While many
efforts to optimize diffusion models focus on achieving acceptable image
quality in extremely few steps (1-4 steps), our emphasis is on matching best
case results (typically achieved in 20 steps) while significantly reducing
runtime. ILF achieves this balance effectively, demonstrating strong
performance for both class-to-image generation with diffusion transformer (DiT)
and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The
quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP
Image Quality Assessment, ImageReward, and qualitative comparisons. Project
information is available at https://mgwillia.github.io/ilf.",['cs.CV'],2503.04268," In this report, I present an inpainting framework named \textit{ControlFill},
which involves training two distinct prompts: one for generating plausible
objects within a designated mask (\textit{creation}) and another for filling
the region by extending the background (\textit{removal}). During the inference
stage, these learned embeddings guide a diffusion network that operates without
requiring heavy text encoders. By adjusting the relative significance of the
two prompts and employing classifier-free guidance, users can control the
intensity of removal or creation. Furthermore, I introduce a method to
spatially vary the intensity of guidance by assigning different scales to
individual pixels.",['cs.CV'],False,,,,Accelerate High-Quality Diffusion Models with Inner Loop Feedback,ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning
neg-d2-394,2025-03-14,,2503.11383," Using $e^+e^-$ annihilation data corresponding to a total integrated
luminosity of 7.33 $\rm fb^{-1}$ collected at center-of-mass energies between
4.128 and 4.226~GeV with the BESIII detector, we provide the first amplitude
analysis and absolute branching fraction measurement of the hadronic decay
$D_{s}^{+} \to K_{S}^{0}K_{L}^{0}\pi^{+}$. The branching fraction of $D_{s}^{+}
\to K_{S}^{0}K_{L}^{0}\pi^{+}$ is determined to be $(1.86\pm0.06_{\rm
stat}\pm0.03_{\rm syst})\%$.
  Combining the $\mathcal{B}(D_{s}^{+} \to \phi(\to K_{S}^0K_{L}^0) \pi^+)$
obtained in this work and the world average of $\mathcal{B}(D_{s}^{+} \to
\phi(\to K^+K^-) \pi^+)$, we measure the relative branching fraction
$\mathcal{B}(\phi \to K_S^0K_L^0)/\mathcal{B}(\phi \to K^+K^-)$=($0.597 \pm
0.023_{\rm stat} \pm 0.018_{\rm syst} \pm 0.016_{\rm PDG}$), which deviates
from the PDG value by more than 3$\sigma$. Furthermore, the asymmetry of the
branching fractions of $D^+_s\to K_{S}^0K^{*}(892)^{+}$ and $D^+_s\to
K_{L}^0K^{*}(892)^{+}$, $\frac{\mathcal{B}(D_{s}^{+} \to
K_{S}^0K^{*}(892)^{+})-\mathcal{B}(D_{s}^{+} \to
K_{L}^0K^{*}(892)^{+})}{\mathcal{B}(D_{s}^{+} \to
K_{S}^0K^{*}(892)^{+})+\mathcal{B}(D_{s}^{+} \to K_{L}^0K^{*}(892)^{+})}$, is
determined to be $(-13.4\pm5.0_{\rm stat}\pm3.4_{\rm syst})\%$.",['hep-ex'],2501.18979," The electric dipole moments~(EDM) of fundamental particles inherently violate
parity~(P) and time-reversal~(T) symmetries. By virtue of the CPT theorem in
quantum field theory, the latter also implies the violation of the combined
charge-conjugation and parity~(CP) symmetry. We aim to measure the EDM of the
muon using the frozen-spin technique within a compact storage trap. This method
exploits the high effective electric field, \$E \approx 165\$ MV/m, experienced
in the rest frame of the muon with a momentum of about 23 MeV/c when it passes
through a solenoidal magnetic field of \$|\vec{B}|=2.5\$ T. In this paper, we
outline the fundamental considerations for a muon EDM search and present a
conceptual design for a demonstration experiment to be conducted at secondary
muon beamlines of the Paul Scherrer Institute in Switzerland. In Phase~I, with
an anticipated data acquisition period of 200 days, the expected sensitivity to
a muon EDM is 4E-21 ecm. In a subsequent phase, Phase~II, we propose to improve
the sensitivity to 6E-23 ecm using a dedicated instrument installed on a
different beamline that produces muons of momentum 125 MeV/c}.",['hep-ex'],False,,,,"Study of $\phi\to K\bar{K}$ and $K_{S}^{0}-K_{L}^{0}$ asymmetry in the
  amplitude analysis of $D_{s}^{+} \to K_{S}^{0}K_{L}^{0}\pi^{+}$ decay","A compact frozen-spin trap for the search for the electric dipole moment
  of the muon"
neg-d2-395,2025-03-10,,2503.07822," Magneto-electric coupling enables the manipulation of magnetization by
electric fields and vice versa. While typically found in heavy element
materials with large spin-orbit coupling, recent experiments on
rhombohedral-stacked pentalayer graphene (RPG) have demonstrated a {\it
longitudinal magneto-electric coupling} (LMC) without spin-orbit coupling. Here
we present a microscopic theory of LMC in multilayer graphene and identify how
it is controlled by a ``layer-space'' quantum geometry and interaction-driven
valley polarization. Strikingly, we find that the interplay between
valley-polarized order and LMC produces a butterfly shaped magnetic hysteresis
controlled by out-of-plane electric field: a signature of LMC and a
multiferroic valley order. Furthermore, we identify a nonlinear LMC in
multilayer graphene under time-reversal symmetry, while the absence of
centrosymmetry enables the generation of a second-order nonlinear electric
dipole moment in response to an out-of-plane magnetic field. Our theoretical
framework provides a quantitative understanding of LMC, as well as the emergent
magneto-electric properties of multilayer graphene.",['cond-mat.mes-hall'],2502.15232," In-situ control over band mass inversion is crucial for developing materials
with topologically protected edge modes. In this Letter, we report the direct
observation of displacement field $D$ control of band mass and Berry phase in
Bernal stacked trilayer graphene (TLG) in the region where trigonal warping
distorts the quadratic band into off-center Dirac points, referred to as `Dirac
Gullies.' Using Shubnikov-de-Haas (SdH) oscillations, we map the Fermi surface
contours of the Dirac gullies and the $D$-dependent band structure. With
increasing $D$-field, the Berry phase undergoes multiple transitions from
$\Phi_B=2\pi$ $\rightarrow$ $\pi$ $\rightarrow$ $2\pi$ as $D$ is varied.
Concurrently, measurement of the effective mass reveals a series of transitions
between massless and massive bands, signaling the closure and reopening
(accompanied by a possible band inversion) of the band gap at a critical value
of $D$. Interestingly, the expected Dirac-like behavior of the Dirac gullies
($\Phi_B=\pi$) persists only over a narrow range of $D$. Our study directly
confirms recent predictions of $ D$-field-induced band inversion in the
low-energy regions of TLG. It is a significant step towards achieving control
over pure valley transport in multilayer graphene.",['cond-mat.mes-hall'],False,,,,Orbital Longitudinal Magneto-electric Coupling in Multilayer Graphene,Gate tunable Dirac mass and Berry phase in Trilayer graphene
neg-d2-396,2025-01-02,,2501.01134," This paper enriches the topological horseshoe theory using finite subshift
theory in symbolic dynamical systems, and develops an elementary framework
addressing incomplete crossing and semi-horseshoes. Two illustrative examples
are provided: one from the perturbed Duffing system and another from a
polynomial system proposed by Chen, demonstrating the prevalence of
semi-horseshoes in chaotic systems. Moreover, the semi-topological horseshoe
theory enhances the detection of chaos and improves the accuracy of topological
entropy estimation.",['math.DS'],2502.19814," A system of inhomogeneous second-order difference equations with linear parts
given by noncommutative matrix coefficients are considered. Closed form of its
solution is derived by means of newly defined delayed matrix sine/cosine using
the Z-transform and determining function.",['math.DS'],False,,,,Incomplete crossing and semi-topological horseshoes,Explicit solution of second-order delayed discrete equations
neg-d2-397,2025-02-05,,2502.02992," We present the results obtained by performing global fits of
two-Higgs-doublet models (2HDMs) using the full Run 1 and Run 2 Higgs datasets
collected at the LHC. Avoiding unwanted tree-level flavor-changing neutral
currents and including the wrong-sign cases, we consider 12 scenarios across
six types of 2HDMs: Inert, type I, type II, type III, type IV, and Aligned
2HDMs. Our main results are presented in Table 3 and Fig. 1. We find that the
type-I 2HDM provides the best fit, while the wrong-sign scenarios of the
type-II and type-IV 2HDMs, where the normalized Yukawa coupling to down-type
quarks is opposite in sign to the Standard Model (SM), are disfavored. We also
observe that the Aligned 2HDM gives the second-best fit when the Yukawa
couplings to down-type quarks take the same sign as in the SM, regardless of
the sign of the Yukawa couplings to the charged leptons.",['hep-ph'],2502.01668," The literature establishes that the light fermions contributions to the
decays $H\to Z\gamma$ and $H\to\gamma\gamma$ are negligible since their
coupling with the Higgs is proportional to $m_f$. In the present letter, we
show that although such a conclusion is true for leptons, the light quark
contributions are zero when we consider their non-perturbative effects.",['hep-ph'],False,,,,"Higgs boson precision analysis of two Higgs doublet models: Full LHC Run
  1 and Run 2 data",Light quark contributions to Higgs decays
neg-d2-398,2025-01-21,,2501.12455," In this work we report on a self-assembled growth of a Ge quantum dot lattice
in a single 600-nm-thick Ge+Al2O3 layer during magnetron sputtering deposition
of a Ge+Al2O3 mixture at an elevated substrate temperature. The self-assembly
results in the formation of a well-ordered threedimensional body-centered
tetragonal quantum dot lattice within the whole deposited volume. The quantum
dots formed are very small in size less than 4.0 nm, have a narrow size
distribution and a large packing density. The parameters of the quantum dot
lattice can be tuned by changing the deposition parameters. The self-ordering
of the quantum dots is explained by diffusionmediated nucleation and
surface-morphology effects and simulated by a kinetic Monte Carlo model.",['cond-mat.mtrl-sci'],2502.00337," Exotic nondiffusive heat transfer regimes such as the second sound, where
heat propagates as a damped wave at speeds comparable to those of mechanical
disturbances, often occur at cryogenic temperatures (T) and nanosecond
timescales in semiconductors. First-principles prediction of such rapid, low-T
phonon dynamics requires finely-resolved temporal tracking of large, dense, and
coupled linear phonon dynamical systems arising from the governing linearized
Peierls-Boltzmann equation (LPBE). Here, we uncover a rigorous low-rank
representation of these linear dynamical systems, derived from the spectral
properties of the phonon collision matrix, that accelerates the
first-principles prediction of phonon dynamics by a factor of over a million
without compromising on the computational accuracy. By employing this low-rank
representation of the LPBE, we predict strong amplification of the wave-like
second sound regime upon isotopic enrichment in diamond - a finding that would
have otherwise been computationally intractable using the conventional
brute-force approaches. Our framework enables a rapid and accurate discovery of
the conditions under which wave-like heat flow can be realized in common
semiconductors.",['cond-mat.mtrl-sci'],False,,,,Self-assembling of Ge quantum dots in an alumina matrix,"Efficient calculation of phonon dynamics through a low-rank solution of
  the Boltzmann equation"
neg-d2-399,2025-03-12,,2503.09104," A central black hole can attract a dark matter cluster and generate a spike
in the density profile, as demonstrated by detailed analysis of Schwarzschild
and Kerr black holes in the past. Do different black holes attract dark matter
differently? To get a fair answer to this question, we customize a relativistic
framework to grow general static spherical black holes in dark matter halos and
investigate how deviations from the Schwarzschild geometry modify the dark
matter spike for the first time. The framework is applied to a class of
Schwarzschild-like black hole solutions in Lorentz-violated gravity models --
one in the bumblebee model and two in the Kalb-Ramond model. For these black
holes, the answer is no if initially the dark matter has a constant
distribution, but the answer is yes if it has a Hernquist profile initially.",['gr-qc'],2503.17689," In this paper, we discuss the existence of ghost star models in the
Einstein-Maxwell framework. In order to explore these objects, we put forward
the idea of Zeldovich and Novikov by keeping in mind that the energy density of
such models lie in the negative range in some regions of the spacetime
geometry. We proceed by taking into account a static sphere and develop the
field equations for a charged anisotropic fluid configuration. The two
generating functions are then considered and we rewrite the field equations in
terms of the mass and these physical quantities. Afterwards, we formulate two
different models using the conformally flatness condition along with the
considered generating functions. Further, we adopt the vanishing complexity
constraint as well as null active gravitational mass to find two more
solutions. The energy density for all developed models is also graphically
shown. We conclude that the ghost stars exist in the presence of charge as the
energy density for all the resulting solutions lie in the negative region for a
particular range of the radial coordinate.",['gr-qc'],False,,,,"Dark matter distributions around Schwarzschild-like black holes in
  bumblebee and Kalb-Ramond models","Possible Existence of Ghost Stars in the context of Electromagnetic
  Field"
neg-d2-400,2025-03-10,,2503.07196," Quantum Key Distribution (QKD) promises information-theoretic security, yet
integrating QKD into existing protocols like TLS remains challenging due to its
fundamentally different operational model. In this paper, we propose a hybrid
QKD-KEM protocol with two distinct integration approaches: a client-initiated
flow compatible with both ETSI 004 and 014 specifications, and a
server-initiated flow similar to existing work but limited to stateless ETSI
014 APIs. Unlike previous implementations, our work specifically addresses the
integration of stateful QKD key exchange protocols (ETSI 004) which is
essential for production QKD networks but has remained largely unexplored. By
adapting OpenSSL's provider infrastructure to accommodate QKD's pre-distributed
key model, we maintain compatibility with current TLS implementations while
offering dual layers of security. Performance evaluations demonstrate the
feasibility of our hybrid scheme with acceptable overhead, showing that robust
security against quantum threats is achievable while addressing the unique
requirements of different QKD API specifications.",['cs.CR'],2503.12952," As quantum computing advances, modern cryptographic standards face an
existential threat, necessitating a transition to post-quantum cryptography
(PQC). The National Institute of Standards and Technology (NIST) has selected
CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure
key exchange and digital signatures, respectively. This study conducts a
comprehensive performance analysis of these algorithms by benchmarking
execution times across cryptographic operations such as key generation,
encapsulation, decapsulation, signing, and verification. Additionally, the
impact of AVX2 optimizations is evaluated to assess hardware acceleration
benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient
execution times, outperforming classical cryptographic schemes such as RSA and
ECDSA at equivalent security levels. Beyond technical performance, the
real-world deployment of PQC introduces challenges in telecommunications
networks, where large-scale infrastructure upgrades, interoperability with
legacy systems, and regulatory constraints must be addressed. This paper
examines the feasibility of PQC adoption in telecom environments, highlighting
key transition challenges, security risks, and implementation strategies.
Through industry case studies, we illustrate how telecom operators are
integrating PQC into 5G authentication, subscriber identity protection, and
secure communications. Our analysis provides insights into the computational
trade-offs, deployment considerations, and standardization efforts shaping the
future of quantum-safe cryptographic infrastructure.",['cs.CR'],False,,,,QKD-KEM: Hybrid QKD Integration into TLS with OpenSSL Providers,"Performance Analysis and Industry Deployment of Post-Quantum
  Cryptography Algorithms"
neg-d2-401,2025-02-13,,2502.09598," The continuous operation of Earth-orbiting satellites generates vast and
ever-growing archives of Remote Sensing (RS) images. Natural language presents
an intuitive interface for accessing, querying, and interpreting the data from
such archives. However, existing Vision-Language Models (VLMs) are
predominantly trained on web-scraped, noisy image-text data, exhibiting limited
exposure to the specialized domain of RS. This deficiency results in poor
performance on RS-specific tasks, as commonly used datasets often lack
detailed, scientifically accurate textual descriptions and instead emphasize
solely on attributes like date and location. To bridge this critical gap, we
introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and
multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated
RS image-text pairs, representing a diverse range of RS modalities associated
to different spatial resolutions. Unlike existing vision-language datasets in
RS, GAIA specifically focuses on capturing a diverse range of RS applications,
providing unique information about environmental changes, natural disasters,
and various other dynamic phenomena. The dataset provides a spatially and
temporally balanced distribution, spanning across the globe, covering the last
25 years with a balanced temporal distribution of observations. GAIA's
construction involved a two-stage process: (1) targeted web-scraping of images
and accompanying text from reputable RS-related sources, and (2) generation of
five high-quality, scientifically grounded synthetic captions for each image
using carefully crafted prompts that leverage the advanced vision-language
capabilities of GPT-4o. Our extensive experiments, including fine-tuning of
CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance
on RS image classification, cross-modal retrieval and image captioning tasks.",['cs.CV'],2502.10259," The ability to observe the world is fundamental to reasoning and making
informed decisions on how to interact with the environment. However, optical
perception can often be disrupted due to common occurrences, such as
occlusions, which can pose challenges to existing vision systems. We present
MITO, the first millimeter-wave (mmWave) dataset of diverse, everyday objects,
collected using a UR5 robotic arm with two mmWave radars operating at different
frequencies and an RGB-D camera. Unlike visible light, mmWave signals can
penetrate common occlusions (e.g., cardboard boxes, fabric, plastic) but each
mmWave frame has much lower resolution than typical cameras. To capture
higher-resolution mmWave images, we leverage the robot's mobility and fuse
frames over the synthesized aperture. MITO captures over 24 million mmWave
frames and uses them to generate 550 high-resolution mmWave (synthetic
aperture) images in line-of-sight and non-light-of-sight (NLOS), as well as
RGB-D images, segmentation masks, and raw mmWave signals, taken from 76
different objects. We develop an open-source simulation tool that can be used
to generate synthetic mmWave images for any 3D triangle mesh. Finally, we
demonstrate the utility of our dataset and simulator for enabling broader NLOS
perception by developing benchmarks for NLOS segmentation and classification.",['cs.CV'],False,,,,"GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for
  Remote Sensing Image Analysis","MITO: A Millimeter-Wave Dataset and Simulator for Non-Line-of-Sight
  Perception"
neg-d2-402,2025-02-11,,2502.07709," Open-ended learning agents must efficiently prioritize goals in vast
possibility spaces, focusing on those that maximize learning progress (LP).
When such autotelic exploration is achieved by LLM agents trained with online
RL in high-dimensional and evolving goal spaces, a key challenge for LP
prediction is modeling one's own competence, a form of metacognitive
monitoring. Traditional approaches either require extensive sampling or rely on
brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive
framework that lets LLM agents learn to predict their competence and LP online.
By capturing semantic relationships between goals, MAGELLAN enables
sample-efficient LP estimation and dynamic adaptation to evolving goal spaces
through generalization. In an interactive learning environment, we show that
MAGELLAN improves LP prediction efficiency and goal prioritization, being the
only method allowing the agent to fully master a large and evolving goal space.
These results demonstrate how augmenting LLM agents with a metacognitive
ability for LP predictions can effectively scale curriculum learning to
open-ended goal spaces.",['cs.AI'],2502.09294," Automatic Affect Prediction (AAP) uses computational analysis of input data
such as text, speech, images, and physiological signals to predict various
affective phenomena (e.g., emotions or moods). These models are typically
constructed using supervised machine-learning algorithms, which rely heavily on
labeled training datasets. In this position paper, we posit that all AAP
training data are derived from human Affective Interpretation Processes,
resulting in a form of Affective Meaning. Research on human affect indicates a
form of complexity that is fundamental to such meaning: it can possess what we
refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing
Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of
confidence regarding meanings' correctness), Ambiguity (meaning contains
mutually exclusive concepts) and Vagueness (meaning is situated at different
levels in a nested hierarchy). Failing to appropriately consider QIs leads to
results incapable of meaningful and reliable predictions. Based on this
premise, we argue that a crucial step in adequately addressing indeterminacy in
AAP is the development of data collection practices for modeling corpora that
involve the systematic consideration of 1) a relevant set of QIs and 2) context
for the associated interpretation processes. To this end, we are 1) outlining a
conceptual model of AIPs and the QIs associated with the meaning these produce
and a conceptual structure of relevant context, supporting understanding of its
role. Finally, we use our framework for 2) discussing examples of
context-sensitivity-related challenges for addressing QIs in data collection
setups. We believe our efforts can stimulate a structured discussion of both
the role of aspects of indeterminacy and context in research on AAP, informing
the development of better practices for data collection and analysis.",['cs.AI'],False,,,,"MAGELLAN: Metacognitive predictions of learning progress guide autotelic
  LLM agents in large goal spaces","Indeterminacy in Affective Computing: Considering Meaning and Context in
  Data Collection Practices"
neg-d2-403,2025-01-06,,2501.02939," Three-dimensional atomic force microscopy (3D-AFM) has been a powerful tool
to probe the atomic-scale structure of solid-liquid interfaces. As a nanoprobe
moves along the 3D volume of interfacial liquid, the probe-sample interaction
force is sensed and mapped, providing information on not only the solid
morphology, but also the liquid density distribution. To date 3D-AFM force maps
of a diverse set of solid-liquid interfaces have been recorded, revealing
remarkable force oscillations that are typically attributed to solvation layers
or electrical double layers. However, despite the high resolution down to
sub-angstrom level, quantitative interpretation of the 3D force maps has been
an outstanding challenge. Here we will review the technical details of 3D-AFM
and the existing approaches for quantitative data interpretation. Based on
evidences in recent literature, we conclude that the perturbation-induced AFM
force paradoxically represents the intrinsic, unperturbed liquid density
profile. We will further discuss how the oscillatory force profiles can be
attributed to the probe-modulation of the liquid configurational entropy, and
how the quantitative, atomic-scale liquid density distribution can be derived
from the force maps.",['physics.chem-ph'],2501.06341," Colloidal two-dimensional lead chalcogenide nanocrystals represent an
intriguing new class of materials that push the boundaries of quantum
confinement by combining a crystal thickness down to the monolayer with
confinement in the lateral dimension. In particular flat PbSe quantum dots
exhibit efficient telecommunication band-friendly photoluminescence (1.43 -
0.83 eV with up to 61% quantum yield) that is highly interesting for
fiber-optics information processing. By using cryogenic scanning tunneling
microscopy and spectroscopy, we probe distinct single layer-defined PbSe
quantum dot populations down to a monolayer with in-gap state free quantum
dot-like density of states, in agreement with theoretical tight binding
calculations. Cryogenic ensemble photoluminescence spectra reveal mono-, bi-,
and trilayer contribution, confirming the structural, electronic and
theoretical results. From larger timescale shifts and ratio changes in the
optical spectra we infer Ostwald ripening in solution and fusing in deposited
samples of thinner flat PbSe quantum dots, which can be slowed down by surface
passivation with PbI2. By uncovering the interplay between thickness, lateral
size and density of states, as well as the synthetic conditions and
post-synthetic handling, our findings enable the target-oriented synthesis of
two-dimensional PbSe quantum dots with precisely tailored optical properties at
telecom wavelengths.",['physics.chem-ph'],False,,,,"Towards Quantitative Interpretation of 3D Atomic Force Microscopy at
  Solid-Liquid Interfaces","Monolayer-Defined Flat Colloidal PbSe Quantum Dots in Extreme
  Confinement"
neg-d2-404,2025-03-03,,2503.01257," Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on
mobile devices. However, due to the manufacturing constraints of compact
devices and the inherent physical principles of imaging, dToF depth maps are
sparse and noisy. In this paper, we propose a novel video depth completion
method, called SVDC, by fusing the sparse dToF data with the corresponding RGB
guidance. Our method employs a multi-frame fusion scheme to mitigate the
spatial ambiguity resulting from the sparse dToF imaging. Misalignment between
consecutive frames during multi-frame fusion could cause blending between
object edges and the background, which results in a loss of detail. To address
this, we introduce an adaptive frequency selective fusion (AFSF) module, which
automatically selects convolution kernel sizes to fuse multi-frame features.
Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to
enhance features and generates an attention map as fusion weights. The AFSF
ensures edge detail recovery while suppressing high-frequency noise in smooth
regions. To further enhance temporal consistency, We propose a cross-window
consistency loss to ensure consistent predictions across different windows,
effectively reducing flickering. Our proposed SVDC achieves optimal accuracy
and consistency on the TartanAir and Dynamic Replica datasets. Code is
available at https://github.com/Lan1eve/SVDC.",['cs.CV'],2502.1815," Recent approaches to jointly reconstruct 3D humans and objects from a single
RGB image represent 3D shapes with template-based or coarse models, which fail
to capture details of loose clothing on human bodies. In this paper, we
introduce a novel implicit approach for jointly reconstructing realistic 3D
clothed humans and objects from a monocular view. For the first time, we model
both the human and the object with an implicit representation, allowing to
capture more realistic details such as clothing. This task is extremely
challenging due to human-object occlusions and the lack of 3D information in 2D
images, often leading to poor detail reconstruction and depth ambiguity. To
address these problems, we propose a novel attention-based neural implicit
model that leverages image pixel alignment from both the input human-object
image for a global understanding of the human-object scene and from local
separate views of the human and object images to improve realism with, for
example, clothing details. Additionally, the network is conditioned on semantic
features derived from an estimated human-object pose prior, which provides 3D
spatial information about the shared space of humans and objects. To handle
human occlusion caused by objects, we use a generative diffusion model that
inpaints the occluded regions, recovering otherwise lost details. For training
and evaluation, we introduce a synthetic dataset featuring rendered scenes of
inter-occluded 3D human scans and diverse objects. Extensive evaluation on both
synthetic and real-world datasets demonstrates the superior quality of the
proposed human-object reconstructions over competitive methods.",['cs.CV'],False,,,,"SVDC: Consistent Direct Time-of-Flight Video Depth Completion with
  Frequency Selective Fusion","Realistic Clothed Human and Object Joint Reconstruction from a Single
  Image"
neg-d2-405,2025-03-05,,2503.03192," Reliable simultaneous localization and mapping (SLAM) algorithms are
necessary for safety-critical autonomous navigation. In the
communication-constrained multi-agent setting, navigation systems increasingly
use point-to-point range sensors as they afford measurements with low bandwidth
requirements and known data association. The state estimation problem for these
systems takes the form of range-aided (RA) SLAM. However, distributed
algorithms for solving the RA-SLAM problem lack formal guarantees on the
quality of the returned estimate. To this end, we present the first distributed
algorithm for RA-SLAM that can efficiently recover certifiably globally optimal
solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),
achieves this via the Riemannian Staircase method, where computational
procedures developed for distributed certifiably correct pose graph
optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's
efficacy on real-world multi-agent datasets by achieving absolute trajectory
errors comparable to those of a state-of-the-art centralized certifiably
correct RA-SLAM algorithm. Additionally, we perform a parametric study on the
structure of the RA-SLAM problem using synthetic data, revealing how common
parameters affect DCORA's performance.",['cs.RO'],2503.16559," This paper proposes a design scheme of reward function that constantly
evaluates both driving states and actions for applying reinforcement learning
to automated driving. In the field of reinforcement learning, reward functions
often evaluate whether the goal is achieved by assigning values such as +1 for
success and -1 for failure. This type of reward function can potentially obtain
a policy that achieves the goal, but the process by which the goal is reached
is not evaluated. However, process to reach a destination is important for
automated driving, such as keeping velocity, avoiding risk, retaining distance
from other cars, keeping comfortable for passengers. Therefore, the reward
function designed by the proposed scheme is suited for automated driving by
evaluating driving process. The effects of the proposed scheme are demonstrated
on simulated circuit driving and highway cruising. Asynchronous Advantage
Actor-Critic is used, and models are trained under some situations for
generalization. The result shows that appropriate driving positions are
obtained, such as traveling on the inside of corners, and rapid deceleration to
turn along sharp curves. In highway cruising, the ego vehicle becomes able to
change lane in an environment where there are other vehicles with suitable
deceleration to avoid catching up to a front vehicle, and acceleration so that
a rear vehicle does not catch up to the ego vehicle.",['cs.RO'],False,,,,Distributed Certifiably Correct Range-Aided SLAM,"Design of Reward Function on Reinforcement Learning for Automated
  Driving"
neg-d2-406,2025-01-28,,2501.17118," For each $f\!:\!\mathbb{R}\to\mathbb{C}$ that is Henstock--Kurzweil
integrable on the real line, or is a distribution in the completion of the
space of Henstock--Kurzweil integrable functions in the Alexiewicz norm, it is
shown that the Fourier transform is the second distributional derivative of a
H\""older continuous function. The space of such Fourier transforms is
isometrically isomorphic to the completion of the Henstock--Kurzweil integrable
functions. There is an exchange theorem, inversion in norm and convolution
results. Sufficient conditions are given for an $L^1$ function to have a
Fourier transform that is of bounded variation. Pointwise inversion of the
Fourier transform is proved for functions in $L^p$ spaces for $1<p<\infty$. The
exchange theorem is used to evaluate an integral that does not appear in
published tables.",['math.CA'],2502.13786," Let $G$ be a locally compact abelian group, and let $\widehat{G}$ denote its
dual group, equipped with a Haar measure. A variant of the uncertainty
principle states that for any $S \subset G$ and $\Sigma \subset \widehat{G}$,
there exists a constant $C(S, \Sigma)$ such that for any $f \in L^2(G)$, the
following inequality holds: \[\|f\|_{L^2(G)} \leq C(S, \Sigma) \bigl(
\|f\|_{L^2(G \setminus S)} + \|\widehat{f}\|_{L^2(\widehat{G} \setminus
\Sigma)} \bigr),\] where $\widehat{f}$ denotes the Fourier transform of $f$.
This variant of the uncertainty principle is particularly useful in
applications such as signal processing and control theory.The purpose of this
paper is to show that such estimates can be strengthened when $S$ or $\Sigma$
satisfies a restriction theorem and to provide an estimate for the constant
$C(S, \Sigma)$. This result serves as a quantitative counterpart to a recent
finding by the first and last author. In the setting of finite groups, the
results also extend those of Matolcsi-Sz\""ucs and Donoho-Stark.",['math.CA'],False,,,,"The Fourier transform with Henstock--Kurzweil and continuous primitive
  integrals","Uncertainty Principle, annihilating pairs and Fourier restriction"
neg-d2-407,2025-03-21,,2503.17147," We consider entangling operations in a single nitrogen-vacancy (NV) center in
diamond where the hyperfine-coupled nuclear spin qubits are addressed with
radio-frequency (rf) pulses conditioned on the state of the central electron
spin. Limiting factors for the gate fidelity are coherent errors due to
off-resonant driving of neighboring transitions in the dense, hyperfine-split
energy spectrum of the defect and non-negligible perpendicular hyperfine tensor
components that narrow the choice of $^{13}\rm C$ nuclear spin qubits. We
address these issues by presenting protocols based on synchronization effects
that allow for a complete suppression of both error sources in state-of-the-art
CNOT gate schemes. This is possible by a suitable choice of parameter sets that
incorporate the error into the scheme instead of avoiding it. These results
contribute to the recent progress toward scalable quantum computation with
defects in solids.",['quant-ph'],2502.04996," The Tilloy-Di\'osi (TD) prescription allows to turn any Markovian spontaneous
collapse model that can be formally regarded as a continuous measurement
process of the mass density into a hybrid classical-quantum theory in which the
gravitational Newtonian field enters as a classical field. We study the
application of a similar idea to the Poissonian Spontaneous Localization (PSL)
model. As in the TD case, the Newtonian classical field is again recovered upon
averaging, and additional decoherence appears due to the gravitational
back-reaction. We study general features of this model and investigate the
dynamics of a single particle and a rigid spherical body. With respect to the
TD models, the PSL model presents some notable differences such as the absence
of long-range decoherence due to the gravitational back-reaction noise and the
absence of negative mass measurements.",['quant-ph'],False,,,,"Suppression of coherent errors during entangling operations in NV
  centers in diamond",Newtonian Gravity from a Poissonian Spontaneous Collapse Model
neg-d2-408,2025-03-04,,2503.02441," Security researchers grapple with the surge of malicious files, necessitating
swift identification and classification of malware strains for effective
protection. Visual classifiers and in particular Convolutional Neural Networks
(CNNs) have emerged as vital tools for this task. However, issues of robustness
and explainability, common in other high risk domain like medicine and
autonomous vehicles, remain understudied in current literature. Although deep
learning visualization classifiers presented in research obtain great results
without the need for expert feature extraction, they have not been properly
studied in terms of their replicability. Additionally, the literature is not
clear on how these types of classifiers arrive to their answers. Our study
addresses these gaps by replicating six CNN models and exploring their
pitfalls. We employ Class Activation Maps (CAMs), like GradCAM and HiResCAM, to
assess model explainability. We evaluate the CNNs' performance and
interpretability on two standard datasets, MalImg and Big2015, and a newly
created called VX-Zoo. We employ these different CAM techniques to gauge the
explainability of each of the models. With these tools, we investigate the
underlying factors contributing to different interpretations of inputs across
the different models, empowering human researchers to discern patterns crucial
for identifying distinct malware families and explain why CNN models arrive at
their conclusions. Other then highlighting the patterns found in the
interpretability study, we employ the extracted heatmpas to enhance Visual
Transformers classifiers' performance and explanation quality. This approach
yields substantial improvements in F1 score, ranging from 2% to 8%, across the
datasets compared to benchmark values.",['cs.CR'],2501.12034," Like most computer systems, a manycore can also be the target of security
attacks. It is essential to ensure the security of the NoC since all
information travels through its channels, and any interference in the traffic
of messages can reflect on the entire chip, causing communication problems.
Among the possible attacks on NoC, Denial of Service (DoS) attacks are the most
cited in the literature. The state of the art shows a lack of work that can
detect such attacks through learning techniques. On the other hand, these
techniques are widely explored in computer network security via an Intrusion
Detection System (IDS). In this context, the main goal of this document is to
present the progress of a work that explores an IDS technique using machine
learning and temporal series for detecting DoS attacks in NoC-based manycore
systems. To fulfill this goal, it is necessary to extract traffic data from a
manycore NoC and execute the learning techniques in the extracted data.
However, while low-level platforms offer precision and slow execution,
high-level platforms offer higher speed and data incompatible with reality.
Therefore, a platform is being developed using the OVP tool, which has a higher
level of abstraction. To solve the low precision problem, the developed
platform will have its data validated with a low-level platform.",['cs.CR'],False,,,,"Through the Static: Demystifying Malware Visualization via
  Explainability","Application of Machine Learning Techniques for Secure Traffic in
  NoC-based Manycores"
neg-d2-409,2025-03-22,,2503.1782," Interactive segmentation aims to segment the specified target on the image
with positive and negative clicks from users. Interactive ambiguity is a
crucial issue in this field, which refers to the possibility of multiple
compliant outcomes with the same clicks, such as selecting a part of an object
versus the entire object, a single object versus a combination of multiple
objects, and so on. The existing methods cannot provide intuitive guidance to
the model, which leads to unstable output results and makes it difficult to
meet the large-scale and efficient annotation requirements for specific targets
in some scenarios. To bridge this gap, we introduce RefCut, a reference-based
interactive segmentation framework designed to address part ambiguity and
object ambiguity in segmenting specific targets. Users only need to provide a
reference image and corresponding reference masks, and the model will be
optimized based on them, which greatly reduces the interactive burden on users
when annotating a large number of such targets. In addition, to enrich these
two kinds of ambiguous data, we propose a new Target Disassembly Dataset which
contains two subsets of part disassembly and object disassembly for evaluation.
In the combination evaluation of multiple datasets, our RefCut achieved
state-of-the-art performance. Extensive experiments and visualized results
demonstrate that RefCut advances the field of intuitive and controllable
interactive segmentation. Our code will be publicly available and the demo
video is in https://www.lin-zheng.com/refcut.",['cs.CV'],2502.19896," Existing point cloud completion methods, which typically depend on predefined
synthetic training datasets, encounter significant challenges when applied to
out-of-distribution, real-world scans. To overcome this limitation, we
introduce a zero-shot completion framework, termed GenPC, designed to
reconstruct high-quality real-world scans by leveraging explicit 3D generative
priors. Our key insight is that recent feed-forward 3D generative models,
trained on extensive internet-scale data, have demonstrated the ability to
perform 3D generation from single-view images in a zero-shot setting. To
harness this for completion, we first develop a Depth Prompting module that
links partial point clouds with image-to-3D generative models by leveraging
depth images as a stepping stone. To retain the original partial structure in
the final results, we design the Geometric Preserving Fusion module that aligns
the generated shape with input by adaptively adjusting its pose and scale.
Extensive experiments on widely used benchmarks validate the superiority and
generalizability of our approach, bringing us a step closer to robust
real-world scan completion.",['cs.CV'],False,,,,RefCut: Interactive Segmentation with Reference Guidance,GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors
neg-d2-410,2025-03-06,,2503.05131," We perform a systematic study of the possible molecular states composed of a
pair of heavy mesons such as $D^{(*)}D^{(*)}$, $D^{(*)}\bar{D}^{(*)}$ in the
framework of the one-boson-exchange model. The exchanged bosons include the
pseudoscalar, scalar and vector mesons($\pi$, $\sigma$, $\rho$, $\omega$). We
use the Bonn approximation to get the interaction potential of
one-boson-exchange model, then apply the complex scaling method to calculate
the bound and resonant states. The results indicate that the $D^{(*)}D^{(*)}$
and $D^{(*)}\bar{D}^{(*)}$ system can not only form several bound states, but
also a P-wave resonant state. The hadron molecular state model can explain the
structure of $T_{cc}^+$ as a bound state $DD^{*}$ with quantum number $I(J^P) =
0(1^+)$. In addition, we also discovered other bound and resonant states, which
have the potential to be observed experimentally.",['hep-ph'],2501.14015," Progress in the theoretical understanding of parton branching dynamics within
an expanding Quark Gluon Plasma relies on detailed and fair comparisons with
experimental data for reconstructed jets. Such comparisons are only meaningful
when the computed jet, be it analytically or via event generation, accounts for
the complexity of jets reconstructed in the challenging environment of
heavy-ion collisions. Jet reconstruction in heavy ion collisions involves a
necessarily imperfect subtraction of the large and fluctuating underlying
event: reconstructed jets always include underlying event contamination. To
identify true jet quenching effects, modifications due to the interaction of
the branching partonic system with the Quark Gluon Plasma, we establish a
baseline that accounts for possible background contamination on unmodified
jets. In practical terms, jet quenching effects are only those not present in
jets produced in proton-proton collisions that have been embedded in a
realistic heavy-ion background and where subtraction has been carried out
analogously to that in the heavy ion case. With this setup, we assess the
sensitivity to underlying event of commonly discussed jet quenching observables
and its impact on the robustness of Machine Learning studies, aimed at
classifying jets according to their degree of modification by the Quark Gluon
Plasma, that rely on those observables. We find the discrimination power of a
simple Boosted Decision Tree to be robust in the realistic scenario where both
medium response and underlying event are present, giving support to portability
to the experimental context.",['hep-ph'],False,,,,"The bound and resonant states of $D^{(*)}D^{(*)}$ and
  $D^{(*)}\bar{D}^{(*)}$ with the complex scaling method","Apples to Apples in Jet Quenching: robustness of Machine Learning
  classification of quenched jets to Underlying Event contamination"
neg-d2-411,2025-03-17,,2503.13771," We present components of an AI-assisted academic writing system including
citation recommendation and introduction writing. The system recommends
citations by considering the user's current document context to provide
relevant suggestions. It generates introductions in a structured fashion,
situating the contributions of the research relative to prior work. We
demonstrate the effectiveness of the components through quantitative
evaluations. Finally, the paper presents qualitative research exploring how
researchers incorporate citations into their writing workflows. Our findings
indicate that there is demand for precise AI-assisted writing systems and
simple, effective methods for meeting those needs.",['cs.AI'],2502.06656," The recent development of powerful AI systems has highlighted the need for
robust risk management frameworks in the AI industry. Although companies have
begun to implement safety frameworks, current approaches often lack the
systematic rigor found in other high-risk industries. This paper presents a
comprehensive risk management framework for the development of frontier AI that
bridges this gap by integrating established risk management principles with
emerging AI-specific practices. The framework consists of four key components:
(1) risk identification (through literature review, open-ended red-teaming, and
risk modeling), (2) risk analysis and evaluation using quantitative metrics and
clearly defined thresholds, (3) risk treatment through mitigation measures such
as containment, deployment controls, and assurance processes, and (4) risk
governance establishing clear organizational structures and accountability.
Drawing from best practices in mature industries such as aviation or nuclear
power, while accounting for AI's unique challenges, this framework provides AI
developers with actionable guidelines for implementing robust risk management.
The paper details how each component should be implemented throughout the
life-cycle of the AI system - from planning through deployment - and emphasizes
the importance and feasibility of conducting risk management work prior to the
final training run to minimize the burden associated with it.",['cs.AI'],False,,,,Towards AI-assisted Academic Writing,"A Frontier AI Risk Management Framework: Bridging the Gap Between
  Current AI Practices and Established Risk Management"
neg-d2-412,2025-03-11,,2503.08894," Origami metamaterials made of repeating unit cells of parallelogram panels
joined at folds dramatically change their shape through a collective motion of
their cells. Here we develop an effective elastic model and numerical method to
study the large deformation response of these metamaterials under a broad class
of loads. The model builds on an effective plate theory derived in our prior
work [64]. The theory captures the overall shape change of all slightly
stressed parallelogram origami deformations through nonlinear geometric
compatibility constraints that couple the origami's (cell averaged) effective
deformation to an auxiliary angle field quantifying its cell-by-cell actuation.
It also assigns to each such origami deformation a plate energy associated to
these effective fields. Seeking a constitutive model that is faithful to the
theory but also practical to simulate, we relax the geometric constraints via
corresponding elastic energy penalties; we also simplify the plate energy
density to embrace its essential character as a regularization to the geometric
penalties. The resulting model for parallelogram origami is a generalized
elastic continuum that is nonlinear in the effective deformation gradient and
angle field and regularized by high-order gradients thereof. We provide a
finite element formulation of this model using the $C^0$ interior penalty
method to handle second gradients of deformation, and implement it using the
open source computing platform Firedrake. We end by using the model and
numerical method to study two canonical parallelogram origami patterns, in
Miura and Eggbox origami, under a variety of loading conditions.",['cond-mat.soft'],2502.03959," The hard-sphere potential has become a cornerstone in the study of both
molecular and complex fluids. Despite its mathematical simplicity, its
implementation in fixed time-step molecular simulations remains a formidable
challenge due to the discontinuity at contact. To circumvent the issues
associated with the ill-defined force at contact, a continuous
potential--referred to here as the pseudo-hard-sphere (pHS) potential--has
recently been proposed [J. Chem, Phys. 149, 164907 (2018)]. This potential is
constructed to match the second virial coefficient of the hard-sphere potential
and is expected to mimic its thermodynamic properties. However, this hypothesis
has only been partially validated within the fluid region of the phase diagram
for hard-sphere dispersions in two and three dimensions. In this contribution,
we examine the ability of the continuous pHS potential to reproduce the
equation of state of a hard-sphere fluid, not only in the fluid phase but also
across the fluid-solid coexistence region. Our focus is primarily on
hard-sphere systems in three and four dimensions. We compare the results
obtained from Brownian dynamics simulations of the pHS potential with those
derived from refined event-driven simulations of the corresponding hard-sphere
potential. Furthermore, we provide a comparative analysis with theoretical
equations of state based on both mean-field and integral equation
approximations.",['cond-mat.soft'],False,,,,"Modeling and computation of the effective elastic behavior of
  parallelogram origami metamaterials","Phase diagram of the hard-sphere potential model in three and four
  dimensions using a pseudo-hard-sphere potential"
neg-d2-413,2025-02-20,,2502.14766," We propose a structural default model for portfolio-wide valuation
adjustments (xVAs) and represent it as a system of coupled backward stochastic
differential equations. The framework is divided into four layers, each
capturing a key component: (i) clean values, (ii) initial margin and Collateral
Valuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments
(CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding
Valuation Adjustment (FVA). Because these layers depend on one another through
collateral and default effects, a naive Monte Carlo approach would require
deeply nested simulations, making the problem computationally intractable.
  To address this challenge, we use an iterative deep BSDE approach, handling
each layer sequentially so that earlier outputs serve as inputs to the
subsequent layers. Initial margin is computed via deep quantile regression to
reflect margin requirements over the Margin Period of Risk. We also adopt a
change-of-measure method that highlights rare but significant defaults of the
bank or counterparty, ensuring that these events are accurately captured in the
training process.
  We further extend Han and Long's (2020) a posteriori error analysis to BSDEs
on bounded domains. Due to the random exit from the domain, we obtain an order
of convergence of $\mathcal{O}(h^{1/4-\epsilon})$ rather than the usual
$\mathcal{O}(h^{1/2})$.
  Numerical experiments illustrate that this method drastically reduces
computational demands and successfully scales to high-dimensional,
non-symmetric portfolios. The results confirm its effectiveness and accuracy,
offering a practical alternative to nested Monte Carlo simulations in
multi-counterparty xVA analyses.",['q-fin.CP'],2503.11053," This paper develops general approaches for pricing various types of
American-style Parisian options (down-in/-out, perpetual/finite-maturity) with
general payoff functions based on continuous-time Markov chain (CTMC)
approximation under general 1D time-inhomogeneous Markov models. For the
down-in types, by conditioning on the Parisian stopping time, we reduce the
pricing problem to that of a series of vanilla American options with different
maturities and their prices integrated with the distribution function of the
Parisian stopping time yield the American Parisian down-in option price. This
facilitates an efficient application of CTMC approximation to obtain the
approximate option price by calculating the required quantities. For the
perpetual down-in cases under time-homogeneous models, significant
computational cost can be reduced. The down-out cases are more complicated, for
which we use the state augmentation approach to record the excursion duration
and then the approximate option price is obtained by solving a series of
variational inequalities recursively with the Lemke's pivoting method. We show
the convergence of CTMC approximation for all the types of American Parisian
options under general time-inhomogeneous Markov models, and the accuracy and
efficiency of our algorithms are confirmed with extensive numerical
experiments.",['q-fin.CP'],False,,,,"Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and
  Convergence Analysis","Pricing American Parisian Options under General Time-Inhomogeneous
  Markov Models"
neg-d2-414,2025-02-14,,2502.10591," Binary black holes (BBH) are expected to form and merge in active galactic
nuclei (AGN), deep in the potential well of a supermassive black hole (SMBH),
from populations that exist in a nuclear star cluster (NSC). Here we
investigate the gravitational wave (GW) signature of a BBH lensed by a nearby
SMBH. For a fiducial GW150914-like BBH orbiting close to a $10^{8}M_{\odot}$
SMBH located at $z=0.1$, the lensed GW signal varies in a predictable manner in
and out of the LISA detectability band and across frequencies. The occurrence
of such signatures has the potential to confound LISA global fit models if they
are not modelled. Detection of these sources provide an independent measure of
AGN inclination angles, along with detecting warping of the inner disk, and
measuring the SMBH spin.",['astro-ph.HE'],2502.11434," The KM3Net Collaboration has recently reported on the observation of a
remarkable event KM3-230213A that could have been produced by an ultra high
energy cosmic neutrino. The origin of this event is still unclear. In
particular, the cosmogenic neutrino scenario is not favoured due to the
non-observation of a similar event by the IceCube detector, and most galactic
scenarios are disfavoured as well. We show that the blazar PKS 0605-085 is a
viable source of the KM3-230213A event. In particular, even though this blazar
is located at 2.4$^{\circ}$ from the KM3-230213A event, the association between
the blazar and the event is not unlikely due to a sizable direction systematic
uncertainty of $\approx 1.5^{\circ}$ reported by the KM3Net Collaboration.
Furthermore, we show that the observation of a $\approx$72 PeV neutrino from
PKS 0605-085 is entirely possible given that a $\approx$7.5 PeV neutrino could
have been observed from another blazar TXS 0506+056. Finally, we consider
$\gamma$-ray constraints on the number of observable neutrino events and show
that for the case of the external photon field production mechanism these
constraints could be relaxed due to the often-neglected effect of the
isotropisation of the hadronically-produced electrons in the magnetic field of
the blob. We encourage further multi-wavelength observations of the blazar PKS
0605-085.",['astro-ph.HE'],False,,,,Evolution of LISA Observables for Binary Black Holes Lensed by an SMBH,"The blazar PKS 0605-085 as the origin of the KM3-230213A ultra high
  energy neutrino event"
neg-d2-415,2025-03-13,,2503.10964," The Linear Quadratic Regulator (LQR) is a cornerstone of optimal control
theory, widely studied in both model-based and model-free approaches. Despite
its well-established nature, certain foundational aspects remain subtle. In
this paper, we revisit three key properties of policy optimization in LQR: (i)
strong duality in the nonconvex policy optimization formulation, (ii) the
gradient dominance property, examining when it holds and when it fails, and
(iii) the global optimality of linear static policies. Using primal-dual
analysis and convex reformulation, we refine and clarify existing results by
leveraging Riccati equations/inequalities, semidefinite programming (SDP)
duality, and a recent framework of Extended Convex Lifting (\texttt{ECL}). Our
analysis confirms that LQR 1) behaves almost like a convex problem (e.g.,
strong duality) under the standard assumptions of stabilizability and
detectability and 2) exhibits strong convexity-like properties (e.g., gradient
dominance) under slightly stronger conditions. In particular, we establish a
broader characterization under which gradient dominance holds using
\texttt{ECL} and the notion of Cauchy directions. By clarifying and refining
these theoretical insights, we hope this work contributes to a deeper
understanding of LQR and may inspire further developments beyond LQR.",['math.OC'],2501.0339," The Pseudo-Boolean problem deals with linear or polynomial constraints with
integer coefficients over Boolean variables. The objective lies in optimizing a
linear objective function, or finding a feasible solution, or finding a
solution that satisfies as many constraints as possible. In the 2024
Pseudo-Boolean competition, solvers incorporating the SCIP framework won five
out of six categories it was competing in. From a total of 1,207 instances,
SCIP successfully solved 759, while its parallel version FiberSCIP solved 776.
Based on the results from the competition, we further enhanced SCIP's
Pseudo-Boolean capabilities. This article discusses the results and presents
the winning algorithmic ideas.",['math.OC'],False,,,,"Revisiting Strong Duality, Hidden Convexity, and Gradient Dominance in
  the Linear Quadratic Regulator",State-of-the-art Methods for Pseudo-Boolean Solving with SCIP
neg-d2-416,2025-01-21,,2501.12069," When formulating a model there is a trade-off between model complexity and
(biological) realism. In the present paper we demonstrate how model reduction
from a precise mechanistic ""super model"" to simpler conceptual models using
Tikhonov-Fenichel reductions, an algebraic approach to singular perturbation
theory, can mitigate this problem. Compared to traditional methods for time
scale separations (Tikhonov's theorem, quasi-steady state assumption),
Tikhonov-Fenichel reductions have the advantage that we can compute a reduction
directly for a separation of rates into slow and fast ones instead of a
separation of components of the system. Moreover, we can find all such
reductions algorithmically.
  In the present paper we use Tikhonov-Fenichel reductions to analyse a
mutualism model tailored towards lichens with an explicit description of the
interaction. We find that (1) the implicit description of the interaction given
in the reductions by interaction terms (functional responses) varies depending
on the scenario, (2) there is a tendency for the mycobiont, an obligate
mutualist, to always benefit from the interaction while it can be detrimental
for the photobiont, a facultative mutualist, depending on the parameters, (3)
our model is capable of describing the shift from mutualism to parasitism, (4)
our model can produce bistability with multiple stable fixed points in the
interior of the first orthant. To analyse the reductions we formalize and
discuss a mathematical criterion that categorizes two-species interactions.
Throughout the paper we focus on the relation between the mathematics behind
Tikhonov-Fenichel reductions and their biological interpretation.",['q-bio.PE'],2501.12069," When formulating a model there is a trade-off between model complexity and
(biological) realism. In the present paper we demonstrate how model reduction
from a precise mechanistic ""super model"" to simpler conceptual models using
Tikhonov-Fenichel reductions, an algebraic approach to singular perturbation
theory, can mitigate this problem. Compared to traditional methods for time
scale separations (Tikhonov's theorem, quasi-steady state assumption),
Tikhonov-Fenichel reductions have the advantage that we can compute a reduction
directly for a separation of rates into slow and fast ones instead of a
separation of components of the system. Moreover, we can find all such
reductions algorithmically.
  In the present paper we use Tikhonov-Fenichel reductions to analyse a
mutualism model tailored towards lichens with an explicit description of the
interaction. We find that (1) the implicit description of the interaction given
in the reductions by interaction terms (functional responses) varies depending
on the scenario, (2) there is a tendency for the mycobiont, an obligate
mutualist, to always benefit from the interaction while it can be detrimental
for the photobiont, a facultative mutualist, depending on the parameters, (3)
our model is capable of describing the shift from mutualism to parasitism, (4)
our model can produce bistability with multiple stable fixed points in the
interior of the first orthant. To analyse the reductions we formalize and
discuss a mathematical criterion that categorizes two-species interactions.
Throughout the paper we focus on the relation between the mathematics behind
Tikhonov-Fenichel reductions and their biological interpretation.",['q-bio.PE'],False,,,,"Tikhonov-Fenichel Reductions and their Application to a Novel Modelling
  Approach for Mutualism","Tikhonov-Fenichel Reductions and their Application to a Novel Modelling
  Approach for Mutualism"
neg-d2-417,2025-01-10,,2501.06341," Colloidal two-dimensional lead chalcogenide nanocrystals represent an
intriguing new class of materials that push the boundaries of quantum
confinement by combining a crystal thickness down to the monolayer with
confinement in the lateral dimension. In particular flat PbSe quantum dots
exhibit efficient telecommunication band-friendly photoluminescence (1.43 -
0.83 eV with up to 61% quantum yield) that is highly interesting for
fiber-optics information processing. By using cryogenic scanning tunneling
microscopy and spectroscopy, we probe distinct single layer-defined PbSe
quantum dot populations down to a monolayer with in-gap state free quantum
dot-like density of states, in agreement with theoretical tight binding
calculations. Cryogenic ensemble photoluminescence spectra reveal mono-, bi-,
and trilayer contribution, confirming the structural, electronic and
theoretical results. From larger timescale shifts and ratio changes in the
optical spectra we infer Ostwald ripening in solution and fusing in deposited
samples of thinner flat PbSe quantum dots, which can be slowed down by surface
passivation with PbI2. By uncovering the interplay between thickness, lateral
size and density of states, as well as the synthetic conditions and
post-synthetic handling, our findings enable the target-oriented synthesis of
two-dimensional PbSe quantum dots with precisely tailored optical properties at
telecom wavelengths.",['physics.chem-ph'],2501.10042," We present a theoretical model to investigate the dynamics and spectroscopic
properties of a plexciton system consisting of a molecular exciton coupled to a
single short-lived plasmonic mode. The exciton is described as a two-level
system (TLS), while the plasmonic mode is treated as a dissipative harmonic
oscillator. The hierarchical equations of motion method is employed to simulate
energy transfer dynamics, absorption spectra, and two-dimensional electronic
spectra (2DES) of the system across a range of coupling strengths. It is shown
that increasing the exciton-plasmon coupling strength drives a transition in
the absorption spectra from an asymmetric Fano line shape to a Rabi splitting
pattern, while coupling the TLS to intramolecular vibrational modes reduces the
central dip of the absorption spectra and makes the line shape more symmetric.
The simulated 2DES exhibit distinct features compared to those of a coupled
molecular dimer, highlighting the unique nonlinear response of plexciton
systems. In addition, a ""breathing mod"" pattern observed in the strong coupling
regime can serve as a direct evidence of Rabi oscillation.",['physics.chem-ph'],False,,,,"Monolayer-Defined Flat Colloidal PbSe Quantum Dots in Extreme
  Confinement",A theoretical model for linear and nonlinear spectroscopy of plexcitons
neg-d2-418,2025-03-05,,2503.03892," Cislunar space is the volume between Earth's geosynchronous orbit and beyond
the Moon, including the lunar Lagrange points. Understanding the stability of
orbits within this space is crucial for the successful planning and execution
of space missions. Orbits in cislunar space are influenced by the gravitational
forces of the Sun, Earth, Moon, and other Solar System planets leading to
typically unpredictable and chaotic behavior. It is therefore difficult to
predict the stability of an orbit from a set of initial orbital elements. We
simulate one million cislunar orbits and use a self-organizing map (SOM) to
cluster the orbits into families based on how long they remain stable within
the cislunar regime. Utilizing Lawrence Livermore National Laboratory's (LLNL)
High Performance Computers (HPC) we develop a highly adaptable SOM capable of
efficiently characterizing observations from individual events. We are able to
predict the lifetime from the initial three line element (TLE) to within 10
percent for 8 percent of the test dataset, within 50 percent for 43 percent of
the dataset, and within 100 percent for 75 percent of the dataset. The
fractional absolute deviation peaks at 1 for all lifetimes. Multi-modal
clustering in the SOM suggests that a variety of orbital morphologies have
similar lifetimes. The trained SOMs use an average of 2.73 milliseconds of
computational time to produce an orbital stability prediction. The outcomes of
this research enhance our understanding of cislunar orbital dynamics and also
provide insights for mission planning, enabling the rapid identification of
stable orbital regions and pathways for future space exploration. As
demonstrated in this study, an SOM can generate orbital lifetime estimates from
minimal observational data, such as a single TLE, making it essential for early
warning systems and large-scale sensor network operations.",['astro-ph.EP'],2503.05433," The IRAC camera on the Spitzer Space Telescope observed 2175 Near Earth
Objects (NEOs) during its Warm Mission phase, primarily in three large surveys,
and also in a small number of a dedicated projects. In this paper we present
the final reprocessing of the NEO data and determine fluxes at 3.6 microns
(where available) and 4.5 microns. The observing windows range from minutes to
nearly ten hours, which means that for 39 NEOs we observe a complete
lightcurve, and for these objects we present period and amplitude estimates and
derive minimum cohesive strengths for the objects with well-determined periods.
For an additional 128 objects we detect a significant fraction of a complete
lightcurve, and present estimated lower limits to their rotation periods. This
paper presents the final and definitive Spitzer/IRAC NEO flux catalog.",['astro-ph.EP'],False,,,,Predicting Cislunar Orbit Lifetimes from Initial Orbital Elements,"Infrared Fluxes and Light Curves of Near-Earth Objects: The full Spitzer
  Sample"
neg-d2-419,2025-03-04,,2503.02364," Blending Painlev\'e property with singularity confinement for a general
arbitrary order Sawada-Kotera differential-difference equation, we find a
proliferation of ``tau-functions'' (coming from strictly confined patterns).
However only one of these function enters into the Hirota bilinear form (the
others give multi-linear expressions) but has specific relations with all
others. We also discuss the case of two modifications of Sawada-Kotera showing
that periodic patterns appear in addition to strictly confined ones. Fully
discretisations and express method for computing algebraic entropy are
discussed.",['nlin.SI'],2503.02364," Blending Painlev\'e property with singularity confinement for a general
arbitrary order Sawada-Kotera differential-difference equation, we find a
proliferation of ``tau-functions'' (coming from strictly confined patterns).
However only one of these function enters into the Hirota bilinear form (the
others give multi-linear expressions) but has specific relations with all
others. We also discuss the case of two modifications of Sawada-Kotera showing
that periodic patterns appear in addition to strictly confined ones. Fully
discretisations and express method for computing algebraic entropy are
discussed.",['nlin.SI'],False,,,,"Singularity confinement and proliferation of tau functions for a general
  differential-difference Sawada-Kotera equation","Singularity confinement and proliferation of tau functions for a general
  differential-difference Sawada-Kotera equation"
neg-d2-420,2025-01-23,,2501.13537," Precision determination of the hyperfine splitting of cadmium ions is
essential to study space-time variation of fundamental physical constants and
isotope shifts. In this work, we present the precision frequency measurement of
the excited-state $^2{P}_{3/2}$ hyperfine splitting of
$^{111,113}\mathrm{Cd}^+$ ions using the laser-induced fluorescence technique.
By introducing the technology of sympathetic cooling and setting up free-space
beat detection unit based on the optical comb, the uncertainties are improved
to 14.8 kHz and 10.0 kHz, respectively, two orders of magnitude higher than the
reported results from the linear transformation of isotope shifts. The magnetic
dipole constants $A_{P_{3/2}}$ of $^{111}\mathrm{Cd}^+$ and
$^{113}\mathrm{Cd}^+$ are estimated to be 395 938.8(7.4) kHz and 411 276.0(5.0)
kHz, respectively. The difference between the measured and theoretical
hyperfine structure constants indicates that more physical effects are required
to be considered in the theoretical calculation, and provides critical data for
the examination of deviation from King-plot linearity in isotope shifts.",['physics.atom-ph'],2503.17367," In this study, we develop and implement a specialized coupled-cluster (CC)
approach tailored for accurately describing atoms and molecules in strong
magnetic fields. Using the open-source Ghent Quantum Chemistry Package
(\texttt{GQCP}) in conjunction with the Python-based Simulations of Chemistry
Framework (\texttt{PySCF}), we calculate potential energy curves, permanent and
transient dipole moments, as well as vibrational spectra for the diatomic
molecules H$_2$, HeH$^+$ and LiH under various magnetic field strengths
adopting a fully non-perturbative treatment. The main computational
difficulties stem from the inclusion of the magnetic field in the Hamiltonian,
in particular, from the presence of the angular momentum operator, which leads
to a complication of the wave function and introduces a gauge-origin
dependence. Addressing these challenges requires advanced modifications to
existing routines, which we achieve by implementing gauge-comprising atomic
orbitals (GIAOs) by using \texttt{GQCP}, and the capabilities offered by
\texttt{PySCF}. This approach enhances the accuracy and reliability of the CC
theory, opening pathways for more comprehensive investigations in molecular
quantum chemistry at strong magnetic fields.",['physics.atom-ph'],False,,,,"Precision determination of the excited-state hyperfine splitting of
  Cadmium ions","Exploring the Properties of Light Diatomic Molecules in Strong Magnetic
  Fields"
neg-d2-421,2025-02-11,,2502.08112," The enigmatic ultraviolet (UV) extinction bump at 2175 Angstrom, the
strongest spectroscopic absorption feature superimposed on the interstellar
extinction curve, has recently been detected at the cosmic dawn by the James
Webb Space Telescope (JWST) in JADES-GS-z6-0, a distant galaxy at redshift
z=6.71, corresponding to a cosmic age of just 800 million years after the Big
Bang. Although small graphite grains have historically long been suggested as
the carrier of the 2175 Angstrom extinction bump and graphite grains are
expected to have already been pervasive in the early Universe, in this work we
demonstrate that small graphite grains are not responsible for the UV
extinction bump seen at the cosmic dawn in JADES-GS-z6-0, as the extinction
bump arising from small graphite grains is too broad and peaks at wavelengths
that are too short to be consistent with what is seen in JADES-GS-z6-0.",['astro-ph.GA'],2501.01613," The cold and hot interstellar medium (ISM) in star forming galaxies resembles
the reservoir for star formation and associated heating by stellar winds and
explosions during stellar evolution, respectively. We utilize data from deep
$Chandra$ observations and archival millimeter surveys to study the
interconnection between these two phases and the relation to star formation
activities in M51 on kiloparsec scales. A sharp radial decrease is present in
the hot gas surface brightness profile within the inner 2 kpc of M51. The ratio
between the total infrared luminosity ($L_{\rm IR}$) and the hot gas luminosity
($L_{\rm 0.5 - 2\,keV}^{\rm gas}$) shows a positive correlation with the
galactic radius in the central region. For the entire galaxy, a twofold
correlation is revealed in the $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm
IR}$ diagram, where $L_{\rm 0.5 - 2\,keV}^{\rm gas}$ sharply increases with
$L_{\rm IR}$ in the center but varies more slowly in the disk. The best fit
gives a steep relation of ${\rm log}(L_{\rm 0.5-2\,keV}^{\rm gas} /{\rm
erg\,s^{-1}})=1.82\,{\rm log}(L_{\rm IR} /{L_{\rm \odot}})+22.26$ for the
center of M51. The similar twofold correlations are also found in the $L_{\rm
0.5 - 2\,keV}^{\rm gas}$${-}$molecular line luminosity ($L^\prime_{\rm gas}$)
relations for the four molecular emission lines CO(1-0), CO(2-1), HCN(1-0), and
HCO$^+$(1-0). We demonstrate that the core-collapse supernovae (SNe) are the
primary source of energy for heating gas in the galactic center of M51, leading
to the observed steep $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm IR}$ and
$L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L^\prime_{\rm gas}$ relations, as their
X-ray radiation efficiencies ($\eta$ $\equiv$ $L_{\rm 0.5 - 2\,keV}^{\rm
gas}$/$\dot{E}_\mathrm{SN}$) increase with the star formation rate surface
densities, where $\dot{E}_\mathrm{SN}$ is the SN mechanical energy input rate.",['astro-ph.GA'],False,,,,What causes the ultraviolet extinction bump at the cosmic dawn?,"Fire and Ice in the Whirlpool: Spatially Resolved Scaling Relations
  between X-ray Emitting Hot Gas and Cold Molecular Gas in M51"
neg-d2-422,2025-01-19,,2501.11237," The solar transition region (TR) is a narrow interface between the
chromosphere and corona, where emitted radiation contains critical information
pertinent to coronal heating processes. We conducted 2-dimensional radiation
magnetohydrodynamics simulations using adaptive mesh refinement to spatially
resolve the fine structure of the TR while simultaneously capturing the
larger-scale dynamics originating from surface convection. The time evolution
of ionization fractions for oxygen ions is computed alongside the simulations.
A minimum grid size of 1.25 km is achieved in the TR, enabling adequate
resolution of the upper TR (log$_{10}T \gtrsim$ 5), although the lower TR
(log$_{10}T \lesssim$ 5) remains under-resolved. Doppler shifts and nonthermal
widths synthesized from TR lines exhibit convergence with grid sizes as coarse
as 40 km, though some discrepancies persist between our results and observed TR
line properties. A notable enhancement in emission from \ion{O}{6} lines,
converging at a grid size of 2.5 km, shows an intensity 1.2 times that expected
under ionization equilibrium, attributable to shock interactions with the TR.
While model refinements are still required, our ability to resolve the TR
offers critical insights into TR line characteristics arising from
non-equilibrium ionization states, advancing our understanding of the coronal
heating problem.",['astro-ph.SR'],2502.11838," The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is
among the early evidences that they contain multiple populations of stars.
Indeed, the location of each star along the HB depends both on its initial
helium content (Y) and on the global average mass loss along the red giant
branch ($\mu$). In most GCs, it is generally straightforward to analyse the
first stellar population (standard Y), and the most extreme one (largest Y),
while it is more tricky to look at the ""intermediate"" populations (mildly
enhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever
possible the helium abundance for each stellar populations is constrained by
using independent measurements present in the literature. We compare population
synthesis models with photometric catalogues from the Hubble Space Telescope
Treasury survey to derive the parameters of these HB stars. We find that the
location of helium enriched stars on the HB is reproduced only by adopting a
higher value of $\mu$ with respect to the first generation stars in all the
analysed stellar populations. We also find that $\mu$ correlates with the
helium enhancement of the populations. This holds for both clusters. This
finding is naturally predicted by the model of ''pre-main sequence disc early
loss'', previously suggested in the literature, and is consistent with the
findings of multiple-populations formation models that foresee the formation of
second generation stars in a cooling flow.",['astro-ph.SR'],False,,,,"Modeling the Solar Transition Region: Effects of Spatial Resolution on
  the Atmospheric Structure, Emission and Non-Equilibrium Ionization","Mass loss along the red giant branch of the intermediate stellar
  populations in NGC6752 and NGC2808"
neg-d2-423,2025-03-03,,2503.01963," Most of our knowledge regarding molecular clouds and the early stages of star
formation stems from molecular spectral-line observations. However, the various
chemical and radiative-transfer effects, in combination with projection
effects, can lead to a distorted view of molecular clouds. Our objective is to
simultaneously study all of these effects by creating synthetic spectral-line
observations based on a chemo-dynamical simulation of a collapsing molecular
cloud. We performed a 3D ideal MHD simulation of a supercritical turbulent
collapsing molecular cloud where the dynamical evolution was coupled to a
nonequilibrium gas-grain chemical network consisting of 115 species, the
evolution of which was governed by >1600 chemical reactions. We post-processed
this simulation with a multilevel non-LTE radiative-transfer code to produce
synthetic PPV data cubes of the CO, HCO+, HCN, and N2H+ (J = 1-0) transitions
under various projection angles with respect to the mean component of the
magnetic field. We find that the chemical abundances of various species in our
simulated cloud tend to be over-predicted in comparison to observationally
derived abundances and attribute this discrepancy to the fact that the cloud
collapses rapidly and therefore the various species do not have enough time to
deplete onto dust grains. This suggests that our initial conditions may not
correspond to the initial conditions of real molecular clouds and cores. We
show that the projection angle has a notable effect on the moment maps of the
species for which we produced synthetic observations. Specifically, the
integrated emission and velocity dispersion of CO, HCO+, and HCN are higher
when the cloud is observed ""face on"" compared to ""edge on,"" whereas column
density maps exhibit an opposite trend. Finally, we show that only N2H+ is an
accurate tracer of the column density of the cloud across all projection
angles.",['astro-ph.GA'],2502.14972," We present GalactiKit, a data-driven methodology for estimating the lookback
infall time, stellar mass, halo mass and mass ratio of the disrupted
progenitors of Milky Way-like galaxies at the time of infall. GalactiKit uses
simulation-based inference to extract the information on galaxy formation
processes encoded in the Auriga cosmological MHD simulations of Milky Way-mass
halos to create a model that relates the properties of mergers to those of the
corresponding merger debris at $z=0$. We investigate how well GalactiKit can
reconstruct the merger properties given the dynamical, chemical, and the
combined chemo-dynamical information of debris. For this purpose, three models
were implemented considering the following properties of merger debris: (a)
total energy and angular momentum, (b) iron-to-hydrogen and alpha-to-iron
abundance ratios, and (c) a combination of all of these. We find that the
kinematics of the debris can be used to trace the lookback time at which the
progenitor was first accreted into the main halo. However, chemical information
is necessary for inferring the stellar and halo masses of the progenitors. In
both models (b) and (c), the stellar masses are predicted more accurately than
the halo masses, which could be related to the scatter in the stellar mass-halo
mass relation. Model (c) provides the most accurate predictions for the merger
parameters, which suggests that combining chemical and dynamical data of debris
can significantly improve the reconstruction of the Milky Way's assembly
history.",['astro-ph.GA'],False,,,,"Projection-angle effects when ""observing"" a turbulent magnetized
  collapsing molecular cloud. I. Chemistry and line transfer","GalactiKit: reconstructing mergers from $z=0$ debris using
  simulation-based inference in Auriga"
neg-d2-424,2025-03-21,,2503.16832," We introduce a novel approach for simultaneous self-supervised video
alignment and action segmentation based on a unified optimal transport
framework. In particular, we first tackle self-supervised video alignment by
developing a fused Gromov-Wasserstein optimal transport formulation with a
structural prior, which trains efficiently on GPUs and needs only a few
iterations for solving the optimal transport problem. Our single-task method
achieves the state-of-the-art performance on multiple video alignment
benchmarks and outperforms VAVA, which relies on a traditional Kantorovich
optimal transport formulation with an optimality prior. Furthermore, we extend
our approach by proposing a unified optimal transport framework for joint
self-supervised video alignment and action segmentation, which requires
training and storing a single model and saves both time and memory consumption
as compared to two different single-task models. Extensive evaluations on
several video alignment and action segmentation datasets demonstrate that our
multi-task method achieves comparable video alignment yet superior action
segmentation results over previous methods in video alignment and action
segmentation respectively. Finally, to the best of our knowledge, this is the
first work to unify video alignment and action segmentation into a single
model.",['cs.CV'],2502.05482," Implicit Neural Representations (INRs) employ neural networks to represent
continuous functions by mapping coordinates to the corresponding values of the
target function, with applications e.g., inverse graphics. However, INRs face a
challenge known as spectral bias when dealing with scenes containing varying
frequencies. To overcome spectral bias, the most common approach is the Fourier
features-based methods such as positional encoding. However, Fourier
features-based methods will introduce noise to output, which degrades their
performances when applied to downstream tasks. In response, this paper
initially hypothesizes that combining multi-layer perceptrons (MLPs) with
Fourier feature embeddings mutually enhances their strengths, yet
simultaneously introduces limitations inherent in Fourier feature embeddings.
By presenting a simple theorem, we validate our hypothesis, which serves as a
foundation for the design of our solution. Leveraging these insights, we
propose the use of multi-layer perceptrons (MLPs) without additive",['cs.CV'],False,,,,Joint Self-Supervised Video Alignment and Action Segmentation,"Robustifying Fourier Features Embeddings for Implicit Neural
  Representations"
neg-d2-425,2025-02-07,,2502.05296," Mobile messaging apps offer an increasing range of emotional expressions,
such as emojis to help users manually augment their texting experiences.
Accessibility of such augmentations is limited in voice messaging. With the
term ""speejis"" we refer to accessible emojis and other visual speech emotion
cues that are created automatically from speech input alone. The paper presents
an implementation of speejis and reports on a user study (N=12) comparing the
UX of voice messaging with and without speejis. Results show significant
differences in measures such as attractiveness and stimulation and a clear
preference of all participants for messaging with speejis. We highlight the
benefits of using paralinguistic speech processing and continuous emotion
models to enable finer grained augmentations of emotion changes and transitions
within a single message in addition to augmentations of the overall tone of the
message.",['cs.HC'],2502.05324," The prevailing methodologies for visualizing AI risks have focused on
technical issues such as data biases and model inaccuracies, often overlooking
broader societal risks like job loss and surveillance. Moreover, these
visualizations are typically designed for tech-savvy individuals, neglecting
those with limited technical skills. To address these challenges, we propose
the Atlas of AI Risks-a narrative-style tool designed to map the broad risks
associated with various AI technologies in a way that is understandable to
non-technical individuals as well. To both develop and evaluate this tool, we
conducted two crowdsourcing studies. The first, involving 40 participants,
identified the design requirements for visualizing AI risks for decision-making
and guided the development of the Atlas. The second study, with 140
participants reflecting the US population in terms of age, sex, and ethnicity,
assessed the usability and aesthetics of the Atlas to ensure it met those
requirements. Using facial recognition technology as a case study, we found
that the Atlas is more user-friendly than a baseline visualization, with a more
classic and expressive aesthetic, and is more effective in presenting a
balanced assessment of the risks and benefits of facial recognition. Finally,
we discuss how our design choices make the Atlas adaptable for broader use,
allowing it to generalize across the diverse range of technology applications
represented in a database that reports various AI incidents.",['cs.HC'],False,,,,"Speejis: Enhancing User Experience of Mobile Voice Messaging with
  Automatic Visual Speech Emotion Cues",Atlas of AI Risks: Enhancing Public Understanding of AI Risks
neg-d2-426,2025-03-12,,2503.09831," It is well-known that intersection type assignment systems can be used to
characterize strong normalization (SN). Typical proofs that typable
lambda-terms are SN in these systems rely on semantical techniques. In this
work, we study $\Lambda_\cap^e$, a variant of Coppo and Dezani's (Curry-style)
intersection type system, and we propose a syntactical proof of strong
normalization for it. We first design $\Lambda_\cap^i$, a Church-style version,
in which terms closely correspond to typing derivations. Then we prove that
typability in $\Lambda_\cap^i$ implies SN through a measure that, given a term,
produces a natural number that decreases along with reduction. Finally, the
result is extended to $\Lambda_\cap^e$, since the two systems simulate each
other.",['cs.LO'],2501.08063," System requirements related to concepts like information flow, knowledge, and
robustness cannot be judged in terms of individual system executions, but
rather require an analysis of the relationship between multiple executions.
Such requirements belong to the class of hyperproperties, which generalize
classic trace properties to properties of sets of traces. During the past
decade, a range of new specification logics has been introduced with the goal
of providing a unified theory for reasoning about hyperproperties. This paper
gives an overview on the current landscape of logics for the specification of
hyperproperties and on algorithms for satisfiability checking, model checking,
monitoring, and synthesis.",['cs.LO'],False,,,,"Strong normalization through idempotent intersection types: a new
  syntactical approach",Logics and Algorithms for Hyperproperties
neg-d2-427,2025-02-13,,2502.09771," This paper introduces DSrepair, a knowledge-enhanced program repair method
designed to repair the buggy code generated by LLMs in the data science domain.
DSrepair uses knowledge graph based RAG for API knowledge retrieval as well as
bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to
enable knowledge graph based API retrieval, we construct DS-KG (Data Science
Knowledge Graph) for widely used data science libraries. For bug knowledge
enrichment, we employ an abstract syntax tree (AST) to localize errors at the
AST node level. DSrepair's effectiveness is evaluated against five
state-of-the-art LLM-based repair baselines using four advanced LLMs on the
DS-1000 dataset. The results show that DSrepair surpasses all five baselines.
Specifically, when compared to the second-best baseline, DSrepair demonstrates
significant improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code
snippets for each of the four evaluated LLMs, respectively. Additionally, it
achieves greater efficiency, reducing the number of tokens required per code
task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.",['cs.SE'],2503.15626," Selecting the combination of security controls that will most effectively
protect a system's assets is a difficult task. If the wrong controls are
selected, the system may be left vulnerable to cyber-attacks that can impact
the confidentiality, integrity, and availability of critical data and services.
In practical settings, as standardized control catalogues can be quite large,
it is not possible to select and implement every control possible. Instead,
considerations, such as budget, effectiveness, and dependencies among various
controls, must be considered to choose a combination of security controls that
best achieve a set of system security objectives. In this paper, we present a
game-theoretic approach for selecting effective combinations of security
controls based on expected attacker profiles and a set budget. The control
selection problem is set up as a two-person zero-sum one-shot game. Valid
control combinations for selection are generated using an algebraic formalism
to account for dependencies among selected controls. Using a software tool, we
apply the approach on a fictional Canadian military system with Canada's
standardized control catalogue, ITSG-33. Through this case study, we
demonstrate the approach's scalability to assist in selecting an effective set
of security controls for large systems. The results illustrate how a security
analyst can use the proposed approach and supporting tool to guide and support
decision-making in the control selection activity when developing secure
systems of all sizes.",['cs.SE'],False,,,,Knowledge-Enhanced Program Repair for Data Science Code,"A Scalable Game-Theoretic Approach for Selecting Security Controls from
  Standardized Catalogues"
neg-d2-428,2025-01-05,,2501.05468," Systematic literature reviews and meta-analyses are essential for
synthesizing research insights, but they remain time-intensive and
labor-intensive due to the iterative processes of screening, evaluation, and
data extraction. This paper introduces and evaluates LatteReview, a
Python-based framework that leverages large language models (LLMs) and
multi-agent systems to automate key elements of the systematic review process.
Designed to streamline workflows while maintaining rigor, LatteReview utilizes
modular agents for tasks such as title and abstract screening, relevance
scoring, and structured data extraction. These agents operate within
orchestrated workflows, supporting sequential and parallel review rounds,
dynamic decision-making, and iterative refinement based on user feedback.
LatteReview's architecture integrates LLM providers, enabling compatibility
with both cloud-based and locally hosted models. The framework supports
features such as Retrieval-Augmented Generation (RAG) for incorporating
external context, multimodal reviews, Pydantic-based validation for structured
inputs and outputs, and asynchronous programming for handling large-scale
datasets. The framework is available on the GitHub repository, with detailed
documentation and an installable package.",['cs.CL'],2501.0599," The paper discusses the role of WordNet-based semantic classification in the
formalization of constructions, and more specifically in the semantic
annotation of schematic fillers, in the Italian Constructicon. We outline how
the Italian Constructicon project uses Open Multilingual WordNet topics to
represent semantic features and constraints of constructions.",['cs.CL'],False,,,,"LatteReview: A Multi-Agent Framework for Systematic Review Automation
  Using Large Language Models","Constraining constructions with WordNet: pros and cons for the semantic
  annotation of fillers in the Italian Constructicon"
neg-d2-429,2025-01-31,,2501.19051," Elastic computing enables dynamic scaling to meet workload demands, and
Remote Direct Memory Access (RDMA) enhances this by providing high-throughput,
low-latency network communication. However, integrating RDMA into elastic
computing remains a challenge, particularly in control plane operations for
RDMA connection setup.
  This paper revisits the assumptions of prior work on high-performance RDMA
for elastic computing, and reveals that extreme microsecond-level control plane
optimizations are often unnecessary. By challenging the conventional beliefs on
the slowness of user-space RDMA control plane and the difficulty of user-space
RDMA resource sharing, we uncover new design opportunities. Our key insight is
that user-space RDMA connection setup can be significantly improved with
caching, while RDMA resources can be efficiently shared among processes using
fork. In light of this, we propose Swift, a simple yet effective solution that
co-designs RDMA with a serverless framework to optimize performance for elastic
computing. At its very core, Swift handles cold and warm serverless requests by
swiftly initializing the RDMA control plane with cache-optimized libibverbs,
and manages fork requests by leveraging the RDMA's fork capability. Implemented
with OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and
18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared
to prior solutions.",['cs.NI'],2501.08644," In future wireless communication systems, millimeter waves (mmWaves) will
play an important role in meeting high data rates. However, due to their short
wavelengths, these mmWaves present high propagation losses and are highly
attenuated by blocking. In this chapter, we seek to increase the indoor radio
coverage at 60 GHz in non line-of-sight (NLOS) environments. Firstly, a
metallic passive reflector is used in an L-shaped corridor. Secondly, an array
of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80
grooves) is used in a T-shaped corridor. Next, the study focuses on the
blockage losses caused by the human body. The results obtained in these
different configurations show that it is possible to use beamforming to exploit
a reflected path when the direct path is blocked.",['cs.NI'],False,,,,Swift: Rethinking RDMA Control Plane for Elastic Computing,"Extension of indoor mmW link radio coverage in non line-of-sight
  conditions"
neg-d2-430,2025-02-20,,2502.14282," In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code is available at
https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.",['cs.CV'],2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],False,,,,"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC",Interpretable Failure Detection with Human-Level Concepts
neg-d2-431,2025-03-19,,2503.15158," This paper focuses on an integrated sensing and communication (ISAC) system
in the presence of signal-dependent modulated jamming (SDMJ). Our goal is to
suppress jamming while carrying out simultaneous communications and sensing. We
minimize the integrated sidelobe level (ISL) of the mismatch filter output for
the transmitted waveform and the integrated level (IL) of the mismatch filter
output for the jamming, under the constraints of the loss in-processing gain
(LPG) and the peak-to-average power ratio (PAPR) of the transmitted waveform.
Meanwhile, the similarity constraint is introduced for information-bearing
transmit waveform. We develop a decoupled majorization minimization (DMM)
algorithm to solve the proposed multi-constrained optimization problem. In
contrast to the existing approaches, the proposed algorithm transforms the
difficult optimization problem involving two variables into two parallel
sub-problems with one variable, thus significantly speeding up the convergence
rate. Furthermore, fast Fourier transform (FFT) is introduced to compute the
closed-form solution of each sub-problem, giving rise to a greatly reduced
computation complexity. Simulation results demonstrate the capabilities of the
proposed ISAC system which strikes a proper trade-off among sensing and jamming
suppression.",['eess.SP'],2502.18636," In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.",['eess.SP'],False,,,,"Waveform and Filter Design for Integrated Sensing and Communication
  Against Signal-dependent Modulated Jamming","Transfer Learning Assisted Fast Design Migration Over Technology Nodes:
  A Study on Transformer Matching Network"
neg-d2-432,2025-01-17,,2501.10189," Structured sparsity has been proposed as an efficient way to prune the
complexity of Machine Learning (ML) applications and to simplify the handling
of sparse data in hardware. Accelerating ML models, whether for training, or
inference, heavily relies on matrix multiplications that can be efficiently
executed on vector processors, or custom matrix engines. This work aims to
integrate the simplicity of structured sparsity into vector execution to speed
up the corresponding matrix multiplications. Initially, the implementation of
structured-sparse matrix multiplication using the current RISC-V instruction
set vector extension is comprehensively explored. Critical parameters that
affect performance, such as the impact of data distribution across the scalar
and vector register files, data locality, and the effectiveness of loop
unrolling are analyzed both qualitatively and quantitatively. Furthermore, it
is demonstrated that the addition of a single new instruction would reap even
higher performance. The newly proposed instruction is called vindexmac, i.e.,
vector index-multiply-accumulate. It allows for indirect reads from the vector
register file and it reduces the number of instructions executed per matrix
multiplication iteration, without introducing additional dependencies that
would limit loop unrolling. The proposed new instruction was integrated in a
decoupled RISC-V vector processor with negligible hardware cost. Experimental
results demonstrate the runtime efficiency and the scalability offered by the
introduced optimizations and the new instruction for the execution of
state-of-the-art Convolutional Neural Networks. More particularly, the addition
of a custom instruction improves runtime by 25% and 33% when compared with
highly-optimized vectorized kernels that use only the currently defined RISC-V
instructions.",['cs.AR'],2501.0733," ML and HPC applications increasingly combine dense and sparse memory access
computations to maximize storage efficiency. However, existing CPUs and GPUs
struggle to flexibly handle these heterogeneous workloads with consistently
high compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,
dual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical
interconnect and in-core streaming units (SUs) designed to accelerate dense and
sparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets
in 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense
linear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On
stencil codes, Occamy reaches an FPU utilization of 83% and a
technology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading
state-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On
sparse-dense linear algebra (LA), it achieves 42% FPU utilization and a
normalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x
and 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up
to 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.
Finally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and
graph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available
under a permissive open-source license.",['cs.AR'],False,,,,"Optimizing Structured-Sparse Matrix Multiplication in RISC-V Vector
  Processors","Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System
  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET"
neg-d2-433,2025-01-10,,2501.05862," Depicting novel classes with language descriptions by observing few-shot
samples is inherent in human-learning systems. This lifelong learning
capability helps to distinguish new knowledge from old ones through the
increase of open-world learning, namely Few-Shot Class-Incremental Learning
(FSCIL). Existing works to solve this problem mainly rely on the careful tuning
of visual encoders, which shows an evident trade-off between the base knowledge
and incremental ones. Motivated by human learning systems, we propose a new
Language-inspired Relation Transfer (LRT) paradigm to understand objects by
joint visual clues and text depictions, composed of two major steps. We first
transfer the pretrained text knowledge to the visual domains by proposing a
graph relation transformation module and then fuse the visual and language
embedding by a text-vision prototypical fusion module. Second, to mitigate the
domain gap caused by visual finetuning, we propose context prompt learning for
fast domain alignment and imagined contrastive learning to alleviate the
insufficient text data during alignment. With collaborative learning of domain
alignments and text-image transfer, our proposed LRT outperforms the
state-of-the-art models by over $13\%$ and $7\%$ on the final session of
mini-ImageNet and CIFAR-100 FSCIL benchmarks.",['cs.CV'],2502.1815," Recent approaches to jointly reconstruct 3D humans and objects from a single
RGB image represent 3D shapes with template-based or coarse models, which fail
to capture details of loose clothing on human bodies. In this paper, we
introduce a novel implicit approach for jointly reconstructing realistic 3D
clothed humans and objects from a monocular view. For the first time, we model
both the human and the object with an implicit representation, allowing to
capture more realistic details such as clothing. This task is extremely
challenging due to human-object occlusions and the lack of 3D information in 2D
images, often leading to poor detail reconstruction and depth ambiguity. To
address these problems, we propose a novel attention-based neural implicit
model that leverages image pixel alignment from both the input human-object
image for a global understanding of the human-object scene and from local
separate views of the human and object images to improve realism with, for
example, clothing details. Additionally, the network is conditioned on semantic
features derived from an estimated human-object pose prior, which provides 3D
spatial information about the shared space of humans and objects. To handle
human occlusion caused by objects, we use a generative diffusion model that
inpaints the occluded regions, recovering otherwise lost details. For training
and evaluation, we introduce a synthetic dataset featuring rendered scenes of
inter-occluded 3D human scans and diverse objects. Extensive evaluation on both
synthetic and real-world datasets demonstrates the superior quality of the
proposed human-object reconstructions over competitive methods.",['cs.CV'],False,,,,"Language-Inspired Relation Transfer for Few-shot Class-Incremental
  Learning","Realistic Clothed Human and Object Joint Reconstruction from a Single
  Image"
neg-d2-434,2025-01-23,,2501.14015," Progress in the theoretical understanding of parton branching dynamics within
an expanding Quark Gluon Plasma relies on detailed and fair comparisons with
experimental data for reconstructed jets. Such comparisons are only meaningful
when the computed jet, be it analytically or via event generation, accounts for
the complexity of jets reconstructed in the challenging environment of
heavy-ion collisions. Jet reconstruction in heavy ion collisions involves a
necessarily imperfect subtraction of the large and fluctuating underlying
event: reconstructed jets always include underlying event contamination. To
identify true jet quenching effects, modifications due to the interaction of
the branching partonic system with the Quark Gluon Plasma, we establish a
baseline that accounts for possible background contamination on unmodified
jets. In practical terms, jet quenching effects are only those not present in
jets produced in proton-proton collisions that have been embedded in a
realistic heavy-ion background and where subtraction has been carried out
analogously to that in the heavy ion case. With this setup, we assess the
sensitivity to underlying event of commonly discussed jet quenching observables
and its impact on the robustness of Machine Learning studies, aimed at
classifying jets according to their degree of modification by the Quark Gluon
Plasma, that rely on those observables. We find the discrimination power of a
simple Boosted Decision Tree to be robust in the realistic scenario where both
medium response and underlying event are present, giving support to portability
to the experimental context.",['hep-ph'],2501.0106," Using the pinch technique, we compute the one-loop vertices of weak
interactions in the B-LSSM and incorporate their pinch contributions into the
gauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$
parameters in the Standard Model based on the $SU(2)_L\otimes U(1)_Y$ group,
the corresponding parameters in the B-LSSM are modified. We provide these
redefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the
results. In the framework of the low-energy effective Lagrangian for weak
interactions, the $S$, $T$, and $U$ parameters can be expressed as functions of
certain parameters in the B-LSSM. The updated experimental and fitting results
constrain the parameter space of the B-LSSM strongly.",['hep-ph'],False,,,,"Apples to Apples in Jet Quenching: robustness of Machine Learning
  classification of quenched jets to Underlying Event contamination","$S$, $T$, $U$ Parameters in The B-LSSM"
neg-d2-435,2025-01-28,,2501.17258," Conversational AI agents are commonly applied within single-user, turn-taking
scenarios. The interaction mechanics of these scenarios are trivial: when the
user enters a message, the AI agent produces a response. However, the
interaction dynamics are more complex within group settings. How should an
agent behave in these settings? We report on two experiments aimed at
uncovering users' experiences of an AI agent's participation within a group, in
the context of group ideation (brainstorming). In the first study, participants
benefited from and preferred having the AI agent in the group, but participants
disliked when the agent seemed to dominate the conversation and they desired
various controls over its interactive behaviors. In the second study, we
created functional controls over the agent's behavior, operable by group
members, to validate their utility and probe for additional requirements.
Integrating our findings across both studies, we developed a taxonomy of
controls for when, what, and where a conversational AI agent in a group should
respond, who can control its behavior, and how those controls are specified and
implemented. Our taxonomy is intended to aid AI creators to think through
important considerations in the design of mixed-initiative conversational
agents.",['cs.HC'],2502.06985," Online communities can offer many benefits for youth including peer learning,
cultural expression, and skill development. However, most HCI research on
youth-focused online communities has centered communities developed by adults
for youth rather than by the youth themselves. In this work, we interviewed 11
teenagers (ages 13-17) who moderate online Discord communities created by
youth, for youth. Participants were identified by Discord platform staff as
leaders of well-moderated servers through an intensive exam and
application-based process. We also interviewed 2 young adults who volunteered
as mentors of some of our teen participants. We present our findings about the
benefits, motivations, and risks of teen-led online communities, as well as the
role of external stakeholders of these youth spaces. We contextualize our work
within the broader teen online safety landscape to provide recommendations to
better support, encourage, and protect teen moderators and their online
communities. This empirical work contributes one of the first studies to date
with teen Discord moderators and aims to empower safe youth-led online
communities.",['cs.HC'],False,,,,"Controlling AI Agent Participation in Group Conversations: A
  Human-Centered Approach","""It's Great Because It's Ran By Us"": Empowering Teen Volunteer Discord
  Moderators to Design Healthy and Engaging Youth-Led Online Communities"
neg-d2-436,2025-02-17,,2502.1194," This paper presents the full dynamic model of the UR10 industrial robot. A
triple-stage identification approach is adopted to estimate the manipulator's
dynamic coefficients. First, linear parameters are computed using a standard
linear regression algorithm. Subsequently, nonlinear friction parameters are
estimated according to a sigmoidal model. Lastly, motor drive gains are devised
to map estimated joint currents to torques. The overall identified model can be
used for both control and planning purposes, as the accompanied ROS2 software
can be easily reconfigured to account for a generic payload. The estimated
robot model is experimentally validated against a set of exciting trajectories
and compared to the state-of-the-art model for the same manipulator, achieving
higher current prediction accuracy (up to a factor of 4.43) and more precise
motor gains. The related software is available at
https://codeocean.com/capsule/8515919/tree/v2.",['cs.RO'],2502.15613," Current robotic pick-and-place policies typically require consistent gripper
configurations across training and inference. This constraint imposes high
retraining or fine-tuning costs, especially for imitation learning-based
approaches, when adapting to new end-effectors. To mitigate this issue, we
present a diffusion-based policy with a hybrid learning-optimization framework,
enabling zero-shot adaptation to novel grippers without additional data
collection for retraining policy. During training, the policy learns
manipulation primitives from demonstrations collected using a base gripper. At
inference, a diffusion-based optimization strategy dynamically enforces
kinematic and safety constraints, ensuring that generated trajectories align
with the physical properties of unseen grippers. This is achieved through a
constrained denoising procedure that adapts trajectories to gripper-specific
parameters (e.g., tool-center-point offsets, jaw widths) while preserving
collision avoidance and task feasibility. We validate our method on a Franka
Panda robot across six gripper configurations, including 3D-printed fingertips,
flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a
93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion
policy baselines), supporting tool-center-point variations of 16-23.5 cm and
jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion
enables robust cross-gripper manipulation while maintaining the sample
efficiency of imitation learning, eliminating the need for gripper-specific
retraining. Video and code are available at https://github.com/yaoxt3/GADP.",['cs.RO'],False,,,,The Dynamic Model of the UR10 Robot and its ROS2 Integration,"Pick-and-place Manipulation Across Grippers Without Retraining: A
  Learning-optimization Diffusion Policy Approach"
neg-d2-437,2025-03-11,,2503.08666," This paper provides evidence that stock returns, after truncation, might be
modeled by a special type of continuous mixtures or normals, so-called
$q$-Gaussians. Negative binomial distributions might model the counts for
extreme returns. A generalized jump-diffusion model is proposed, and an
explicit option pricing formula is obtained.",['q-fin.MF'],2503.08666," This paper provides evidence that stock returns, after truncation, might be
modeled by a special type of continuous mixtures or normals, so-called
$q$-Gaussians. Negative binomial distributions might model the counts for
extreme returns. A generalized jump-diffusion model is proposed, and an
explicit option pricing formula is obtained.",['q-fin.MF'],False,,,,Modeling Stock Return Distributions and Pricing Options,Modeling Stock Return Distributions and Pricing Options
neg-d2-438,2025-02-14,,2502.10256," In this work, we investigate the application of an advanced nonlinear
torsion- and shear-free Kirchhoff rod model, enhanced with a penalty-based
barrier function (to simulate the seabed contact), intended for studying the
static and dynamic behavior of mooring lines. The formulation incorporates
conservative and non-conservative external loads, including those coming from
the surrounding flow (added mass, tangential drag, and normal drag). To
illustrate the favorable features of this model, we consider some key scenarios
such as static configurations, pulsating force applications at the fairlead,
and fluid-structure interaction between mooring lines and the surrounding flow.
Verification against well-established solutions, including catenary
configurations and OpenFAST simulations, shows excellent accuracy in predicting
mooring line responses for a floating offshore wind turbine. Among the most
important results, we can mention that under normal pulsating loads at the
fairlead, the mooring line exhibits a transition from a drag-dominated regime
at low frequencies to an added-mass-dominated regime at higher frequencies.
Furthermore, tangential forcing at the fairlead reveals a strong coupling
between axial and bending dynamics, contrasting with normal forcing scenarios
where axial dynamics remain largely unaffected. These findings underscore the
potential of the proposed approach for advanced mooring line simulations.",['physics.flu-dyn'],2503.08331," Modern telescopes provide breathtaking images of nebulae, clouds and galaxies
shaped by gravity-driven interactions between complex bodies. While such
structures are prevalent on an astrophysical scale, they are rarely observed at
the human scale. In this letter, we report the observations of the complex
orbits, collision, and coalescence of droplets on a soap film, forming
structures such as bridges and spiral arms, reminiscent of their astrophysical
counterparts. These dynamics emerge from attractive forces caused by
gravito-capillary-driven distortions of the supporting soap film. Long orbits
and intricate coalescence mechanisms are enabled by the small dissipation in
the soap film and the fluidic nature of the droplets and supporting film,
respectively. The existence of stable droplets within the soap film featuring a
universal radius, as well as the attractive potentials, are explained through a
careful comparison of experimental data with models computing the distortions
of the supporting soap film. This work opens perspectives to",['physics.flu-dyn'],False,,,,On the use of an advanced Kirchhoff rod model to study mooring lines,"Orbiting, colliding and merging droplets on a soap film: toward
  gravitational analogues"
neg-d2-439,2025-03-05,,2503.03209," Isolated skyrmion solutions to the 2D Landau-Lifshitz equation with the
Dzyaloshinskii-Moriya interaction, Zeeman interaction, and easy-plane
anisotropy are considered. In a wide range of parameters illustrating the
various interaction strengths, we construct exact solutions and examine their
monotonicity, exponential decay, and stability using a careful mathematical
analysis. We also estimate the distance between the constructed solutions and
the harmonic maps by exploiting the structure of the linearized equation and by
proving a resolvent estimate for the linearized operator that is uniform in
extra implicit potentials.",['math.AP'],2502.00184," We prove a Brunn-Minkowski type inequality for the first (nontrivial)
Dirichlet eigenvalue of the weighted $p$-operator \[
-\Delta_{p,\gamma}u=-\text{div}(|\nabla u|^{p-2} \nabla u)+(x,\nabla u)|\nabla
u|^{p-2}, \] where $p>1$, in the class of bounded Lipschitz domains in
$\mathbb{R}^n$. We also prove that any corresponding positive eigenfunction is
log-concave if the domain is convex.",['math.AP'],False,,,,"Global perturbation of isolated equivariant chiral skyrmions from the
  harmonic maps","Geometric properties of solutions to elliptic PDE's in Gauss space and
  related Brunn-Minkowski type inequalities"
neg-d2-440,2025-03-01,,2503.00484," Realizing a quantum critical point (QCP) in clean ferromagnetic (FM) metals
has remained elusive due to the coupling of magnetization to the electronic
soft modes that drive the transition to be of first order. However, by
introducing a suitable amount of quenched disorder, one can still establish a
QCP in ferromagnets. In this study, we ascertain that the itinerant ferromagnet
Ni$_{1-x}$Mo$_{x}$ exhibits a FM QCP at a critical doping of $x_c \simeq
0.125$. Through magnetization and muon-spin relaxation measurements, we
demonstrate that the FM ordering temperature is suppressed continuously to zero
at $x_c$, while the magnetic volume fraction remains $100\%$ up to $x_c$,
indicating a second-order phase transition. The QCP is accompanied by a
non-Fermi liquid behavior, as evidenced by the logarithmic divergence of the
specific heat and the linear temperature dependence of the low-temperature
resistivity. Our findings reveal a minimal effect of disorder on the critical
spin dynamics of Ni$_{1-x}$Mo$_{x}$ at $x_c$, highlighting it as one of the
rare systems to exhibit a clean FM QCP.",['cond-mat.str-el'],2503.00484," Realizing a quantum critical point (QCP) in clean ferromagnetic (FM) metals
has remained elusive due to the coupling of magnetization to the electronic
soft modes that drive the transition to be of first order. However, by
introducing a suitable amount of quenched disorder, one can still establish a
QCP in ferromagnets. In this study, we ascertain that the itinerant ferromagnet
Ni$_{1-x}$Mo$_{x}$ exhibits a FM QCP at a critical doping of $x_c \simeq
0.125$. Through magnetization and muon-spin relaxation measurements, we
demonstrate that the FM ordering temperature is suppressed continuously to zero
at $x_c$, while the magnetic volume fraction remains $100\%$ up to $x_c$,
indicating a second-order phase transition. The QCP is accompanied by a
non-Fermi liquid behavior, as evidenced by the logarithmic divergence of the
specific heat and the linear temperature dependence of the low-temperature
resistivity. Our findings reveal a minimal effect of disorder on the critical
spin dynamics of Ni$_{1-x}$Mo$_{x}$ at $x_c$, highlighting it as one of the
rare systems to exhibit a clean FM QCP.",['cond-mat.str-el'],False,,,,"Magnetic order and spin dynamics across the ferromagnetic quantum
  critical point in Ni\boldmath{$_{1-x}$}Mo\boldmath{$_{x}$}","Magnetic order and spin dynamics across the ferromagnetic quantum
  critical point in Ni\boldmath{$_{1-x}$}Mo\boldmath{$_{x}$}"
neg-d2-441,2025-01-11,,2501.06569," The palette of a vertex v in a graph G is the set of colors assigned to the
edges incident to v. The palette index of G is the minimum number of distinct
palettes among the vertices, taken over all proper edge colorings of G. This
paper presents results on the palette index of the Cartesian product $G \Box
H$, where one of the factor graphs is a path or a cycle. Additionally, it
provides exact results and bounds on the palette index of the Cartesian product
of two graphs, where one factor graph is isomorphic to a regular or class 1
nearly regular graph.",['math.CO'],2503.12909," In this paper, we refer to a asymptotic degree sequence as
$\mathscr{D}=(d_1,d_2,\dots,d_n)$. The examination of topological indices on
trees gives us a general overview through bounds to find the maximum and
minimum bounds which reflect the maximum and minimum number of edges incident
to every vertex in the graph, Albertson index known as $\sum_{uv\in E(G)}\lvert
d_u(G)-d_v(G) \rvert$, Sigma index $\sigma(G)$ among $\mathscr{D}$ of tree $T$
when $d_n\geqslant \dots \geqslant d_1$. According to the first zegrb we show
for a degree sequence of order $n=4$,
$\operatorname{irr}(T)=M_1(T)^2-2\sqrt{M_1(T)}+\sum_{i=1}^4\left|x_i-x_{i+1}\right|-(b+c)-1$.",['math.CO'],False,,,,"The palette index of the Cartesian product of paths, cycles and regular
  graphs",Topological Indices With Degree Sequence $\mathscr{D}$ of Tree
neg-d2-442,2025-01-11,,2501.06599," We study curvatures of the groups of measure-preserving diffeomorphisms of
non-orientable compact surfaces. For the cases of the Klein bottle and the real
projective plane we compute curvatures, their asymptotics and the normalized
Ricci curvatures in many directions. Extending the approach of V. Arnold, and
A. Lukatskii we provide estimates of weather unpredictability for natural
models of trade wind currents on the Klein bottle and the projective plane.",['math.DG'],2501.08525," In this paper we classify a kind of special Calabi hypersurfaces with
negative constant sectional curvature in Calabi affine geometry. Meanwhile, we
find a class of new Euclidean complete and Calabi complete affine
hypersurfaces, which satisfy the affine maximal type equation and the Abreu
equation with negative constant scalar curvatures.",['math.DG'],False,,,,"Curvature of Measure-Preserving Diffeomorphism Groups of Non-Orientable
  Surfaces",A class of new complete affine maximal type hypersurfaces
neg-d2-443,2025-02-12,,2502.08389," We numerically study the Kitaev honeycomb model with the additional XX Ising
interaction between the nearest and the next nearest neighbors
(Kitaev-Ising-$J_1$-$J_2$ model), by using the density matrix renormalization
group (DMRG) method. Such additional interaction correspond to the nearest and
diagonal interactions on the square lattice. Phase diagram of the bare Kitaev
model consist of low entangled commensurate magnetic phases and entagled Kitaev
spin liquid. Anisotropic Ising interaction allows the entangled incommensurate
magnetic phases in the phase diagram, which previously was predicted only for
more complex type of interactions. We study the scaling law of the entanglement
entropy and the bond dimension of the matrix product state with the size of the
system. In addition, we propose an optimization algorithm to prevent DMRG from
getting stuck in the low-entangled phases.",['cond-mat.str-el'],2503.129," Neutron three-axis spectrometry has been used to determine the interplanar
magnetic exchange parameter in the magnetic van der Waals compound CoPS$_3$.
The exchange is found to be small and antiferromagnetic, estimated to be 0.020
$\pm$ 0.001 meV, which is surprising considering that the magnetic structure is
correlated ferromagnetically between the ab planes. A possible explanation,
involving a small anisotropy in the exchanges, is proposed. The results are
discussed with reference to the other members of the transition metal-PS3
compounds.",['cond-mat.str-el'],False,,,,"Kitaev-Ising-$J_1$-$J_2$ model: a density matrix renormalization group
  study",Interplanar magnetic exchange in CoPS$_3$
neg-d2-444,2025-03-11,,2503.08487," Black holes can launch powerful jets through the Blandford-Znajek process.
This relies on enough plasma in the jet funnel to conduct the necessary
current. However, in some low luminosity active galactic nuclei, the plasma
supply near the jet base may be an issue. It has been proposed that spark gaps
-- local regions with unscreened electric field -- can form in the
magnetosphere, accelerating particles to initiate pair cascades, thus filling
the jet funnel with plasma. In this paper, we carry out 2D general relativistic
particle-in-cell (GRPIC) simulations of the gap, including self-consistent
treatment of inverse Compton scattering and pair production. We observe gap
dynamics that is fully consistent with our earlier 1D GRPIC simulations. We
find strong dependence of the gap power on the soft photon spectrum and energy
density, as well as the strength of the horizon magnetic field. We derive
physically motivated scaling relations, and applying to M87, we find that the
gap may be energetically viable for the observed TeV flares. For Sgr A$^*$, the
energy dissipated in the gap may also be sufficient to power the X-ray flares.",['astro-ph.HE'],2502.11511," GRB 240529A is a long-duration gamma-ray burst (GRB) whose light curve of
prompt emission is composed of a triple-episode structure, separated by
quiescent gaps of tens to hundreds of seconds. More interestingly, its X-ray
light curve of afterglow exhibits two-plateau emissions, namely, an internal
plateau emission that is smoothly connected with a $\sim t^{-0.1}$ segment and
followed by a $\sim t^{-2}$ power-law decay. The three episodes in the prompt
emission, together with two plateau emissions in X-ray, are unique in the Swift
era. They are very difficult to explain with the standard internal/external
shock model by invoking a black hole central engine. However, it could be
consistent with the prediction of a supramassive magnetar as the central
engine, the physical process of phase transition from magnetar to strange star,
as well as the cooling and spin-down of the strange star. In this paper, we
propose that the first- and second-episode emissions in the prompt $\gamma-$ray
of GRB 240529A are from the jet emission of a massive star collapsing into a
supramassive magnetar and the re-activity of central engine, respectively.
Then, the third-episode emission of prompt is attributed to the phase
transition from a magnetar to a strange star. Finally, the first- and
second-plateau emissions of the X-ray afterglow are powered by the cooling and
spin-down of the strange star, respectively. The observational data of each
component of GRB 240529A are roughly coincident with the estimations of the
above physical picture.",['astro-ph.HE'],False,,,,"Physics of Pair Producing Gaps in Black Hole Magnetospheres: Two
  Dimensional General Relativistic Particle-in-cell Simulations",Signature of strange star as the central engine of GRB 240529A
neg-d2-445,2025-01-25,,2501.15386," We investigate time-independent solutions of a discrete optical cavity model
featuring saturable Kerr nonlinearity, a discrete version of the
Lugiato-Lefever equation. This model supports continuous wave (uniform) and
localized (discrete soliton) solutions. Stationary bright solitons arise
through the interaction of dark and bright uniform states, forming a homoclinic
snaking bifurcation diagram within the Pomeau pinning region. As the system
approaches the anti-continuum limit (weak coupling), this snaking bifurcation
widens and transitions into $\subset$-shaped isolas. We propose a
one-active-site approximation that effectively captures the system's behavior
in this regime. The approximation also provides insight into the stability
properties of soliton states. Numerical continuation and spectral analysis
confirm the accuracy of this semianalytical method, showing excellent agreement
with the full model.",['nlin.PS'],2502.14695," The breather solution found by M. Tajiri and Y. Murakami for the Boussinesq
equation is studied analytically. The new parameterization of the solution is
proposed, allowing us to find exactly the existence boundary of the Boussinesq
breather and to show that such a nonlinear excitation emerges from the linear
localized mode of the kink solution corresponding to a shock wave analog in a
crystal. We explicitly find the first integrals, namely the energy and the
field momentum, and faithfully construct the adiabatic invariant for the
Boussinesq breather. As a result, we carry out the quasiclassical quantization
of the nonlinear oscillating solution, obtaining its energy spectrum, i.e., the
energy dependence on the momentum and the number of states, and reveal the
Hamiltonian equations for this particle-like excitation.",['nlin.PS'],False,,,,"From Snaking to Isolas: A One-Active-Site Approximation in Discrete
  Optical Cavities","Quasiclassical quantization of the Boussinesq breather emerging from the
  kink localized mode"
neg-d2-446,2025-03-21,,2503.17226," Deep neural networks trained with Empirical Risk Minimization (ERM) perform
well when both training and test data come from the same domain, but they often
fail to generalize to out-of-distribution samples. In image classification,
these models may rely on spurious correlations that often exist between labels
and irrelevant features of images, making predictions unreliable when those
features do not exist. We propose a technique to generate training samples with
text-to-image (T2I) diffusion models for addressing the spurious correlation
problem. First, we compute the best describing token for the visual features
pertaining to the causal components of samples by a textual inversion
mechanism. Then, leveraging a language segmentation method and a diffusion
model, we generate new samples by combining the causal component with the
elements from other classes. We also meticulously prune the generated samples
based on the prediction probabilities and attribution scores of the ERM model
to ensure their correct composition for our objective. Finally, we retrain the
ERM model on our augmented dataset. This process reduces the model's reliance
on spurious correlations by learning from carefully crafted samples for in
which this correlation does not exist. Our experiments show that across
different benchmarks, our technique achieves better worst-group accuracy than
the existing state-of-the-art methods.",['cs.CV'],2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],False,,,,Leveraging Text-to-Image Generation for Handling Spurious Correlation,Interpretable Failure Detection with Human-Level Concepts
neg-d2-447,2025-02-13,,2502.09124," We propose the scheme realizing the two-level control over the unitary
operators $U_k$ creating the required quantum state of the system $S$. These
operators are controlled by the superposition state of the auxiliary subsystem
$R$ which is governed by two control centers. The
  first-level control center (main control) creates the equal-probability pure
state of $R$ with certain distribution of phase factors that, in turn, govern
the power of the second-level control center $C$ that applies the special
$V$-operators to the same subsystem $R$ changing its state and thus controlling
the applicability of $U_k$. In addition, the above phases are responsible for
the entanglement in the subsystem $R$. We find the direct relation between this
entanglement and the number of operators $U_k$ that can be controlled by $C$.
The simple example of a two-level control system governing the creation of
entangled state of the two-qubit system $S$ is presented.",['quant-ph'],2501.075," This work exploits a framework whereby a graph (in the mathematical sense)
serves to connect a classical system to a state space that we call
`quantum-like' (QL). The QL states comprise arbitrary superpositions of states
in a tensor product basis. The graph plays a special dual role by directing
design of the classical system and defining the state space. We study a
specific example of a large, dynamical classical system -- a system of coupled
phase oscillators -- that maps, via a graph, to the QL state space. We
investigate how mixedness of the state diminishes or increases as the
underlying classical system synchronizes or de-synchronizes respectively. This
shows the interplay between the nonlinear dynamics of the variables of the
classical system and the QL state space. We prove that maps from one time point
to another in the state space are linear maps. In the limit of a strongly
phase-locked classical network -- that is, where couplings between phase
oscillators are very large -- the state space evolves according to unitary
dynamics, whereas in the cases of weaker synchronization, the classical
variables act as a hidden environment that promotes decoherence of
superpositions.",['quant-ph'],False,,,,"Two-level control over quantum state creation via entangled
  equal-probability state","Dynamics in an emergent quantum-like state space generated by a
  nonlinear classical network"
neg-d2-448,2025-03-18,,2503.14626," Natural language misinformation detection approaches have been, to date,
largely dependent on sequence classification methods, producing opaque systems
in which the reasons behind classification as misinformation are unclear. While
an effort has been made in the area of automated fact-checking to propose
explainable approaches to the problem, this is not the case for automated
reason-checking systems. In this paper, we propose a new explainable framework
for both factual and rational misinformation detection based on the theory of
Argumentation Schemes and Critical Questions. For that purpose, we create and
release NLAS-CQ, the first corpus combining 3,566 textbook-like natural
language argumentation scheme instances and 4,687 corresponding answers to
critical questions related to these arguments. On the basis of this corpus, we
implement and validate our new framework which combines classification with
question answering to analyse arguments in search of misinformation, and
provides the explanations in form of critical questions to the human user.",['cs.CL'],2502.11405," Despite being pretrained on multilingual corpora, large language models
(LLMs) exhibit suboptimal performance on low-resource languages. Recent
approaches have leveraged multilingual encoders alongside LLMs by introducing
trainable parameters connecting the two models. However, these methods
typically focus on the encoder's output, overlooking valuable information from
other layers. We propose \aname (\mname), a framework that integrates
representations from all encoder layers, coupled with the \attaname mechanism
to enable layer-wise interaction between the LLM and the multilingual encoder.
Extensive experiments on multilingual reasoning tasks, along with analyses of
learned representations, show that our approach consistently outperforms
existing baselines.",['cs.CL'],False,,,,"An Explainable Framework for Misinformation Identification via Critical
  Question Answering","LayAlign: Enhancing Multilingual Reasoning in Large Language Models via
  Layer-Wise Adaptive Fusion and Alignment Strategy"
neg-d2-449,2025-01-27,,2501.16656," Data mining in transportation networks (DMTNs) refers to using diverse types
of spatio-temporal data for various transportation tasks, including pattern
analysis, traffic prediction, and traffic controls. Graph neural networks
(GNNs) are essential in many DMTN problems due to their capability to represent
spatial correlations between entities. Between 2016 and 2024, the notable
applications of GNNs in DMTNs have extended to multiple fields such as traffic
prediction and operation. However, existing reviews have primarily focused on
traffic prediction tasks. To fill this gap, this study provides a timely and
insightful summary of GNNs in DMTNs, highlighting new progress in prediction
and operation from academic and industry perspectives since 2023. First, we
present and analyze various DMTN problems, followed by classical and recent GNN
models. Second, we delve into key works in three areas: (1) traffic prediction,
(2) traffic operation, and (3) industry involvement, such as Google Maps, Amap,
and Baidu Maps. Along these directions, we discuss new research opportunities
based on the significance of transportation problems and data availability.
Finally, we compile resources such as data, code, and other learning materials
to foster interdisciplinary communication. This review, driven by recent trends
in GNNs in DMTN studies since 2023, could democratize abundant datasets and
efficient GNN methods for various transportation problems including prediction
and operation.",['cs.LG'],2501.0461," Collaborative learning in peer-to-peer networks offers the benefits of
distributed learning while mitigating the risks associated with single points
of failure inherent in centralized servers. However, adversarial workers pose
potential threats by attempting to inject malicious information into the
network. Thus, ensuring the resilience of peer-to-peer learning emerges as a
pivotal research objective. The challenge is exacerbated in the presence of
non-convex loss functions and non-iid data distributions. This paper introduces
a resilient aggregation technique tailored for such scenarios, aimed at
fostering similarity among peers' learning processes. The aggregation weights
are determined through an optimization procedure, and use the loss function
computed using the neighbor's models and individual private data, thereby
addressing concerns regarding data privacy in distributed machine learning.
Theoretical analysis demonstrates convergence of parameters with non-convex
loss functions and non-iid data distributions. Empirical evaluations across
three distinct machine learning tasks support the claims. The empirical
findings, which encompass a range of diverse attack models, also demonstrate
improved accuracy when compared to existing methodologies.",['cs.LG'],False,,,,"Data Mining in Transportation Networks with Graph Neural Networks: A
  Review and Outlook",Resilient Peer-to-peer Learning based on Adaptive Aggregation
neg-d2-450,2025-03-17,,2503.12909," In this paper, we refer to a asymptotic degree sequence as
$\mathscr{D}=(d_1,d_2,\dots,d_n)$. The examination of topological indices on
trees gives us a general overview through bounds to find the maximum and
minimum bounds which reflect the maximum and minimum number of edges incident
to every vertex in the graph, Albertson index known as $\sum_{uv\in E(G)}\lvert
d_u(G)-d_v(G) \rvert$, Sigma index $\sigma(G)$ among $\mathscr{D}$ of tree $T$
when $d_n\geqslant \dots \geqslant d_1$. According to the first zegrb we show
for a degree sequence of order $n=4$,
$\operatorname{irr}(T)=M_1(T)^2-2\sqrt{M_1(T)}+\sum_{i=1}^4\left|x_i-x_{i+1}\right|-(b+c)-1$.",['math.CO'],2503.17187," Cigler considered certain shifted Hankel determinants of convolution powers
of Catalan numbers and conjectured identities for these determinants. Recently,
Fulmek gave a bijective proof of Cigler's conjecture. Cigler then provided a
computational proof. We extend Cigler's determinant identities to the
convolution of general power series $F(x)$, where $F(x)$ satisfies a certain
type of quadratic equation. As an application, we present the Hankel
determinant identities of convolution powers of Motzkin numbers.",['math.CO'],False,,,,Topological Indices With Degree Sequence $\mathscr{D}$ of Tree,"Hankel Determinants for Convolution of Power Series: An Extension of
  Cigler's Results"
neg-d2-451,2025-01-29,,2501.17821," Scene flow enables an understanding of the motion characteristics of the
environment in the 3D world. It gains particular significance in the
long-range, where object-based perception methods might fail due to sparse
observations far away. Although significant advancements have been made in
scene flow pipelines to handle large-scale point clouds, a gap remains in
scalability with respect to long-range. We attribute this limitation to the
common design choice of using dense feature grids, which scale quadratically
with range. In this paper, we propose Sparse Scene Flow (SSF), a general
pipeline for long-range scene flow, adopting a sparse convolution based
backbone for feature extraction. This approach introduces a new challenge: a
mismatch in size and ordering of sparse feature maps between time-sequential
point scans. To address this, we propose a sparse feature fusion scheme, that
augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater
importance to faraway points. Our method, SSF, achieves state-of-the-art
results on the Argoverse2 dataset, demonstrating strong performance in
long-range scene flow estimation. Our code will be released at
https://github.com/KTH-RPL/SSF.git.",['cs.CV'],2503.09496," The integrative analysis of histopathological images and genomic data has
received increasing attention for survival prediction of human cancers.
However, the existing studies always hold the assumption that full modalities
are available. As a matter of fact, the cost for collecting genomic data is
high, which sometimes makes genomic data unavailable in testing samples. A
common way of tackling such incompleteness is to generate the genomic
representations from the pathology images. Nevertheless, such strategy still
faces the following two challenges: (1) The gigapixel whole slide images (WSIs)
are huge and thus hard for representation. (2) It is difficult to generate the
genomic embeddings with diverse function categories in a unified generative
framework. To address the above challenges, we propose a Conditional Latent
Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal
survival prediction, even with missing genomic data. Specifically, a
Variational Information Bottleneck Transformer (VIB-Trans) module is proposed
to learn compressed pathological representations from the gigapixel WSIs. To
generate different functional genomic features, we develop a novel Latent
Differentiation Variational AutoEncoder (LD-VAE) to learn the common and
specific posteriors for the genomic embeddings with diverse functions. Finally,
we use the product-of-experts technique to integrate the genomic common
posterior and image posterior for the joint latent distribution estimation in
LD-CVAE. We test the effectiveness of our method on five different cancer
datasets, and the experimental results demonstrate its superiority in both
complete and missing modality scenarios.",['cs.CV'],False,,,,SSF: Sparse Long-Range Scene Flow for Autonomous Driving,"Robust Multimodal Survival Prediction with the Latent Differentiation
  Conditional Variational AutoEncoder"
neg-d2-452,2025-02-20,,2502.14617," Large Language Model (LLM) inference workloads handled by global cloud
providers can include both latency-sensitive and insensitive tasks, creating a
diverse range of Service Level Agreement (SLA) requirements. Managing these
mixed workloads is challenging due to the complexity of the inference stack,
which includes multiple LLMs, hardware configurations, and geographic
distributions. Current optimization strategies often silo these tasks to ensure
that SLAs are met for latency-sensitive tasks, but this leads to significant
under-utilization of expensive GPU resources despite the availability of spot
and on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a
comprehensive LLM serving framework that employs adaptive control knobs at
varying time scales, ensuring SLA compliance while maximizing the utilization
of valuable GPU resources. Short-term optimizations include efficient request
routing to data center regions, while long-term strategies involve scaling GPU
VMs out/in and redeploying models to existing VMs to align with traffic
patterns. These strategies are formulated as an optimization problem for
resource allocation and solved using Integer Linear Programming (ILP). We
perform empirical and simulation studies based on production workload traces
with over 8M requests using four open-source models deployed across three
regions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining
tail latency and satisfying all SLOs, and it reduces the scaling overhead
compared to baselines by up to 80%, confirming the effectiveness of our
proposal. In terms of dollar cost, this can save cloud providers up to $2M over
the course of a month.",['cs.DC'],2503.10525," This paper explores the potential of affine frequency division multiplexing
(AFDM) to mitigate the multiuser interference (MUI) problem by employing
time-domain precoding in extremely-large-scale multiple-input multiple-output
(XL-MIMO) systems. In XL-MIMO systems, user mobility significantly improves
network capacity and transmission quality. Meanwhile, the robustness of AFDM to
Doppler shift is enhanced in user mobility scenarios, which further improves
the system performance. However, the multicarrier nature of AFDM has attracted
much attention, and it leads to a significant increase in precoding complexity.
However, the serious problem is that the multicarrier use of AFDM leads to a
sharp increase in precoding complexity. Therefore, we employ efficient
precoding randomized Kaczmarz (rKA) to reduce the complexity overhead. Through
simulation analysis, we compare the performance of XL-MIMO-AFDM and XL-MIMO
orthogonal frequency division multiplexing (XL-MIMO-OFDM) in mobile scenarios,
and the results show that our proposed AFDM-based XL-MIMO precoding design can
be more efficient.",['cs.DC'],False,,,,"Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing
  Workloads at Scale",Efficient Precoding in XL-MIMO-AFDM System
neg-d2-453,2025-03-13,,2503.10833," Integrated Sensing and Communication (ISAC) systems combine sensing and
communication functionalities within a unified framework, enhancing spectral
efficiency and reducing costs by utilizing shared hardware components. This
paper investigates multipath component power delay profile (MPCPDP)-based joint
range and Doppler estimation for Affine Frequency Division Multiplexing
(AFDM)-ISAC systems. The path resolvability of the equivalent channel in the
AFDM system allows the recognition of Line-of-Sight (LoS) and Non-Line-of-Sight
(NLoS) paths within a single pilot symbol in fast time-varying channels. We
develop a joint estimation model that leverages multipath Doppler shifts and
delays information under the AFDM waveform. Utilizing the MPCPDP, we propose a
novel ranging method that exploits the range-dependent magnitude of the MPCPDP
across its delay spread by constructing a Nakagami-m statistical fading model
for MPC channel fading and correlating the distribution parameters with
propagation distance in AFDM systems. This method eliminates the need for
additional time synchronization or extra hardware. We also transform the
nonlinear Doppler estimation problem into a bilinear estimation problem using a
First-order Taylor expansion. Moreover, we introduce the Expectation
Maximization algorithm to estimate the hyperparameters and leverage the
Expectation Consistent algorithm to cope with high-dimensional integration
challenges. Extensive numerical simulations demonstrate the effectiveness of
our MPCPDP-based joint range and Doppler estimation in ISAC systems.",['eess.SP'],2502.18636," In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.",['eess.SP'],False,,,,"Multipath Component Power Delay Profile Based Joint Range and Doppler
  Estimation for AFDM-ISAC Systems","Transfer Learning Assisted Fast Design Migration Over Technology Nodes:
  A Study on Transformer Matching Network"
neg-d2-454,2025-01-14,,2501.08174," Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.",['cs.CV'],2501.17821," Scene flow enables an understanding of the motion characteristics of the
environment in the 3D world. It gains particular significance in the
long-range, where object-based perception methods might fail due to sparse
observations far away. Although significant advancements have been made in
scene flow pipelines to handle large-scale point clouds, a gap remains in
scalability with respect to long-range. We attribute this limitation to the
common design choice of using dense feature grids, which scale quadratically
with range. In this paper, we propose Sparse Scene Flow (SSF), a general
pipeline for long-range scene flow, adopting a sparse convolution based
backbone for feature extraction. This approach introduces a new challenge: a
mismatch in size and ordering of sparse feature maps between time-sequential
point scans. To address this, we propose a sparse feature fusion scheme, that
augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater
importance to faraway points. Our method, SSF, achieves state-of-the-art
results on the Argoverse2 dataset, demonstrating strong performance in
long-range scene flow estimation. Our code will be released at
https://github.com/KTH-RPL/SSF.git.",['cs.CV'],False,,,,"Object-Centric 2D Gaussian Splatting: Background Removal and
  Occlusion-Aware Pruning for Compact Object Models",SSF: Sparse Long-Range Scene Flow for Autonomous Driving
neg-d2-455,2025-02-03,,2502.01881," In light of breakthroughs in superconductivity under high pressure, and
considering that record critical temperatures (T$_c$s) across various systems
have been achieved under high pressure, the primary challenge for higher Tc
should no longer solely be to increase T$_c$ under extreme conditions but also
to reduce, or ideally eliminate, the need for applied pressure in retaining
pressure-induced or -enhanced superconductivity. The topological semiconductor
Bi$_{0.5}$Sb$_{1.5}$Te$_3$ (BST) was chosen to demonstrate our approach to
addressing this challenge and exploring its intriguing physics. Under pressures
up to ~ 50 GPa, three superconducting phases (BST-I, -II, and -III) were
observed. A superconducting phase in BST-I appears at ~ 4 GPa, without a
structural transition, suggesting the possible topological nature of this
phase. Using the pressure-quench protocol (PQP) recently developed by us, we
successfully retained this pressure-induced phase at ambient pressure and
revealed the bulk nature of the state. Significantly, this demonstrates
recovery of a pressure-quenched sample from a diamond anvil cell at room
temperature with the pressure-induced phase retained at ambient pressure. Other
superconducting phases were retained in BST-II and -III at ambient pressure and
subjected to thermal and temporal stability testing. Superconductivity was also
found in BST with T$_c$ up to 10.2 K, the record for this compound series.
While PQP maintains superconducting phases in BST at ambient pressure, both
depressurization and PQP enhance its T$_c$, possibly due to microstructures
formed during these processes, offering an added avenue to raise T$_c$. These
findings are supported by our density-functional theory calculations.",['cond-mat.supr-con'],2503.17241," Superconducting electronics represents a promising technology, offering not
only efficient integration with quantum computing systems, but also the
potential for significant power reduction in high-performance computing.
Nonetheless, the lack of superconducting memories better than conventional
metal-oxide semiconductor (CMOS) memories represent a major obstacle towards
the development of computing systems entirely based on superconducting
electronics. In this work, we combine the emerging concept of gate-controlled
supercurrent (GCS) with the well-established mechanism of charge-trapping
memory to demonstrate a novel, highly scalable, voltage-controlled and
non-volatile superconducting memory. GCS denotes the observation that the
supercurrent in a superconducting constriction can be suppressed by applying a
certain gate voltage (VG) to it. Our findings show that charge trapping within
the gate dielectric, here sapphire, influences the voltage threshold needed to
suppress the supercurrent. We demonstrate reliable reading and reversible
writing of two distinct charge-trapping memory states, associated with
different supercurrent values. Based on our memory device demonstrator, we
discuss its integration into a NOT AND (NAND) gate layout, outlining the
significant improvements offered by this novel memory concept over other
existing NAND memory technologies.",['cond-mat.supr-con'],False,,,,"Creation, stabilization, and study at ambient pressure of
  pressure-induced superconductivity in Bi$_{0.5}$Sb$_{1.5}$Te$_3$","Superconducting non-volatile memory based on charge trapping and
  gate-controlled superconductivity"
neg-d2-456,2025-03-18,,2503.14607," In this paper, we introduce MapBench-the first dataset specifically designed
for human-readable, pixel-based map-based outdoor navigation, curated from
complex path finding scenarios. MapBench comprises over 1600 pixel space map
path finding problems from 100 diverse maps. In MapBench, LVLMs generate
language-based navigation instructions given a map image and a query with
beginning and end landmarks. For each map, MapBench provides Map Space Scene
Graph (MSSG) as an indexing data structure to convert between natural language
and evaluate LVLM-generated results. We demonstrate that MapBench significantly
challenges state-of-the-art LVLMs both zero-shot prompting and a
Chain-of-Thought (CoT) augmented reasoning framework that decomposes map
navigation into sequential cognitive processes. Our evaluation of both
open-source and closed-source LVLMs underscores the substantial difficulty
posed by MapBench, revealing critical limitations in their spatial reasoning
and structured decision-making capabilities. We release all the code and
dataset in https://github.com/taco-group/MapBench.",['cs.CV'],2503.1612," Multi-species animal pose estimation has emerged as a challenging yet
critical task, hindered by substantial visual diversity and uncertainty. This
paper challenges the problem by efficient prompt learning for Vision-Language
Pretrained (VLP) models, \textit{e.g.} CLIP, aiming to resolve the
cross-species generalization problem. At the core of the solution lies in the
prompt designing, probabilistic prompt modeling and cross-modal adaptation,
thereby enabling prompts to compensate for cross-modal information and
effectively overcome large data variances under unbalanced data distribution.
To this end, we propose a novel probabilistic prompting approach to fully
explore textual descriptions, which could alleviate the diversity issues caused
by long-tail property and increase the adaptability of prompts on unseen
category instance. Specifically, we first introduce a set of learnable prompts
and propose a diversity loss to maintain distinctiveness among prompts, thus
representing diverse image attributes. Diverse textual probabilistic
representations are sampled and used as the guidance for the pose estimation.
Subsequently, we explore three different cross-modal fusion strategies at
spatial level to alleviate the adverse impacts of visual uncertainty. Extensive
experiments on multi-species animal pose benchmarks show that our method
achieves the state-of-the-art performance under both supervised and zero-shot
settings. The code is available at https://github.com/Raojiyong/PPAP.",['cs.CV'],False,,,,Can Large Vision Language Models Read Maps Like a Human?,Probabilistic Prompt Distribution Learning for Animal Pose Estimation
neg-d2-457,2025-03-07,,2503.05623," There is now a large body of techniques, many based on formal methods, for
describing and realizing complex robotics tasks, including those involving a
variety of rich goals and time-extended behavior. This paper explores the
limits of what sorts of tasks are specifiable, examining how the precise
grounding of specifications, that is, whether the specification is given in
terms of the robot's states, its actions and observations, its knowledge, or
some other information,is crucial to whether a given task can be specified.
While prior work included some description of particular choices for this
grounding, our contribution treats this aspect as a first-class citizen: we
introduce notation to deal with a large class of problems, and examine how the
grounding affects what tasks can be posed. The results demonstrate that certain
classes of tasks are specifiable under different combinations of groundings.",['cs.RO'],2503.12265," In this letter, we demonstrate that previously proposed improved state
parameterizations for soft and continuum robots are specific cases of Clarke
coordinates. By explicitly deriving these improved parameterizations from a
generalized Clarke transformation matrix, we unify various approaches into one
comprehensive mathematical framework. This unified representation provides
clarity regarding their relationships and generalizes them beyond existing
constraints, including arbitrary joint numbers, joint distributions, and
underlying modeling assumptions. This unification consolidates prior insights
and establishes Clarke coordinates as a foundational tool, enabling systematic
knowledge transfer across different subfields within soft and continuum
robotics.",['cs.RO'],False,,,,Limits of specifiability for sensor-based robotic planning tasks,"Clarke Coordinates Are Generalized Improved State Parametrization for
  Continuum Robots"
neg-d2-458,2025-03-20,,2503.16017," In this work, we study groupoids and their approximation properties,
generalizing both the definitions and some known results for the group case.
More precisely, we introduce weak amenability for groupoids using the
definition of the Fourier algebra given by Renault. We prove that weakly
amenable groupoids are inner exact. We also generalize its algebraic
counterpart, the CBAP. To do this we introduce the notion of a quasi Cartan
pair $(B,A)$ and see that $(C_r^*(G),C_0(G^0))$ can be viewed as such. We then
define what it means for a pair $(B,A)$ to have the CBAP. We introduce the
Cowling-Haagerup constants associated to these approximation properties and
prove that $\Lambda_{\text{cb}}(C_r^*(G),C_0(G^0)) \leq
\Lambda_{\text{cb}}(G)$. We then study some classes of groupoids where we could
achieve equality, that is, $\Lambda_{\text{cb}}(G) =
\Lambda_{\text{cb}}(C_r^*(G),C_0(G^0))$. They are discrete groupoids and
groupoids arising from partial actions of a discrete group $\Gamma$ on a
locally compact Hausdorff space $X$.",['math.OA'],2502.1719," We develop a theory of type semigroups for arbitrary twisted, not necessarily
Hausdorff \'etale groupoids. The type semigroup is a dynamical version of the
Cuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable
finiteness of the reduced and essential C*-algebras. If the reduced C*-algebra
of a twisted groupoid is simple and the type semigroup satisfies a weak version
of almost unperforation, then the C*-algebra is either stably finite or purely
infinite. We apply our theory to Cartan inclusions. We calculate the type
semigroup for the possibly non-Hausdorff groupoids associated to self-similar
group actions on graphs and deduce a dichotomy for the resulting Exel-Pardo
algebras.",['math.OA'],False,,,,On weakly amenable groupoids,"Type semigroups for twisted groupoids and a dichotomy for groupoid
  C*-algebras"
neg-d2-459,2025-01-16,,2501.09697," In this paper, we consider the family of monic polynomials with prime
coefficients and the family of all polynomials with prime coefficients. We
determine the number of $f(x)$ in each of these families having: squarefree
discriminant; $\mathbb{Z}[x]/(f(x))$ as the maximal order in
$\mathbb{Q}[x]/(f(x))$.",['math.NT'],2502.09359," In the theory of integral weight harmonic Maass forms of manageable growth,
two key differential operators, the Bol operator and the shadow operator, play
a fundamental role. Harmonic Maass forms of manageable growth canonically split
into two parts, and each operator controls one of these parts. A third
operator, called the flipping operator, exchanges the role of these two parts.
Maass--Poincar\'e series (of parabolic type) form a convenient basis of
negative weight harmonic Maass forms of manageable growth, and flipping has the
effect of negating an index. Recently, there has been much interest in locally
harmonic Maass forms defined by the first author, Kane, and Kohnen. These are
lifts of Poincar\'e series of hyperbolic type, and are intimately related to
the Shimura and Shintani lifts. In this note, we prove that a similar property
holds for the flipping operator applied to these Poincar\'e series.",['math.NT'],False,,,,Squarefree discriminants of polynomials with prime coefficients,Flipping operators and locally harmonic Maass forms
neg-d2-460,2025-01-12,,2501.16645," Optical interferometry has dramatically advanced the development of modern
science and technology. Here we introduce an interesting centroid evolution
phenomenon of orbital angular momentum (OAM) interference fields with broken
rotational symmetry, and establish a novel interferometric paradigm by fully
exploiting centroid orbiting information. The centroid positions and their
geometric trajectories can provide more detectable information in a
two-dimensional plane to sense the interferometric perturbations, compared with
the conventional interferometry. We first investigate centroid orbital
evolution under the inclined angle perturbation that allows for ultra-sensitive
angle distinguishment with arc-second resolution. We also show centroid ellipse
evolution under spatial phase perturbation that enables geometric
characterization of arbitrary OAM superpositions on modal Poincar\'e spheres.
Furthermore, based on the angle subdivision of centroid orbiting, we
demonstrate the environmentally robust nanoscale displacement measurement with
polarization synchronous detection, and particularly the high-resolution, fast,
and large-range linear movement monitoring using commercial four-quadrant
photodetectors. This novel centroid orbiting interferometry may open new
opportunities to advance metrological technologies beyond the conventional
interferometers.",['physics.optics'],2502.18739," Real time, singleshot multispectral imaging systems are crucial for
environment monitoring and biomedical imaging. Most singleshot multispectral
imagers rely on complex computational backends, which precludes real time
operations. In this work, we leverage the spectral selectivity afforded by
engineered photonic materials to perform bulk of the multispectral data
extraction in the optical domain, thereby circumventing the need for heavy
backend computation. We use our imager to extract multispectral data for two
real world objects at 8 predefined spectral channels in the 400 to 900 nm
wavelength range. For both objects, an RGB image constructed using extracted
multispectral data shows good agreement with an image taken using a phone
camera, thereby validating our imaging approach. We believe that the proposed
system can provide new avenues for the development of highly compact and low
latency multispectral imaging technologies.",['physics.optics'],False,,,,Optical centroid orbiting metrology,Singleshot Multispectral Imaging via a Chromatic Metalens Array
neg-d2-461,2025-02-13,,2502.09816," Inferring adverse events (AEs) of medical products from Spontaneous Reporting
Systems (SRS) databases is a core challenge in contemporary pharmacovigilance.
Bayesian methods for pharmacovigilance are attractive for their rigorous
ability to simultaneously detect potential AE signals and estimate their
strengths/degrees of relevance. However, existing Bayesian and empirical
Bayesian methods impose restrictive parametric assumptions and/or demand
substantial computational resources, limiting their practical utility. This
paper introduces a suite of novel, scalable empirical Bayes methods for
pharmacovigilance that utilize flexible non-parametric priors and custom,
efficient data-driven estimation techniques to enhance signal detection and
signal strength estimation at a low computational cost. Our highly flexible
methods accommodate a broader range of data and achieve signal detection
performance comparable to or better than existing Bayesian and empirical
Bayesian approaches. More importantly, they provide coherent and high-fidelity
estimation and uncertainty quantification for potential AE signal strengths,
offering deeper insights into the comparative importance and relevance of AEs.
Extensive simulation experiments across diverse data-generating scenarios
demonstrate the superiority of our methods in terms of accurate signal strength
estimation, as measured by replication root mean squared errors. Additionally,
our methods maintain or exceed the signal detection performance of
state-of-the-art techniques, as evaluated by frequentist false discovery rates
and sensitivity metrics. Applications on FDA FAERS data for the statin group of
drugs reveal interesting insights through Bayesian posterior probabilities.",['stat.ME'],2502.11072," This work presents a novel simulation-based approach for constructing
confidence regions in parametric models, which is particularly suited for
generative models and situations where limited data and conventional asymptotic
approximations fail to provide accurate results. The method leverages the
concept of data depth and depends on creating random hyper-rectangles, i.e.
boxes, in the sample space generated through simulations from the model,
varying the input parameters. A probabilistic acceptance rule allows to
retrieve a Depth-Confidence Distribution for the model parameters from which
point estimators as well as calibrated confidence sets can be read-off. The
method is designed to address cases where both the parameters and test
statistics are multivariate.",['stat.ME'],False,,,,"Flexible Empirical Bayesian Approaches to Pharmacovigilance for
  Simultaneous Signal Detection and Signal Strength Estimation in Spontaneous
  Reporting Systems Data",Box Confidence Depth: simulation-based inference with hyper-rectangles
neg-d2-462,2025-02-07,,2502.0482," This paper presents a parallel and fully conservative adaptive mesh
refinement (AMR) implementation of a finite-volume-based kinetic solver for
compressible flows. Time-dependent H-type refinement is combined with a
two-population quasi-equilibrium Bhatnagar-Gross-Krook discrete velocity
Boltzmann model. A validation has shown that conservation laws are strictly
preserved through the application of refluxing operations at coarse-fine
interfaces. Moreover, the targeted macroscopic moments of Euler and
Navier-Stokes-Fourier level flows were accurately recovered with correct and
Galilean invariant dispersion rates for a temperature range over three orders
of magnitude and dissipation rates of all eigen-modes up to Mach of order 1.8.
Results for one- and two-dimensional benchmarks up to Mach numbers of 3.2 and
temperature ratios of 7, such as the Sod and Lax shock tubes, the Shu-Osher and
several Riemann problems, as well as viscous shock-vortex interactions, have
demonstrated that the solver precisely captures reference solutions. Excellent
performance in obtaining sensitive quantities was proven, for example in the
test case involving nonlinear acoustics, whilst, for the same accuracy and
fidelity of the solution, the AMR methodology significantly reduced
computational cost and memory footprints. Over all demonstrated two-dimensional
problems, up to a 4- to 9-fold reduction was achieved and an upper limit of the
AMR overhead of 30% was found in a case with very cost-intensive parameter
choice. The proposed solver marks an accurate, efficient and scalable framework
for kinetic simulations of compressible flows with moderate supersonic speeds
and discontinuities, offering a valuable tool for studying complex problems in
fluid dynamics.",['physics.flu-dyn'],2501.08178," Quasiperiodicity, a partially synchronous state that precedes the onset of
forced synchronization in hydrodynamic systems, exhibits distinct geometrical
patterns based on the specific route to lock-in. In this study, we explore
these dynamic behaviors using recurrence quantification analysis. Focusing on a
self-excited hydrodynamic system-a low-density jet subjected to external
acoustic forcing at varying frequencies and amplitudes. We generate recurrence
plots from unsteady velocity time traces. These recurrence plots provide
insight into the synchronization dynamics and pathways of the jet under forced
conditions. Further, we show that recurrence quantities are helpful to detect
and distinguish between different routes to lock-in.",['physics.flu-dyn'],False,,,,"A fully conservative discrete velocity Boltzmann solver with parallel
  adaptive mesh refinement for compressible flows","Detection and analysis of synchronization routes in an axially forced
  globally unstable jet using recurrence quantification"
neg-d2-463,2025-01-14,,2501.08257," Recent work shows fascinating links between ensemble averaging and the
Swampland program. In order to break the emerging global symmetries of the
ensemble averaging as dictated by the no global symmetries conjecture, one may
consider fluctuations away from the average given by deviations in the
Siegel-Weil formula. In this work, we investigate the physical interpretation
of these fluctuations in the bulk physics and pinpoint the states giving rise
to them. For this purpose, we explore an ensemble of generalised Narain CFTs
and build the AdS$_{3}$ gravitational dual in the Chern-Simons (CS) framework.
We study the associated charged BTZ black hole solution and assess its
stability. Using the Swampland weak gravity conjecture, we show that the
fluctuations of the ensemble average are below the BTZ threshold and correspond
to a sublattice of superextremal states emitted by the black hole. We exploit
the logarithmic density of states to derive bounds on the charged vectors of
the abelian CS symmetry and introduce a novel formulation of the density
function to ensure consistency with the sublattice WGC. We establish bounds
that allows to distinguish heavy states contributing to the average from light
states generating fluctuations around it.",['hep-th'],2501.1391," The $\frac12$-BPS indices of $\mathcal{N}=4$ Super Yang-Mills theory with
unitary, orthogonal, and symplectic groups all admit $q$-expansions suggesting
an interpretation in terms of D-branes in the dual bulk AdS$_5$ string
theories. We present a derivation of these expansions in the corresponding bulk
duals by quantizing the moduli space of $\frac12$-BPS giant gravitons using
supersymmetric localization, extending and clarifying our study in
arxiv:2312.14921. We perform a detailed analysis of the one-loop fluctuations
around the maximal giants (the fixed points), and show how the Hamiltonian
analysis is recovered from the functional integral for the equivariant index.
We show that the analytic continuation for these giant graviton expansions
observed in the literature maps precisely to a wall-crossing phenomenon for the
index. In the case of orthogonal and symplectic gauge groups, the
$\mathbb{Z}_2$ quotient in the bulk leads to a corresponding projection in the
$q$-expansion. Additional terms in the expansion related to the Pfaffian
operator arise from topologically stable branes in the bulk dual on AdS$_5
\times \mathbb{RP}^5$.",['hep-th'],False,,,,Fluctuating Ensemble Averages and the BTZ Threshold,Localization and wall-crossing of giant graviton expansions in AdS$_5$
neg-d2-464,2025-03-04,,2503.03144," Spiking Neural Networks (SNNs), inspired by the human brain, offer
significant computational efficiency through discrete spike-based information
transfer. Despite their potential to reduce inference energy consumption, a
performance gap persists between SNNs and Artificial Neural Networks (ANNs),
primarily due to current training methods and inherent model limitations. While
recent research has aimed to enhance SNN learning by employing knowledge
distillation (KD) from ANN teacher networks, traditional distillation
techniques often overlook the distinctive spatiotemporal properties of SNNs,
thus failing to fully leverage their advantages. To overcome these challenge,
we propose a novel logit distillation method characterized by temporal
separation and entropy regularization. This approach improves existing SNN
distillation techniques by performing distillation learning on logits across
different time steps, rather than merely on aggregated output features.
Furthermore, the integration of entropy regularization stabilizes model
optimization and further boosts the performance. Extensive experimental results
indicate that our method surpasses prior SNN distillation strategies, whether
based on logit distillation, feature distillation, or a combination of both.
The code will be available on GitHub.",['cs.CV'],2503.15871," In this work, we tackle action-scene hallucination in Video Large Language
Models (Video-LLMs), where models incorrectly predict actions based on the
scene context or scenes based on observed actions. We observe that existing
Video-LLMs often suffer from action-scene hallucination due to two main
factors. First, existing Video-LLMs intermingle spatial and temporal features
by applying an attention operation across all tokens. Second, they use the
standard Rotary Position Embedding (RoPE), which causes the text tokens to
overemphasize certain types of tokens depending on their sequential orders. To
address these issues, we introduce MASH-VLM, Mitigating Action-Scene
Hallucination in Video-LLMs through disentangled spatial-temporal
representations. Our approach includes two key innovations: (1) DST-attention,
a novel attention mechanism that disentangles the spatial and temporal tokens
within the LLM by using masked attention to restrict direct interactions
between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the
dimensionality of the positional IDs, allowing the spatial and temporal tokens
to maintain balanced positions relative to the text tokens. To evaluate the
action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark
with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that
MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as
on existing video understanding benchmarks.",['cs.CV'],False,,,,"Temporal Separation with Entropy Regularization for Knowledge
  Distillation in Spiking Neural Networks","MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through
  Disentangled Spatial-Temporal Representations"
neg-d2-465,2025-03-06,,2503.04104," Large Language Models (LLMs) have shown remarkable capabilities across tasks,
yet they often require additional prompting techniques when facing complex
problems. While approaches like self-correction and response selection have
emerged as popular solutions, recent studies have shown these methods perform
poorly when relying on the LLM itself to provide feedback or selection
criteria. We argue this limitation stems from the fact that common LLM
post-training procedures lack explicit supervision for discriminative judgment
tasks. In this paper, we propose Generative Self-Aggregation (GSA), a novel
prompting method that improves answer quality without requiring the model's
discriminative capabilities. GSA first samples multiple diverse responses from
the LLM, then aggregates them to obtain an improved solution. Unlike previous
approaches, our method does not require the LLM to correct errors or compare
response quality; instead, it leverages the model's generative abilities to
synthesize a new response based on the context of multiple samples. While GSA
shares similarities with the self-consistency (SC) approach for response
aggregation, SC requires specific verifiable tokens to enable majority voting.
In contrast, our approach is more general and can be applied to open-ended
tasks. Empirical evaluation demonstrates that GSA effectively improves response
quality across various tasks, including mathematical reasoning, knowledge-based
problems, and open-ended generation tasks such as code synthesis and
conversational responses.",['cs.CL'],2502.2062," Large language models (LLMs) can exhibit advanced reasoning yet still
generate incorrect answers. We hypothesize that such errors frequently stem
from spurious beliefs, propositions the model internally considers true but are
incorrect. To address this, we propose a method to rectify the belief space by
suppressing these spurious beliefs while simultaneously enhancing true ones,
thereby enabling more reliable inferences. Our approach first identifies the
beliefs that lead to incorrect or correct answers by prompting the model to
generate textual explanations, using our Forward-Backward Beam Search (FBBS).
We then apply unlearning to suppress the identified spurious beliefs and
enhance the true ones, effectively rectifying the model's belief space.
Empirical results on multiple QA datasets and LLMs show that our method
corrects previously misanswered questions without harming overall model
performance. Furthermore, our approach yields improved generalization on unseen
data, suggesting that rectifying a model's belief space is a promising
direction for mitigating errors and enhancing overall reliability.",['cs.CL'],False,,,,LLMs Can Generate a Better Answer by Aggregating Their Own Responses,Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning
neg-d2-466,2025-01-28,,2501.16748," Large Language Models (LLMs) have shown remarkable advancements but also
raise concerns about cultural bias, often reflecting dominant narratives at the
expense of under-represented subcultures. In this study, we evaluate the
capacity of LLMs to recognize and accurately respond to the Little Traditions
within Indian society, encompassing localized cultural practices and
subcultures such as caste, kinship, marriage, and religion. Through a series of
case studies, we assess whether LLMs can balance the interplay between dominant
Great Traditions and localized Little Traditions. We explore various prompting
strategies and further investigate whether using prompts in regional languages
enhances the models cultural sensitivity and response quality. Our findings
reveal that while LLMs demonstrate an ability to articulate cultural nuances,
they often struggle to apply this understanding in practical, context-specific
scenarios. To the best of our knowledge, this is the first study to analyze
LLMs engagement with Indian subcultures, offering critical insights into the
challenges of embedding cultural diversity in AI systems.",['cs.CL'],2501.02157," As large language models (LLMs) evolve, their ability to deliver personalized
and context-aware responses offers transformative potential for improving user
experiences. Existing personalization approaches, however, often rely solely on
user history to augment the prompt, limiting their effectiveness in generating
tailored outputs, especially in cold-start scenarios with sparse data. To
address these limitations, we propose Personalized Graph-based
Retrieval-Augmented Generation (PGraphRAG), a framework that leverages
user-centric knowledge graphs to enrich personalization. By directly
integrating structured user knowledge into the retrieval process and augmenting
prompts with user-relevant context, PGraphRAG enhances contextual understanding
and output quality. We also introduce the Personalized Graph-based Benchmark
for Text Generation, designed to evaluate personalized text generation tasks in
real-world settings where user history is sparse or unavailable. Experimental
results show that PGraphRAG significantly outperforms state-of-the-art
personalization methods across diverse tasks, demonstrating the unique
advantages of graph-based retrieval for personalization.",['cs.CL'],False,,,,"Through the Prism of Culture: Evaluating LLMs' Understanding of Indian
  Subcultures and Traditions",Personalized Graph-Based Retrieval for Large Language Models
neg-d2-467,2025-01-12,,2501.0693," We give tightness criteria for random variables taking values in the space of
all compact sets of cadlag real-valued paths, in terms of both the Skorohod J1
and M1 topologies. This extends earlier work motivated by the study of the
Brownian web that was concerned only with continuous paths. In the M1 case, we
give a natural extension of our tightness criteria which ensures that
non-crossing systems of paths have weak limit points that are also
non-crossing. This last result is exemplified through a rescaling of heavy
tailed Poisson trees and a more general application to weaves.",['math.PR'],2501.02862," We show that a substantial portion of stochastic calculus can be developed
along similar lines to ordinary calculus, with derivative-based concepts
driving the development. We define a notion of stopping derivative, which is a
form of right derivative with respect to stopping times. Using this, we define
the drift and variance rate of a process as stopping derivatives for
(generalised) conditional expectation and conditional variance respectively.
Applying elementary, derivative-based methods, we derive a calculus of rules
describing how drift and variance rate transform under constructions on
processes, culminating in a version of the multi-dimensional It\^o formula. Our
approach connects with the standard machinery of stochastic calculus via a
theorem establishing that continuous processes with zero drift coincide with
random translations of continuous local martingales. This equivalence allows us
to derive a Fundamental Theorem of Calculus for stopping derivatives, which
relates the quantities of drift and variance rate, defined as stopping
derivatives, to parameters used in the description of a process as a stochastic
integral.",['math.PR'],False,,,,Tightness criteria for random compact sets of cadlag paths,Stochastic Calculus via Stopping Derivatives
neg-d2-468,2025-01-12,,2501.06966," Networks of coupled nonlinear optical resonators have emerged as an important
class of systems in ultrafast optical science, enabling richer and more complex
nonlinear dynamics compared to their single-resonator or travelling-wave
counterparts. In recent years, these coupled nonlinear optical resonators have
been applied as application-specific hardware accelerators for computing
applications including combinatorial optimization and artificial intelligence.
In this work, we rigorously prove a fundamental result showing that coupled
nonlinear optical resonators are Turing-complete computers, which endows them
with much greater computational power than previously thought. Furthermore, we
show that the minimum threshold of hardware complexity needed for
Turing-completeness is surprisingly low, which has profound physical
consequences. In particular, we show that several problems of interest in the
study of coupled nonlinear optical resonators are formally undecidable. These
theoretical findings can serve as the foundation for better understanding the
promise of next-generation, ultrafast all-optical computers.",['physics.optics'],2502.1248," Soliton microcombs are a cornerstone of integrated frequency comb
technologies, with applications spanning photonic computing, ranging, microwave
synthesis, optical communications, and quantum light generation. In nearly all
such applications, electro-optic (EO) components play a critical role in
generating, monitoring, stabilizing, and modulating the solitons. Towards
building photonic integrated circuits for next-generation applications, that
will simultaneously maximize system performance and minimize size, weight, and
power consumption metrics, achieving soliton microcombs and efficient EO
modulation on a chip is essential. X-cut thin-film lithium niobate (TFLN) has
emerged as a leading photonic platform for the realization of high-performance
integrated EO devices and systems. However, despite extensive research, soliton
microcombs have remained elusive to X-cut TFLN due to its multiple strong
Raman-active modes, in-plane refractive index anisotropy, and photorefractive
effects. Here, we address this long-standing challenge and demonstrate
versatile soliton microcombs on X-cut TFLN, with repetition-rates spanning from
the gigahertz (~26 GHz) up to the millimeter-wave (~0.156 THz) regime. The
combs feature exceptional long-term stability, maintaining a direct
injection-locked state for over 90 minutes (manually terminated), with
repetition-rate phase noise closely tracking that of a high-quality electronic
microwave synthesizer. Our finding broadly advances both the fundamental
science and practical applications of integrated comb sources by enabling
efficient EO modulation and broadband coherent solitons to be monolithically
combined on the same chip.",['physics.optics'],False,,,,"Turing-Completeness and Undecidability in Coupled Nonlinear Optical
  Resonators","Stable gigahertz- and mmWave-repetition-rate soliton microcombs on X-cut
  lithium niobate"
neg-d2-469,2025-02-13,,2502.08984," We present [NII] 205 $\mu$m fine structure line observations of three
submillimeter galaxies (SMGs) and three quasar host galaxies at
4$\lesssim$z$\lesssim$6 using the Institut de radioastronomie millim\'etrique
(IRAM) interferometer. The [NII] emission is detected in three sources, and we
report detections of the underlying dust continuum emission in all sources. The
observed [NII]-to-infrared luminosity ratio spans at least 0.5 dex for our
sources. Comparing our estimates with sources detected in the [NII] 205 $\mu$m
at similar redshifts shows that the overall [NII]-to-IR luminosity ratio spans
over a dex in magnitude from L$_{[NII]}$/L$_{IR}$ ~ 10$^{-4}$ - 10$^{-5}$ and
follows the trend of the so-called [NII] fine structure line deficit observed
in (ultra)-luminous infrared galaxies in the local Universe. The [CII]-to-[NII]
luminosity ratio is >10 for most of our sources, indicating that the bulk of
the [CII] 158 $\mu$m line emission (f([CII]$^{PDR}$)>75%) arises from the
neutral medium. From our analysis, we do not find significant differences in
the [NII] 205 $\mu$m emission and the respective ratios between SMGs and QSOs,
suggesting a negligible contribution to the boosting of [NII] 205 $\mu$m
emission due to the active galactic nucleus (AGN) photoionization. Future
investigations involving other fine structure lines and optical diagnostics
will provide further insight into a suite of ionized medium properties and
reveal the diversity between AGN and non-AGN environments.",['astro-ph.GA'],2502.15943," HeH$^+$ belongs to the class of ""reactive"" ions that can be destroyed so
quickly that chemical formation and destruction rates may compete with
inelastic rates and should be considered when solving the statistical
equilibrium equations. This so-called chemical ""pumping"" or ""excitation"" effect
is investigated here for the first time in HeH$^+$. The chemical evolution of
HeH$^+$ in NGC 7027 is modeled with the CLOUDY photoionization code using
updated reaction rate coefficients. The non-LTE analysis of the three observed
HeH$^+$ emission lines is then performed with the CLOUDY and RADEX codes using
an extensive set of spectroscopic and inelastic collisional data suitable for
the specific high-temperature environment of NGC 7027. In a second approach,
chemical formation and destruction rates of HeH$^+$ are implemented in RADEX.
This code is combined with MCMC sampling (performed on the RADEX-parameters
space) to extract the best-fit HeH$^+$ column density and physical conditions
from the observed line fluxes. The CLOUDY and RADEX non-LTE results are found
to be in good agreement, and the $\upsilon=1-0 \ P(2)/P(1)$ line ratio is
better than 20%. Agreement to better than a factor of 2.3 is obtained when
including the reaction between He($2^3S$) and H as an additional source of
HeH$^+$. The RADEX/MCMC model with chemical pumping is found to reproduce both
the observed line fluxes and the line ratio to 20%. Our results suggest that
additional HeH$^+$ lines must be detected in NGC 7027 to better constrain the
physical conditions via non-LTE models. Uncertainties in collisional (reactive
and inelastic) data of HeH$^+$ have been largely reduced in this work. The
three observed lines are not sensitive to chemical pumping while excited
""short-lived"" levels are significantly overpopulated with respect to a non-LTE
model neglecting chemical excitation.",['astro-ph.GA'],False,,,,The [NII] 205 $\mu$m line emission from high-z SMGs and QSOs,"Chemistry and ro-vibrational excitation of HeH$^+$ in the Planetary
  Nebula NGC 7027"
neg-d2-470,2025-02-10,,2502.06427," Hyperspectral image (HSI) classification plays a pivotal role in domains such
as environmental monitoring, agriculture, and urban planning. However, it faces
significant challenges due to the high-dimensional nature of the data and the
complex spectral-spatial relationships inherent in HSI. Traditional methods,
including conventional machine learning and convolutional neural networks
(CNNs), often struggle to effectively capture these intricate spectral-spatial
features and global contextual information. Transformer-based models, while
powerful in capturing long-range dependencies, often demand substantial
computational resources, posing challenges in scenarios where labeled datasets
are limited, as is commonly seen in HSI applications. To overcome these
challenges, this work proposes GraphMamba, a hybrid model that combines
spectral-spatial token generation, graph-based token prioritization, and
cross-attention mechanisms. The model introduces a novel hybridization of
state-space modeling and Gated Recurrent Units (GRU), capturing both linear and
nonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model
complex spatial-spectral relationships while maintaining scalability and
computational efficiency across diverse HSI datasets. Through comprehensive
experiments, we demonstrate that GraphMamba outperforms existing
state-of-the-art models, offering a scalable and robust solution for complex
HSI classification tasks.",['cs.CV'],2503.14482," Image generation has witnessed significant advancements in the past few
years. However, evaluating the performance of image generation models remains a
formidable challenge. In this paper, we propose ICE-Bench, a unified and
comprehensive benchmark designed to rigorously assess image generation models.
Its comprehensiveness could be summarized in the following key features: (1)
Coarse-to-Fine Tasks: We systematically deconstruct image generation into four
task categories: No-ref/Ref Image Creating/Editing, based on the presence or
absence of source images and reference images. And further decompose them into
31 fine-grained tasks covering a broad spectrum of image generation
requirements, culminating in a comprehensive benchmark. (2) Multi-dimensional
Metrics: The evaluation framework assesses image generation capabilities across
6 dimensions: aesthetic quality, imaging quality, prompt following, source
consistency, reference consistency, and controllability. 11 metrics are
introduced to support the multi-dimensional evaluation. Notably, we introduce
VLLM-QA, an innovative metric designed to assess the success of image editing
by leveraging large models. (3) Hybrid Data: The data comes from real scenes
and virtual generation, which effectively improves data diversity and
alleviates the bias problem in model evaluation. Through ICE-Bench, we conduct
a thorough analysis of existing generation models, revealing both the
challenging nature of our benchmark and the gap between current model
capabilities and real-world generation requirements. To foster further
advancements in the field, we will open-source ICE-Bench, including its
dataset, evaluation code, and models, thereby providing a valuable resource for
the research community.",['cs.CV'],False,,,,"Hybrid State-Space and GRU-based Graph Tokenization Mamba for
  Hyperspectral Image Classification","ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and
  Editing"
neg-d2-471,2025-02-26,,2502.18893," In multi-robot system (MRS) applications, efficient task assignment is
essential not only for coordinating agents and ensuring mission success but
also for maintaining overall system security. In this work, we first propose an
optimization-based distributed task assignment algorithm that dynamically
assigns mandatory security-critical tasks and optional tasks among teams.
Leveraging an inexact Alternating Direction Method of Multipliers (ADMM)-based
approach, we decompose the task assignment problem into separable and
non-separable subproblems. The non-separable subproblems are transformed into
an inexact ADMM update by projected gradient descent, which can be performed
through several communication steps within the team.
  In the second part of this paper, we formulate a comprehensive framework that
enables MRS under plan-deviation attacks to handle online tasks without
compromising security. The process begins with a security analysis that
determines whether an online task can be executed securely by a robot and, if
so, the required time and location for the robot to rejoin the team. Next, the
proposed task assignment algorithm is used to allocate security-related tasks
and verified online tasks. Finally, task fulfillment is managed using a Control
Lyapunov Function (CLF)-based controller, while security enforcement is ensured
through a Control Barrier Function (CBF)-based security filter. Through
simulations, we demonstrate that the proposed framework allows MRS to
effectively respond to unplanned online tasks while maintaining security
guarantees.",['cs.MA'],2501.00881," The evolution of agentic systems represents a significant milestone in
artificial intelligence and modern software systems, driven by the demand for
vertical intelligence tailored to diverse industries. These systems enhance
business outcomes through adaptability, learning, and interaction with dynamic
environments. At the forefront of this revolution are Large Language Model
(LLM) agents, which serve as the cognitive backbone of these intelligent
systems. In response to the need for consistency and scalability, this work
attempts to define a level of standardization for Vertical AI agent design
patterns by identifying core building blocks and proposing a \textbf{Cognitive
Skills } Module, which incorporates domain-specific, purpose-built inference
capabilities. Building on these foundational concepts, this paper offers a
comprehensive introduction to agentic systems, detailing their core components,
operational patterns, and implementation strategies. It further explores
practical use cases and examples across various industries, highlighting the
transformative potential of LLM agents in driving industry-specific
applications.",['cs.MA'],False,,,,"Distributed Online Task Assignment via Inexact ADMM for unplanned online
  tasks and its Applications to Security","Agentic Systems: A Guide to Transforming Industries with Vertical AI
  Agents"
neg-d2-472,2025-01-30,,2501.18763," Carbon stars (with atmospheric C/O$>1$) range widely in temperature and
luminosity, from low mass dwarfs to asymptotic giant branch stars (AGB). The
main sequence dwarf carbon (dC) stars have inherited carbon-rich material from
an AGB companion, which has since transitioned to a white dwarf. The dC stars
are far more common than C giants, but no reliable estimates of dC space
density have been published to date. We present results from an all-sky survey
for carbon stars using the low-resolution XP spectra from Gaia DR3. We
developed and measured a set of spectral indices contrasting C$_{\rm 2}$ and CN
molecular band strengths in carbon stars against common absorption features
found in normal (C/O$<1$) stars such as CaI, TiO and Balmer lines. We combined
these indices with the XP spectral coefficients as input to supervised
machine-learning algorithms trained on a vetted sample of known C stars from
LAMOST. We describe the selection of the carbon candidate sample, and provide a
catalog of 43,574 candidates dominated by cool C giants in the Magellanic
Clouds and at low galactic latitude in the Milky Way. We report the
confirmation of candidate C stars using intermediate ($R\sim 1800$) resolution
optical spectroscopy from the Fred Lawrence Whipple Observatory, and provide
estimates of sample purity and completeness. From a carefully-vetted sample of
over 600 dCs, we measure their local space density to be
$\rho_0\,=\,1.96^{+0.14}_{-0.12}\times10^{-6}\,\text{pc}^{-3}$ (about one dC in
every local disk volume of radius 50\,pc), with a relatively large disk scale
height of $H_z\,=\,856^{+49}_{-43}\,$pc.",['astro-ph.SR'],2503.16204," The existence of the binary system SDSS J1257+5428 has been described as
paradoxical. Here we investigate under which conditions SDSS J1257+5428 could
be understood as a descendant of a cataclysmic variable with an evolved donor
star, which is a scenario that has never been explored in detail. We used the
BSE code for pre-common-envelope (CE) evolution and the MESA code for post-CE
evolution to run binary evolution simulations and searched for potential
formation pathways for SDSS J1257+5428 that lead to its observed
characteristics. For the post-CE evolution, we adopted a boosted version of the
CARB model. We find that SDSS J1257+5428 can be explained as a
post-cataclysmic-variable system if (i) the progenitor of the extremely
low-mass WD was initially a solar-type star that evolved into a subgiant before
the onset of mass transfer and underwent hydrogen shell flashes after the mass
transfer stopped, (ii) the massive WD was highly or entirely rejuvenated during
the cataclysmic variable evolution, and (iii) magnetic braking was strong
enough to make the evolution convergent. In this case, the torques due to
magnetic braking need to be stronger than those provided by the CARB model by a
factor of ${\sim100}$. We conclude that SDSS J1257+5428 can be reasonably well
explained as having originated from a cataclysmic variable that hosted an
evolved donor star and should no longer be regarded as paradoxical. If our
formation channel is correct, our findings provide further support that
stronger magnetic braking acts on progenitors of (i) close detached WD
binaries, (ii) close detached millisecond pulsar with extremely low-mass WDs,
(iii) AM CVn binaries, and (iv) ultra-compact X-ray binaries, in comparison to
the magnetic braking strength required to explain binaries hosting
main-sequence stars and single main-sequence stars.",['astro-ph.SR'],False,,,,Carbon Stars From Gaia DR3 and the Space Density of Dwarf Carbon Stars,"Resolution of a paradox: SDSS J1257+5428 can be explained as a
  descendant of a cataclysmic variable with an evolved donor"
neg-d2-473,2025-01-23,,2501.13889," We propose a trait-specific image generation method that models forehead
creases geometrically using B-spline and B\'ezier curves. This approach ensures
the realistic generation of both principal creases and non-prominent crease
patterns, effectively constructing detailed and authentic forehead-crease
images. These geometrically rendered images serve as visual prompts for a
diffusion-based Edge-to-Image translation model, which generates corresponding
mated samples. The resulting novel synthetic identities are then used to train
a forehead-crease verification network. To enhance intra-subject diversity in
the generated samples, we employ two strategies: (a) perturbing the control
points of B-splines under defined constraints to maintain label consistency,
and (b) applying image-level augmentations to the geometric visual prompts,
such as dropout and elastic transformations, specifically tailored to crease
patterns. By integrating the proposed synthetic dataset with real-world data,
our method significantly improves the performance of forehead-crease
verification systems under a cross-database verification protocol.",['cs.CV'],2503.1612," Multi-species animal pose estimation has emerged as a challenging yet
critical task, hindered by substantial visual diversity and uncertainty. This
paper challenges the problem by efficient prompt learning for Vision-Language
Pretrained (VLP) models, \textit{e.g.} CLIP, aiming to resolve the
cross-species generalization problem. At the core of the solution lies in the
prompt designing, probabilistic prompt modeling and cross-modal adaptation,
thereby enabling prompts to compensate for cross-modal information and
effectively overcome large data variances under unbalanced data distribution.
To this end, we propose a novel probabilistic prompting approach to fully
explore textual descriptions, which could alleviate the diversity issues caused
by long-tail property and increase the adaptability of prompts on unseen
category instance. Specifically, we first introduce a set of learnable prompts
and propose a diversity loss to maintain distinctiveness among prompts, thus
representing diverse image attributes. Diverse textual probabilistic
representations are sampled and used as the guidance for the pose estimation.
Subsequently, we explore three different cross-modal fusion strategies at
spatial level to alleviate the adverse impacts of visual uncertainty. Extensive
experiments on multi-species animal pose benchmarks show that our method
achieves the state-of-the-art performance under both supervised and zero-shot
settings. The code is available at https://github.com/Raojiyong/PPAP.",['cs.CV'],False,,,,"Generating Realistic Forehead-Creases for User Verification via
  Conditioned Piecewise Polynomial Curves",Probabilistic Prompt Distribution Learning for Animal Pose Estimation
neg-d2-474,2025-03-17,,2503.13187," Magnetoelectric composites integrate the coupling between magnetic and
piezoelectric materials to create new functionalities for potential
technological applications. This coupling is typically achieved through the
exchange of magnetic, electric, or elastic energy across the interfaces between
the different constituent materials. Tailoring the strength of the
magnetoelectric effect is primarily accomplished by selecting suitable
materials for each constituent and by optimizing geometrical and
microstructural designs. Various composite architectures, such as (0-3), (2-2),
(1-3) and core-shell connectivities, have been studied to enhance
magnetoelectric coupling and other required physical properties in composites.
This review examines the latest advancements in magnetoelectric materials,
focusing on the impact of different interphase connectivity types on their
properties and performance. Before exploring magnetic-electric coupling, a
brief overview of the historical background of multiferroic magnetoelectric
composites is provided. Fundamental concepts underlying the magnetoelectric
effect, piezoelectricity, and the magnetostrictive effect are explained,
including their origins and examples of these materials' properties. So far,
three types of magnetoelectric composite connectivities have been investigated
experimentally: particulate composites (0-3), laminated and thin films (2-2),
sticks embedded in matrix, core-shell particles, and coaxial fibers. An outlook
on the prospects and scientific challenges in the field of multiferroic
magnetoelectric composites is given at the end of this review.",['cond-mat.mtrl-sci'],2502.17988," Oscillatory signals from coherently excited phonons are regularly observed in
ultrafast pump-probe experiments on condensed matter samples. Electron-phonon
coupling implies that coherent phonons also modulate the electronic band
structure. These oscillations can be probed with energy and momentum resolution
using time- and angle-resolved photoemission spectroscopy (trARPES) which
reveals the orbital dependence of the electron-phonon coupling for a specific
phonon mode. However, a comprehensive analysis remains challenging when
multiple coherent phonon modes couple to multiple electronic bands. Complex
spectral line shapes due to strong correlations in quantum materials add to
this challenge. In this work, we examine how the frequency domain
representation of trARPES data facilitates a quantitative analysis of coherent
oscillations of the electronic bands. We investigate the frequency domain
representation of the photoemission intensity and \tred{the first moment of the
energy distribution curves}. Both quantities provide complimentary information
and are able to distinguish oscillations of binding energy, linewidth and
intensity.We analyze a representative trARPES dataset of the transition metal
dichalcogenide WTe$_2$ and construct composite spectra which intuitively
illustrate how much each electronic band is affected by a specific phonon mode.
We also show how a linearly chirped probe pulse can generate extrinsic
artifacts that are distinct from the intrinsic coherent phonon signal.",['cond-mat.mtrl-sci'],False,,,,"Current Advances in Magnetoelectric Composites with Various Interphase
  Connectivity Types","Analysis methodology of coherent oscillations in time- and
  angle-resolved photoemission spectroscopy"
neg-d2-475,2025-01-24,,2501.14646," Generating talking avatar driven by audio remains a significant challenge.
Existing methods typically require high computational costs and often lack
sufficient facial detail and realism, making them unsuitable for applications
that demand high real-time performance and visual quality. Additionally, while
some methods can synchronize lip movement, they still face issues with
consistency between facial expressions and upper body movement, particularly
during silent periods. In this paper, we introduce SyncAnimation, the first
NeRF-based method that achieves audio-driven, stable, and real-time generation
of speaking avatar by combining generalized audio-to-pose matching and
audio-to-expression synchronization. By integrating AudioPose Syncer and
AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression
generation, progressively producing audio-synchronized upper body, head, and
lip shapes. Furthermore, the High-Synchronization Human Renderer ensures
seamless integration of the head and upper body, and achieves audio-sync lip.
The project page can be found at https://syncanimation.github.io",['cs.CV'],2502.19699," Although efficient extraction of discriminative spatial-spectral features is
critical for hyperspectral images classification (HSIC), it is difficult to
achieve these features due to factors such as the spatial-spectral
heterogeneity and noise effect. This paper presents a Spatial-Spectral
Diffusion Contrastive Representation Network (DiffCRN), based on denoising
diffusion probabilistic model (DDPM) combined with contrastive learning (CL)
for HSIC, with the following characteristics. First,to improve spatial-spectral
feature representation, instead of adopting the UNets-like structure which is
widely used for DDPM, we design a novel staged architecture with spatial
self-attention denoising module (SSAD) and spectral group self-attention
denoising module (SGSAD) in DiffCRN with improved efficiency for
spectral-spatial feature learning. Second, to improve unsupervised feature
learning efficiency, we design new DDPM model with logarithmic absolute error
(LAE) loss and CL that improve the loss function effectiveness and increase the
instance-level and inter-class discriminability. Third, to improve feature
selection, we design a learnable approach based on pixel-level spectral angle
mapping (SAM) for the selection of time steps in the proposed DDPM model in an
adaptive and automatic manner. Last, to improve feature integration and
classification, we design an Adaptive weighted addition modul (AWAM) and Cross
time step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wise
features and perform classification. Experiments conducted on widely used four
HSI datasets demonstrate the improved performance of the proposed DiffCRN over
the classical backbone models and state-of-the-art GAN, transformer models and
other pretrained methods. The source code and pre-trained model will be made
available publicly.",['cs.CV'],False,,,,"SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human
  Pose and Talking Head Animation","Spatial-Spectral Diffusion Contrastive Representation Network for
  Hyperspectral Image Classification"
neg-d2-476,2025-03-10,,2503.07138," The floating wind turbines sector has great energy potential. However,
minimizing the movement of the structure under the combined effect of wind and
waves while ensuring maximum power extraction over a wide operating range is
one of the main challenges for the control of these turbines. This paper
presents a review of control methods for floating wind turbines from the recent
literature. The limitations of these controllers are discussed, before
introducing a presentation of several promising data-based methods. In
particular, this paper focuses on artificial intelligence techniques associated
with data-based control methods. Finally, the CREATIF project dealing with
real-time simulation of floating wind turbines and their intelligent controls
is presented.",['math.OC'],2501.04034," This paper is devoted to the variational inequality problems. We consider two
classes of problems, the first is classical constrained variational inequality
and the second is the same problem with functional (inequality type)
constraints. To solve these problems, we propose mirror descent-type methods
with a weighting scheme for the generated points in each iteration of the
algorithms. This scheme assigns smaller weights to the initial points and
larger weights to the most recent points, thus it improves the convergence rate
of the proposed methods. For the variational inequality problem with functional
constraints, the proposed method switches between adaptive and non-adaptive
steps in the dependence on the values of the functional constraints at
iterations. We analyze the proposed methods for the time-varying step sizes and
prove the optimal convergence rate for variational inequality problems with
bounded and monotone operators. The results of numerical experiments of the
proposed methods for classical constrained variational inequality problems show
a significant improvement over the modified projection method.",['math.OC'],False,,,,"Strat{\'e}gies de contr{\^o}le pour les {\'e}oliennes flottantes :
  {\'e}tat de l'art et perspectives","Mirror Descent Methods with Weighting Scheme for Outputs for Constrained
  Variational Inequality Problems"
neg-d2-477,2025-01-10,,2501.06438," This paper presents Qffusion, a dual-frame-guided framework for portrait
video editing. Specifically, we consider a design principle of ``animation for
editing'', and train Qffusion as a general animation framework from two still
reference images while we can use it for portrait video editing easily by
applying modified start and end frames as references during inference.
Leveraging the powerful generative power of Stable Diffusion, we propose a
Quadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which
arranges the latent codes of two reference images and that of four facial
conditions into a four-grid fashion, separately. Then, we fuse features of
these two modalities and use self-attention for both appearance and temporal
learning, where representations at different times are jointly modeled under
QGA. Our Qffusion can achieve stable video editing without additional networks
or complex training stages, where only the input format of Stable Diffusion is
modified. Further, we propose a Quadrant-grid Propagation (QGP) inference
strategy, which enjoys a unique advantage on stable arbitrary-length video
generation by processing reference and condition frames recursively. Through
extensive experiments, Qffusion consistently outperforms state-of-the-art
techniques on portrait video editing.",['cs.CV'],2502.16826," Building on recent advances in Bayesian statistics and image denoising, we
propose Noise2Score3D, a fully unsupervised framework for point cloud denoising
that addresses the critical challenge of limited availability of clean data.
Noise2Score3D learns the gradient of the underlying point cloud distribution
directly from noisy data, eliminating the need for clean data during training.
By leveraging Tweedie's formula, our method performs inference in a single
step, avoiding the iterative processes used in existing unsupervised methods,
thereby improving both performance and efficiency. Experimental results
demonstrate that Noise2Score3D achieves state-of-the-art performance on
standard benchmarks, outperforming other unsupervised methods in Chamfer
distance and point-to-mesh metrics, and rivaling some supervised approaches.
Furthermore, Noise2Score3D demonstrates strong generalization ability beyond
training datasets. Additionally, we introduce Total Variation for Point Cloud,
a criterion that allows for the estimation of unknown noise parameters, which
further enhances the method's versatility and real-world utility.",['cs.CV'],False,,,,"Qffusion: Controllable Portrait Video Editing via Quadrant-Grid
  Attention Learning",Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising
neg-d2-478,2025-02-27,,2502.2071," One of the predominant causes of program distortion in the real quantum
computing system may be attributed to the probability deviation caused by
thermal relaxation. We introduce Barber (Balancing reAdout Results using
Bit-invErted ciRcuits), a method designed to counteract the asymmetric thermal
relaxation deviation and improve the reliability of near-term quantum programs.
Barber collaborates with a bit-inverted quantum circuit, where the excited
quantum state of qubits is assigned to the $\lvert 0 \rangle$ and the unexcited
state to the $\lvert 1 \rangle$. In doing so, bit-inverted quantum circuits can
experience thermal relaxation in the opposite direction compared to standard
quantum circuits. Barber can effectively suppress the thermal relaxation
deviation in program's readout results by selectively merging distributions
from the standard and bit-inverted circuits.",['quant-ph'],2502.07605," In contrast to the commonly used qubit resonator transverse coupling via the
$\sigma_{xy}$-degree of freedom, longitudinal coupling through $\sigma_z$
presents a tantalizing alternative: it does not hybridize the modes,
eliminating Purcell decay, and it enables quantum-non-demolishing qubit readout
independent of the qubit-resonator frequency detuning. Here, we demonstrate
longitudinal coupling between a {Cr$_7$Ni} molecular spin qubit ensemble and
the kinetic inductance of a granular aluminum superconducting microwave
resonator. The inherent frequency-independence of this coupling allows for the
utilization of a 7.8 GHz readout resonator to measure the full {Cr$_7$Ni}
magnetization curve spanning 0-600 mT, corresponding to a spin frequency range
of $f_\text{spin}=$0-15 GHz. For 2 GHz detuning from the readout resonator, we
measure a $1/e$ spin relaxation time $\tau=$0.38 s, limited by phonon decay to
the substrate. Based on these results, we propose a path towards longitudinal
coupling of single spins to a superconducting fluxonium qubit.",['quant-ph'],False,,,,"Balancing Thermal Relaxation Deviations of Near-Future Quantum Computing
  Results via Bit-Inverted Programs",Kinetic inductance coupling for circuit QED with spins
neg-d2-479,2025-03-20,,2503.16591," Monocular 3D estimation is crucial for visual perception. However, current
methods fall short by relying on oversimplified assumptions, such as pinhole
camera models or rectified images. These limitations severely restrict their
general applicability, causing poor performance in real-world scenarios with
fisheye or panoramic images and resulting in substantial context loss. To
address this, we present UniK3D, the first generalizable method for monocular
3D estimation able to model any camera. Our method introduces a spherical 3D
representation which allows for better disentanglement of camera and scene
geometry and enables accurate metric 3D reconstruction for unconstrained camera
models. Our camera component features a novel, model-independent representation
of the pencil of rays, achieved through a learned superposition of spherical
harmonics. We also introduce an angular loss, which, together with the camera
module design, prevents the contraction of the 3D outputs for wide-view
cameras. A comprehensive zero-shot evaluation on 13 diverse datasets
demonstrates the state-of-the-art performance of UniK3D across 3D, depth, and
camera metrics, with substantial gains in challenging large-field-of-view and
panoramic settings, while maintaining top accuracy in conventional pinhole
small-field-of-view domains. Code and models are available at
github.com/lpiccinelli-eth/unik3d .",['cs.CV'],2501.12218," Point tracking in videos is a fundamental task with applications in robotics,
video editing, and more. While many vision tasks benefit from pre-trained
feature backbones to improve generalizability, point tracking has primarily
relied on simpler backbones trained from scratch on synthetic data, which may
limit robustness in real-world scenarios. Additionally, point tracking requires
temporal awareness to ensure coherence across frames, but using
temporally-aware features is still underexplored. Most current methods often
employ a two-stage process: an initial coarse prediction followed by a
refinement stage to inject temporal information and correct errors from the
coarse stage. These approach, however, is computationally expensive and
potentially redundant if the feature backbone itself captures sufficient
temporal information.
  In this work, we introduce Chrono, a feature backbone specifically designed
for point tracking with built-in temporal awareness. Leveraging pre-trained
representations from self-supervised learner DINOv2 and enhanced with a
temporal adapter, Chrono effectively captures long-term temporal context,
enabling precise prediction even without the refinement stage. Experimental
results demonstrate that Chrono achieves state-of-the-art performance in a
refiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, among
common feature backbones used in point tracking as well as DINOv2, with
exceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/",['cs.CV'],False,,,,UniK3D: Universal Camera Monocular 3D Estimation,Exploring Temporally-Aware Features for Point Tracking
neg-d2-480,2025-02-27,,2502.20202," Effective field theory (EFT) provides a powerful model-independent
theoretical framework for illuminating complicated interactions across a wide
range of physics areas and subfields. In this work, we consider the low-energy
deuteron-Helium-4, triton-Helium-4, and helion-Helium-4 systems at low energies
in cluster EFT. In particular, we focus on the deuteron + Helium-4 cluster
configuration of the Lithium-6 nucleus, the triton + Helium-4 cluster
configuration of the Lithium-7 nucleus, and the Helium-3-Helium-4 configuration
of the Beryllium-7 nucleus, respectively. We illustrate how to directly extract
the asymptotic normalization coefficient and several observables using
experimental measurement of the electromagnetic form factors of these nuclei.",['nucl-th'],2503.10951," This work presents the microscopic calculation of energy rates ({\gamma} ray
heating and (anti)neutrino cooling rates) due to weak decay of selected Fe
isotopes. The isotopes have astrophysical significance during the presupernova
evolution of massive stars. The energy rates are calculated using the pn QRPA
model and compared with the independent particle model (IPM), large scale shell
model (LSSM) and recent shell model calculation (GXPF1J). The reported
(anti)neutrino cooling rates are smaller by up to two orders of magnitude at
low core temperature values than the IPM rates. The two calculations compare
well at T = 30 GK. The comparison of cooling rates with the LSSM is
interesting. The pn QRPA cooling rates due to even even Fe isotopes are smaller
(up to 2 orders of magnitude). For the odd A isotopes, the reported rates are
bigger up to an order of magnitude. The pn QRPA computed cooling rates are, up
to 2 orders of magnitude, bigger when compared with the GXPF1J calculation. The
{\gamma} ray heating rates due to electron capture rates rise with the
temperature and density values of the stellar core. On the other hand, the
{\gamma} ray heating due to \b{eta} decay increases with the core temperature
values but decreases by orders of magnitude when the stellar core stiffens. The
pn QRPA computed {\gamma} heating rates are bigger (up to 3 orders of
magnitude) at high temperatures and densities (for the case of 55 56Fe) when
compared with the recent shell model results. Owing to the importance of energy
rates, this study may contribute to a realistic simulation of presupernova
evolution of massive stars.",['nucl-th'],False,,,,"Electromagnetic form factors of ${}^6$Li, ${}^7$Li, and ${}^7$Be in
  cluster effective field theory","Energy rates due to Fe isotopes during presupernova evolution of massive
  stars"
neg-d2-481,2025-02-08,,2502.05532," This paper investigates the existence and qualitative properties of
minimizers for a class of nonlocal micromagnetic energy functionals defined on
bounded domains. The considered energy functional consists of a symmetric
exchange interaction, which penalizes spatial variations in magnetization, and
a magnetostatic self-energy term that accounts for long-range dipolar
interactions. Motivated by the extension of Brown's fundamental theorem on fine
ferromagnetic particles to nonlocal settings, we develop a rigorous variational
framework in $L^2(\Omega;\mathbb{S}^2)$ under mild assumptions on the
interaction kernel \( j \), including symmetry, L\'evy-type integrability, and
prescribed singular behavior. For spherical domains, we generalize Browns
fundamental results by identifying critical radii $R^*$ and $R^{**}$ that
delineate distinct energetic regimes: for \( R \leq R^* \), the uniform
magnetization state is energetically preferable (\emph{small-body regime}),
whereas for $R \geq R^{**}$, non-uniform magnetization configurations become
dominant (\emph{large-body regime}). These transitions are analyzed through
Poincar\'e-type inequalities and explicit energy comparisons between uniform
and vortex-like magnetization states.
  Our results directly connect classical micromagnetic theory and contemporary
nonlocal models, providing new insights into domain structure formation in
nanoscale magnetism. Furthermore, the mathematical framework developed in this
work contributes to advancing theoretical foundations for applications in
spintronics and data storage technologies.",['math.AP'],2502.00184," We prove a Brunn-Minkowski type inequality for the first (nontrivial)
Dirichlet eigenvalue of the weighted $p$-operator \[
-\Delta_{p,\gamma}u=-\text{div}(|\nabla u|^{p-2} \nabla u)+(x,\nabla u)|\nabla
u|^{p-2}, \] where $p>1$, in the class of bounded Lipschitz domains in
$\mathbb{R}^n$. We also prove that any corresponding positive eigenfunction is
log-concave if the domain is convex.",['math.AP'],False,,,,"Nonlocal Micromagnetics: Compactness Criteria, Existence of Minimizers,
  and Brown's Fundamental Theorem","Geometric properties of solutions to elliptic PDE's in Gauss space and
  related Brunn-Minkowski type inequalities"
neg-d2-482,2025-01-02,,2501.01613," The cold and hot interstellar medium (ISM) in star forming galaxies resembles
the reservoir for star formation and associated heating by stellar winds and
explosions during stellar evolution, respectively. We utilize data from deep
$Chandra$ observations and archival millimeter surveys to study the
interconnection between these two phases and the relation to star formation
activities in M51 on kiloparsec scales. A sharp radial decrease is present in
the hot gas surface brightness profile within the inner 2 kpc of M51. The ratio
between the total infrared luminosity ($L_{\rm IR}$) and the hot gas luminosity
($L_{\rm 0.5 - 2\,keV}^{\rm gas}$) shows a positive correlation with the
galactic radius in the central region. For the entire galaxy, a twofold
correlation is revealed in the $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm
IR}$ diagram, where $L_{\rm 0.5 - 2\,keV}^{\rm gas}$ sharply increases with
$L_{\rm IR}$ in the center but varies more slowly in the disk. The best fit
gives a steep relation of ${\rm log}(L_{\rm 0.5-2\,keV}^{\rm gas} /{\rm
erg\,s^{-1}})=1.82\,{\rm log}(L_{\rm IR} /{L_{\rm \odot}})+22.26$ for the
center of M51. The similar twofold correlations are also found in the $L_{\rm
0.5 - 2\,keV}^{\rm gas}$${-}$molecular line luminosity ($L^\prime_{\rm gas}$)
relations for the four molecular emission lines CO(1-0), CO(2-1), HCN(1-0), and
HCO$^+$(1-0). We demonstrate that the core-collapse supernovae (SNe) are the
primary source of energy for heating gas in the galactic center of M51, leading
to the observed steep $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm IR}$ and
$L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L^\prime_{\rm gas}$ relations, as their
X-ray radiation efficiencies ($\eta$ $\equiv$ $L_{\rm 0.5 - 2\,keV}^{\rm
gas}$/$\dot{E}_\mathrm{SN}$) increase with the star formation rate surface
densities, where $\dot{E}_\mathrm{SN}$ is the SN mechanical energy input rate.",['astro-ph.GA'],2501.01613," The cold and hot interstellar medium (ISM) in star forming galaxies resembles
the reservoir for star formation and associated heating by stellar winds and
explosions during stellar evolution, respectively. We utilize data from deep
$Chandra$ observations and archival millimeter surveys to study the
interconnection between these two phases and the relation to star formation
activities in M51 on kiloparsec scales. A sharp radial decrease is present in
the hot gas surface brightness profile within the inner 2 kpc of M51. The ratio
between the total infrared luminosity ($L_{\rm IR}$) and the hot gas luminosity
($L_{\rm 0.5 - 2\,keV}^{\rm gas}$) shows a positive correlation with the
galactic radius in the central region. For the entire galaxy, a twofold
correlation is revealed in the $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm
IR}$ diagram, where $L_{\rm 0.5 - 2\,keV}^{\rm gas}$ sharply increases with
$L_{\rm IR}$ in the center but varies more slowly in the disk. The best fit
gives a steep relation of ${\rm log}(L_{\rm 0.5-2\,keV}^{\rm gas} /{\rm
erg\,s^{-1}})=1.82\,{\rm log}(L_{\rm IR} /{L_{\rm \odot}})+22.26$ for the
center of M51. The similar twofold correlations are also found in the $L_{\rm
0.5 - 2\,keV}^{\rm gas}$${-}$molecular line luminosity ($L^\prime_{\rm gas}$)
relations for the four molecular emission lines CO(1-0), CO(2-1), HCN(1-0), and
HCO$^+$(1-0). We demonstrate that the core-collapse supernovae (SNe) are the
primary source of energy for heating gas in the galactic center of M51, leading
to the observed steep $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm IR}$ and
$L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L^\prime_{\rm gas}$ relations, as their
X-ray radiation efficiencies ($\eta$ $\equiv$ $L_{\rm 0.5 - 2\,keV}^{\rm
gas}$/$\dot{E}_\mathrm{SN}$) increase with the star formation rate surface
densities, where $\dot{E}_\mathrm{SN}$ is the SN mechanical energy input rate.",['astro-ph.GA'],False,,,,"Fire and Ice in the Whirlpool: Spatially Resolved Scaling Relations
  between X-ray Emitting Hot Gas and Cold Molecular Gas in M51","Fire and Ice in the Whirlpool: Spatially Resolved Scaling Relations
  between X-ray Emitting Hot Gas and Cold Molecular Gas in M51"
neg-d2-483,2025-03-06,,2503.04478," General-purpose AI models, particularly those designed for text and vision,
demonstrate impressive versatility across a wide range of deep-learning tasks.
However, they often underperform in specialised domains like medical imaging,
where domain-specific solutions or alternative knowledge transfer approaches
are typically required. Recent studies have noted that general-purpose models
can exhibit similar latent spaces when processing semantically related data,
although this alignment does not occur naturally. Building on this insight, it
has been shown that applying a simple transformation - at most affine -
estimated from a subset of semantically corresponding samples, known as
anchors, enables model stitching across diverse training paradigms,
architectures, and modalities. In this paper, we explore how semantic alignment
- estimating transformations between anchors - can bridge general-purpose AI
with specialised medical knowledge. Using multiple public chest X-ray datasets,
we demonstrate that model stitching across model architectures allows general
models to integrate domain-specific knowledge without additional training,
leading to improved performance on medical tasks. Furthermore, we introduce a
novel zero-shot classification approach for unimodal vision encoders that
leverages semantic alignment across modalities. Our results show that our
method not only outperforms general multimodal models but also approaches the
performance levels of fully trained, medical-specific multimodal solutions",['cs.CV'],2502.12723," This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.",['cs.CV'],False,,,,Semantic Alignment of Unimodal Medical Text and Vision Representations,"myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  Dataset"
neg-d2-484,2025-02-17,,2502.12228," In this note, we show that there does not exist a $C_2$-ring spectrum whose
underlying ring spectrum is $\mathrm{MSpin}^c$ and whose $C_2$-fixed point
spectrum is $\mathrm{MSpin}$.",['math.AT'],2502.12228," In this note, we show that there does not exist a $C_2$-ring spectrum whose
underlying ring spectrum is $\mathrm{MSpin}^c$ and whose $C_2$-fixed point
spectrum is $\mathrm{MSpin}$.",['math.AT'],False,,,,On the nonexistence of a Green functor with values MSpin${}^c$ and MSpin,On the nonexistence of a Green functor with values MSpin${}^c$ and MSpin
neg-d2-485,2025-01-07,,2501.03693," Context. Blazars are a distinct subclass of active galactic nuclei (AGN),
known for their fast variability, high polarization, and intense emission
across the electromagnetic spectrum, from radio waves to gamma rays. Gamma-ray
blazar candidates of uncertain type (BCU) are an ongoing challenge in gamma-ray
astronomy due to difficulties in classification and redshift determination.
Aims. This study continues an optical spectroscopic campaign aimed at
identifying the characteristics of BCUs to improve classification and redshift
estimates, particularly focusing on low-synchrotron-peak sources. Methods. We
conducted a detailed analysis of optical spectroscopic data for a sample of 21
low-synchrotron-peak BCUs plus one bl lac with contradictory results in the
literature, using the 3.58-m Telescopio Nazionale Galileo (TNG, La Palma,
Spain). Results. Our analysis identifies 14 out of the 21 BCUs as flat-spectrum
radio quasars (FSRQs), demonstrating the effectiveness of our selection
criteria. Notably, four FSRQs have redshifts exceeding 1, including 4FGL
J2000.0+4214 at z = 2.04. Six sources are classified as bl lacs, with one of
them, 4FGL J0746.5-0719, showing a featureless spectrum in this work despite
previously exhibiting strong lines, suggesting it may be a changing-look
blazar. One source remains classified as a BCU due to a noisy spectrum.
Additionally, we observed a bl lac object, 4FGL J1054.5+2211, due to
inconsistent redshift estimates in the literature, but we could not confirm any
redshift due to its featureless spectrum. Our findings provide insights into
the classification and redshift estimation of blazar candidates, emphasizing
the need for continued spectroscopic monitoring.",['astro-ph.HE'],2502.16626," The non-detection of periodicity related to rotation challenges the magnetar
model for fast radio bursts (FRBs). Moreover, a bimodal distribution of the
burst waiting times is widely observed in hyper-active FRBs, a significant
deviation from the exponential distribution expected from stationary Poisson
processes. By combining the epidemic-type aftershock sequence (ETAS) earthquake
model and the rotating vector model (RVM) involving the rotation of the
magnetar and orientations of the spin and magnetic axes, we find that starquake
events modulated by the rotation of FRB-emitting magnetar can explain the
bimodal distribution of FRB waiting times, as well as the non-detection of
periodicity in active repeating FRBs. We analyze data from multiple FRB
sources, demonstrating that differences in waiting time distributions and
observed energies can be explained by varying parameters related to magnetar
properties and starquake dynamics. Our results suggest that rotation-modulated
starquakes on magnetars can possibly be a unified source for FRBs. Notably, we
find that active repeaters tend to have small magnetic inclination angles in
order to hide their periodicity. We also show that our model can reproduce the
waiting time distribution of a pulsar phase of the galactic magnetar SGR
J1935+2154 with a larger inclination angle than the active repeaters, which
could explain the detection of spin period and the relatively low observed
energy for FRBs from the magnetar. The spin periods of active repeaters are not
well constrained, but most likely fall in the valley region between the two
peaks of the waiting time distributions.",['astro-ph.HE'],False,,,,"Continuation of an Optical Spectroscopic Campaign of Fermi Blazar
  Candidates with TNG: Discovery of a New Changing-Look Blazar","Hyper-active repeating fast radio bursts from rotation modulated
  starquakes on magnetars"
neg-d2-486,2025-01-16,,2501.09791," Effectively finding and identifying active galactic nuclei (AGNs) in dwarf
galaxies is an important step in studying black hole formation and evolution.
In this work, we examine four mid-IR-selected AGN candidates in dwarf galaxies
with stellar masses between $M_\star \sim 10^8 - 10^9 M_\odot$ , and find that
the galaxies are host to nuclear star clusters (NSCs) that are notably rare in
how young and massive they are. We perform photometric measurements on the
central star clusters in our target galaxies galaxies using Hubble Space
Telescope optical and near-IR imaging and compare their observed properties to
models of stellar population evolution. We find that these galaxies are host to
very massive ($\sim10^7 M_\odot$), extremely young ($\lesssim 8$ Myr), dusty
($0.6 \lesssim \mathrm{A_v} \lesssim 1.8$) nuclear star clusters. Our results
indicate that these galactic nuclei have ongoing star-formation, are still at
least partially obscured by clouds of gas and dust, and are most likely
producing the extremely red AGN-like mid-IR colors. Moreover, prior work has
shown that these galaxies do not exhibit X-ray or optical AGN signatures.
Therefore, we recommend caution when using mid-IR color-color diagnostics for
AGN selection in dwarf galaxies, since, as directly exemplified in this sample,
they can be contaminated by massive star clusters with ongoing star formation.",['astro-ph.GA'],2501.03985," We investigate AGN feedback from an intermediate-mass black hole at the
center of a dwarf spheroidal galaxy, by performing isolated galaxy simulations
using a modified version of the GADGET-3 code. We consider Leo II (PGC 34176)
in the Local Group as our simulation reference model. Beginning with black hole
seeds ranging from $10^3$ to $10^6$ M$_{\odot}$, our simulations focus on
comparing stellar-only feedback with AGN+stellar/SN feedback over 13.7 Gyr of
galactic evolution. Our results indicate that a low-mass AGN in a dwarf galaxy
influences the star formation history under specific physical conditions. While
AGN feedback is generally negative on star formation, instances of positive
feedback were also identified. Despite measurable effects on the evolution of
the dwarf host galaxy, black hole seeds exhibited only marginal growth. We
tested several physical scenarios as modified models in our simulations,
primarily concerning the dynamics of the central black holes, which may wander
within dwarf galaxies rather than being centrally located. However, none of
these adjustments significantly impacted the growth of the black hole seeds.
This suggests that intermediate-mass black holes may struggle to achieve higher
masses in isolated environments, with mergers and interactions likely playing
crucial roles in their growth. Nevertheless, AGN feedback exhibited
non-negligible effects in our simulated dwarf spheroidal galaxies, despite the
assumed dominant role of stellar feedback in the low-mass regime.",['astro-ph.GA'],False,,,,"Star-Forming Nuclear Clusters in Dwarf Galaxies Mimicking AGN Signatures
  in the Mid-Infrared","Exploring the evolution of a dwarf spheroidal galaxy with SPH
  simulations: II. AGN feedback"
neg-d2-487,2025-01-16,,2501.09553," We study the hybridization between plasmons, phonons, and electronic sound in
ionic crystals using the Debye model, where the ionic background is modeled as
a homogeneous, isotropic, elastic medium. We explicitly obtain the energies and
the damping of the hybrid plasmon-sound modes in the hydrodynamic regime and
calculate the corresponding dynamic structure factor. We show that the direct
Coulomb interaction between the ions is essential to obtain a collective sound
mode with linear dispersion.",['cond-mat.str-el'],2501.07843," Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$
possesses novel triple-$\mathbf{q}$ magnetic order instead of conventional
single-$\mathbf{q}$ zigzag order. Here we present dedicated experiments in
search for distinct properties expected of the triple-$\mathbf{q}$ order,
namely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking
fields and fictitious magnetic fields generated by the spin vorticity. In
structurally pristine single crystals, we show that $C_3$ symmetry-breaking
in-plane uniaxial strains do not affect the order's magnetic neutron
diffraction signals. We further show that $\mathbf{c}$-axis propagating light
exhibits large Faraday rotations in the ordered state due to the spin
vorticity, the sign of which can be trained via the system's ferrimagnetic
moment. These results are in favor of the triple-$\mathbf{q}$ order in
Na$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",['cond-mat.str-el'],False,,,,Plasmon-sound hybridization in ionic crystals,"Robust triple-q magnetic order with trainable spin vorticity in
  Na$_2$Co$_2$TeO$_6$"
neg-d2-488,2025-02-27,,2502.19917," To improve Multimodal Large Language Models' (MLLMs) ability to process
images and complex instructions, researchers predominantly curate large-scale
visual instruction tuning datasets, which are either sourced from existing
vision tasks or synthetically generated using LLMs and image descriptions.
However, they often suffer from critical flaws, including misaligned
instruction-image pairs and low-quality images. Such issues hinder training
efficiency and limit performance improvements, as models waste resources on
noisy or irrelevant data with minimal benefit to overall capability. To address
this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach
via \textbf{A}gents Collaboration (ViSA), which centers on image quality
assessment and image-instruction relevance evaluation. Specifically, our
approach consists of 1) an image information quantification method via visual
agents collaboration to select images with rich visual information, and 2) a
visual-centric instruction quality assessment method to select high-quality
instruction data related to high-quality images. Finally, we reorganize 80K
instruction data from large open-source datasets. Extensive experiments
demonstrate that ViSA outperforms or is comparable to current state-of-the-art
models on seven benchmarks, using only 2.5\% of the original data, highlighting
the efficiency of our data selection approach. Moreover, we conduct ablation
studies to validate the effectiveness of each component of our method. The code
is available at https://github.com/HITsz-TMG/ViSA.",['cs.CL'],2501.18644," This paper examines the output of cultural items generated by Chat Generative
PreTrained Transformer Pro in response to three structured prompts to translate
three anthologies of African poetry. The first prompt was broad, the second
focused on poetic structure, and the third prompt emphasized cultural
specificity. To support this analysis, four comparative tables were created.
The first table presents the results of the cultural items produced after the
three prompts, the second categorizes these outputs based on Aixela framework
of Proper nouns and Common expressions, the third table summarizes the cultural
items generated by human translators, a custom translation engine, and a Large
Language Model. The final table outlines the strategies employed by Chat
Generative PreTrained Transformer Pro following the culture specific prompt.
Compared to the outputs of cultural items from reference human translation and
the custom translation engine in prior studies the findings indicate that the
culture oriented prompts used with Chat Generative PreTrained Transformer Pro
did not yield significant enhancements of cultural items during the translation
of African poetry from English to French. Among the fifty four cultural items,
the human translation produced thirty three cultural items in repetition, the
custom translation engine generated Thirty eight cultural items in repetition
while Chat Generative PreTrained Transformer Pro produced forty one cultural
items in repetition. The untranslated cultural items revealed inconsistencies
in Large language models approach to translating cultural items in African
poetry from English to French.",['cs.CL'],False,,,,"Picking the Cream of the Crop: Visual-Centric Data Selection with
  Collaborative Agents","Prompt-oriented Output of Culture-Specific Items in Translated African
  Poetry by Large Language Model: An Initial Multi-layered Tabular Review"
neg-d2-489,2025-03-10,,2503.08," According to Faraday's law in classical physics, a varying magnetic field
stimulates an electric eddy field. Intuitively, when a classical field is
constant and imposed on a lattice, the Wannier-Stark ladders (WSL) can be
established, resulting in Bloch oscillations. In this work, we investigate the
dynamics of an interacting system on a (generalized) ring lattice threaded by a
varying magnetic flux. Based on the rigorious results, we demonstrate that
there exist many invariant subspaces in which the dynamics is periodic when the
flux varies linearly over time. Nevertheless, for a given initial state, the
evolved state differs from that driven by a linear field. However, the
probability distributions of the two states are identical, referred to as the
quantum analogue of Faraday's law. Our results are ubiquitous for a wide
variety of interacting systems. We demonstrate these results through numerical
simulations in an extended fermi-Hubbard model.",['cond-mat.str-el'],2502.19259," Quantum spin liquid represents an intriguing state where electron spins are
highly entangled yet spin fluctuation persists even at 0 K. Recently, the
hexaaluminates \textit{R}MgAl$_{11}$O$_{19}$ (\textit{R} = rare earth) have
been proposed to be a platform for realizing the quantum spin liquid state with
dominant Ising anisotropic correlations. Here, we report detailed
low-temperature magnetic susceptibility, muon spin relaxation, and
thermodynamic studies on the CeMgAl$_{11}$O$_{19}$ single crystal. Ising
anisotropy is revealed by magnetic susceptibility measurements. Muon spin
relaxation and ac susceptibility measurements rule out any long-range magnetic
ordering or spin freezing down to 50 mK despite the onset of spin correlations
below $\sim$0.8 K. Instead, the spins keep fluctuating at a rate of 1.0(2) MHz
at 50 mK. Specific heat results indicate a gapless excitation with a power-law
dependence on temperature, $C_m(T) \propto T^{\alpha}$. The quasi-quadratic
temperature dependence with $\alpha$ = 2.28(4) in zero field and linear
temperature dependence in 0.25 T support the possible realization of the U(1)
Dirac quantum spin liquid state.",['cond-mat.str-el'],False,,,,"Bloch oscillations in interacting systems driven by a time-dependent
  magnetic field","U(1) Dirac quantum spin liquid candidate in triangular-lattice
  antiferromagnet CeMgAl$_{11}$O$_{19}$"
neg-d2-490,2025-01-15,,2501.08659," Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.",['cs.CV'],2502.08377," Recently, the generation of dynamic 3D objects from a video has shown
impressive results. Existing methods directly optimize Gaussians using whole
information in frames. However, when dynamic regions are interwoven with static
regions within frames, particularly if the static regions account for a large
proportion, existing methods often overlook information in dynamic regions and
are prone to overfitting on static regions. This leads to producing results
with blurry textures. We consider that decoupling dynamic-static features to
enhance dynamic representations can alleviate this issue. Thus, we propose a
dynamic-static feature decoupling module (DSFD). Along temporal axes, it
regards the regions of current frame features that possess significant
differences relative to reference frame features as dynamic features.
Conversely, the remaining parts are the static features. Then, we acquire
decoupled features driven by dynamic features and current frame features.
Moreover, to further enhance the dynamic representation of decoupled features
from different viewpoints and ensure accurate motion prediction, we design a
temporal-spatial similarity fusion module (TSSF). Along spatial axes, it
adaptively selects similar information of dynamic regions. Hinging on the
above, we construct a novel approach, DS4D. Experimental results verify our
method achieves state-of-the-art (SOTA) results in video-to-4D. In addition,
the experiments on a real-world scenario dataset demonstrate its effectiveness
on the 4D scene. Our code will be publicly available.",['cs.CV'],False,,,,"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with
  Multi-modality Refinement Module","Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling
  Dynamic-Static Features"
neg-d2-491,2025-02-07,,2502.05353," Sample selection is pervasive in applied economic studies. This paper
develops semiparametric selection models that achieve point identification
without relying on exclusion restrictions, an assumption long believed
necessary for identification in semiparametric selection models. Our
identification conditions require at least one continuously distributed
covariate and certain nonlinearity in the selection process. We propose a
two-step plug-in estimator that is root-n-consistent, asymptotically normal,
and computationally straightforward (readily available in statistical
software), allowing for heteroskedasticity. Our approach provides a middle
ground between Lee (2009)'s nonparametric bounds and Honor\'e and Hu (2020)'s
linear selection bounds, while ensuring point identification. Simulation
evidence confirms its excellent finite-sample performance. We apply our method
to estimate the racial and gender wage disparity using data from the US Current
Population Survey. Our estimates tend to lie outside the Honor\'e and Hu
bounds.",['econ.EM'],2503.06645," This paper investigates the estimation of high-dimensional factor models in
which factor loadings undergo an unknown number of structural changes over
time. Given that a model with multiple changes in factor loadings can be
observationally indistinguishable from one with constant loadings but varying
factor variances, this reduces the high-dimensional structural change problem
to a lower-dimensional one. Due to the presence of multiple breakpoints, the
factor space may expand, potentially causing the pseudo factor covariance
matrix within some regimes to be singular. We define two types of breakpoints:
{\bf a singular change}, where the number of factors in the combined regime
exceeds the minimum number of factors in the two separate regimes, and {\bf a
rotational change}, where the number of factors in the combined regime equals
that in each separate regime. Under a singular change, we derive the properties
of the small eigenvalues and establish the consistency of the QML estimators.
Under a rotational change, unlike in the single-breakpoint case, the pseudo
factor covariance matrix within each regime can be either full rank or
singular, yet the QML estimation error for the breakpoints remains stably
bounded. We further propose an information criterion (IC) to estimate the
number of breakpoints and show that, with probability approaching one, it
accurately identifies the true number of structural changes. Monte Carlo
simulations confirm strong finite-sample performance. Finally, we apply our
method to the FRED-MD dataset, identifying five structural breaks in factor
loadings between 1959 and 2024.",['econ.EM'],False,,,,"Point-Identifying Semiparametric Sample Selection Models with No
  Excluded Variable","Singularity-Based Consistent QML Estimation of Multiple Breakpoints in
  High-Dimensional Factor Models"
neg-d2-492,2025-02-01,,2502.00515," Tensorial, spinorial and helicity formalisms of the curvature and conformal
curvature dynamics are developed. Equations of linearized gravity within that
formalisms are given. Gravitational radiation in linearized gravity in terms of
curvature dynamics is investigated. Equivalence of the Bia\l{}ynicki-Birula
formula for the gravitational energy in linearized gravity and the
Landau-Lifschitz formula is proved. Analogous result is found for the momentum
in linearized gravity.",['gr-qc'],2501.03356," An approach is presented to address singularities in general relativity using
a complex Riemannian spacetime extension. We demonstrate how this method can be
applied to both black hole and cosmological singularities, specifically
focusing on the Schwarzschild and Kerr black holes and the
Friedmann-Lema\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending
the relevant coordinates into the complex plane and carefully choosing
integration contours, we show that it is possible to regularize these
singularities, resulting in physically meaningful, singularity-free solutions
when projected back onto real spacetime. The removal of the singularity at the
Big Bang allows for a bounce cosmology. This approach offers a potential bridge
between classical general relativity and quantum gravity effects, suggesting a
way to resolve longstanding issues in gravitational physics without requiring a
full theory of quantum gravity.",['gr-qc'],False,,,,"Curvature and conformal curvature dynamics formalisms and their
  applications in linearized gravity","Complex Riemannian spacetime and singularity-free black holes and
  cosmology"
neg-d2-493,2025-01-07,,2501.04112," In this work, we provide the first example of an infinite family of branch
groups in the class of non-contracting self-similar groups. We show that these
groups are very strongly fractal, not regular branch, and of exponential
growth. Further, we prove that these groups do not have the congruence subgroup
property by explicitly calculating the structure of their rigid kernels. This
class of groups is also the first example of branch groups with non-torsion
rigid kernels. As a consequence of these results, we also determine the
Hausdorff dimension of these groups.",['math.GR'],2502.09442," In this paper we prove that the Diophantine problem in iterated restricted
wreath products $G$ of arbitrary non-trivial free abelian groups $A_1,\ldots,
A_k$, $k>1$ of finite ranks is undecidable, i.e., there is no algorithm that
given a finite system of group equations with coefficients in $G$ decides
whether or not the system has a solution in $G$.",['math.GR'],False,,,,A Class of Non-Contracting Branch Groups with Non-Torsion Rigid Kernels,"The Diophantine problem in iterated wreath products of free abelian
  groups is undecidable"
neg-d2-494,2025-02-18,,2502.1295," Hybrid traffic laws represent an innovative approach to managing mixed
environments of connected autonomous vehicles (CAVs) and human-driven vehicles
(HDVs) by introducing separate sets of regulations for each vehicle type. These
laws are designed to leverage the unique capabilities of CAVs while ensuring
both types of cars coexist effectively, ultimately aiming to enhance overall
social welfare. This study uses the SUMO simulation platform to explore hybrid
traffic laws in a restricted lane scenario. It evaluates static and dynamic
lane access policies under varying traffic demands and CAV proportions. The
policies aim to minimize average passenger delay and encourage the
incorporation of autonomous vehicles with higher occupancy rates. Results
demonstrate that dynamic policies significantly improve traffic flow,
especially at low CAV proportions, compared to traditional dedicated bus lane
strategies. These findings highlight the potential of hybrid traffic laws to
enhance traffic efficiency and accelerate the transition to autonomous
technology.",['cs.MA'],2503.07702," Collaboration is a fundamental and essential characteristic of many complex
systems, ranging from ant colonies to human societies. Each component within a
complex system interacts with others, even at a distance, to accomplish a given
task. A network of collaboration can be defined to study the collective
behavior of such systems within the framework of complex networks. The nodes in
these networks may represent simple organisms or more sophisticated intelligent
agents, such as humans. In this study, we utilize intelligent agents (nodes)
trained through reinforcement learning techniques to establish connections with
their neighbors, ultimately leading to the emergence of a large-scale
communication cluster. Notably, there is no centralized administrator; instead,
agents must adjust their connections based on information obtained from local
observations. The connection strategy is formulated using a physical
Hamiltonian, thereby categorizing this intelligent system under the paradigm of
""Physics-Guided Machine Learning"".",['cs.MA'],False,,,,"Towards Hybrid Traffic Laws for Mixed Flow of Human-Driven Vehicles and
  Connected Autonomous Vehicles","A Reliable Self-Organized Distributed Complex Network for Communication
  of Smart Agents"
neg-d2-495,2025-01-31,,2502.00087," Ortiz and Ibarra-Castor (2024) have presented a ""Generalized redshift
formula"" taking account of only energy conservation considerations. Contrary to
their claim, we emphasize to invoke both energy and momentum considerations in
order to deduce all three types of redshift (Doppler, gravitational and
cosmological). We formulate our views on the three physical effects in a
consistent manner in addition to addressing the lack of relevant references in
Ref.(Ortizand Ibarra-Castor, 2024).",['physics.gen-ph'],2502.00087," Ortiz and Ibarra-Castor (2024) have presented a ""Generalized redshift
formula"" taking account of only energy conservation considerations. Contrary to
their claim, we emphasize to invoke both energy and momentum considerations in
order to deduce all three types of redshift (Doppler, gravitational and
cosmological). We formulate our views on the three physical effects in a
consistent manner in addition to addressing the lack of relevant references in
Ref.(Ortizand Ibarra-Castor, 2024).",['physics.gen-ph'],False,,,,"Doppler, gravitational and cosmological redshifts","Doppler, gravitational and cosmological redshifts"
neg-d2-496,2025-02-28,,2502.20831," This study introduces a dynamic bus lane (DBL) strategy, referred to as the
dynamic bus priority lane (DBPL) strategy, designed for mixed traffic
environments featuring both manual and automated vehicles. Unlike previous DBL
strategies, this approach accounts for partially connected and autonomous
vehicles (CAVs) capable of autonomous trajectory planning. By leveraging this
capability, the strategy grants certain CAVs Right of Way (ROW) in bus lanes
while utilizing their leading effects in general lanes to guide vehicle
platoons through intersections, thereby indirectly influencing the trajectories
of other vehicles. The ROW allocation is optimized using a mixed-integer linear
programming (MILP) model, aimed at minimizing total vehicle travel time. Since
different CAVs entering the bus lane affect other vehicles travel times, the
model incorporates lane change effects when estimating the states of CAVs,
human-driven vehicles (HDVs), and connected autonomous buses (CABs) as they
approach the stop bar. A dynamic control framework with a rolling horizon
procedure is established to ensure precise execution of the ROW optimization
under varying traffic conditions. Simulation experiments across two scenarios
assess the performance of the proposed DBPL strategy at different CAV market
penetration rates (MPRs).",['math.OC'],2502.18135," This paper introduces a novel method for solving the single-source
localization problem, specifically addressing the case of trilateration. We
formulate the problem as a weighted least-squares problem in the squared
distances and demonstrate how suitable weights are chosen to accommodate
different noise distributions. By transforming this formulation into an
eigenvalue problem, we leverage existing eigensolvers to achieve a fast,
numerically stable, and easily implemented solver. Furthermore, our theoretical
analysis establishes that the globally optimal solution corresponds to the
largest real eigenvalue, drawing parallels to the existing literature on the
trust-region subproblem. Unlike previous works, we give special treatment to
degenerate cases, where multiple and possibly infinitely many solutions exist.
We provide a geometric interpretation of the solution sets and design the
proposed method to handle these cases gracefully. Finally, we validate against
a range of state-of-the-art methods using synthetic and real data,
demonstrating how the proposed method is among the fastest and most numerically
stable.",['math.OC'],False,,,,"A Dynamic Bus Lane Strategy for Integrated Management of Human-Driven
  and Autonomous Vehicles",Single-Source Localization as an Eigenvalue Problem
neg-d2-497,2025-01-29,,2501.18008," We present a full Batalin-Vilkovisky action in the component field formalism
for $\mathcal N=1$ supergravity in ten dimensions coupled to Yang-Mills
multiplets.",['hep-th'],2503.14673," We derive a manifestly superconformally covariant unfolded formulation of the
free (2,0) tensor multiplet. The unfolded system consists of an abelian
two-form and an infinite-dimensional, chiral Weyl zero-form realized using
superoscillators. The construction of the cocycle gluing these forms on a
general superconformal background goes one step beyond previous results in
super-Poincar\'e backgrounds.",['hep-th'],False,,,,"Batalin-Vilkovisky formulation of the $\mathcal N=1$ supergravity in ten
  dimensions",Unfolding the Six-Dimensional Tensor Multiplet
neg-d2-498,2025-02-01,,2502.01668," The literature establishes that the light fermions contributions to the
decays $H\to Z\gamma$ and $H\to\gamma\gamma$ are negligible since their
coupling with the Higgs is proportional to $m_f$. In the present letter, we
show that although such a conclusion is true for leptons, the light quark
contributions are zero when we consider their non-perturbative effects.",['hep-ph'],2502.02992," We present the results obtained by performing global fits of
two-Higgs-doublet models (2HDMs) using the full Run 1 and Run 2 Higgs datasets
collected at the LHC. Avoiding unwanted tree-level flavor-changing neutral
currents and including the wrong-sign cases, we consider 12 scenarios across
six types of 2HDMs: Inert, type I, type II, type III, type IV, and Aligned
2HDMs. Our main results are presented in Table 3 and Fig. 1. We find that the
type-I 2HDM provides the best fit, while the wrong-sign scenarios of the
type-II and type-IV 2HDMs, where the normalized Yukawa coupling to down-type
quarks is opposite in sign to the Standard Model (SM), are disfavored. We also
observe that the Aligned 2HDM gives the second-best fit when the Yukawa
couplings to down-type quarks take the same sign as in the SM, regardless of
the sign of the Yukawa couplings to the charged leptons.",['hep-ph'],False,,,,Light quark contributions to Higgs decays,"Higgs boson precision analysis of two Higgs doublet models: Full LHC Run
  1 and Run 2 data"
neg-d2-499,2025-02-24,,2502.16842," Large Vision-Language Models (LVLMs) integrate image encoders with Large
Language Models (LLMs) to process multi-modal inputs and perform complex visual
tasks. However, they often generate hallucinations by describing non-existent
objects or attributes, compromising their reliability. This study analyzes
hallucination patterns in image captioning, showing that not all tokens in the
generation process are influenced by image input and that image dependency can
serve as a useful signal for hallucination detection. To address this, we
develop an automated pipeline to identify hallucinated objects and train a
token-level classifier using hidden representations from parallel inference
passes-with and without image input. Leveraging this classifier, we introduce a
decoding strategy that effectively controls hallucination rates in image
captioning at inference time.",['cs.CV'],2501.01557," Surround-View System (SVS) is an essential component in Advanced Driver
Assistance System (ADAS) and requires precise calibrations. However,
conventional offline extrinsic calibration methods are cumbersome and
time-consuming as they rely heavily on physical patterns. Additionally, these
methods primarily focus on short-range areas surrounding the vehicle, resulting
in lower calibration quality in more distant zones. To address these
limitations, we propose Click-Calib, a pattern-free approach for offline SVS
extrinsic calibration. Without requiring any special setup, the user only needs
to click a few keypoints on the ground in natural scenes. Unlike other offline
calibration approaches, Click-Calib optimizes camera poses over a wide range by
minimizing reprojection distance errors of keypoints, thereby achieving
accurate calibrations at both short and long distances. Furthermore,
Click-Calib supports both single-frame and multiple-frame modes, with the
latter offering even better results. Evaluations on our in-house dataset and
the public WoodScape dataset demonstrate its superior accuracy and robustness
compared to baseline methods. Code is available at
https://github.com/lwangvaleo/click_calib.",['cs.CV'],False,,,,"Exploring Causes and Mitigation of Hallucinations in Large Vision
  Language Models","Click-Calib: A Robust Extrinsic Calibration Method for Surround-View
  Systems"
neg-d2-500,2025-01-27,,2501.16186," This paper studies the uplink and downlink power allocation for interactive
augmented reality (AR) services, where live video captured by an AR device is
uploaded to the network edge and then the augmented video is subsequently
downloaded. By modeling the AR transmission process as a tandem queuing system,
we derive an upper bound for the probabilistic quality of service (QoS)
requirement concerning end-to-end latency and reliability. The resource
allocation with the QoS constraints results in a functional optimization
problem. To address it, we design a deep neural network to learn the power
allocation policy, leveraging the structure of optimal power allocation to
enhance learning performance. Simulation results demonstrate that the proposed
method effectively reduces transmit powers while meeting the QoS requirement.",['cs.LG'],2503.10115," The purpose of partial multi-label feature selection is to select the most
representative feature subset, where the data comes from partial multi-label
datasets that have label ambiguity issues. For label disambiguation, previous
methods mainly focus on utilizing the information inside the labels and the
relationship between the labels and features. However, the information existing
in the feature space is rarely considered, especially in partial multi-label
scenarios where the noises is considered to be concentrated in the label space
while the feature information is correct. This paper proposes a method based on
latent space alignment, which uses the information mined in feature space to
disambiguate in latent space through the structural consistency between labels
and features. In addition, previous methods overestimate the consistency of
features and labels in the latent space after convergence. We comprehensively
consider the similarity of latent space projections to feature space and label
space, and propose new feature selection term. This method also significantly
improves the positive label identification ability of the selected features.
Comprehensive experiments demonstrate the superiority of the proposed method.",['cs.LG'],False,,,,Learn to Optimize Resource Allocation under QoS Constraint of AR,"Reconsidering Feature Structure Information and Latent Space Alignment
  in Partial Multi-label Feature Selection"
neg-d2-501,2025-03-20,,2503.16622," Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning
of machine learning models. In IoT systems, XAI improves the transparency of
models processing sensor data from multiple heterogeneous devices, ensuring
end-users understand and trust their outputs. Among the many applications, XAI
has also been applied to sensor-based Activities of Daily Living (ADLs)
recognition in smart homes. Existing approaches highlight which sensor events
are most important for each predicted activity, using simple rules to convert
these events into natural language explanations for non-expert users. However,
these methods produce rigid explanations lacking natural language flexibility
and are not scalable. With the recent rise of Large Language Models (LLMs), it
is worth exploring whether they can enhance explanation generation, considering
their proven knowledge of human activities. This paper investigates potential
approaches to combine XAI and LLMs for sensor-based ADL recognition. We
evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition
models, avoiding costly labeled data collection, and b) to automate the
generation of explanations for existing data-driven XAI approaches when
training data is available and the goal is higher recognition rates. Our
critical evaluation provides insights into the benefits and challenges of using
LLMs for explainable ADL recognition.",['cs.CL'],2501.04249," Despite the remarkable advancements and widespread applications of deep
neural networks, their ability to perform reasoning tasks remains limited,
particularly in domains requiring structured, abstract thought. In this paper,
we investigate the linguistic reasoning capabilities of state-of-the-art large
language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from
International Linguistics Olympiad (IOL) problems. This dataset encompasses
diverse problems testing syntax, morphology, phonology, and semantics, all
carefully designed to be self-contained and independent of external knowledge.
These tasks challenge models to engage in metacognitive linguistic reasoning,
requiring the deduction of linguistic rules and patterns from minimal examples.
Through extensive benchmarking of leading LLMs, we find that even the most
advanced models struggle to handle the intricacies of linguistic complexity,
particularly in areas demanding compositional generalization and rule
abstraction. Our analysis highlights both the strengths and persistent
limitations of current models in linguistic problem-solving, offering valuable
insights into their reasoning capabilities. By introducing IOLBENCH, we aim to
foster further research into developing models capable of human-like reasoning,
with broader implications for the fields of computational linguistics and
artificial intelligence.",['cs.CL'],False,,,,"Leveraging Large Language Models for Explainable Activity Recognition in
  Smart Homes: A Critical Evaluation",IOLBENCH: Benchmarking LLMs on Linguistic Reasoning
neg-d2-502,2025-03-14,,2503.11965," We introduce a novel framework for learning in neural networks by decomposing
each neuron's weight vector into two distinct parts, $W_1$ and $W_2$, thereby
modeling contrastive information directly at the neuron level. Traditional
gradient descent stores both positive (target) and negative (non-target)
feature information in a single weight vector, often obscuring fine-grained
distinctions. Our approach, by contrast, maintains separate updates for target
and non-target features, ultimately forming a single effective weight $W = W_1
- W_2$ that is more robust to noise and class imbalance. Experimental results
on both regression (California Housing, Wine Quality) and classification
(MNIST, Fashion-MNIST, CIFAR-10) tasks suggest that this decomposition enhances
generalization and resists overfitting, especially when training data are
sparse or noisy. Crucially, the inference complexity remains the same as in the
standard $WX + \text{bias}$ setup, offering a practical solution for improved
learning without additional inference-time overhead.",['cs.LG'],2502.12874," Causality is widely used in fairness analysis to prevent discrimination on
sensitive attributes, such as genders in career recruitment and races in crime
prediction. However, the current data-based Potential Outcomes Framework (POF)
often leads to untrustworthy fairness analysis results when handling
high-dimensional data. To address this, we introduce a distribution-based POF
that transform fairness analysis into Distributional Closeness Testing (DCT) by
intervening on sensitive attributes. We define counterfactual closeness
fairness as the null hypothesis of DCT, where a sensitive attribute is
considered fair if its factual and counterfactual potential outcome
distributions are sufficiently close. We introduce the Norm-Adaptive Maximum
Mean Discrepancy Treatment Effect (N-TE) as a statistic for measuring
distributional closeness and apply DCT using the empirical estimator of NTE,
referred to Counterfactual Fairness-CLOseness Testing ($\textrm{CF-CLOT}$). To
ensure the trustworthiness of testing results, we establish the testing
consistency of N-TE through rigorous theoretical analysis. $\textrm{CF-CLOT}$
demonstrates sensitivity in fairness analysis through the flexibility of the
closeness parameter $\epsilon$. Unfair sensitive attributes have been
successfully tested by $\textrm{CF-CLOT}$ in extensive experiments across
various real-world scenarios, which validate the consistency of the testing.",['cs.LG'],False,,,,Revisiting Gradient Descent: A Dual-Weight Method for Improved Learning,Testing for Causal Fairness
neg-d2-503,2025-02-19,,2502.13763," In session-based recommender systems, predictions are based on the user's
preceding behavior in the session. State-of-the-art sequential recommendation
algorithms either use graph neural networks to model sessions in a graph or
leverage the similarity of sessions by exploiting item features. In this paper,
we combine these two approaches and propose a novel method, Graph Convolutional
Network Extension (GCNext), which incorporates item features directly into the
graph representation via graph convolutional networks. GCNext creates a
feature-rich item co-occurrence graph and learns the corresponding item
embeddings in an unsupervised manner. We show on three datasets that
integrating GCNext into sequential recommendation algorithms significantly
boosts the performance of nearest-neighbor methods as well as neural network
models. Our flexible extension is easy to incorporate in state-of-the-art
methods and increases the MRR@20 by up to 12.79%.",['cs.IR'],2502.05558," Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.",['cs.IR'],False,,,,"Unsupervised Graph Embeddings for Session-based Recommendation with Item
  Features",Large Memory Network for Recommendation
neg-d2-504,2025-02-05,,2502.03462," Effective noise models are essential for analyzing and understanding the
dynamics of quantum systems, particularly in applications like quantum error
mitigation and correction. However, even when noise processes are
well-characterized in isolation, the effective noise channels impacting target
quantum operations can differ significantly, as different gates experience
noise in distinct ways. Here, we present a noise model construction method that
builds an effective model from a Lindbladian description of the physical noise
processes acting simultaneously to the desired gate operation. It employs the
Magnus expansion and Dyson series, and can be utilized for both low-order
symbolic and high-order numerical approximations of the noise channel of a
multi-qubit quantum gate. We envision multiple use cases of our noise
construction method such as (i) computing the corresponding noise channel from
a learned Lindbladian, and (ii) generating the noise channel starting with
physically motivated Lindbladians for a given hardware architecture. In doing
so, we close the gap between physical Lindbladians and operational level noise
model parameters. We demonstrate a strong agreement between our symbolic noise
construction and full numerical Lindblad simulations for various two-qubit
gates, in isolation and in three- and four-qubit scenarios, for a variety of
physically motivated noise sources. Our symbolic construction provides a useful
breakdown of how noise model parameters depend on the underlying physical noise
parameters, which gives qualitative insight into the structure of errors. For
instance, our theory provides insight into the interplay of Lindblad noise with
the intended gate operations, and can predict how local Lindblad noise can
effectively spread into multi-qubit error.",['quant-ph'],2501.11888," The T center in silicon has recently emerged as a promising candidate for
scalable quantum technologies, due to its telecommunications band optical
transition and microwave addressable ground state spin. The immense promise of
the T center is driven by its silicon host material; silicon is by far the most
mature, manufacturable semiconductor material for integrated photonic and
electronic devices. Here, we present the first study of T-centers in an
electrical device. We study an ensemble of T centers coupled to a buried
lateral P-I-N diode in silicon, observing the T-center's optical response to
static and dynamic electric fields. We utilize the defect's optical response as
a probe of device nonlinearity, observing a phase transition of the carrier
density into a stable oscillatory regime characteristic of negative
differential resistance. These findings provide fundamental insight into the
physics of the T-center for improved quantum device performance and open a
promising new direction for defect-based local quantum sensing in semiconductor
devices.",['quant-ph'],False,,,,Efficient Lindblad synthesis for noise model construction,"Probing negative differential resistance in silicon with a P-I-N
  diode-integrated T center ensemble"
neg-d2-505,2025-01-13,,2501.07748," The vertical ground reaction force (vGRF) and its characteristic weight
acceptance and push-off peaks measured during walking are important for gait
and biomechanical analysis. Current wearable vGRF estimation methods suffer
from drifting errors or low generalization performances, limiting their
practical application. This paper proposes a novel method for reliably
estimating vGRF and its characteristic peaks using data collected from the
smart insole, including inertial measurement unit data and the newly introduced
center of the pressed sensor data. These data were fused with machine learning
algorithms including artificial neural networks, random forest regression, and
bi-directional long-short-term memory. The proposed method outperformed the
state-of-the-art methods with the root mean squared error, normalized root mean
squared error, and correlation coefficient of 0.024 body weight (BW), 1.79% BW,
and 0.997 in intra-participant testing, and 0.044 BW, 3.22% BW, and 0.991 in
inter-participant testing, respectively. The difference between the reference
and estimated weight acceptance and push-off peak values are 0.022 BW and 0.017
BW with a delay of 1.4% and 1.8% of the gait cycle for the intra-participant
testing and 0.044 BW and 0.025 BW with a delay of 1.5% and 2.3% of the gait
cycle for the inter-participant testing. The results indicate that the proposed
vGRF estimation method has the potential to achieve accurate vGRF measurement
during walking in free living environments.",['cs.HC'],2502.16895," Teaching scientific concepts is essential but challenging, and analogies help
students connect new concepts to familiar ideas. Advancements in large language
models (LLMs) enable generating analogies, yet their effectiveness in education
remains underexplored. In this paper, we first conducted a two-stage study
involving high school students and teachers to assess the effectiveness of
LLM-generated analogies in biology and physics through a controlled in-class
test and a classroom field study. Test results suggested that LLM-generated
analogies could enhance student understanding particularly in biology, but
require teachers' guidance to prevent over-reliance and overconfidence.
Classroom experiments suggested that teachers could refine LLM-generated
analogies to their satisfaction and inspire new analogies from generated ones,
encouraged by positive classroom feedback and homework performance boosts.
Based on findings, we developed and evaluated a practical system to help
teachers generate and refine teaching analogies. We discussed future directions
for developing and evaluating LLM-supported teaching and learning by analogy.",['cs.HC'],False,,,,"Reliable Vertical Ground Reaction Force Estimation with Smart Insole
  During Walking","Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies
  for Student Understanding and Classroom Practice?"
neg-d2-506,2025-03-17,,2503.13591," A systematic torque from anisotropic radiation can rapidly spin up irregular
grains to the point of breakup. We apply the standard theory of rotational
disruption from radiative torques to solar system grains, finding that grains
with radii $\sim$0.03 --3 $\mu$m at 1 a.u. from the Sun are spun to the point
of breakup on timescales $\lesssim1$ yr even when assuming them to have an
unrealistically high tensile strength of pure meteoritic iron. Such a rapid
disruption timescale is incompatible with both the abundance of micron-sized
grains detected in the inner solar system and with the low production rate of
$\beta$ meteoroids. We suggest the possibility that zodiacal grains have a
strong propensity to attain rotational equilibrium at low angular velocity (a
so-called low-$J$ attractor) and that the efficacy of rotational disruption in
the Solar System -- and likely elsewhere -- has been greatly overestimated.",['astro-ph.EP'],2503.01599," Of the ~25 directly imaged planets to date, all are younger than 500Myr and
all but 6 are younger than 100Myr. Eps Ind A (HD209100, HIP108870) is a K5V
star of roughly solar age (recently derived as 3.7-5.7Gyr and
3.5$^{+0.8}_{-1.3}$Gyr). A long-term radial velocity trend as well as an
astrometric acceleration led to claims of a giant planet orbiting the nearby
star (3.6384$\pm$0.0013pc). Here we report JWST coronagraphic images that
reveal a giant exoplanet which is consistent with these radial and astrometric
measurements, but inconsistent with the previously claimed planet properties.
The new planet has temperature ~275K, and is remarkably bright at 10.65um and
15.50um. Non-detections between 3.5-5um indicate an unknown opacity source in
the atmosphere, possibly suggesting a high metallicity, high carbon-to-oxygen
ratio planet. The best-fit temperature of the planet is consistent with
theoretical thermal evolution models, which are previously untested at this
temperature range. The data indicates that this is likely the only giant planet
in the system and we therefore refer to it as ``b"", despite it having
significantly different orbital properties than the previously claimed planet
``b"".",['astro-ph.EP'],False,,,,A Rotational Disruption Crisis for Zodiacal Dust,A temperate super-Jupiter imaged with JWST in the mid-infrared
neg-d2-507,2025-02-18,,2502.12895," The breakthrough of generative large language models (LLMs) that can solve
different tasks through chat interaction has led to a significant increase in
the use of general benchmarks to assess the quality or performance of these
models beyond individual applications. There is also a need for better methods
to evaluate and also to compare models due to the ever increasing number of new
models published. However, most of the established benchmarks revolve around
the English language. This paper analyses the benefits and limitations of
current evaluation datasets, focusing on multilingual European benchmarks. We
analyse seven multilingual benchmarks and identify four major challenges.
Furthermore, we discuss potential solutions to enhance translation quality and
mitigate cultural biases, including human-in-the-loop verification and
iterative translation ranking. Our analysis highlights the need for culturally
aware and rigorously validated benchmarks to assess the reasoning and
question-answering capabilities of multilingual LLMs accurately.",['cs.CL'],2502.14734," We propose the Sentence Smith framework that enables controlled and specified
manipulation of text meaning. It consists of three main steps: 1. Parsing a
sentence into a semantic graph, 2. Applying human-designed semantic
manipulation rules, and 3. Generating text from the manipulated graph. A final
filtering step (4.) ensures the validity of the applied transformation. To
demonstrate the utility of Sentence Smith in an application study, we use it to
generate hard negative pairs that challenge text embedding models. Since the
controllable generation makes it possible to clearly isolate different types of
semantic shifts, we can gain deeper insights into the specific strengths and
weaknesses of widely used text embedding models, also addressing an issue in
current benchmarking where linguistic phenomena remain opaque. Human validation
confirms that the generations produced by Sentence Smith are highly accurate.",['cs.CL'],False,,,,"Multilingual European Language Models: Benchmarking Approaches and
  Challenges","Sentence Smith: Formally Controllable Text Transformation and its
  Application to Evaluation of Text Embedding Models"
neg-d2-508,2025-01-07,,2501.04212," In this paper we study a nonlinear free boundary problem on the radial growth
of a two-layer solid tumor with a quiescent core. The tumor surface and its
inner interface separating the proliferating cells and the quiescent cells are
both free boundaries. By deeply analyzing their relationship and employing the
maximum principle, we show this problem is globally well-posed and prove the
existence of a unique positive threshold $\sigma^*$ such that the problem
admits a unique stationary solution with a quiescent core if and only if the
externally supplied nutrient $\bar\sigma> \sigma^*$. The stationary solution is
globally asymptotically stable. The formation of the quiescent core and its
interesting connection with the necrotic core are also given.",['math.AP'],2503.1214," We study the decay properties of non-negative solutions to the
one-dimensional defocusing damped wave equation in the Fujita subcritical case
under a specific initial condition. Specifically, we assume that the initial
data are positive, satisfy a condition ensuring the positiveness of solutions,
and exhibit polynomial decay at infinity.
  To show the decay properties of the solution, we construct suitable
supersolutions composed of an explicit function satisfying an ordinary
differential inequality and the solution of the linear damped wave equation.
Our estimates correspond to the optimal ones inferred from the analysis of the
heat equation.",['math.AP'],False,,,,"Analysis of a nonlinear free boundary problem modeling the radial growth
  of two-layer tumors","Decay estimate for subcritical semilinear damped wave equations with
  slowly decreasing data"
neg-d2-509,2025-02-05,,2502.02968," We extend the Coupon Collector's Problem (CCP) and present a novel
generalized model, referred as the k-LCCP problem, where one is interested in
recovering a bipartite graph with a perfect matching, which represents the
coupons and their matching labels. We show two extra-extensions to this
variation: the heterogeneous sample size case (K-LCCP) and the partly
recovering case.",['cs.DM'],2502.02968," We extend the Coupon Collector's Problem (CCP) and present a novel
generalized model, referred as the k-LCCP problem, where one is interested in
recovering a bipartite graph with a perfect matching, which represents the
coupons and their matching labels. We show two extra-extensions to this
variation: the heterogeneous sample size case (K-LCCP) and the partly
recovering case.",['cs.DM'],False,,,,"The Labeled Coupon Collector Problem with Random Sample Sizes and
  Partial Recovery","The Labeled Coupon Collector Problem with Random Sample Sizes and
  Partial Recovery"
neg-d2-510,2025-03-05,,2503.04014," Dexterous hand manipulation in real-world scenarios presents considerable
challenges due to its demands for both dexterity and precision. While imitation
learning approaches have thoroughly examined these challenges, they still
require a significant number of expert demonstrations and are limited by a
constrained performance upper bound. In this paper, we propose a novel and
efficient Imitation-Bootstrapped Online Reinforcement Learning (IBORL) method
tailored for robotic dexterous hand manipulation in real-world environments.
Specifically, we pretrain the policy using a limited set of expert
demonstrations and subsequently finetune this policy through direct
reinforcement learning in the real world. To address the catastrophic
forgetting issues that arise from the distribution shift between expert
demonstrations and real-world environments, we design a regularization term
that balances the exploration of novel behaviors with the preservation of the
pretrained policy. Our experiments with real-world tasks demonstrate that our
method significantly outperforms existing approaches, achieving an almost 100%
success rate and a 23% improvement in cycle time. Furthermore, by finetuning
with online reinforcement learning, our method surpasses expert demonstrations
and uncovers superior policies. Our code and empirical results are available in
https://hggforget.github.io/iborl.github.io/.",['cs.RO'],2502.01256," With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.",['cs.RO'],False,,,,"Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online
  Reinforcement Learning",Soft is Safe: Human-Robot Interaction for Soft Robots
neg-d2-511,2025-02-02,,2502.00986," One method for deriving a factorization for QCD processes is to use
successive integration over fields in the functional integral. In this
approach, we separate the fields into two categories: dynamical fields with
momenta above a relevant cutoff, and background fields with momenta below the
cutoff. The dynamical fields are then integrated out in the background of the
low-momentum background fields. This strategy works well at tree level,
allowing us to quickly derive QCD factorization formulas at leading order.
However, to extend the approach to higher loops, it is necessary to rigorously
define the functional integral over dynamical fields in an arbitrary background
field. This framework was carefully developed for the calculation of the
effective action in a background field at the two-loop level in the classic
paper by Abbott [1]. Building on this work, I specify the renormalized
background-field Lagrangian and define the notion of the quantum average of an
operator in a background field, consistent with the ``separation of scales''
scheme mentioned earlier. As examples, I discuss the evolution of the twist-2
gluon light-ray operator and the one-loop gluon propagator in a background
field near the light cone.",['hep-ph'],2503.13638," The Standard Model Effective Field Theory (SMEFT) is a widely utilized
framework for exploring new physics effects in a model-independent manner. In
previous studies, Drell-Yan collider data has emerged as a promising signature
due to its energy enhancement relative to Standard Model predictions. We
present recent works, extending this approach by also considering the ""missing
energy + jet"" signature, which can constrain related dineutrino couplings and
similarly benefits from energy enhancement. The combination of these
observables allows for constraining a broader selection of operators and helps
resolve flat directions in a global analysis. Overall, the bounds probe the
multi-TeV range, with the strongest reaching up to $10\; \text{TeV}$ for
four-fermion interactions and $7 \;\text{TeV}$ for gluonic dipole interactions.
Furthermore, we find that low energy flavor observables improve limits by up to
a factor of three for dipole operators. We also estimate sensitivities to new
physics at future hadron colliders including the $\sqrt{s} = 27 \;\text{TeV}$
HE-LHC and the $\sqrt{s} = 100 \; \text{TeV}$ FCC-hh.",['hep-ph'],False,,,,Background-field method and QCD factorization,Teaming up MET plus jet with Drell-Yan in the SMEFT
neg-d2-512,2025-02-10,,2502.06656," The recent development of powerful AI systems has highlighted the need for
robust risk management frameworks in the AI industry. Although companies have
begun to implement safety frameworks, current approaches often lack the
systematic rigor found in other high-risk industries. This paper presents a
comprehensive risk management framework for the development of frontier AI that
bridges this gap by integrating established risk management principles with
emerging AI-specific practices. The framework consists of four key components:
(1) risk identification (through literature review, open-ended red-teaming, and
risk modeling), (2) risk analysis and evaluation using quantitative metrics and
clearly defined thresholds, (3) risk treatment through mitigation measures such
as containment, deployment controls, and assurance processes, and (4) risk
governance establishing clear organizational structures and accountability.
Drawing from best practices in mature industries such as aviation or nuclear
power, while accounting for AI's unique challenges, this framework provides AI
developers with actionable guidelines for implementing robust risk management.
The paper details how each component should be implemented throughout the
life-cycle of the AI system - from planning through deployment - and emphasizes
the importance and feasibility of conducting risk management work prior to the
final training run to minimize the burden associated with it.",['cs.AI'],2501.06461," This study explores the use of artificial intelligence (AI) as a
complementary tool for grading essay-type questions in higher education,
focusing on its consistency with human grading and potential to reduce biases.
Using 70 handwritten exams from an introductory sociology course, we evaluated
generative pre-trained transformers (GPT) models' performance in transcribing
and scoring students' responses. GPT models were tested under various settings
for both transcription and grading tasks. Results show high similarity between
human and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in
accuracy. For grading, GPT demonstrated strong correlations with the human
grader scores, especially when template answers were provided. However,
discrepancies remained, highlighting GPT's role as a ""second grader"" to flag
inconsistencies for assessment reviewing rather than fully replace human
evaluation. This study contributes to the growing literature on AI in
education, demonstrating its potential to enhance fairness and efficiency in
grading essay-type questions.",['cs.AI'],False,,,,"A Frontier AI Risk Management Framework: Bridging the Gap Between
  Current AI Practices and Established Risk Management","Assessing instructor-AI cooperation for grading essay-type questions in
  an introductory sociology course"
neg-d2-513,2025-01-08,,2501.04294," We construct a $C^\infty$-flow on the four-dimensional sphere whose
nonwandering set contains an attached hyperbolic singularity yet possesses the
standard shadowing property. This gives a counterexample to a conjecture given
by Arbieto, L\'{o}pez, Rego and S\'{a}nchez (Math. Annalen 390:417-437).",['math.DS'],2501.14416," We classify the global dynamics of the five-parameter family of planar
Kolmogorov systems \begin{equation*}
  \begin{split}
  \dot{y}&=y \left( b_0+ b_1 y z + b_2 y + b_3 z\right),
  \dot{z}&=z\left( c_0 + b_1 y z + b_2 y + b_3 z\right),
  \end{split} \end{equation*} which is obtained from the Lotka-Volterra systems
of dimension three. These systems have infinitely many singular points at
inifnity. We give the topological classification of their phase portraits in
the Poincar\'e disc, so we can describe the dynamics of these systems near
infinity. We prove that these systems have 13 topologically distinct global
phase portraits.",['math.DS'],False,,,,A shadowable chain recurrent set with an attached hyperbolic singularity,"Planar Kolmogorov systems with infinitely many singular points at
  infinity"
neg-d2-514,2025-01-09,,2501.05162," This paper proposes a novel and efficient key conditional quotient filter
(KCQF) for the estimation of state in the nonlinear system which can be either
Gaussian or non-Gaussian, and either Markovian or non-Markovian. The core idea
of the proposed KCQF is that only the key measurement conditions, rather than
all measurement conditions, should be used to estimate the state. Based on key
measurement conditions, the quotient-form analytical integral expressions for
the conditional probability density function, mean, and variance of state are
derived by using the principle of probability conservation, and are calculated
by using the Monte Carlo method, which thereby constructs the KCQF. Two
nonlinear numerical examples were given to demonstrate the superior estimation
accuracy of KCQF, compared to seven existing filters.",['cs.CE'],2501.1113," This study proposes a new full-field approach for modeling grain boundary
pinning by second phase particles in two-dimensional polycrystals. These
particles are of great importance during thermomechanical treatments, as they
produce deviations from the microstructural evolution that the alloy produces
in the absence of particles. This phenomenon, well-known as Smith-Zener
pinning, is widely used by metallurgists to control the grain size during the
metal forming process of many alloys. Predictive tools are then needed to
accurately model this phenomenon. This article introduces a new methodology for
the simulation of microstructural evolutions subjected to the presence of
second phase particles. The methodology employs a Lagrangian 2D front-tracking
methodology, while the particles are modeled using discretized circular shapes
or pinning nodes. The evolution of the particles can be considered and modeled
using a constant velocity of particle shrinking. This approach has the
advantages of improving the limited description made of the phenomenon in
vertex approaches, to be usable for a wide range of second-phase particle sizes
and to improve calculation times compared to front-capturing type approaches.",['cs.CE'],False,,,,"A Key Conditional Quotient Filter for Nonlinear, non-Gaussian and
  non-Markovian System","Efficient and accurate simulation of the Smith-Zener pinning mechanism
  during grain growth using a front-tracking numerical framework"
neg-d2-515,2025-01-14,,2501.08063," System requirements related to concepts like information flow, knowledge, and
robustness cannot be judged in terms of individual system executions, but
rather require an analysis of the relationship between multiple executions.
Such requirements belong to the class of hyperproperties, which generalize
classic trace properties to properties of sets of traces. During the past
decade, a range of new specification logics has been introduced with the goal
of providing a unified theory for reasoning about hyperproperties. This paper
gives an overview on the current landscape of logics for the specification of
hyperproperties and on algorithms for satisfiability checking, model checking,
monitoring, and synthesis.",['cs.LO'],2501.15913," Stream-based runtime monitoring frameworks are safety assurance tools that
check the runtime behavior of a system against a formal specification. This
tutorial provides a hands-on introduction to RTLola, a real-time monitoring
toolkit for cyber-physical systems and networks. RTLola processes, evaluates,
and aggregates streams of input data, such as sensor readings, and provides a
real-time analysis in the form of comprehensive statistics and logical
assessments of the system's health. RTLola has been applied successfully in
monitoring autonomous systems such as unmanned aircraft. The tutorial guides
the reader through the development of a stream-based specification for an
autonomous drone observing other flying objects in its flight path. Each
tutorial section provides an intuitive introduction, highlighting useful
language features and specification patterns, and gives a more in-depth
explanation of technical details for the advanced reader. Finally, we discuss
how runtime monitors generated from RTLola specifications can be integrated
into a variety of systems and discuss different monitoring applications.",['cs.LO'],False,,,,Logics and Algorithms for Hyperproperties,A Tutorial on Stream-based Monitoring
neg-d2-516,2025-01-06,,2501.03345," The solar system planetary architecture has been proposed to be consistent
with the terrestrial and giant planets forming from material rings at ~1 au and
~5 au, respectively. Here, we show that super-Earths and mini-Neptunes may
share a similar formation pathway. In our simulations conducted with a disk
alpha-viscosity of 4e-3, super-Earths accrete from rings of rocky material in
the inner disk, growing predominantly via planetesimal accretion. Mini-Neptunes
primarily originate from rings located beyond the water snowline, forming via
pebble accretion. Our simulations broadly match the period-ratio distribution,
the intra-system size uniformity, and the planet multiplicity distribution of
exoplanets. The radius valley constrains the typical total mass available for
rocky planet formation to be less than 3-6 Earth masses. Our results predict
that planets at ~1 au in systems with close-in super-Earths and mini-Neptunes
are predominantly water-rich. Though relatively uncommon, at ~1% level, such
systems might also host rocky Earth-sized planets in the habitable zone that
underwent late giant impacts, akin to the Moon-forming event.",['astro-ph.EP'],2503.01599," Of the ~25 directly imaged planets to date, all are younger than 500Myr and
all but 6 are younger than 100Myr. Eps Ind A (HD209100, HIP108870) is a K5V
star of roughly solar age (recently derived as 3.7-5.7Gyr and
3.5$^{+0.8}_{-1.3}$Gyr). A long-term radial velocity trend as well as an
astrometric acceleration led to claims of a giant planet orbiting the nearby
star (3.6384$\pm$0.0013pc). Here we report JWST coronagraphic images that
reveal a giant exoplanet which is consistent with these radial and astrometric
measurements, but inconsistent with the previously claimed planet properties.
The new planet has temperature ~275K, and is remarkably bright at 10.65um and
15.50um. Non-detections between 3.5-5um indicate an unknown opacity source in
the atmosphere, possibly suggesting a high metallicity, high carbon-to-oxygen
ratio planet. The best-fit temperature of the planet is consistent with
theoretical thermal evolution models, which are previously untested at this
temperature range. The data indicates that this is likely the only giant planet
in the system and we therefore refer to it as ``b"", despite it having
significantly different orbital properties than the previously claimed planet
``b"".",['astro-ph.EP'],False,,,,Formation of super-Earths and mini-Neptunes from rings of planetesimals,A temperate super-Jupiter imaged with JWST in the mid-infrared
neg-d2-517,2025-01-18,,2501.10774," Model monitoring involves analyzing AI algorithms once they have been
deployed and detecting changes in their behaviour. This thesis explores machine
learning model monitoring ML before the predictions impact real-world decisions
or users. This step is characterized by one particular condition: the absence
of labelled data at test time, which makes it challenging, even often
impossible, to calculate performance metrics.
  The thesis is structured around two main themes: (i) AI alignment, measuring
if AI models behave in a manner consistent with human values and (ii)
performance monitoring, measuring if the models achieve specific accuracy goals
or desires.
  The thesis uses a common methodology that unifies all its sections. It
explores feature attribution distributions for both monitoring dimensions.
Using these feature attribution explanations, we can exploit their theoretical
properties to derive and establish certain guarantees and insights into model
monitoring.",['cs.LG'],2501.12739," Stochastic Gradient Descent (SGD) is the foundation of modern deep learning
optimization but becomes increasingly inefficient when training convolutional
neural networks (CNNs) on high-resolution data. This paper introduces
Multiscale Stochastic Gradient Descent (Multiscale-SGD), a novel optimization
approach that exploits coarse-to-fine training strategies to estimate the
gradient at a fraction of the cost, improving the computational efficiency of
SGD type methods while preserving model accuracy. We derive theoretical
criteria for Multiscale-SGD to be effective, and show that while standard
convolutions can be used, they can be suboptimal for noisy data. This leads us
to introduce a new class of learnable, scale-independent Mesh-Free Convolutions
(MFCs) that ensure consistent gradient behavior across resolutions, making them
well-suited for multiscale training. Through extensive empirical validation, we
demonstrate that in practice, (i) our Multiscale-SGD approach can be used to
train various architectures for a variety of tasks, and (ii) when the noise is
not significant, standard convolutions benefit from our multiscale training
framework. Our results establish a new paradigm for the efficient training of
deep networks, enabling practical scalability in high-resolution and multiscale
learning tasks.",['cs.LG'],False,,,,"Model Monitoring in the Absence of Labeled Data via Feature Attributions
  Distributions","Multiscale Stochastic Gradient Descent: Efficiently Training
  Convolutional Neural Networks"
neg-d2-518,2025-03-17,,2503.12915," For the composite multi-objective optimization problem composed of two
nonsmooth terms, a smoothing method is used to overcome the nonsmoothness of
the objective function, making the objective function contain at most one
nonsmooth term. Then, inspired by the design idea of the aforementioned
backtracking strategy, an update rule is proposed by constructing a
relationship between an estimation sequence of the Lipschitz constant and a
smoothing factor, which results in a backtracking strategy suitable for this
problem, allowing the estimation sequence to be updated in a non-increasing
manner. On this basis, a smoothing accelerated proximal gradient algorithm
based on the backtracking strategy is further proposed. Under appropriate
conditions, it is proven that all accumulation points of the sequence generated
by this algorithm are weak Pareto optimal solutions. Additionally, the
convergence rate of the algorithm under different parameters is established
using a utility function. Numerical experiments show that, compared with the
subgradient algorithm, the proposed algorithm demonstrates significant
advantages in terms of runtime, iteration count, and function evaluations.",['math.OC'],2503.09179," This paper concerns the problem of reachability of a given state for a
multiagent control system in $\mathbb{R}^d$. In such a system, at every time
each agent can choose his/her velocity which depends both on his/her position
and on the position of the whole crowd of agents (modeled by a probability
measure on $ \mathbb{R}^d$). The main contribution of the paper is to study the
above reachability problem with a given rate of attainability through a
Lyapunov method adapted to the Wasserstein space of probability measures. As a
byproduct we obtain a new comparison result for viscosity solutions of Hamilton
Jacobi equations in the Wasserstein space.",['math.OC'],False,,,,"Smoothing Accelerated Proximal Gradient Method with Backtracking for
  Nonsmooth Multiobjective Optimization",Reachability for multiagent control systems via Lyapunov functions
neg-d2-519,2025-03-16,,2503.12539," 3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.",['cs.CV'],2501.1687," Understanding emotional signals in older adults is crucial for designing
virtual assistants that support their well-being. However, existing affective
computing models often face significant limitations: (1) limited availability
of datasets representing older adults, especially in non-English-speaking
populations, and (2) poor generalization of models trained on younger or
homogeneous demographics. To address these gaps, this study evaluates
state-of-the-art affective computing models -- including facial expression
recognition, text sentiment analysis, and smile detection -- using videos of
older adults interacting with either a person or a virtual avatar. As part of
this effort, we introduce a novel dataset featuring Spanish-speaking older
adults engaged in human-to-human video interviews. Through three comprehensive
analyses, we investigate (1) the alignment between human-annotated labels and
automatic model outputs, (2) the relationships between model outputs across
different modalities, and (3) individual variations in emotional signals. Using
both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover
limited agreement between human annotations and model predictions, weak
consistency across modalities, and significant variability among individuals.
These findings highlight the shortcomings of generalized emotion perception
models and emphasize the need of incorporating personal variability and
cultural nuances into future systems.",['cs.CV'],False,,,,"BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature
  Analysis","Experimenting with Affective Computing Models in Video Interviews with
  Spanish-speaking Older Adults"
neg-d2-520,2025-02-04,,2502.03489," In quantum mechanics, the time evolution of particles is given by the
Schr\""odinger equation. It is valid in a nonrelativistic regime where the
interactions with the particle can be modelled by a potential and quantised
fields are not required. This has been verified in countless experiments when
the interaction is of electromagnetic origin, but also corrections due to the
quantised field are readily observed. When the interaction is due to gravity,
then one cannot expect to see effects of the quantised field in
current-technology Earth-bound experiments. However, this does not yet
guarantee that in the accessible regime, the time evolution is accurately given
by the Schr\""odinger equation. Here we propose to measure the effects of an
asymmetric mass configuration on a quantum particle in an interferometer. For
this setup we show that with parameters within experimental reach, one can be
sensitive to possible deviations from the Schr\""odinger equation, beyond the
already verified lowest-order regime. Performing this experiment will hence
directly test the nonclassical behaviour of a quantum particle in the
gravitational field.",['quant-ph'],2501.03521," In this study, we propose a new method for constrained combinatorial
optimization using variational quantum circuits. Quantum computers are
considered to have the potential to solve large combinatorial optimization
problems faster than classical computers. Variational quantum algorithms, such
as Variational Quantum Eigensolver (VQE), have been studied extensively because
they are expected to work on noisy intermediate scale devices. Unfortunately,
many optimization problems have constraints, which induces infeasible solutions
during VQE process. Recently, several methods for efficiently solving
constrained combinatorial optimization problems have been proposed by designing
a quantum circuit so as to output only the states that satisfy the constraints.
However, the types of available constraints are still limited. Therefore, we
have started to develop variational quantum circuits that can handle a wider
range of constraints. The proposed method utilizes a forwarding operation that
maps from feasible states for subproblems to those for larger subproblems. As
long as appropriate forwarding operations can be defined, iteration of this
process can inductively construct variational circuits outputting feasible
states even in the case of multiple and complex constraints. In this paper, the
proposed method was applied to facility location problem and was found to
increase the probability for measuring feasible solutions or optimal solutions.
In addition, the cost of the obtained circuit was comparable to that of
conventional variational circuits.",['quant-ph'],False,,,,"Probing the nonclassical dynamics of a quantum particle in a
  gravitational field","Inductive Construction of Variational Quantum Circuit for Constrained
  Combinatorial Optimization"
neg-d2-521,2025-01-21,,2501.11995," 3D printing has progressed significantly, allowing objects to be produced
using a wide variety of materials. Recent advances have employed focused
ultrasound in 3D printing, to allow printing inside acoustically transparent
materials. Here we introduce a Selective Ultrasonic Melting (SUM) method for 3D
printing of poly ({\epsilon}-caprolactone) (PCL) powder mixed with water. The
printing was done by mechanically moving a focused ultrasound transducer. The
microstructure and porosity of the prints were analyzed with micro-computed
tomography ({\mu}CT). The open porosity of the printed samples was determined
using the water intrusion method and by passing fluorescent microspheres
through the structure. The cytocompatibility of the printed structures was
confirmed by seeding NIH-3T3 fibroblast cells on the scaffolds, followed by
analysis using live/dead fluorescent assay. and visualization using scanning
electron microscopy (SEM). We demonstrated that SUM is a viable technique to
print structures with active control of their porosity This method provides an
alternative to methods such as fused deposition modelling (FDM) and material
jetting.",['physics.app-ph'],2501.09812," The Benguela Upwelling System (BUS), off the south-western African coast, is
one of the four major eastern boundary upwelling ecosystems in the oceans.
However, despite its very interesting characteristics, this area has been
almost overlooked in the field of environmental radioactivity. In this work, it
has been carried out for the first time the combined study of $^{236}$U and
$^{237}$Np in the coast of Namibia within the northern BUS. Surface seawater
exhibited similar $^{236}$U and $^{237}$Np concentrations, ranging from
$3.9\times10^6$ to $5.6\times10^6$ atoms/kg and from $4.6\times10^6$ to
$8.5\times10^6$ atoms/kg, respectively. The observed inventories in a water
column from the continental margin, of $(2.10 \pm 0.11)\times10^12$ atoms
m$^{-2}$ for $^{236}$U and $(3.48 \pm 0.13)\times10^{12}$ atoms/m$^{-2}$ for
$^{237}$Np, were in agreement with the global fallout (GF) source term in the
Southern Hemisphere, which was recognized as the main source of actinides to
the region. A pattern was observed in the surface samples, with $^{237}$Np
concentrations that decreased by 25-30% when moving from inshore to offshore
stations, but such an effect could not be clearly discerned in the case of
$^{236}$U within the data uncertainties. An explanation based on the larger
particle reactivity of GF $^{237}$Np compared to GF $^{236}$U was proposed.
Such an effect would have been important at the studied site due to the enhance
presence of particles in the continental shelf triggered by the upwelling
phenomenon. A value of $1.77 \pm 0.20$ was obtained for the
$^{237}$Np/$^{236}$U atom ratio for the GF source term in the marine
environment.",['physics.app-ph'],False,,,,"Fabrication of Poly ({\epsilon}-Caprolactone) 3D scaffolds with
  controllable porosity using ultrasound","Presence of $^{236}$U and $^{237}$Np in a marine ecosystem: the northern
  Benguela Upwelling System, a case study"
neg-d2-522,2025-03-06,,2503.04585," First formulated by Sir Isaac Newton in his work ""Philosophiae Naturalis
Principia Mathematica"", the concept of the Three-Body Problem was put forth as
a study of the motion of the three celestial bodies within the Earth-Sun-Moon
system. In a generalized definition, it seeks to predict the motion for an
isolated system composed of three point masses freely interacting under
Newton's law of universal attraction. This proves to be analogous to a
multitude of interactions between celestial bodies, and thus, the problem finds
applicability within the studies of celestial mechanics. Despite numerous
attempts by renowned physicists to solve it throughout the last three
centuries, no general closed-form solutions have been reached due to its
inherently chaotic nature for most initial conditions. Current state-of-the-art
solutions are based on two approaches, either numerical high-precision
integration or machine learning-based. Notwithstanding the breakthroughs of
neural networks, these present a significant limitation, which is their
ignorance of any prior knowledge of the chaotic systems presented. Thus, in
this work, we propose a novel method that utilizes Physics-Informed Neural
Networks (PINNs). These deep neural networks are able to incorporate any prior
system knowledge expressible as an Ordinary Differential Equation (ODE) into
their learning processes as a regularizing agent. Our findings showcase that
PINNs surpass current state-of-the-art machine learning methods with comparable
prediction quality. Despite a better prediction quality, the usability of
numerical integrators suffers due to their prohibitively high computational
cost. These findings confirm that PINNs are both effective and time-efficient
open-form solvers of the Three-Body Problem that capitalize on the extensive
knowledge we hold of classical mechanics.",['cs.LG'],2503.00485," Graph spectra are an important class of structural features on graphs that
have shown promising results in enhancing Graph Neural Networks (GNNs). Despite
their widespread practical use, the theoretical understanding of the power of
spectral invariants -- particularly their contribution to GNNs -- remains
incomplete. In this paper, we address this fundamental question through the
lens of homomorphism expressivity, providing a comprehensive and quantitative
analysis of the expressive power of spectral invariants. Specifically, we prove
that spectral invariant GNNs can homomorphism-count exactly a class of specific
tree-like graphs which we refer to as parallel trees. We highlight the
significance of this result in various contexts, including establishing a
quantitative expressiveness hierarchy across different architectural variants,
offering insights into the impact of GNN depth, and understanding the subgraph
counting capabilities of spectral invariant GNNs. In particular, our results
significantly extend Arvind et al. (2024) and settle their open questions.
Finally, we generalize our analysis to higher-order GNNs and answer an open
question raised by Zhang et al. (2024).",['cs.LG'],False,,,,"Advancing Solutions for the Three-Body Problem Through Physics-Informed
  Neural Networks",Homomorphism Expressivity of Spectral Invariant Graph Neural Networks
neg-d2-523,2025-02-26,,2502.19639," The origin of the slow solar wind is not well understood, unlike the fast
solar wind which originates from coronal holes. In-situ elemental abundances of
the slow solar wind suggest that it originates from initially closed field
lines that become open. Coronal hole boundary regions are a potential source of
slow solar wind as there open field lines interact with the closed loops
through interchange reconnection. Our primary aim is to quantify the role of
interchange reconnection at the boundaries of coronal holes. To this end, we
have measured the relative abundances of different elements at these
boundaries. Reconnection is expected to modulate the relative abundances
through the first ionization potential (FIP) effect. For our analysis we used
spectroscopic data from the extreme ultraviolet imaging spectrometer (EIS) on
board Hinode. To account for the temperature structure of the observed region
we computed the differential emission measure (DEM). Using the DEM we were able
to infer the ratio between coronal and photospheric abundances, known as the
FIP bias. By examining the variation of the FIP bias moving from the coronal
hole to the quiet Sun, we have been able to constrain models of interchange
reconnection. The FIP bias variation in the boundary region around the coronal
hole has an approximate width of 30-50 Mm, comparable to the size of
supergranules. This boundary region is also a source of open flux into
interplanetary space. We find that there is an additional ~30% open flux that
originates from this boundary region.",['astro-ph.SR'],2502.19256," Observations of stars other than the Sun are sensitive to oscillations of
only low degree. Many are high-order acoustic modes. Acoustic frequencies of
main-sequence stars, for example, satisfy a well-known pattern, which some
astronomers have adopted even for red-giant stars. That is not wise, because
the internal structures of these stars can be quite different from those on the
Main Sequence, which is populated by stars whose structure is regular. Here I
report on pondering this matter, and point out two fundamental deviations from
the commonly adopted relation. There are aspects of the regular relation that
are connected in a simple way to gross properties of the star, such as the
dependence of the eigenfrequencies on the linear combination
$n+\textstyle{\frac {1}{2}}l$ of the order $n$ and degree $l$, which is
characteristic of a regular spherical acoustic cavity. That is not a feature of
red-giant frequencies, because, as experienced by the waves, red-giant stars
appear to have (phantom) singular centres, which substantially modify the
propagation of waves. That requires a generalization of the eigenfrequency
relation, which I present here. When fitted to the observed frequencies of the
Sun, the outcome is consistent with the Sun being round, with no singularity in
the core. That is hardly novel, but at least it provides some assurance that
our understanding of stellar acoustic wave dynamics is on a sound footing.",['astro-ph.SR'],False,,,,"Elemental Abundances at Coronal Hole Boundaries as a Means to
  Investigate Interchange Reconnection and the Solar Wind",Some musings on erythrogigantoacoustics
neg-d2-524,2025-02-12,,2502.08164," This work explores dynamical models of the Milky Way (MW) by analyzing a
sample of 86,109 K giant stars selected through cross-matching the LAMOST DR8
and Gaia EDR3 surveys. Our earlier torus models in Wang et al. (2017) did not
include Gaia data, making them incompatible with the new proper motion
distributions of samples. Here, we refine the construction of action-based,
self-consistent models to constrain the three-dimensional velocity distribution
of K giants over a larger parameter space, drawing on a series of existing MW
models. This approach produces several new MW models. Our best-fit model for
the local kinematics near the Sun indicates a MW virial mass of 1.35 $\times
10^{12} M_\odot$, a local stellar density of 0.0696 $\rm M_\odot pc^{-3}$, and
a local dark matter density of 0.0115 $\rm M_\odot pc^{-3}$. Our main
conclusion supports a thicker and more extended thick disk, alongside a cooler
thin disk, compared to the best-fitting model in Wang et al. (2017). Near the
Sun, our model aligns well with observations, but is less satisfactory at
distances far from the Galactic center, perhaps implying unidentified
structures. Further high-precision observations will be critical for
understanding the dynamics in these outer Galactic regions, and will require a
more realistic model.",['astro-ph.GA'],2501.04089," We present deep optical observations of the stellar halo of NGC 300, an
LMC-mass galaxy, acquired with the DEEP sub-component of the DECam Local Volume
Exploration survey (DELVE) using the 4 m Blanco Telescope. Our resolved star
analysis reveals a large, low surface brightness stellar stream
($M_{V}\sim-8.5$; [Fe/H] $= -1.4\pm0.15$) extending more than 40 kpc north from
the galaxy's center. We also find other halo structures, including potentially
an additional stream wrap to the south, which may be associated with the main
stream. The morphology and derived low metallicities of the streams and shells
discovered surrounding NGC 300 are highly suggestive of a past accretion event.
Assuming a single progenitor, the accreted system is approximately Fornax-like
in luminosity, with an inferred mass ratio to NGC 300 of approximately $1:15$.
We also present the discovery of a metal-poor globular cluster
($R_{\rm{proj}}=23.3$~kpc; $M_{V}=-8.99\pm0.16$; [Fe/H] $\approx-1.6\pm0.6$) in
the halo of NGC 300, the furthest identified globular cluster associated with
NGC 300. The stellar structures around NGC 300 represent the richest features
observed in a Magellanic Cloud analog to date, strongly supporting the idea
that accretion and subsequent disruption is an important mechanism in the
assembly of dwarf galaxy stellar halos.",['astro-ph.GA'],False,,,,"Dynamical Models of the Milky Way in Action Space with LAMOST DR8 and
  GAIA EDR3","Streams, Shells, and Substructures in the Accretion-Built Stellar Halo
  of NGC 300"
neg-d2-525,2025-02-18,,2502.12935," In this review, we examine computational models that explore the role of
neural oscillations in speech perception, spanning from early auditory
processing to higher cognitive stages. We focus on models that use rhythmic
brain activities, such as gamma, theta, and delta oscillations, to encode
phonemes, segment speech into syllables and words, and integrate linguistic
elements to infer meaning. We analyze the mechanisms underlying these models,
their biological plausibility, and their potential applications in processing
and understanding speech in real time, a computational feature that is achieved
by the human brain but not yet implemented in speech recognition models.
Real-time processing enables dynamic adaptation to incoming speech, allowing
systems to handle the rapid and continuous flow of auditory information
required for effective communication, interactive applications, and accurate
speech recognition in a variety of real-world settings. While significant
progress has been made in modeling the neural basis of speech perception,
challenges remain, particularly in accounting for the complexity of semantic
processing and the integration of contextual influences. Moreover, the high
computational demands of biologically realistic models pose practical
difficulties for their implementation and analysis. Despite these limitations,
these models provide valuable insights into the neural mechanisms of speech
perception. We conclude by identifying current limitations, proposing future
research directions, and suggesting how these models can be further developed
to achieve a more comprehensive understanding of speech processing in the human
brain.",['q-bio.NC'],2502.13661," Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation
(DBS) in DYT1 dystonia and disease progression under DBS was studied.
Twenty-six patients of this open-label study were divided into two groups: (A)
with single bilateral GPi lead, (B) with a second bilateral GPi lead implanted
owning to subsequent worsening of symptomatology. Dystonia was assessed with
the Burke Scale. Appearance of new symptoms and distribution according to body
region were recorded. In the whole cohort, significant decreases in motor and
disability subscores (P < 0.0001) were observed at 1 year and maintained up to
10 years. Group B showed worsening of the symptoms. At 1 year, there were no
significant differences between Groups A (without subsequent worsening) and B;
at 5 years, a significant difference was found for motor and disability scores.
Within Group B, four patients exhibited additional improvement after the second
DBS surgery. In the 26 patients, significant difference (P = 0.001) was found
between the number of body regions affected by dystonia preoperatively and over
the whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10
years (two patients). New symptoms appear with long-term follow-up and may
improve with additional leads in a subgroup of patients.",['q-bio.NC'],False,,,,Neuro-oscillatory models of cortical speech processing,"Long-term follow-up of DYT1 dystonia patients treated by deep brain
  stimulation: an open-label study"
neg-d2-526,2025-03-10,,2503.0788," High-resolution helioseismology observations with the Helioseismic and
Magnetic Imager (HMI) onboard Solar Dynamics Observatory (SDO) provide a unique
three-dimensional view of the solar interior structure and dynamics, revealing
a tremendous complexity of the physical processes inside the Sun. We present an
overview of the results of the HMI helioseismology program and discuss their
implications for modern theoretical models and simulations of the solar
interior.",['astro-ph.SR'],2502.19256," Observations of stars other than the Sun are sensitive to oscillations of
only low degree. Many are high-order acoustic modes. Acoustic frequencies of
main-sequence stars, for example, satisfy a well-known pattern, which some
astronomers have adopted even for red-giant stars. That is not wise, because
the internal structures of these stars can be quite different from those on the
Main Sequence, which is populated by stars whose structure is regular. Here I
report on pondering this matter, and point out two fundamental deviations from
the commonly adopted relation. There are aspects of the regular relation that
are connected in a simple way to gross properties of the star, such as the
dependence of the eigenfrequencies on the linear combination
$n+\textstyle{\frac {1}{2}}l$ of the order $n$ and degree $l$, which is
characteristic of a regular spherical acoustic cavity. That is not a feature of
red-giant frequencies, because, as experienced by the waves, red-giant stars
appear to have (phantom) singular centres, which substantially modify the
propagation of waves. That requires a generalization of the eigenfrequency
relation, which I present here. When fitted to the observed frequencies of the
Sun, the outcome is consistent with the Sun being round, with no singularity in
the core. That is hardly novel, but at least it provides some assurance that
our understanding of stellar acoustic wave dynamics is on a sound footing.",['astro-ph.SR'],False,,,,"Structure and Dynamics of the Sun's Interior Revealed by Helioseismic
  and Magnetic Imager",Some musings on erythrogigantoacoustics
neg-d2-527,2025-01-13,,2501.07216," Soft pneumatic fingers are of great research interest. However, their
significant potential is limited as most of them can generate only one motion,
mostly bending. The conventional design of soft fingers does not allow them to
switch to another motion mode. In this paper, we developed a novel multi-modal
and single-actuated soft finger where its motion mode is switched by changing
the finger's temperature. Our soft finger is capable of switching between three
distinctive motion modes: bending, twisting, and extension-in approximately
five seconds. We carried out a detailed experimental study of the soft finger
and evaluated its repeatability and range of motion. It exhibited repeatability
of around one millimeter and a fifty percent larger range of motion than a
standard bending actuator. We developed an analytical model for a
fiber-reinforced soft actuator for twisting motion. This helped us relate the
input pressure to the output twist radius of the twisting motion. This model
was validated by experimental verification. Further, a soft robotic gripper
with multiple grasp modes was developed using three actuators. This gripper can
adapt to and grasp objects of a large range of size, shape, and stiffness. We
showcased its grasping capabilities by successfully grasping a small berry, a
large roll, and a delicate tofu cube.",['cs.RO'],2501.07295," This paper introduces GestLLM, an advanced system for human-robot interaction
that enables intuitive robot control through hand gestures. Unlike conventional
systems, which rely on a limited set of predefined gestures, GestLLM leverages
large language models and feature extraction via MediaPipe to interpret a
diverse range of gestures. This integration addresses key limitations in
existing systems, such as restricted gesture flexibility and the inability to
recognize complex or unconventional gestures commonly used in human
communication.
  By combining state-of-the-art feature extraction and language model
capabilities, GestLLM achieves performance comparable to leading
vision-language models while supporting gestures underrepresented in
traditional datasets. For example, this includes gestures from popular culture,
such as the ``Vulcan salute"" from Star Trek, without any additional
pretraining, prompt engineering, etc. This flexibility enhances the naturalness
and inclusivity of robot control, making interactions more intuitive and
user-friendly.
  GestLLM provides a significant step forward in gesture-based interaction,
enabling robots to understand and respond to a wide variety of hand gestures
effectively. This paper outlines its design, implementation, and evaluation,
demonstrating its potential applications in advanced human-robot collaboration,
assistive robotics, and interactive entertainment.",['cs.RO'],False,,,,Temperature Driven Multi-modal/Single-actuated Soft Finger,"GestLLM: Advanced Hand Gesture Interpretation via Large Language Models
  for Human-Robot Interaction"
neg-d2-528,2025-02-07,,2502.05108," [Background] Emotional Intelligence (EI) can impact Software Engineering (SE)
outcomes through improved team communication, conflict resolution, and stress
management. SE workers face increasing pressure to develop both technical and
interpersonal skills, as modern software development emphasizes collaborative
work and complex team interactions. Despite EI's documented importance in
professional practice, SE education continues to prioritize technical knowledge
over emotional and social competencies. [Objective] This paper analyzes SE
students' self-perceptions of their EI after a two-month cooperative learning
project, using Mayer and Salovey's four-ability model to examine how students
handle emotions in collaborative development. [Method] We conducted a case
study with 29 SE students organized into four squads within a project-based
learning course, collecting data through questionnaires and focus groups that
included brainwriting and sharing circles, then analyzing the data using
descriptive statistics and open coding. [Results] Students demonstrated
stronger abilities in managing their own emotions compared to interpreting
others' emotional states. Despite limited formal EI training, they developed
informal strategies for emotional management, including structured planning and
peer support networks, which they connected to improved productivity and
conflict resolution. [Conclusion] This study shows how SE students perceive EI
in a collaborative learning context and provides evidence-based insights into
the important role of emotional competencies in SE education.",['cs.SE'],2501.1938," Background: Software engineering requires both technical skills and creative
problem-solving. Blind and low-vision software professionals (BLVSPs) encounter
numerous workplace challenges, including inaccessible tools and collaboration
hurdles with sighted colleagues. Objective: This study explores the innovative
strategies employed by BLVSPs to overcome these accessibility barriers,
focusing on their custom solutions and the importance of supportive
communities. Methodology: We conducted semi-structured interviews with 30
BLVSPs and used reflexive thematic analysis to identify key themes. Results:
Findings reveal that BLVSPs are motivated to develop creative and adaptive
solutions, highlighting the vital role of collaborative communities in
fostering shared problem-solving. Conclusion: For BLVSPs, creative
problem-solving is essential for navigating inaccessible work environments, in
contrast to sighted peers, who pursue optimization. This study enhances
understanding of how BLVSPs navigate accessibility challenges through
innovation.",['cs.SE'],False,,,,"Towards Emotionally Intelligent Software Engineers: Understanding
  Students' Self-Perceptions After a Cooperative Learning Experience","Creative Problem-Solving: A Study with Blind and Low Vision Software
  Professionals"
neg-d2-529,2025-02-20,,2502.15112," Categorical response data are ubiquitous in complex survey applications, yet
few methods model the dependence across different outcome categories when the
response is ordinal. Likewise, few methods exist for the common combination of
a longitudinal design and categorical data. By modeling individual survey
responses at the unit-level, it is possible to capture both ordering
information in ordinal responses and any longitudinal correlation. However,
accounting for a complex survey design becomes more challenging in the
unit-level setting. We propose a Bayesian hierarchical, unit-level, model-based
approach for categorical data that is able to capture ordering among response
categories, can incorporate longitudinal dependence, and accounts for the
survey design. To handle computational scalability, we develop efficient Gibbs
samplers with appropriate data augmentation as well as variational Bayes
algorithms. Using public-use microdata from the Household Pulse Survey, we
provide an analysis of an ordinal response that asks about the frequency of
anxiety symptoms at the beginning of the COVID-19 pandemic. We compare both
design-based and model-based estimators and demonstrate superior performance
for the proposed approaches.",['stat.ME'],2502.02148," As new Model-X knockoff construction techniques are developed, primarily
concerned with determining the correct conditional distribution from which to
sample, we focus less on deriving the correct multivariate distribution and
instead ask if ``perfect'' knockoffs can be constructed using linear algebra.
Using mean absolute correlation between knockoffs and features as a measure of
quality, we do produce knockoffs that are pseudo-perfect, however, the
optimization algorithm is computationally very expensive. We outline a series
of methods to significantly reduce the computation time of the algorithm.",['stat.ME'],False,,,,"Bayesian Unit-level Modeling of Categorical Survey Data with a
  Longitudinal Design",Can linear algebra create perfect knockoffs?
neg-d2-530,2025-01-15,,2501.0902," Compute Express Link (CXL) is widely-supported interconnect standard that
promises to enable memory disaggregation in data centers. CXL allows for memory
pooling, which can be used to create a shared memory space across multiple
servers. However, CXL does not specify how to actually build a memory pool.
Existing proposals for CXL memory pools are expensive, as they require CXL
switches or large multi-headed devices. In this paper, we propose a new design
for CXL memory pools that is cost-effective. We call these designs Octopus
topologies. Our design uses small CXL devices that can be made cheaply and
offer fast access latencies. Specifically, we propose asymmetric CXL topologies
where hosts connect to different sets of CXL devices. This enables pooling and
sharing memory across multiple hosts even as each individual CXL device is only
connected to a small number of hosts. Importantly, this uses hardware that is
readily available today. We also show the trade-off in terms of CXL pod size
and cost overhead per host. Octopus improves the Pareto frontier defined by
prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost
per host.",['cs.AR'],2501.0733," ML and HPC applications increasingly combine dense and sparse memory access
computations to maximize storage efficiency. However, existing CPUs and GPUs
struggle to flexibly handle these heterogeneous workloads with consistently
high compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,
dual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical
interconnect and in-core streaming units (SUs) designed to accelerate dense and
sparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets
in 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense
linear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On
stencil codes, Occamy reaches an FPU utilization of 83% and a
technology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading
state-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On
sparse-dense linear algebra (LA), it achieves 42% FPU utilization and a
normalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x
and 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up
to 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.
Finally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and
graph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available
under a permissive open-source license.",['cs.AR'],False,,,,Octopus: Scalable Low-Cost CXL Memory Pooling,"Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System
  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET"
neg-d2-531,2025-03-15,,2503.12268," In this note, we show that every Noetherian graded ring with an affine degree
zero part is affine. As a result, a Noetherian graded Hopf algebra whose degree
zero component is a commutative or a cocommutative Hopf subalgebra is affine.
Moreover, we show that the braided Hopf algebra of a Noetherian graded Hopf
algebra is affine.",['math.RA'],2503.06715," We develop a technique to show the Morita equivalence of certain subrings of
a ring with local units. We then apply this technique to develop conditions
that are sufficient to show the Morita equivalence of subalgebras induced by
partial subactions on generalized Boolean algebras and, subsequently, strongly
$E^{\ast}$-unitary inverse subsemigroups. As an application, we prove that the
Leavitt path algebra of a graph is Morita equivalent to the Leavitt path
algebra of certain subgraphs and use this to calculate the Morita equivalence
class of some Leavitt path algebras. Finally, as the main application, we prove
a desingularization result for labelled Leavitt path algebras.",['math.RA'],False,,,,"Affineness on Noetherian graded rings, algebras and Hopf algebras","Morita Equivalence of Subrings with Applications to Inverse Semigroup
  Algebras"
neg-d2-532,2025-03-07,,2503.0555," Google Scholar is a vital tool for engineering scholars, enabling efficient
literature searches and facilitating academic dissemination. Elsevier, as one
of the largest publishers of engineering journals, produces essential research
that scholars rely on. The pre-proof policy, adopted by Elsevier for certain
journals, allows articles to be published online in their accepted draft form
before final proofreading and formatting. However, this study empirically
demonstrates that the pre-proof publication policy hinders comprehensive
indexing by Google Scholar. Articles published under this policy are only
partially indexed, often limited to titles and abstracts, while crucial
sections such as introductions, methods, results, discussions, conclusions,
appendices, and data availability statements remain unsearchable. This problem
has persisted for years, resulting in reduced visibility and accessibility of
certain Elsevier articles. To improve academic dissemination, both Elsevier and
Google Scholar must address this problem by modifying publishing policies or
enhancing indexing practices. Additionally, this paper explores strategies that
authors can use to mitigate the issue and ensure broader discoverability of
their research.",['cs.DL'],2503.0555," Google Scholar is a vital tool for engineering scholars, enabling efficient
literature searches and facilitating academic dissemination. Elsevier, as one
of the largest publishers of engineering journals, produces essential research
that scholars rely on. The pre-proof policy, adopted by Elsevier for certain
journals, allows articles to be published online in their accepted draft form
before final proofreading and formatting. However, this study empirically
demonstrates that the pre-proof publication policy hinders comprehensive
indexing by Google Scholar. Articles published under this policy are only
partially indexed, often limited to titles and abstracts, while crucial
sections such as introductions, methods, results, discussions, conclusions,
appendices, and data availability statements remain unsearchable. This problem
has persisted for years, resulting in reduced visibility and accessibility of
certain Elsevier articles. To improve academic dissemination, both Elsevier and
Google Scholar must address this problem by modifying publishing policies or
enhancing indexing practices. Additionally, this paper explores strategies that
authors can use to mitigate the issue and ensure broader discoverability of
their research.",['cs.DL'],False,,,,Elsevier's Pre-proof Policy Blocks Google Scholar Indexing,Elsevier's Pre-proof Policy Blocks Google Scholar Indexing
neg-d2-533,2025-02-03,,2502.0115," This study analyzes the multi-wavelength flaring activity of the distant flat
spectrum radio quasar (FSRQ) OP 313 (z=0.997) during November 2023 to March
2024, using data from Fermi-Large Area Telescope, Swift X-ray Telescope, and
Ultraviolet and Optical Telescope. The analysis highlights two significant very
high energy(VHE) detection epochs and GeV gamma-ray flaring episodes, providing
insight into jet emission processes and radiative mechanisms. Key findings
include broadband spectral energy distribution (SED) evolution, including
enigmatic X-ray spectral changes. Modeling of the multi-wavelength SED with a
one-zone leptonic radiative processes attributes the emissions to synchrotron
radiation, Synchrotron Self-Compton (SSC), and External Compton (EC)
mechanisms, with torus photons as the primary source for EC processes. The
results suggest that the gamma-ray emitting region lies outside the broad-line
region but within the dusty torus. Furthermore, we find that the radiated power
is significantly smaller than the total jet power, suggesting that most of the
bulk energy remains within the jet even after passing through the blazar
emission zone. These findings advance our understanding of particle
acceleration, jet dynamics, and photon field interactions in FSRQs.",['astro-ph.HE'],2503.04306," We present multiband observations and analysis of EP240801a, a low-energy,
extremely soft gamma-ray burst (GRB) discovered on August 1, 2024 by the
Einstein Probe (EP) satellite, with a weak contemporaneous signal also detected
by Fermi/GBM. Optical spectroscopy of the afterglow, obtained by GTC and Keck,
identified the redshift of $z = 1.6734$. EP240801a exhibits a burst duration of
148 s in X-rays and 22.3 s in gamma-rays, with X-rays leading by 80.61 s.
Spectral lag analysis indicates the gamma-ray signal arrived 8.3 s earlier than
the X-rays. Joint spectral fitting of EP/WXT and Fermi/GBM data yields an
isotropic energy $E_{\gamma,\rm{iso}} = (5.57^{+0.54}_{-0.50})\times
10^{51}\,\rm{erg}$, a peak energy $E_{\rm{peak}} =
14.90^{+7.08}_{-4.71}\,\rm{keV}$, a fluence ratio $\rm
S(25-50\,\rm{keV})/S(50-100\,\rm{keV}) = 1.67^{+0.74}_{-0.46}$, classifying
EP240801a as an X-ray flash (XRF). The host-galaxy continuum spectrum, inferred
using Prospector, was used to correct its contribution for the observed
outburst optical data. Unusual early $R$-band behavior and EP/FXT observations
suggest multiple components in the afterglow. Three models are considered:
two-component jet model, forward-reverse shock model and forward-shock model
with energy injection. Both three provide reasonable explanations. The
two-component jet model and the energy injection model imply a relatively small
initial energy and velocity of the jet in the line of sight, while the
forward-reverse shock model remains typical. Under the two-component jet model,
EP240801a may resemble GRB 221009A (BOAT) if the bright narrow beam is viewed
on-axis. Therefore, EP240801a can be interpreted as an off-beam (narrow) jet or
an intrinsically weak GRB jet. Our findings provide crucial clues for
uncovering the origin of XRFs.",['astro-ph.HE'],False,,,,"Deciphering the Multi-Wavelength Flares of the Most Distant Very
  High-Energy (>100 GeV) Gamma-ray Emitting Blazar","EP240801a/XRF 240801B: An X-ray Flash Detected by the Einstein Probe and
  Implications of its Multiband Afterglow"
neg-d2-534,2025-03-09,,2503.06715," We develop a technique to show the Morita equivalence of certain subrings of
a ring with local units. We then apply this technique to develop conditions
that are sufficient to show the Morita equivalence of subalgebras induced by
partial subactions on generalized Boolean algebras and, subsequently, strongly
$E^{\ast}$-unitary inverse subsemigroups. As an application, we prove that the
Leavitt path algebra of a graph is Morita equivalent to the Leavitt path
algebra of certain subgraphs and use this to calculate the Morita equivalence
class of some Leavitt path algebras. Finally, as the main application, we prove
a desingularization result for labelled Leavitt path algebras.",['math.RA'],2503.12268," In this note, we show that every Noetherian graded ring with an affine degree
zero part is affine. As a result, a Noetherian graded Hopf algebra whose degree
zero component is a commutative or a cocommutative Hopf subalgebra is affine.
Moreover, we show that the braided Hopf algebra of a Noetherian graded Hopf
algebra is affine.",['math.RA'],False,,,,"Morita Equivalence of Subrings with Applications to Inverse Semigroup
  Algebras","Affineness on Noetherian graded rings, algebras and Hopf algebras"
neg-d2-535,2025-02-19,,2502.13483," This paper presents PyJobShop, an open-source Python library for solving
scheduling problems with constraint programming. PyJobShop provides an
easy-to-use modeling interface that supports a wide variety of scheduling
problems, including well-known variants such as the flexible job shop problem
and the resource-constrained project scheduling problem. PyJobShop integrates
two state-of-the-art constraint programming solvers: Google's OR-Tools CP-SAT
and IBM ILOG's CP Optimizer. We leverage PyJobShop to conduct large-scale
numerical experiments on more than 9,000 benchmark instances from the machine
scheduling and project scheduling literature, comparing the performance of
OR-Tools and CP Optimizer. While CP Optimizer performs better on permutation
scheduling and large-scale problems, OR-Tools is highly competitive on job shop
scheduling and project scheduling problems--while also being fully open-source.
By providing an accessible and tested implementation of constraint programming
for scheduling, we hope that PyJobShop will enable researchers and
practitioners to use constraint programming for real-world scheduling problems.",['math.OC'],2501.08719," For a class of sparse optimization problems with the penalty function of
$\|(\cdot)_+\|_0$, we first characterize its local minimizers and then propose
an extrapolated hard thresholding algorithm to solve such problems. We show
that the iterates generated by the proposed algorithm with $\epsilon>0$ (where
$\epsilon$ is the dry friction coefficient) have finite length, without relying
on the Kurdyka-{\L}ojasiewicz inequality. Furthermore, we demonstrate that the
algorithm converges to an $\epsilon$-local minimizer of this problem. For the
special case that $\epsilon=0$, we establish that any accumulation point of the
iterates is a local minimizer of the problem. Additionally, we analyze the
convergence when an error term is present in the algorithm, showing that the
algorithm still converges in the same manner as before, provided that the
errors asymptotically approach zero. Finally, we conduct numerical experiments
to verify the theoretical results of the proposed algorithm.",['math.OC'],False,,,,"PyJobShop: Solving scheduling problems with constraint programming in
  Python","Extrapolated Hard Thresholding Algorithms with Finite Length for
  Composite $\ell_0$ Penalized Problems"
neg-d2-536,2025-01-22,,2501.12847," Working in the hybrid framework of the high energy $pA$ collisions we
identify a new contribution to transverse single spin asymmetry (SSA). The
phase necessary for the SSA is provided by the Pomeron-Odderon interference in
the dense nuclear target. The complete formula for the $pA \to h X$ polarized
cross section also contains the transversity distribution for the polarized
projectile as well as the real part of the twist-3 fragmentation function. We
numerically estimate the asymmetry $A_N$ and its nuclear dependence. Based on a
model computation we find that $A_N$ can be a percent level in the forward and
low-$P_{h\perp}$ region. For large nuclei we find significant suppression, with
$A_N \propto A^{-7/6}$ parametrically. As a notable feature we find a node of
$A_N$ as a function of the $P_{h\perp}$ around the values of the initial
saturation scale that could be used to test this mechanism experimentally.",['hep-ph'],2502.01668," The literature establishes that the light fermions contributions to the
decays $H\to Z\gamma$ and $H\to\gamma\gamma$ are negligible since their
coupling with the Higgs is proportional to $m_f$. In the present letter, we
show that although such a conclusion is true for leptons, the light quark
contributions are zero when we consider their non-perturbative effects.",['hep-ph'],False,,,,"Single spin asymmetry in forward $pA$ collisions from Pomeron-Odderon
  interference",Light quark contributions to Higgs decays
neg-d2-537,2025-03-02,,2503.01063," This paper investigates the potential for large language models (LLMs) to
develop private tonal languages for machine-to-machine (M2M) communication.
Inspired by cryptophasia in human twins (affecting up to 50% of twin births)
and natural tonal languages like Mandarin and Vietnamese, we implement a
precise character-to-frequency mapping system that encodes the full ASCII
character set (32-126) using musical semitones. Each character is assigned a
unique frequency, creating a logarithmic progression beginning with space (220
Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,
with higher characters deliberately mapped to ultrasonic frequencies beyond
human perception (>20 kHz). Our implemented software prototype demonstrates
this encoding through visualization, auditory playback, and ABC musical
notation, allowing for analysis of information density and transmission speed.
Testing reveals that tonal encoding can achieve information rates exceeding
human speech while operating partially outside human perceptual boundaries.
This work responds directly to concerns about AI systems catastrophically
developing private languages within the next five years, providing a concrete
prototype software example of how such communication might function and the
technical foundation required for its emergence, detection, and governance.",['cs.CL'],2501.01028," As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.",['cs.CL'],False,,,,"AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond
  Human Understanding",KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
neg-d2-538,2025-03-07,,2503.05273," The LHCb Ring-Imaging Cherenkov detectors are built to provide charged hadron
identification over a large range of momentum. The upgraded detectors are also
capable of providing an independent measurement of the luminosity for the LHCb
experiment during LHC Run 3. The modelling of the opto-electronics chain, the
application of the powering strategy during operations, the calibration
procedures and the proof of principle of a novel technique for luminosity
determination are presented. In addition, the preliminary precision achieved
during the 2023 data-taking year for real-time and offline luminosity
measurements is reported.",['hep-ex'],2503.17186," This paper presents searches for the direct pair production of charged
light-flavour sleptons, each decaying into a stable neutralino and an
associated Standard Model lepton. The analyses focus on the challenging
""corridor"" region, where the mass difference, $\Delta m$, between the slepton
($\tilde{e}$ or $\tilde{\mu}$) and the lightest neutralino
($\tilde{\chi}^{0}_{1}$) is less or similar to the mass of the $W$ boson,
$m(W)$, with the aim to close a persistent gap in sensitivity to models with
$\Delta m \lesssim m(W)$. Events are required to contain a high-energy jet,
significant missing transverse momentum, and two same-flavour opposite-sign
leptons ($e$ or $\mu$). The analysis uses $pp$ collision data at $\sqrt{s} =
13$ TeV recorded by the ATLAS detector, corresponding to an integrated
luminosity of 140 fb$^{-1}$. Several kinematic selections are applied,
including a set of boosted decision trees. These are each optimised for
different $\Delta m$ to provide expected sensitivity for the first time across
the full $\Delta m$ corridor. The results are generally consistent with the
Standard Model, with the most significant deviations observed with a local
significance of 2.0 $\sigma$ in the selectron search, and 2.4 $\sigma$ in the
smuon search. While these deviations weaken the observed exclusion reach in
some parts of the signal parameter space, the previously present sensitivity
gap to this corridor is largely reduced. Constraints at the 95% confidence
level are set on simplified models of selectron and smuon pair production,
where selectrons (smuons) with masses up to 300 (350) GeV can be excluded for
$\Delta m$ between 2 GeV and 100 GeV.",['hep-ex'],False,,,,Luminosity measurement with the LHCb RICH detectors in Run 3,"Searches for direct slepton production in the compressed-mass corridor
  in $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector"
neg-d2-539,2025-01-12,,2501.06901," The field of serious games for health has grown significantly, demonstrating
effectiveness in various clinical contexts such as stroke, spinal cord injury,
and degenerative neurological diseases. Despite their potential benefits,
therapists face barriers to adopting serious games in rehabilitation, including
limited training and game literacy, concerns about cost and equipment
availability, and a lack of evidence-based research on game effectiveness.
Serious games for rehabilitation often involve repetitive exercises, which can
be tedious and reduce motivation for continued rehabilitation, treating clients
as passive recipients of clinical outcomes rather than players. This study
identifies gaps and provides essential insights for advancing serious games in
rehabilitation, aiming to enhance their engagement for clients and
effectiveness as a therapeutic tool. Addressing these challenges requires a
paradigm shift towards developing and co-creating serious games for
rehabilitation with therapists, researchers, and stakeholders. Furthermore,
future research is crucial to advance the development of serious games,
ensuring they adhere to evidence-based principles and engage both clients and
therapists. This endeavor will identify gaps in the field, inspire new
directions, and support the creation of practical guidelines for serious games
research.",['cs.HC'],2503.16114," Interfaces for interacting with large language models (LLMs) are often
designed to mimic human conversations, typically presenting a single response
to user queries. This design choice can obscure the probabilistic and
predictive nature of these models, potentially fostering undue trust and
over-anthropomorphization of the underlying model. In this paper, we
investigate (i) the effect of displaying multiple responses simultaneously as a
countermeasure to these issues, and (ii) how a cognitive support
mechanism-highlighting structural and semantic similarities across
responses-helps users deal with the increased cognitive load of that
intervention. We conducted a within-subjects study in which participants
inspected responses generated by an LLM under three conditions: one response,
ten responses with cognitive support, and ten responses without cognitive
support. Participants then answered questions about workload, trust and
reliance, and anthropomorphization. We conclude by reporting the results of
these studies and discussing future work and design opportunities for future
LLM interfaces.",['cs.HC'],False,,,,"Games! What are they good for? The Struggle of Serious Game Adoption for
  Rehabilitation","The Impact of Revealing Large Language Model Stochasticity on Trust,
  Reliability, and Anthropomorphization"
neg-d2-540,2025-01-27,,2501.16106," Recently, multimodal depression recognition for clinical interviews (MDRC)
has recently attracted considerable attention. Existing MDRC studies mainly
focus on improving task performance and have achieved significant development.
However, for clinical applications, model transparency is critical, and
previous works ignore the interpretability of decision-making processes. To
address this issue, we propose an Explainable Multimodal Depression Recognition
for Clinical Interviews (EMDRC) task, which aims to provide evidence for
depression recognition by summarizing symptoms and uncovering underlying
causes. Given an interviewer-participant interaction scenario, the goal of
EMDRC is to structured summarize participant's symptoms based on the eight-item
Patient Health Questionnaire depression scale (PHQ-8), and predict their
depression severity. To tackle the EMDRC task, we construct a new dataset based
on an existing MDRC dataset. Moreover, we utilize the PHQ-8 and propose a
PHQ-aware multimodal multi-task learning framework, which captures the
utterance-level symptom-related semantic information to help generate
dialogue-level summary. Experiment results on our annotated dataset demonstrate
the superiority of our proposed methods over baseline systems on the EMDRC
task.",['cs.CL'],2501.01028," As retrieval-augmented generation prevails in large language models,
embedding models are becoming increasingly crucial. Despite the growing number
of general embedding models, prior work often overlooks the critical role of
training data quality. In this work, we introduce KaLM-Embedding, a general
multilingual embedding model that leverages a large quantity of cleaner, more
diverse, and domain-specific training data. Our model has been trained with key
techniques proven to enhance performance: (1) persona-based synthetic data to
create diversified examples distilled from LLMs, (2) ranking consistency
filtering to remove less informative samples, and (3) semi-homogeneous task
batch sampling to improve training efficacy. Departing from traditional
BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,
facilitating the adaptation of auto-regressive language models for general
embedding tasks. Extensive evaluations of the MTEB benchmark across multiple
languages show that our model outperforms others of comparable size, setting a
new standard for multilingual embedding models with <1B parameters.",['cs.CL'],False,,,,"Towards Explainable Multimodal Depression Recognition for Clinical
  Interviews",KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model
neg-d2-541,2025-02-17,,2502.11523," The characterization of pore-throat structures in tight sandstones is crucial
for understanding fluid flow in hydrocarbon reservoirs and groundwater systems.
Both thin-section and Mercury Intrusion Capillary Pressure (MICP) offer
insights rock petrophysical parameters. However, thin-section analysis is
limited by its 2D nature and subjective interpretation, while MICP provides 3D
pore-throat distributions, it lacks direct visualization of pore morphology.
This study evaluates AI-assisted thin-section image analysis for pore-throat
characterization by comparing its results to MICP-derived measurements. A
machine learning-based workflow was developed using color thresholding, K-Means
clustering, and medial axis transformation to segment pore structures in
thin-section images. Throat width, porosity, and permeability were
quantitatively assessed against MICP to determine the accuracy and reliability
of the technique. The analysis of 26 sandstone samples outlined differences
between the two methods. Thin-section analysis showed porosity values from
1.37% to 53.37%, with average pore-throat sizes between 5.63 micron and 30.09
micron, while permeability estimates ranged from 0.01 mD to 344.35 mD.
Correlation analysis showed moderate agreement for throat size (r=0.62) and
permeability (r=0.61), but weaker for porosity (r=0.32), highlighting the
differences in how each method captures pore connectivity. Results demonstrate
that the AI-assisted segmentation provides a scalable and reproducible approach
but is constrained by thin-section imaging resolution. While MICP remains
reliable for permeability evaluation, its comparison with AI-driven image
analysis helps assess the reliability of the method. Future research should
refine segmentation algorithms, incorporate pretrained data to validate
AI-derived pore-throat attributes for improved reservoir quality assessment and
predictive modeling.",['physics.geo-ph'],2502.11523," The characterization of pore-throat structures in tight sandstones is crucial
for understanding fluid flow in hydrocarbon reservoirs and groundwater systems.
Both thin-section and Mercury Intrusion Capillary Pressure (MICP) offer
insights rock petrophysical parameters. However, thin-section analysis is
limited by its 2D nature and subjective interpretation, while MICP provides 3D
pore-throat distributions, it lacks direct visualization of pore morphology.
This study evaluates AI-assisted thin-section image analysis for pore-throat
characterization by comparing its results to MICP-derived measurements. A
machine learning-based workflow was developed using color thresholding, K-Means
clustering, and medial axis transformation to segment pore structures in
thin-section images. Throat width, porosity, and permeability were
quantitatively assessed against MICP to determine the accuracy and reliability
of the technique. The analysis of 26 sandstone samples outlined differences
between the two methods. Thin-section analysis showed porosity values from
1.37% to 53.37%, with average pore-throat sizes between 5.63 micron and 30.09
micron, while permeability estimates ranged from 0.01 mD to 344.35 mD.
Correlation analysis showed moderate agreement for throat size (r=0.62) and
permeability (r=0.61), but weaker for porosity (r=0.32), highlighting the
differences in how each method captures pore connectivity. Results demonstrate
that the AI-assisted segmentation provides a scalable and reproducible approach
but is constrained by thin-section imaging resolution. While MICP remains
reliable for permeability evaluation, its comparison with AI-driven image
analysis helps assess the reliability of the method. Future research should
refine segmentation algorithms, incorporate pretrained data to validate
AI-derived pore-throat attributes for improved reservoir quality assessment and
predictive modeling.",['physics.geo-ph'],False,,,,"AI-Assisted Thin Section Image Processing for Pore-Throat
  Characterization in Tight Clastic Rocks","AI-Assisted Thin Section Image Processing for Pore-Throat
  Characterization in Tight Clastic Rocks"
neg-d2-542,2025-01-06,,2501.03521," In this study, we propose a new method for constrained combinatorial
optimization using variational quantum circuits. Quantum computers are
considered to have the potential to solve large combinatorial optimization
problems faster than classical computers. Variational quantum algorithms, such
as Variational Quantum Eigensolver (VQE), have been studied extensively because
they are expected to work on noisy intermediate scale devices. Unfortunately,
many optimization problems have constraints, which induces infeasible solutions
during VQE process. Recently, several methods for efficiently solving
constrained combinatorial optimization problems have been proposed by designing
a quantum circuit so as to output only the states that satisfy the constraints.
However, the types of available constraints are still limited. Therefore, we
have started to develop variational quantum circuits that can handle a wider
range of constraints. The proposed method utilizes a forwarding operation that
maps from feasible states for subproblems to those for larger subproblems. As
long as appropriate forwarding operations can be defined, iteration of this
process can inductively construct variational circuits outputting feasible
states even in the case of multiple and complex constraints. In this paper, the
proposed method was applied to facility location problem and was found to
increase the probability for measuring feasible solutions or optimal solutions.
In addition, the cost of the obtained circuit was comparable to that of
conventional variational circuits.",['quant-ph'],2501.17526," We investigate the charging dynamics of a frequency-modulated quantum battery
(QB) placed within a dissipative cavity environment. Our study focuses on the
interaction of such a battery under both weak and strong coupling regimes,
employing a model in which the quantum battery and charger are represented as
frequency-modulated qubits indirectly coupled through a zero-temperature
environment. It is demonstrated that both the modulation frequency and
amplitude are crucial for optimizing the charging process and the ergotropy of
the quantum battery. Specifically, high-amplitude, low-frequency modulation
significantly enhances charging performance and work extraction in the strong
coupling regime. As an intriguing result, it is deduced that modulation at very
low frequencies leads to the emergence of energy storage and work extraction in
the weak coupling regime. Such a result can never be achieved without
modulation in the weak coupling regime. These results highlight the importance
of adjusting modulation parameters to optimize the performance of quantum
batteries for real-world applications in quantum technologies.",['quant-ph'],False,,,,"Inductive Construction of Variational Quantum Circuit for Constrained
  Combinatorial Optimization",Amplified quantum battery via dynamical modulation
neg-d2-543,2025-03-07,,2503.0567," Epsilon-near-zero (ENZ) systems exhibit unconventional electromagnetic
response close to their zero permittivity regime. Here, we explore the ability
of ultrathin ENZ films to modulate the transmission of radiation from an
underlying quantum emitter through active control of the carrier density of the
ENZ film. The achievable on/off switching ratio is shown to be constrained by
the material's loss parameter, particularly in the ENZ regime, where
transmissivity increases with higher material loss. The finite loss in real
materials limit the more extraordinary potential of ideal near-zero-index
systems. Along with an in-depth discussion on the material parameters vis-a-vis
the underlying physics, this work provides avenues to overcome the shortcomings
of finite loss in real materials. These findings are intended to guide material
development and offer valuable insights for designing on-chip optical
modulators and beam steering devices operating in the near-infrared regime.",['physics.optics'],2501.15069," We investigate magnomechanically induced transparency (MMIT) in a microwave
3D copper cavity with two YIG spheres under varying interaction parameters.
Numerical simulations show that the steady-state magnon number increases with
stronger coupling between cavity photons and magnons, and is sensitive to both
bias and drive magnetic fields. Pronounced peaks in the magnon population near
resonant fields highlight the importance of the bias field in energy transfer.
The transparency windows are tunable, with up to quadruple windows depending on
the coupling and magnon-phonon interactions, as seen in the transmission
spectrum. Dispersion analysis reveals normal and anomalous regions, enabling
slow and fast light propagation modulated by coupling strength. Phase and group
delay variations, influenced by the drive field, further validate the
tunability of transparency windows. This study demonstrates the potential of
MMIT for precise control with out any additional non-linearity over
light-matter interactions, with applications in quantum information processing
and optical communications.",['physics.optics'],False,,,,Leveraging Epsilon Near Zero phenomena for on-chip photonic modulation,"Magnetic Field induced control and Multiple Magnomechanically Induced
  Transparency in Single Cavity"
neg-d2-544,2025-03-05,,2503.03545," Patients with semantic dementia (SD) present with remarkably consistent
atrophy of neurons in the anterior temporal lobe and behavioural impairments,
such as graded loss of category knowledge. While relearning of lost knowledge
has been shown in acute brain injuries such as stroke, it has not been widely
supported in chronic cognitive diseases such as SD. Previous research has shown
that deep linear artificial neural networks exhibit stages of semantic learning
akin to humans. Here, we use a deep linear network to test the hypothesis that
relearning during disease progression rather than particular atrophy cause the
specific behavioural patterns associated with SD. After training the network to
generate the common semantic features of various hierarchically organised
objects, neurons are successively deleted to mimic atrophy while retraining the
model. The model with relearning and deleted neurons reproduced errors specific
to SD, including prototyping errors and cross-category confusions. This
suggests that relearning is necessary for artificial neural networks to
reproduce the behavioural patterns associated with SD in the absence of
\textit{output} non-linearities. Our results support a theory of SD progression
that results from continuous relearning of lost information. Future research
should revisit the role of relearning as a contributing factor to cognitive
diseases.",['cs.LG'],2501.03654," Deep learning (DL) models have gained prominence in domains such as computer
vision and natural language processing but remain underutilized for regression
tasks involving tabular data. In these cases, traditional machine learning (ML)
models often outperform DL models. In this study, we propose and evaluate
various data augmentation (DA) techniques to improve the performance of DL
models for tabular data regression tasks. We compare the performance gain of
Neural Networks by different DA strategies ranging from a naive method of
duplicating existing observations and adding noise to a more sophisticated DA
strategy that preserves the underlying statistical relationship in the data.
Our analysis demonstrates that the advanced DA method significantly improves DL
model performance across multiple datasets and regression tasks, resulting in
an average performance increase of over 10\% compared to baseline models
without augmentation. The efficacy of these DA strategies was rigorously
validated across 30 distinct datasets, with multiple iterations and evaluations
using three different automated deep learning (AutoDL) frameworks: AutoKeras,
H2O, and AutoGluon. This study demonstrates that by leveraging advanced DA
techniques, DL models can realize their full potential in regression tasks,
thereby contributing to broader adoption and enhanced performance in practical
applications.",['cs.LG'],False,,,,Revisiting the Role of Relearning in Semantic Dementia,"Data Augmentation for Deep Learning Regression Tasks by Machine Learning
  Models"
neg-d2-545,2025-03-24,,2503.18328," Inverse rendering aims to recover scene geometry, material properties, and
lighting from multi-view images. Given the complexity of light-surface
interactions, importance sampling is essential for the evaluation of the
rendering equation, as it reduces variance and enhances the efficiency of Monte
Carlo sampling. Existing inverse rendering methods typically use pre-defined
non-learnable importance samplers in prior manually, struggling to effectively
match the spatially and directionally varied integrand and resulting in high
variance and suboptimal performance. To address this limitation, we propose the
concept of learning a spatially and directionally aware importance sampler for
the rendering equation to accurately and flexibly capture the unconstrained
complexity of a typical scene. We further formulate TensoFlow, a generic
approach for sampler learning in inverse rendering, enabling to closely match
the integrand of the rendering equation spatially and directionally.
Concretely, our sampler is parameterized by normalizing flows, allowing both
directional sampling of incident light and probability density function (PDF)
inference. To capture the characteristics of the sampler spatially, we learn a
tensorial representation of the scene space, which imposes spatial conditions,
together with reflected direction, leading to spatially and directionally aware
sampling distributions. Our model can be optimized by minimizing the difference
between the integrand and our normalizing flow. Extensive experiments validate
the superiority of TensoFlow over prior alternatives on both synthetic and
real-world benchmarks.",['cs.CV'],2503.08201," Human-centric visual perception (HVP) has recently achieved remarkable
progress due to advancements in large-scale self-supervised pretraining (SSP).
However, existing HVP models face limitations in adapting to real-world
applications, which require general visual patterns for downstream tasks while
maintaining computationally sustainable costs to ensure compatibility with edge
devices. These limitations primarily arise from two issues: 1) the pretraining
objectives focus solely on specific visual patterns, limiting the
generalizability of the learned patterns for diverse downstream tasks; and 2)
HVP models often exhibit excessively large model sizes, making them
incompatible with real-world applications. To address these limitations, we
introduce Scale-Aware Image Pretraining (SAIP), a novel SSP framework enabling
lightweight vision models to acquire general patterns for HVP. Specifically,
SAIP incorporates three learning objectives based on the principle of
cross-scale consistency: 1) Cross-scale Matching (CSM) which contrastively
learns image-level invariant patterns from multi-scale single-person images; 2)
Cross-scale Reconstruction (CSR) which learns pixel-level consistent visual
structures from multi-scale masked single-person images; and 3) Cross-scale
Search (CSS) which learns to capture diverse patterns from multi-scale
multi-person images. Three objectives complement one another, enabling
lightweight models to learn multi-scale generalizable patterns essential for
HVP downstream tasks.Extensive experiments conducted across 12 HVP datasets
demonstrate that SAIP exhibits remarkable generalization capabilities across 9
human-centric vision tasks. Moreover, it achieves significant performance
improvements over existing methods, with gains of 3%-13% in single-person
discrimination tasks, 1%-11% in dense prediction tasks, and 1%-6% in
multi-person visual understanding tasks.",['cs.CV'],False,,,,TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering,"Scale-Aware Pre-Training for Human-Centric Visual Perception: Enabling
  Lightweight and Generalizable Models"
neg-d2-546,2025-02-07,,2502.05376," Post-training quantization (PTQ) is a promising approach to reducing the
storage and computational requirements of large language models (LLMs) without
additional training cost. Recent PTQ studies have primarily focused on
quantizing only weights to sub-8-bits while maintaining activations at 8-bits
or higher. Accurate sub-8-bit quantization for both weights and activations
without relying on quantization-aware training remains a significant challenge.
We propose a novel quantization method called block clustered quantization
(BCQ) wherein each operand tensor is decomposed into blocks (a block is a group
of contiguous scalars), blocks are clustered based on their statistics, and a
dedicated optimal quantization codebook is designed for each cluster. As a
specific embodiment of this approach, we propose a PTQ algorithm called
Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block
clustering and codebook design to greedily minimize the quantization mean
squared error. When weight and activation scalars are encoded to W4A4 format
(with 0.5-bits of overhead for storing scaling factors and codebook selectors),
we advance the current state-of-the-art by demonstrating <1% loss in inference
accuracy across several LLMs and downstream tasks.",['cs.LG'],2503.18668," This paper presents an efficient preference elicitation framework for
uncertain matroid optimization, where precise weight information is
unavailable, but insights into possible weight values are accessible. The core
innovation of our approach lies in its ability to systematically elicit user
preferences, aligning the optimization process more closely with
decision-makers' objectives. By incrementally querying preferences between
pairs of elements, we iteratively refine the parametric uncertainty regions,
leveraging the structural properties of matroids. Our method aims to achieve
the exact optimum by reducing regret with a few elicitation rounds.
Additionally, our approach avoids the computation of Minimax Regret and the use
of Linear programming solvers at every iteration, unlike previous methods.
Experimental results on four standard matroids demonstrate that our method
reaches optimality more quickly and with fewer preference queries than existing
techniques.",['cs.LG'],False,,,,BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference,"Geometric Preference Elicitation for Minimax Regret Optimization in
  Uncertainty Matroids"
neg-d2-547,2025-01-10,,2501.06084," The Fisher-Yates shuffle is a well-known algorithm for shuffling a finite
sequence, such that every permutation is equally likely. Despite its
simplicity, it is prone to implementation errors that can introduce bias into
the generated permutations. We verify its correctness in Dafny as follows.
First, we define a functional model that operates on sequences and streams of
random bits. Second, we establish that the functional model has the desired
distribution. Third, we define an executable imperative implementation that
operates on arrays and prove it equivalent to the functional model. The
approach may serve as a blueprint for the verification of more complex
algorithms.",['cs.CR'],2501.06084," The Fisher-Yates shuffle is a well-known algorithm for shuffling a finite
sequence, such that every permutation is equally likely. Despite its
simplicity, it is prone to implementation errors that can introduce bias into
the generated permutations. We verify its correctness in Dafny as follows.
First, we define a functional model that operates on sequences and streams of
random bits. Second, we establish that the functional model has the desired
distribution. Third, we define an executable imperative implementation that
operates on arrays and prove it equivalent to the functional model. The
approach may serve as a blueprint for the verification of more complex
algorithms.",['cs.CR'],False,,,,Verifying the Fisher-Yates Shuffle Algorithm in Dafny,Verifying the Fisher-Yates Shuffle Algorithm in Dafny
neg-d2-548,2025-02-17,,2502.11933," Simulation of interacting fermionic Hamiltonians is one of the most promising
applications of quantum computers. However, the feasibility of analysing
fermionic systems with a quantum computer hinges on the efficiency of
fermion-to-qubit mappings that encode non-local fermionic degrees of freedom in
local qubit degrees of freedom. While recent works have highlighted the
importance of designing fermion-to-qubit mappings that are tailored to specific
problem Hamiltonians, the methods proposed so far are either restricted to a
narrow class of mappings or they use computationally expensive and unscalable
brute-force search algorithms. Here, we address this challenge by designing a
$\mathrm{\textbf{heuristic}}$ numerical optimization framework for
fermion-to-qubit mappings. To this end, we first translate the fermion-to-qubit
mapping problem to a Clifford circuit optimization problem, and then use
simulated annealing to optimize the average Pauli weight of the problem
Hamiltonian. For all fermionic Hamiltonians we have considered, the numerically
optimized mappings outperform their conventional counterparts, including
ternary-tree-based mappings that are known to be optimal for single creation
and annihilation operators. We find that our optimized mappings yield between
$15\%$ to $40\%$ improvements on the average Pauli weight when the simulation
Hamiltonian has an intermediate level of complexity. Most remarkably, the
optimized mappings improve the average Pauli weight for $6 \times 6$
nearest-neighbor hopping and Hubbard models by more than $40\%$ and $20\%$,
respectively. Surprisingly, we also find specific interaction Hamiltonians for
which the optimized mapping outperform $\mathrm{\textbf{any}}$
ternary-tree-based mapping. Our results establish heuristic numerical
optimization as an effective method for obtaining mappings tailored for
specific fermionic Hamiltonian.",['quant-ph'],2503.02934," We propose an approach to generative quantum machine learning that overcomes
the fundamental scaling issues of variational quantum circuits. The core idea
is to use a class of generative models based on instantaneous quantum
polynomial circuits, which we show can be trained efficiently on classical
hardware. Although training is classically efficient, sampling from these
circuits is widely believed to be classically hard, and so computational
advantages are possible when sampling from the trained model on quantum
hardware. By combining our approach with a data-dependent parameter
initialisation strategy, we do not encounter issues of barren plateaus and
successfully circumvent the poor scaling of gradient estimation that plagues
traditional approaches to quantum circuit optimisation. We investigate and
evaluate our approach on a number of real and synthetic datasets, training
models with up to one thousand qubits and hundreds of thousands of parameters.
We find that the quantum models can successfully learn from high dimensional
data, and perform surprisingly well compared to simple energy-based classical
generative models trained with a similar amount of hyperparameter optimisation.
Overall, our work demonstrates that a path to scalable quantum generative
machine learning exists and can be investigated today at large scales.",['quant-ph'],False,,,,"Clifford circuit based heuristic optimization of fermion-to-qubit
  mappings","Train on classical, deploy on quantum: scaling generative quantum
  machine learning to a thousand qubits"
neg-d2-549,2025-03-10,,2503.07982," Achieving precise panoptic segmentation relies on pixel-wise instance
annotations, but obtaining such datasets is costly. Unsupervised instance
segmentation (UIS) eliminates annotation requirements but struggles with
adjacent instance merging and single-instance fragmentation, largely due to the
limitations of DINO-based backbones which lack strong instance separation cues.
Weakly-supervised panoptic segmentation (WPS) reduces annotation costs using
sparse labels (e.g., points, boxes), yet these annotations remain expensive and
introduce human bias and boundary errors. To address these challenges, we
propose DiffEGG (Diffusion-Driven EdGe Generation), a fully annotation-free
method that extracts instance-aware features from pretrained diffusion models
to generate precise instance edge maps. Unlike DINO-based UIS methods,
diffusion models inherently capture fine-grained, instance-aware features,
enabling more precise boundary delineation. For WPS, DiffEGG eliminates
annotation costs and human bias by operating without any form of manual
supervision, addressing the key limitations of prior best methods.
Additionally, we introduce RIP, a post-processing technique that fuses
DiffEGG's edge maps with segmentation masks in a task-agnostic manner. RIP
allows DiffEGG to be seamlessly integrated into various segmentation
frameworks. When applied to UIS, DiffEGG and RIP achieve an average $+4.4\text{
AP}$ improvement over prior best UIS methods. When combined with
weakly-supervised semantic segmentation (WSS), DiffEGG enables WPS without
instance annotations, outperforming prior best point-supervised WPS methods by
$+1.7\text{ PQ}$. These results demonstrate that DiffEGG's edge maps serve as a
cost-effective, annotation-free alternative to instance annotations,
significantly improving segmentation without human intervention. Code is
available at https://github.com/shjo-april/DiffEGG.",['cs.CV'],2501.01121," While current high-resolution depth estimation methods achieve strong
results, they often suffer from computational inefficiencies due to reliance on
heavyweight models and multiple inference steps, increasing inference time. To
address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner
models with lightweight encoders. This reduces model size and inference time
but introduces noisy features. To overcome this, we propose a Coarse-to-Fine
(C2F) module with a Guided Denoising Unit for refining and denoising the
refiner features and a Noisy Pretraining strategy to pretrain the refiner
branch to fully exploit the potential of the lightweight refiner branch.
Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching
(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms
state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy
and speed, using fewer parameters and faster inference. It also shows improved
depth boundary delineation on real-world datasets like CityScape, ScanNet++,
and KITTI, demonstrating its versatility across domains.",['cs.CV'],False,,,,"DiffEGG: Diffusion-Driven Edge Generation as a Pixel-Annotation-Free
  Alternative for Instance Annotation","PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric
  Depth Estimation"
neg-d2-550,2025-02-02,,2502.00958," We study belief revision when information is represented by a set of
probability distributions, or general information. General information extends
the standard event notion while including qualitative information (A is more
likely than B), interval information (A has a ten-to-twenty percent chance),
and more. We behaviorally characterize Inertial Updating: the decision maker's
posterior is of minimal subjective distance from her prior, given the
information constraint. Further, we introduce and characterize a notion of
Bayesian updating for general information and show that Bayesian agents may
disagree. We also behaviorally characterize f-divergences, the class of
distances consistent with Bayesian updating.",['econ.TH'],2501.15548," This paper examines games with strategic complements or substitutes and
incomplete information, where players are uncertain about the opponents'
parameters. We assume that the players' beliefs about the opponent's parameters
are selected from some given set of beliefs. One extreme is the case where
these sets only contain a single belief, representing a scenario where the
players' actual beliefs about the parameters are commonly known among the
players. Another extreme is the situation where these sets contain all possible
beliefs, representing a scenario where the players have no information about
the opponents' beliefs about parameters. But we also allow for intermediate
cases, where these sets contain some, but not all, possible beliefs about the
parameters. We introduce an assumption of weakly increasing differences that
takes both the choice belief and parameter belief of a player into account.
Under this assumption, we demonstrate that greater choice-parameter beliefs
leads to greater optimal choices. Moreover, we show that the greatest and least
point rationalizable choice of a player is increasing in their parameter, and
these can be determined through an iterative procedure. In each round of the
iterative procedure, the lowest surviving choice is optimal for the lowest
choice-parameter belief, while the greatest surviving choice is optimal for the
highest choice-parameter belief.",['econ.TH'],False,,,,Inertial Updating with General Information,Rationalizability and Monotonocity in Games with Incomplete Information
neg-d2-551,2025-01-13,,2501.07657," Narrow linewidth lasers are indispensable for coherent optical systems,
including communications, metrology, and sensing. Although compact
semiconductor lasers with narrow linewidths and low noise have been
demonstrated, their spectral purity typically relies on hybrid or heterogeneous
external cavity feedback. Here, we present a theoretical and experimental
demonstration of a heterogeneous free optical injection locking (HF OIL)
semiconductor laser. By integrating a topological interface state extended
(TISE) laser with a micro ring resonator (MRR) on an AlGaInAs multiple quantum
well platform,we achieve monolithic photon injection and phase locking, thereby
reducing the optical linewidth. We fabricated and characterized a 1550 nm
sidewall HF OIL laser, achieving stable single mode operation over a broad
current range (65 to 300 mA) and a side mode suppression ratio (SMSR) over 50
dB. Under injection locking, the devices Voigt fitted linewidth narrowed from
over 1.7 MHz (free running) to 4.2 kHz, representing a three order of magnitude
improvement over conventional distributed feedback lasers. The intrinsic
linewidth of 1.4 kHz is measured by correlated delayed self-heterodyne
frequency noise power spectrum density (FN PSD) method. Moreover, the HF OIL
laser demonstrated high phase stability and the ability to transition from a
random phased to a phase locked state. These results underscore the potential
of HF-OIL lasers in advancing coherent optical communications and phase
encoders in quantum key distribution (QKD) systems.",['physics.optics'],2501.06966," Networks of coupled nonlinear optical resonators have emerged as an important
class of systems in ultrafast optical science, enabling richer and more complex
nonlinear dynamics compared to their single-resonator or travelling-wave
counterparts. In recent years, these coupled nonlinear optical resonators have
been applied as application-specific hardware accelerators for computing
applications including combinatorial optimization and artificial intelligence.
In this work, we rigorously prove a fundamental result showing that coupled
nonlinear optical resonators are Turing-complete computers, which endows them
with much greater computational power than previously thought. Furthermore, we
show that the minimum threshold of hardware complexity needed for
Turing-completeness is surprisingly low, which has profound physical
consequences. In particular, we show that several problems of interest in the
study of coupled nonlinear optical resonators are formally undecidable. These
theoretical findings can serve as the foundation for better understanding the
promise of next-generation, ultrafast all-optical computers.",['physics.optics'],False,,,,"Heterogeneous-free narrow linewidth semiconductor laser with optical
  injection locking","Turing-Completeness and Undecidability in Coupled Nonlinear Optical
  Resonators"
neg-d2-552,2025-01-31,,2502.00184," We prove a Brunn-Minkowski type inequality for the first (nontrivial)
Dirichlet eigenvalue of the weighted $p$-operator \[
-\Delta_{p,\gamma}u=-\text{div}(|\nabla u|^{p-2} \nabla u)+(x,\nabla u)|\nabla
u|^{p-2}, \] where $p>1$, in the class of bounded Lipschitz domains in
$\mathbb{R}^n$. We also prove that any corresponding positive eigenfunction is
log-concave if the domain is convex.",['math.AP'],2503.16362," In this work, we proved the existence of a unique global mild solution of the
d-dimensional incompressible Navier-Stokes equations, for small initial data in
Besov type spaces based on mixed-Lebesgue spaces; namely, mixed-norm
Besov-Lebesgue spaces and also mixed-norm Fourier-Besov-Lebesgue spaces. The
main tools are the Bernstein's type inequalities, Bony's paraproduct to
estimate the bilinear term and a fixed point scheme in order to get the
well-posedness. Our results complement and cover previous and recents result on
(Fourier-)Besov spaces and, for instance, provide a new class of initial data
possibly not included in BMO^{-1}(R^3) but continuously included in
\dot{B}^{-1}_{\infty,infty}(R^3).",['math.AP'],False,,,,"Geometric properties of solutions to elliptic PDE's in Gauss space and
  related Brunn-Minkowski type inequalities","Global well-posedness for the Navier-Stokes system in new critical
  mixed-norm Besov spaces"
neg-d2-553,2025-03-18,,2503.14131," We investigate the emergence and transformation of pinch-point singularities
in the excitation spectrum of electronic flat band systems on kagome and
pyrochlore lattices with spin-orbit coupling (SOC) and Coulomb interactions.
While pinch points are widely recognized as signatures of classical spin
liquids, they also appear in electronic flat-band systems when there exists a
singular band-touching point to dispersive bands. We explore how SOC modifies
the pinch-point structure in the chiral spin flat-band metallic state, which we
term spinor-ice. The pinch point profile can rotate or redistribute its
spectral weight, governed by a prefactor in the spectral function that
primarily depends on the direction of the ground-state spin polarization, where
we show that SOC flat bands could be experimentally probed by rotating the spin
polarization of the injected electron to infer internal magnetic structures.
These observations are discussed in conjunction with the angle-resolved
photoemission spectroscopy (ARPES) and the application to the potential SOC
flat-band material $\rm CsW_2O_6$. We also demonstrate the persistent residual
pinch-point features under Coulomb interactions and deviations from the ideal
flat-band limit.",['cond-mat.str-el'],2503.11598," We investigate the thermodynamic properties of the Hubbard model on the Bethe
lattice with a coordination number of 3 using the thermal canonical tree tensor
network method. Our findings reveal two distinct thermodynamic phases: a
low-temperature antiferromagnetic phase, where spin SU(2) symmetry is broken,
and a high-temperature paramagnetic phase. A key feature of the system is the
separation of energy scales for charge and spin excitations, which is reflected
in the temperature dependence of thermodynamic quantities and the disparity
between spin and charge gaps extracted from their respective susceptibilities.
At the critical point, both spin and charge susceptibilities exhibit
singularities, suggesting that charge excitations are not fully decoupled from
their spin counterparts. Additionally, the double occupancy number exhibits a
non-monotonic temperature dependence, indicative of an entropy-driven
Pomeranchuk effect. These results demonstrate that the loopless Bethe lattice
effectively captures the essential physics of the Hubbard model while providing
a computationally efficient framework for studying strongly correlated
electronic systems.",['cond-mat.str-el'],False,,,,"Spinor ice correlation in flat-band electronic states on kagome and
  pyrochlore lattices with spin-orbit coupling",Thermodynamics of the Hubbard Model on the Bethe Lattice
neg-d2-554,2025-01-31,,2501.1937," In this project, we propose a Variational Inference algorithm to approximate
posterior distributions. Building on prior methods, we develop the
Gradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This
method introduces a novel loss function that combines a weighted gradient and
the Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The
learning rate is determined through a suboptimal minimization of this loss
function within a gradient descent framework.
  The G-SVGD method is compared against the standard Stein Variational Gradient
Descent (SVGD) approach, employing the ADAM optimizer for learning rate
adaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess
performance in two wave prospection models representing low-contrast and
high-contrast subsurface scenarios. To achieve robust numerical approximations
in the forward model solver, a five-point operator is employed, while the
adjoint method improves accuracy in computing gradients of the log posterior.
  Our findings demonstrate that G-SVGD accelerates convergence and offers
improved performance in scenarios where gradient evaluation is computationally
expensive. The abstract highlights the algorithm's applicability to wave
prospection models and its potential for broader applications in Bayesian
inference. Finally, we discuss the benefits and limitations of G-SVGD,
emphasizing its contribution to advancing computational efficiency in
uncertainty quantification.",['stat.CO'],2501.1937," In this project, we propose a Variational Inference algorithm to approximate
posterior distributions. Building on prior methods, we develop the
Gradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This
method introduces a novel loss function that combines a weighted gradient and
the Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The
learning rate is determined through a suboptimal minimization of this loss
function within a gradient descent framework.
  The G-SVGD method is compared against the standard Stein Variational Gradient
Descent (SVGD) approach, employing the ADAM optimizer for learning rate
adaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess
performance in two wave prospection models representing low-contrast and
high-contrast subsurface scenarios. To achieve robust numerical approximations
in the forward model solver, a five-point operator is employed, while the
adjoint method improves accuracy in computing gradients of the log posterior.
  Our findings demonstrate that G-SVGD accelerates convergence and offers
improved performance in scenarios where gradient evaluation is computationally
expensive. The abstract highlights the algorithm's applicability to wave
prospection models and its potential for broader applications in Bayesian
inference. Finally, we discuss the benefits and limitations of G-SVGD,
emphasizing its contribution to advancing computational efficiency in
uncertainty quantification.",['stat.CO'],False,,,,"Greedy Stein Variational Gradient Descent: An algorithmic approach for
  wave prospection problems","Greedy Stein Variational Gradient Descent: An algorithmic approach for
  wave prospection problems"
neg-d2-555,2025-02-14,,2502.10259," The ability to observe the world is fundamental to reasoning and making
informed decisions on how to interact with the environment. However, optical
perception can often be disrupted due to common occurrences, such as
occlusions, which can pose challenges to existing vision systems. We present
MITO, the first millimeter-wave (mmWave) dataset of diverse, everyday objects,
collected using a UR5 robotic arm with two mmWave radars operating at different
frequencies and an RGB-D camera. Unlike visible light, mmWave signals can
penetrate common occlusions (e.g., cardboard boxes, fabric, plastic) but each
mmWave frame has much lower resolution than typical cameras. To capture
higher-resolution mmWave images, we leverage the robot's mobility and fuse
frames over the synthesized aperture. MITO captures over 24 million mmWave
frames and uses them to generate 550 high-resolution mmWave (synthetic
aperture) images in line-of-sight and non-light-of-sight (NLOS), as well as
RGB-D images, segmentation masks, and raw mmWave signals, taken from 76
different objects. We develop an open-source simulation tool that can be used
to generate synthetic mmWave images for any 3D triangle mesh. Finally, we
demonstrate the utility of our dataset and simulator for enabling broader NLOS
perception by developing benchmarks for NLOS segmentation and classification.",['cs.CV'],2501.17655," 3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene
reconstruction using 3D Gaussians. However, neither the centers nor surfaces of
the Gaussians are accurately aligned to the object surface, complicating their
direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically
produces floater artifacts, increasing the number of Gaussians and storage
requirements. To address these issues, we present FeatureGS, which incorporates
an additional geometric loss term based on an eigenvalue-derived 3D shape
feature into the optimization process of 3DGS. The goal is to improve geometric
accuracy and enhance properties of planar surfaces with reduced structural
entropy in local 3D neighborhoods.We present four alternative formulations for
the geometric loss term based on 'planarity' of Gaussians, as well as
'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We
provide quantitative and qualitative evaluations on 15 scenes of the DTU
benchmark dataset focusing on following key aspects: Geometric accuracy and
artifact-reduction, measured by the Chamfer distance, and memory efficiency,
evaluated by the total number of Gaussians. Additionally, rendering quality is
monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement
in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses
floater artifacts, while maintaining comparable photometric rendering quality.
The geometric loss with 'planarity' from Gaussians provides the highest
geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces
floater artifacts and number of Gaussians the most. This makes FeatureGS a
strong method for geometrically accurate, artifact-reduced and memory-efficient
3D scene reconstruction, enabling the direct use of Gaussian centers for
geometric representation.",['cs.CV'],False,,,,"MITO: A Millimeter-Wave Dataset and Simulator for Non-Line-of-Sight
  Perception","FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for
  Geometrically Accurate and Artifact-Reduced Reconstruction"
neg-d2-556,2025-01-24,,2501.14564," Diffractive optical elements that divide an input beam into a set of replicas
are used in many optical applications ranging from image processing to
communications. Their design requires time-consuming optimization processes,
which, for a given number of generated beams, are to be separately treated for
one-dimensional and two-dimensional cases because the corresponding optimal
efficiencies may be different. After generalizing their Fourier treatment, we
prove that, once a particular divider has been designed, its transmission
function can be used to generate numberless other dividers through affine
transforms that preserve the efficiency of the original element without
requiring any further optimization.",['physics.optics'],2501.06966," Networks of coupled nonlinear optical resonators have emerged as an important
class of systems in ultrafast optical science, enabling richer and more complex
nonlinear dynamics compared to their single-resonator or travelling-wave
counterparts. In recent years, these coupled nonlinear optical resonators have
been applied as application-specific hardware accelerators for computing
applications including combinatorial optimization and artificial intelligence.
In this work, we rigorously prove a fundamental result showing that coupled
nonlinear optical resonators are Turing-complete computers, which endows them
with much greater computational power than previously thought. Furthermore, we
show that the minimum threshold of hardware complexity needed for
Turing-completeness is surprisingly low, which has profound physical
consequences. In particular, we show that several problems of interest in the
study of coupled nonlinear optical resonators are formally undecidable. These
theoretical findings can serve as the foundation for better understanding the
promise of next-generation, ultrafast all-optical computers.",['physics.optics'],False,,,,Affine diffractive beam dividers,"Turing-Completeness and Undecidability in Coupled Nonlinear Optical
  Resonators"
neg-d2-557,2025-02-25,,2502.18619," We study a variant of the voter model on a coevolving network in which
interactions of two individuals with differing opinions only lead to an
agreement on one of these opinions with a fixed probability $q$. Otherwise,
with probability $1-q$, both individuals become offended in the sense that they
never interact again, i.e.\ the corresponding edge is removed from the
underlying network. Eventually, these dynamics reach an absorbing state at
which there is only one opinion present in each connected component of the
network. If globally both opinions are present at absorption we speak of
``segregation'', otherwise of ``consensus''. We rigorously show that
segregation and a weaker form of consensus both occur with positive probability
for every $q \in (0,1)$ and that the segregation probability tends to $1$ as $q
\to 0$. Furthermore, we establish that, if $q \to 1$ fast enough, with high
probability the population reaches consensus while the underlying network is
still densely connected. We provide results from simulations to assess the
obtained bounds and to discuss further properties of the limiting state.",['math.PR'],2503.08837," We study a system of reflecting Brownian motions on the positive half-line in
which each particle has a drift toward the origin determined by the local times
at the origin of all the particles. We show that if this local time drift is
too strong, such systems can exhibit a breakdown in their solutions in that
there is a time beyond which the system cannot be extended. In the finite
particle case we give a complete characterization of the finite time breakdown,
relying on a novel dynamic graph structure. We consider the mean-field limit of
the system in the symmetric case and show that there is a McKean--Vlasov
representation. If the drift is too strong, the solution to the corresponding
Fokker--Planck equation has a blow up in its solution. We also establish the
existence of stationary and self-similar solutions to the McKean--Vlasov
equation in the case where there is no breakdown of the system. This work is
motivated by models for liquidity in financial markets, the supercooled Stefan
problem, and a toy model for cell polarization.",['math.PR'],False,,,,The Offended Voter Model,"Particle Systems and McKean--Vlasov Dynamics with Singular Interaction
  through Local Times"
neg-d2-558,2025-02-18,,2502.12931," We continue our explorations of the transport characteristics in
junction-configurations comprising semimetals with quadratic band-crossings,
observed in the bandstructures of both two- and three-dimensional materials.
Here, we consider short potential barriers/wells modelled by delta-function
potentials. We also generalize our analysis by incorporating tilts in the
dispersion. Due to the parabolic nature of the spectra, caused by
quadratic-in-momentum dependence, there exist evanescent waves, which decay
exponentially as we move away from the junction represented by the location of
the delta-function potential. Investigating the possibility of the appearance
of bound states, we find that their energies appear as pairs of $\pm |E_b |$,
reflecting the presence of the imaginary-valued wavevectors at both positive
and negative values of energies of the propagating quasiparticles.",['cond-mat.mes-hall'],2503.06556," In the context of a growing interdisciplinary interest in the angular
momentum of wave fields, the spin-wave case has yet to be fully explored, with
the extensively studied notion of spin transport being only part of the broader
picture. Here we report experimental evidence for magnon orbital angular
momentum, demonstrating that the mode exhibits rotation rather than remaining
stationary. This conclusion is drawn from observations of the lifted degeneracy
of waves with counter-rotating wave fronts. This requires an unambiguous
formulation of spin and orbital angular momenta for spin waves, which we
provide in full generality based on a systematic application of quantum field
theory techniques. The results unequivocally establish magnetic dipole-dipole
interactions as a magnetic-field controllable spin-orbit interaction for
magnons. Our findings open a new research direction, leveraging the
spectroscopic readability of angular momentum for azimuthal spin waves and
other related systems.",['cond-mat.mes-hall'],False,,,,"Delta-function-potential junctions with quasiparticles occupying tilted
  bands with quadratic-in-momentum dispersion",The Orbital Angular Momentum of Azimuthal Spin-Waves
neg-d2-559,2025-03-13,,2503.10441," We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density
sub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of
observational data (2009-2022), our study incorporates 7 new ground-based
photometric transit observations, three sectors of Transiting Exoplanet Survey
Satellite (TESS) data, and 23 previously published light curves. A total of 46
light curves were analyzed using various analytical models, such as linear,
orbital decay, apsidal precession, and sinusoidal models to investigate the
presence of additional planets. The stellar tidal quality factor ($Q_\star'
\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay
model an unlikely explanation. The apsidal precession model with a $\chi_r^2$
of 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession
rate of 0.0045 rad/epoch. Frequency analysis using the Generalized Lomb-Scargle
(GLS) periodogram identified a significant periodic signal at 0.00415
cycles/day (FAP = 5.1$\times$10$^{-6}$ %), suggesting the influence of an
additional planetary companion. The sinusoidal model provides the lowest
reduced chi-squared value ($\chi_r^2$) of 3.2. Sinusoidal fitting of the timing
residuals estimated this companion to have a mass of approximately 0.02 $M_J$ ,
assuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.
Additionally, the Applegate mechanism, with an amplitude much smaller than the
observed TTV amplitude of 156 s, confirms that stellar activity is not
responsible for the observed variations.",['astro-ph.EP'],2503.05433," The IRAC camera on the Spitzer Space Telescope observed 2175 Near Earth
Objects (NEOs) during its Warm Mission phase, primarily in three large surveys,
and also in a small number of a dedicated projects. In this paper we present
the final reprocessing of the NEO data and determine fluxes at 3.6 microns
(where available) and 4.5 microns. The observing windows range from minutes to
nearly ten hours, which means that for 39 NEOs we observe a complete
lightcurve, and for these objects we present period and amplitude estimates and
derive minimum cohesive strengths for the objects with well-determined periods.
For an additional 128 objects we detect a significant fraction of a complete
lightcurve, and present estimated lower limits to their rotation periods. This
paper presents the final and definitive Spitzer/IRAC NEO flux catalog.",['astro-ph.EP'],False,,,,Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b,"Infrared Fluxes and Light Curves of Near-Earth Objects: The full Spitzer
  Sample"
neg-d2-560,2025-02-05,,2502.03661," While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.",['q-bio.TO'],2502.03661," While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.",['q-bio.TO'],False,,,,"Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues","Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues"
neg-d2-561,2025-02-01,,2502.00337," Exotic nondiffusive heat transfer regimes such as the second sound, where
heat propagates as a damped wave at speeds comparable to those of mechanical
disturbances, often occur at cryogenic temperatures (T) and nanosecond
timescales in semiconductors. First-principles prediction of such rapid, low-T
phonon dynamics requires finely-resolved temporal tracking of large, dense, and
coupled linear phonon dynamical systems arising from the governing linearized
Peierls-Boltzmann equation (LPBE). Here, we uncover a rigorous low-rank
representation of these linear dynamical systems, derived from the spectral
properties of the phonon collision matrix, that accelerates the
first-principles prediction of phonon dynamics by a factor of over a million
without compromising on the computational accuracy. By employing this low-rank
representation of the LPBE, we predict strong amplification of the wave-like
second sound regime upon isotopic enrichment in diamond - a finding that would
have otherwise been computationally intractable using the conventional
brute-force approaches. Our framework enables a rapid and accurate discovery of
the conditions under which wave-like heat flow can be realized in common
semiconductors.",['cond-mat.mtrl-sci'],2502.2008," Recently synthesized Porous 12-Atom-Wide Armchair Graphene Nanoribbons Nano
Lett. 2024, 24, 10718-10723 exhibit tunable properties through periodic
porosity, enabling precise control over their electronic, optical, thermal, and
mechanical behavior. This work presents a comprehensive theoretical
characterization of pristine and porous 12-AGNRs based on density functional
theory (DFT) and molecular dynamics (MD) simulations. DFT calculations reveal
substantial electronic modifications, including band gap widening and the
emergence of localized states. Analyzed within the Bethe-Salpeter equation
(BSE) framework, optical properties highlight strong excitonic effects and
significant absorption shifts. Thermal transport simulations indicate a
pronounced reduction in conductivity due to enhanced phonon scattering at
nanopores. At the same time, MD-based mechanical analysis shows decreased
stiffness and strength while maintaining structural integrity. Despite these
modifications, porous 12-AGNRs remain mechanically and thermally stable. These
findings establish porosity engineering as a powerful strategy for tailoring
graphene nanoribbons' functional properties, reinforcing their potential for
nanoelectronic, optoelectronic, and thermal management applications.",['cond-mat.mtrl-sci'],False,,,,"Efficient calculation of phonon dynamics through a low-rank solution of
  the Boltzmann equation","Computational Characterization of the Recently Synthesized Pristine and
  Porous 12-Atom-Wide Armchair Graphene Nanoribbon"
neg-d2-562,2025-03-19,,2503.15687," In 1990 Kantor introduced the conservative algebra $\mathcal{W}(n)$ of all
algebras (i.e. bilinear maps) on the $n$-dimensional vector space. In case $n
>1$ the algebra $\mathcal{W}(n)$ does not belong to well known classes of
algebras (such as associative, Lie, Jordan, Leibniz algebras). We describe
$\frac{1}{2}$derivations, local (resp. $2$-local) $\frac{1}{2}$-derivations and
biderivations of $\mathcal{W}(2)$. We also study similar problems for the
algebra $\mathcal{W}_2$ of all commutative algebras on the two-dimensional
vector space and the algebra $\mathcal{S}_2$ of all commutative algebras with
trace zero multiplication on the two-dimensional space.",['math.RA'],2503.12268," In this note, we show that every Noetherian graded ring with an affine degree
zero part is affine. As a result, a Noetherian graded Hopf algebra whose degree
zero component is a commutative or a cocommutative Hopf subalgebra is affine.
Moreover, we show that the braided Hopf algebra of a Noetherian graded Hopf
algebra is affine.",['math.RA'],False,,,,On conservative algebras of 2-dimensional Algebras,"Affineness on Noetherian graded rings, algebras and Hopf algebras"
neg-d2-563,2025-02-24,,2502.16924," Large Language Model (LLM)-based cold-start recommendation systems continue
to face significant computational challenges in billion-scale scenarios, as
they follow a ""Text-to-Judgment"" paradigm. This approach processes user-item
content pairs as input and evaluates each pair iteratively. To maintain
efficiency, existing methods rely on pre-filtering a small candidate pool of
user-item pairs. However, this severely limits the inferential capabilities of
LLMs by reducing their scope to only a few hundred pre-filtered candidates. To
overcome this limitation, we propose a novel ""Text-to-Distribution"" paradigm,
which predicts an item's interaction probability distribution for the entire
user set in a single inference. Specifically, we present FilterLLM, a framework
that extends the next-word prediction capabilities of LLMs to billion-scale
filtering tasks. FilterLLM first introduces a tailored distribution prediction
and cold-start framework. Next, FilterLLM incorporates an efficient
user-vocabulary structure to train and store the embeddings of billion-scale
users. Finally, we detail the training objectives for both distribution
prediction and user-vocabulary construction. The proposed framework has been
deployed on the Alibaba platform, where it has been serving cold-start
recommendations for two months, processing over one billion cold items.
Extensive experiments demonstrate that FilterLLM significantly outperforms
state-of-the-art methods in cold-start recommendation tasks, achieving over 30
times higher efficiency. Furthermore, an online A/B test validates its
effectiveness in billion-scale recommendation systems.",['cs.IR'],2502.16924," Large Language Model (LLM)-based cold-start recommendation systems continue
to face significant computational challenges in billion-scale scenarios, as
they follow a ""Text-to-Judgment"" paradigm. This approach processes user-item
content pairs as input and evaluates each pair iteratively. To maintain
efficiency, existing methods rely on pre-filtering a small candidate pool of
user-item pairs. However, this severely limits the inferential capabilities of
LLMs by reducing their scope to only a few hundred pre-filtered candidates. To
overcome this limitation, we propose a novel ""Text-to-Distribution"" paradigm,
which predicts an item's interaction probability distribution for the entire
user set in a single inference. Specifically, we present FilterLLM, a framework
that extends the next-word prediction capabilities of LLMs to billion-scale
filtering tasks. FilterLLM first introduces a tailored distribution prediction
and cold-start framework. Next, FilterLLM incorporates an efficient
user-vocabulary structure to train and store the embeddings of billion-scale
users. Finally, we detail the training objectives for both distribution
prediction and user-vocabulary construction. The proposed framework has been
deployed on the Alibaba platform, where it has been serving cold-start
recommendations for two months, processing over one billion cold items.
Extensive experiments demonstrate that FilterLLM significantly outperforms
state-of-the-art methods in cold-start recommendation tasks, achieving over 30
times higher efficiency. Furthermore, an online A/B test validates its
effectiveness in billion-scale recommendation systems.",['cs.IR'],False,,,,"FilterLLM: Text-To-Distribution LLM for Billion-Scale Cold-Start
  Recommendation","FilterLLM: Text-To-Distribution LLM for Billion-Scale Cold-Start
  Recommendation"
neg-d2-564,2025-03-12,,2503.09103," Recent advancements in Large Language Models (LLMs) have significantly
improved text generation capabilities. However, they also present challenges,
particularly in generating vaccine-related misinformation, which poses risks to
public health. Despite research on human-authored misinformation, a notable gap
remains in understanding how LLMs contribute to vaccine misinformation and how
best to detect it. Existing benchmarks often overlook vaccine-specific
misinformation and the diverse roles of misinformation spreaders. This paper
introduces VaxGuard, a novel dataset designed to address these challenges.
VaxGuard includes vaccine-related misinformation generated by multiple LLMs and
provides a comprehensive framework for detecting misinformation across various
roles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other
LLMs in detecting misinformation, especially when dealing with subtle or
emotionally charged narratives. On the other hand, PHI3 and Mistral show lower
performance, struggling with precision and recall in fear-driven contexts.
Additionally, detection performance tends to decline as input text length
increases, indicating the need for improved methods to handle larger content.
These results highlight the importance of role-specific detection strategies
and suggest that VaxGuard can serve as a key resource for improving the
detection of LLM-generated vaccine misinformation.",['cs.CL'],2502.14451," Natural Language Generation (NLG) popularity has increased owing to the
progress in Large Language Models (LLMs), with zero-shot inference
capabilities. However, most neural systems utilize decoder-only causal
(unidirectional) transformer models, which are effective for English but may
reduce the richness of languages with less strict word order, subject omission,
or different relative clause attachment preferences. This is the first work
that analytically addresses optimal text generation order for non-causal
language models. We present a novel Viterbi algorithm-based methodology for
maximum likelihood word order estimation. We analyze the non-causal
most-likelihood order probability for NLG in Spanish and, then, the probability
of generating the same phrases with Spanish causal NLG. This comparative
analysis reveals that causal NLG prefers English-like SVO structures. We also
analyze the relationship between optimal generation order and causal
left-to-right generation order using Spearman's rank correlation. Our results
demonstrate that the ideal order predicted by the maximum likelihood estimator
is not closely related to the causal order and may be influenced by the
syntactic structure of the target sentence.",['cs.CL'],False,,,,"VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for
  Detecting LLM-Generated Vaccine Misinformation","Optimal word order for non-causal text generation with Large Language
  Models: the Spanish case"
neg-d2-565,2025-01-25,,2501.15145," Current application designers have moved to integrate large language models
(LLMs) into their products. These LLM-integrated applications are vulnerable to
prompt injection vulnerabilities. While attempts have been made to address this
problem by building a detector that can monitor inputs to the LLM and detect
attacks, we find that many detectors are not yet suitable for practical
deployment. To support research in this area, we design the PromptShield
benchmark for evaluating practical prompt injection detectors. We also
construct a new detector, the PromptShield detector, which achieves
significantly better performance at detecting prompt injection attacks than any
prior scheme. Our work suggests that larger models, more training data,
appropriate metrics, and careful curation of training data can contribute to
strong detector performance.",['cs.CR'],2503.02499," CONTEXT. Attack treesare a recommended threat modeling tool, but there is no
established method to compare them. OBJECTIVE. We aim to establish a method to
compare ""real"" attack trees, based on both the structure of the tree itself and
the meaning of the node labels. METHOD. We define four methods of comparison
(three novel and one established) and compare them to a dataset of attack trees
created from a study run on students (n = 39). These attack trees all follow
from the same scenario, but have slightly different labels. RESULTS. We find
that applying semantic similarity as a means of comparing node labels is a
valid approach. Further, we find that treeedit distance (established) and
radical distance (novel) are themost promising methods of comparison in most
circumstances. CONCLUSION. We show that these two methods are valid as means of
comparing attack trees, and suggest a novel technique for using semantic
similarity to compare node labels. We further suggest that these methods can be
used to compare attack trees in a real-world scenario, and that they can be
used to identify similar attack trees.",['cs.CR'],False,,,,PromptShield: Deployable Detection for Prompt Injection Attacks,"Attack Tree Distance: a practical examination of tree difference
  measurement within cyber security"
neg-d2-566,2025-03-14,,2503.11333," Nodal noncentrosymmetric superconductors can host zero-energy flat bands of
Majorana surface states within the projection of the nodal lines onto the
surface Brillouin zone. Thus, these systems can have stationary, localized
Majorana wave packets on certain surfaces, which may be a promising platform
for quantum computation. Such applications require protocols to manipulate the
wave packets in order to move them without destroying their localization or
coherence. As a step in this direction, we explore the idea that the surface
states have a nontrivial spin polarization, which can couple for example to the
magnetization of a ferromagnetic insulator in contact to the surface, via an
exchange term in the Hamiltonian. Such a coupling can make the previously flat
bands weakly dispersive. We aim to model the motion of spatially localized wave
packets under the influence of an exchange field which is changed
adiabatically. We calculate the time-evolved wave packet for a model system and
discuss which factors influence the direction of motion and the broadening of
the wave packet.",['cond-mat.supr-con'],2501.14876," The muon spin rotation ($\mu$SR) technique has been applied to determine the
behavior of the in-plane magnetic penetration depth ($\lambda_{ab}$) in the
vortex state of the unconventional superconductor Sr$_2$RuO$_4$ as a means of
gaining insight into its still unknown superconducting order parameter. A
recent $\mu$SR study of Sr$_2$RuO$_4$ reported a $T$-linear temperature
dependence for $\lambda_{ab}$ at low temperatures that was not identified in an
earlier $\mu$SR study. Here we show that there is no significant difference
between the data in the early and recent $\mu$SR studies and both are
compatible with the limiting low-temperature $\lambda_{ab} \sim T^2$ dependence
expected from measurements of the change in $\lambda_{ab}(T)$ in the Meissner
state by other techniques. However, we argue that at this time there is no
valid theoretical model for reliably determining the absolute value of
$\lambda_{ab}$ in Sr$_2$RuO$_4$ from $\mu$SR measurements. Instead, we identify
the formation of an unusual square vortex lattice that introduces a new
constraint on candidate superconducting order parameters for Sr$_2$RuO$_4$.",['cond-mat.supr-con'],False,,,,"Manipulation of Majorana wave packets at surfaces of nodal
  noncentrosymmetric superconductors","Atypical vortex lattice and the magnetic penetration depth in
  superconducting Sr$_2$RuO$_4$ deduced by $\mu$SR"
neg-d2-567,2025-01-15,,2501.08837," Long-term dense action anticipation is very challenging since it requires
predicting actions and their durations several minutes into the future based on
provided video observations. To model the uncertainty of future outcomes,
stochastic models predict several potential future action sequences for the
same observation. Recent work has further proposed to incorporate uncertainty
modelling for observed frames by simultaneously predicting per-frame past and
future actions in a unified manner. While such joint modelling of actions is
beneficial, it requires long-range temporal capabilities to connect events
across distant past and future time points. However, the previous work
struggles to achieve such a long-range understanding due to its limited and/or
sparse receptive field. To alleviate this issue, we propose a novel MANTA
(MAmba for ANTicipation) network. Our model enables effective long-term
temporal modelling even for very long sequences while maintaining linear
complexity in sequence length. We demonstrate that our approach achieves
state-of-the-art results on three datasets - Breakfast, 50Salads, and
Assembly101 - while also significantly improving computational and memory
efficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .",['cs.CV'],2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],False,,,,"MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation",Interpretable Failure Detection with Human-Level Concepts
neg-d2-568,2025-01-06,,2501.03125," We revise the role that ultraviolet divergences of quantum fields play in
slow-roll inflation, and discuss the renormalization of cosmological
observables from a space-time perspective, namely the angular power spectrum.
We first derive an explicit expression for the multipole coefficients
$C_{\ell}$ in the Sachs-Wolfe regime in terms of the two-point function of
primordial perturbations. We then analyze the ultraviolet behavior, and point
out that the standard result in the literature is equivalent to a
renormalization of $C_{\ell}$ at zero ``adiabatic'' order. We further argue
that renormalization at second ``adiabatic'' order may be more appropriate from
the viewpoint of standard quantum field theory. This may change significantly
the predictions for $C_{\ell}$, while maintaining scale invariance.",['gr-qc'],2502.06451," The population of black holes observed via gravitational waves currently
covers the local universe up to a redshift $z\lesssim 1$, for the most massive
merging binaries, or $z\lesssim 0.25$ for low-mass BH binaries (BBH). Evolution
of the BBH mass spectrum over cosmic time will be a significant probe of
formation channels and environments. We demonstrate a reconstruction of the BBH
merger rate, allowing for general dependence on binary masses and luminosity
distance or redshift and accounting for selection effects, via iterative kernel
density estimation (KDE) with optimized multidimensional bandwidths. Performing
such reconstructions under a range of detailed assumptions, we see no
significant evidence for the evolution of BBH masses with redshift, over the
range where detected events are available; at most, a possible trend towards
increasing merger rate with redshift for primary masses $m_1\gtrsim
50\,M_\odot$ is supported. We compare these findings with previous
investigations and caution against over-interpreting the current, sparse, data.
Significantly upgraded detectors and/or facilities, and longer observing times,
are required to harness any correlations of the BBH mass distribution with
redshift.",['gr-qc'],False,,,,"On the renormalization of ultraviolet divergences in the inflationary
  angular power spectrum","Looking To The Horizon: Probing Evolution in the Black Hole Spectrum
  With GW Catalogs"
neg-d2-569,2025-02-23,,2502.16832," Federated learning (FL) has shown great potential in medical image computing
since it provides a decentralized learning paradigm that allows multiple
clients to train a model collaboratively without privacy leakage. However,
current studies have shown that data heterogeneity incurs local learning bias
in classifiers and feature extractors of client models during local training,
leading to the performance degradation of a federation system. To address these
issues, we propose a novel framework called Federated Bias eliMinating (FedBM)
to get rid of local learning bias in heterogeneous federated learning (FL),
which mainly consists of two modules, i.e., Linguistic Knowledge-based
Classifier Construction (LKCC) and Concept-guided Global Distribution
Estimation (CGDE). Specifically, LKCC exploits class concepts, prompts and
pre-trained language models (PLMs) to obtain concept embeddings. These
embeddings are used to estimate the latent concept distribution of each class
in the linguistic space. Based on the theoretical derivation, we can rely on
these distributions to pre-construct a high-quality classifier for clients to
achieve classification optimization, which is frozen to avoid classifier bias
during local training. CGDE samples probabilistic concept embeddings from the
latent concept distributions to learn a conditional generator to capture the
input space of the global model. Three regularization terms are introduced to
improve the quality and utility of the generator. The generator is shared by
all clients and produces pseudo data to calibrate updates of local feature
extractors. Extensive comparison experiments and ablation studies on public
datasets demonstrate the superior performance of FedBM over state-of-the-arts
and confirm the effectiveness of each module, respectively. The code is
available at https://github.com/CUHK-AIM-Group/FedBM.",['cs.CV'],2502.14282," In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code is available at
https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.",['cs.CV'],False,,,,"FedBM: Stealing Knowledge from Pre-trained Language Models for
  Heterogeneous Federated Learning","PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC"
neg-d2-570,2025-01-22,,2501.13169," Understanding solar turbulent convection and its influence on differential
rotation has been a challenge over the past two decades. Current models often
overestimate giant convection cells amplitude, leading to an effective Rossby
number too large and a shift towards an anti-solar rotation regime. This
Convective Conundrum, underscores the need for improved comprehension of solar
convective dynamics. We propose a numerical experiment in the parameter space
that controls $Ro$ while increasing the Reynolds number ($Re$) and maintaining
solar parameters. By controlling the Nusselt number ($Nu$), we limit the energy
transport by convection while reducing viscous dissipation. This approach
enabled us to construct a Sun-like rotating model (SBR97n035) with strong
turbulence ($Re \sim 800$) that exhibits prograde equatorial rotation and
aligns with observational data from helioseismology. We compare this model with
an anti-solar rotating counterpart, and provide an in-depth spectral analysis
to investigate the changes in convective dynamics. We also find the appearance
of vorticity rings near the poles, which existence on the Sun could be probed
in the future. The Sun-like model shows reduced buoyancy over the spectrum, as
well as an extended quasi-geostrophic equilibrium towards smaller scales. This
promotes a Coriolis-Inertia (CI) balance rather than a
Coriolis-Inertia-Archimedes (CIA) balance, in order to favor the establishment
of a prograde equator. The presence of convective columns in the bulk of the
convection zone, with limited surface manifestations, also hints at such
structures potentially occurring in the Sun.",['astro-ph.SR'],2502.11838," The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is
among the early evidences that they contain multiple populations of stars.
Indeed, the location of each star along the HB depends both on its initial
helium content (Y) and on the global average mass loss along the red giant
branch ($\mu$). In most GCs, it is generally straightforward to analyse the
first stellar population (standard Y), and the most extreme one (largest Y),
while it is more tricky to look at the ""intermediate"" populations (mildly
enhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever
possible the helium abundance for each stellar populations is constrained by
using independent measurements present in the literature. We compare population
synthesis models with photometric catalogues from the Hubble Space Telescope
Treasury survey to derive the parameters of these HB stars. We find that the
location of helium enriched stars on the HB is reproduced only by adopting a
higher value of $\mu$ with respect to the first generation stars in all the
analysed stellar populations. We also find that $\mu$ correlates with the
helium enhancement of the populations. This holds for both clusters. This
finding is naturally predicted by the model of ''pre-main sequence disc early
loss'', previously suggested in the literature, and is consistent with the
findings of multiple-populations formation models that foresee the formation of
second generation stars in a cooling flow.",['astro-ph.SR'],False,,,,"Global Turbulent Solar Convection: a Numerical Path Investigating Key
  Force Balances in the context of the Convective Conundrum","Mass loss along the red giant branch of the intermediate stellar
  populations in NGC6752 and NGC2808"
neg-d2-571,2025-01-28,,2501.17407," In quantum mechanics time is generally treated as a parameter rather than an
observable. For instance wave functions are treated as extending in space, but
not in time. But from relativity we expect time and space should be treated on
the same basis. What are the effects if time is an observable? Are these
effects observable with current technology?
  In earlier work we showed we should see effects in various high energy
scattering processes. We here extend that work to include bound states. The
critical advantage of working with bound states is that the predictions are
significantly more definite, taking the predictions from testable to
falsifiable.
  We estimate the time dispersion for hydrogen as $.177$ attoseconds, possibly
below the current threshold for detection. But the time dispersion should scale
as the $3/2$ power of the principle quantum number $n$. Rydberg atoms can have
$n$ of order $100$, implying a boost by a factor of $1000$. This takes the the
time dispersion to $177$ attoseconds, well within reach of current technology.
  There are a wide variety of experimental targets: any time-dependent
processes should show effects. Falsification will be technically challenging
(due to the short time scales) but immediate and unambiguous. Confirmation
would have significant implications for attosecond physics, quantum computing
and communications, quantum gravity, and the measurement problem. And would
suggest practical uses in these areas as well as circuit design, high speed
biochemistry, cryptography, fusion research, and any area involving change at
attosecond time scales.",['quant-ph'],2503.1183," We study the time-optimal robust control of a two-level quantum system
subjected to field inhomogeneities. We apply the Pontryagin Maximum Principle
and we introduce a reduced space onto which the optimal dynamics is projected
down. This reduction leads to a complete analytical derivation of the optimal
solution in terms of elliptic functions and elliptic integrals. Necessary
optimality conditions are then obtained for the original system. These
conditions are verified numerically and lead to the optimal control protocol.
Various examples, ranging from state-to-state transfer to the generation of a
Not gate, illustrate this study. The connection with other geometric
optimization approaches that have been used to solve this problem is also
discussed.",['quant-ph'],False,,,,Time dispersion in bound states,"Application of the Pontryagin Maximum Principle to the robust
  time-optimal control of two-level quantum systems"
neg-d2-572,2025-01-21,,2501.1215," Deferred neural rendering (DNR) is an emerging computer graphics pipeline
designed for high-fidelity rendering and robotic perception. However, DNR
heavily relies on datasets composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the rendering
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
rendering. We first propose a novel view selector for deferred neural rendering
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
rendering. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity rendering results. We conduct detailed experiments and
ablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness
of DNRSelect. The code will be released.",['cs.CV'],2503.14607," In this paper, we introduce MapBench-the first dataset specifically designed
for human-readable, pixel-based map-based outdoor navigation, curated from
complex path finding scenarios. MapBench comprises over 1600 pixel space map
path finding problems from 100 diverse maps. In MapBench, LVLMs generate
language-based navigation instructions given a map image and a query with
beginning and end landmarks. For each map, MapBench provides Map Space Scene
Graph (MSSG) as an indexing data structure to convert between natural language
and evaluate LVLM-generated results. We demonstrate that MapBench significantly
challenges state-of-the-art LVLMs both zero-shot prompting and a
Chain-of-Thought (CoT) augmented reasoning framework that decomposes map
navigation into sequential cognitive processes. Our evaluation of both
open-source and closed-source LVLMs underscores the substantial difficulty
posed by MapBench, revealing critical limitations in their spatial reasoning
and structured decision-making capabilities. We release all the code and
dataset in https://github.com/taco-group/MapBench.",['cs.CV'],False,,,,DNRSelect: Active Best View Selection for Deferred Neural Rendering,Can Large Vision Language Models Read Maps Like a Human?
neg-d2-573,2025-01-29,,2501.17549," Graph-structured data plays a vital role in numerous domains, such as social
networks, citation networks, commonsense reasoning graphs and knowledge graphs.
While graph neural networks have been employed for graph processing, recent
advancements have explored integrating large language models for graph-based
tasks. In this paper, we propose a novel approach named Learnable Graph Pooling
Token (LGPT), which addresses the limitations of the scalability issues in
node-level projection and information loss in graph-level projection. LGPT
enables flexible and efficient graph representation by introducing learnable
parameters that act as tokens in large language models, balancing fine-grained
and global graph information. Additionally, we investigate an Early Query
Fusion technique, which fuses query context before constructing the graph
representation, leading to more effective graph embeddings. Our method achieves
a 4.13\% performance improvement on the GraphQA benchmark without training the
large language model, demonstrating significant gains in handling complex
textual-attributed graph data.",['cs.CL'],2502.11008," Counterfactual reasoning is widely recognized as one of the most challenging
and intricate aspects of causality in artificial intelligence. In this paper,
we evaluate the performance of large language models (LLMs) in counterfactual
reasoning. In contrast to previous studies that primarily focus on commonsense
causal reasoning, where LLMs often rely on prior knowledge for inference, we
specifically assess their ability to perform counterfactual inference using a
set of formal rules. To support this evaluation, we introduce a new benchmark
dataset, CounterBench, comprising 1K counterfactual reasoning questions. The
dataset is designed with varying levels of difficulty, diverse causal graph
structures, distinct types of counterfactual questions, and multiple
nonsensical name variants. Our experiments demonstrate that counterfactual
reasoning poses a significant challenge for LLMs, with most models performing
at levels comparable to random guessing. To enhance LLM's counterfactual
reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides
LLMs through iterative reasoning and backtracking to systematically explore
counterfactual solutions. Experimental results show that our method
significantly improves LLM performance on counterfactual reasoning tasks and
consistently enhances performance across different LLMs.Our dataset is
available at https://huggingface.co/datasets/CounterBench/CounterBench.",['cs.CL'],False,,,,"Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language
  Models","CounterBench: A Benchmark for Counterfactuals Reasoning in Large
  Language Models"
neg-d2-574,2025-03-07,,2503.06005," We consider the linearized perturbations of near-horizon extremal
Reissner-Nordstr\""om black holes in $d$-dimensional
Einstein-Maxwell-Gauss-Bonnet gravity and seven-dimensional third-order
Lovelock gravity. We find the solutions for the gravitational perturbations as
a function of the higher-derivative coupling coefficients, which we treat
nonperturbatively. Consequently, we observe a breakdown in perturbation theory
for large harmonics for the six-derivative corrections.",['hep-th'],2502.06928," We derive a formula for the half-BPS interface entropy between any pair of
${\cal N}=(4,4)$ theories on the same conformal manifold. This generalizes the
diastasis formula derived in arXiv:1311.2202 for ${\cal N}=(2,2)$ theories,
which is restricted to the conformal submanifolds generated by either chiral or
twisted chiral multiples of ${\cal N}=(2,2)$ supersymmetry. To derive the
${\cal N}=(4,4)$ formula, we use the fact that the conformal manifold of ${\cal
N}=(4,4)$ theories is symmetric and quaternionic-K\""ahler and that its isotropy
group contains the $SU(2) \otimes SU(2)$ external automorphism of the ${\cal
N}=(4,4)$ superconformal algebra. As an application of the formula, we prove a
supersymmetric non-renormalization theorem, which explains the observation in
arXiv:1005.4433 that the interface entropy for half-BPS Janus solutions in type
IIB supergravity on ${\it AdS}_3 \times S^3 \times T^4$ coincides with the
corresponding quantity in their free conformal field limits.",['hep-th'],False,,,,Remarks on nonperturbative perturbations,"Nonrenormalization Theorem for ${\cal N}=(4,4)$ Interface Entropy"
neg-d2-575,2025-01-31,,2501.18979," The electric dipole moments~(EDM) of fundamental particles inherently violate
parity~(P) and time-reversal~(T) symmetries. By virtue of the CPT theorem in
quantum field theory, the latter also implies the violation of the combined
charge-conjugation and parity~(CP) symmetry. We aim to measure the EDM of the
muon using the frozen-spin technique within a compact storage trap. This method
exploits the high effective electric field, \$E \approx 165\$ MV/m, experienced
in the rest frame of the muon with a momentum of about 23 MeV/c when it passes
through a solenoidal magnetic field of \$|\vec{B}|=2.5\$ T. In this paper, we
outline the fundamental considerations for a muon EDM search and present a
conceptual design for a demonstration experiment to be conducted at secondary
muon beamlines of the Paul Scherrer Institute in Switzerland. In Phase~I, with
an anticipated data acquisition period of 200 days, the expected sensitivity to
a muon EDM is 4E-21 ecm. In a subsequent phase, Phase~II, we propose to improve
the sensitivity to 6E-23 ecm using a dedicated instrument installed on a
different beamline that produces muons of momentum 125 MeV/c}.",['hep-ex'],2503.17186," This paper presents searches for the direct pair production of charged
light-flavour sleptons, each decaying into a stable neutralino and an
associated Standard Model lepton. The analyses focus on the challenging
""corridor"" region, where the mass difference, $\Delta m$, between the slepton
($\tilde{e}$ or $\tilde{\mu}$) and the lightest neutralino
($\tilde{\chi}^{0}_{1}$) is less or similar to the mass of the $W$ boson,
$m(W)$, with the aim to close a persistent gap in sensitivity to models with
$\Delta m \lesssim m(W)$. Events are required to contain a high-energy jet,
significant missing transverse momentum, and two same-flavour opposite-sign
leptons ($e$ or $\mu$). The analysis uses $pp$ collision data at $\sqrt{s} =
13$ TeV recorded by the ATLAS detector, corresponding to an integrated
luminosity of 140 fb$^{-1}$. Several kinematic selections are applied,
including a set of boosted decision trees. These are each optimised for
different $\Delta m$ to provide expected sensitivity for the first time across
the full $\Delta m$ corridor. The results are generally consistent with the
Standard Model, with the most significant deviations observed with a local
significance of 2.0 $\sigma$ in the selectron search, and 2.4 $\sigma$ in the
smuon search. While these deviations weaken the observed exclusion reach in
some parts of the signal parameter space, the previously present sensitivity
gap to this corridor is largely reduced. Constraints at the 95% confidence
level are set on simplified models of selectron and smuon pair production,
where selectrons (smuons) with masses up to 300 (350) GeV can be excluded for
$\Delta m$ between 2 GeV and 100 GeV.",['hep-ex'],False,,,,"A compact frozen-spin trap for the search for the electric dipole moment
  of the muon","Searches for direct slepton production in the compressed-mass corridor
  in $\sqrt{s}=13$ TeV $pp$ collisions with the ATLAS detector"
neg-d2-576,2025-03-12,,2503.09533," Designing strategyproof mechanisms for multi-facility location that optimize
social costs based on agent preferences had been challenging due to the
extensive domain knowledge required and poor worst-case guarantees. Recently,
deep learning models have been proposed as alternatives. However, these models
require some domain knowledge and extensive hyperparameter tuning as well as
lacking interpretability, which is crucial in practice when transparency of the
learned mechanisms is mandatory. In this paper, we introduce a novel approach,
named LLMMech, that addresses these limitations by incorporating large language
models (LLMs) into an evolutionary framework for generating interpretable,
hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.
Our experimental results, evaluated on various problem settings where the
social cost is arbitrarily weighted across agents and the agent preferences may
not be uniformly distributed, demonstrate that the LLM-generated mechanisms
generally outperform existing handcrafted baselines and deep learning models.
Furthermore, the mechanisms exhibit impressive generalizability to
out-of-distribution agent preferences and to larger instances with more agents.",['cs.LG'],2502.06738," Recent work showed that small changes in benchmark questions can reduce LLMs'
reasoning and recall. We explore two such changes: pairing questions and adding
more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We
find that for more capable models, these predictably reduce performance,
essentially heightening the performance ceiling of a benchmark and unsaturating
it again. We suggest this approach can resurrect old benchmarks.",['cs.LG'],False,,,,Large Language Models for Multi-Facility Location Mechanism Design,Resurrecting saturated LLM benchmarks with adversarial encoding
neg-d2-577,2025-03-13,,2503.10275," In recent times, bound soliton states have often been referred to as soliton
molecules in the nonlinear optics literature. The striking analogies between
photonic bound states and matter molecular structures in chemistry and physics
have intensified studies on optical soliton molecules in both conservative and
dissipative systems. In this paper, we demonstrate the existence of vector
soliton molecules and their related isomer structures in a conservative optical
fiber system by considering the integrable Manakov equation. We show their
existence by applying the velocity resonance condition and appropriate choice
of temporal separations to the degenerate $N=(\bar{N}+\bar{M})$-soliton
solution. Then, we classify the obtained molecular states as either dissociated
or synthesized molecular states based on the temporal locations of the
constituent solitons. Furthermore, we analyze the collision properties of
vector soliton molecules in the present conservative system. The collision
scenarios reveal that the soliton molecules undergo intriguing energy-sharing
collisions through energy redistribution among the modes. To characterize these
collisions, we have carried out an appropriate asymptotic analysis and found
that elastic collisions arise as a special case of energy-sharing collisions
under specific choices of polarization constants. Finally, we numerically
verify the robustness of vector soliton molecules. We believe that the results
presented in this paper show potential for soliton molecule-based applications
such as optical computation and multi-level encoding for communications.",['nlin.PS'],2502.14695," The breather solution found by M. Tajiri and Y. Murakami for the Boussinesq
equation is studied analytically. The new parameterization of the solution is
proposed, allowing us to find exactly the existence boundary of the Boussinesq
breather and to show that such a nonlinear excitation emerges from the linear
localized mode of the kink solution corresponding to a shock wave analog in a
crystal. We explicitly find the first integrals, namely the energy and the
field momentum, and faithfully construct the adiabatic invariant for the
Boussinesq breather. As a result, we carry out the quasiclassical quantization
of the nonlinear oscillating solution, obtaining its energy spectrum, i.e., the
energy dependence on the momentum and the number of states, and reveal the
Hamiltonian equations for this particle-like excitation.",['nlin.PS'],False,,,,Vector soliton molecules and their collisions,"Quasiclassical quantization of the Boussinesq breather emerging from the
  kink localized mode"
neg-d2-578,2025-01-08,,2501.04364," The underlying data source for web usage mining (WUM) is commonly thought to
be server logs. However, access log files ensure quite limited data about the
clients. Identifying sessions from this messy data takes a considerable effort,
and operations performed for this purpose do not always yield excellent
results. Also, this data cannot be used for web analytics efficiently. This
study proposes an innovative method for user tracking, session management, and
collecting web usage data. The method is mainly based on a new approach for
using collected data for web analytics extraction as the data source in web
usage mining. An application-based API has been developed with a different
strategy from conventional client-side methods to obtain and process log data.
The log data has been successfully gathered by integrating the technique into
an enterprise web application. The results reveal that the homogeneous
structured data collected and stored with this method is more convenient to
browse, filter, and process than web server logs. This data stored on a
relational database can be used effortlessly as a reliable data source for
high-performance web usage mining activity, real-time web analytics, or a
functional recommendation system.",['cs.IR'],2502.05558," Modeling user behavior sequences in recommender systems is essential for
understanding user preferences over time, enabling personalized and accurate
recommendations for improving user retention and enhancing business values.
Despite its significance, there are two challenges for current sequential
modeling approaches. From the spatial dimension, it is difficult to mutually
perceive similar users' interests for a generalized intention understanding;
from the temporal dimension, current methods are generally prone to forgetting
long-term interests due to the fixed-length input sequence. In this paper, we
present Large Memory Network (LMN), providing a novel idea by compressing and
storing user history behavior information in a large-scale memory block. With
the elaborated online deployment strategy, the memory block can be easily
scaled up to million-scale in the industry. Extensive offline comparison
experiments, memory scaling up experiments, and online A/B test on Douyin
E-Commerce Search (ECS) are performed, validating the superior performance of
LMN. Currently, LMN has been fully deployed in Douyin ECS, serving millions of
users each day.",['cs.IR'],False,,,,"An innovative data collection method to eliminate the preprocessing
  phase in web usage mining",Large Memory Network for Recommendation
neg-d2-579,2025-01-18,,2501.10764," Photon-counting detector based computed tomography (PCCT) has greatly
advanced in recent years. However, the spectral inconsistency is still a
serious challenge for PCCT that could directly introduce obvious artifacts and
severe inaccuracies. This work attempts to overcome the challenge by modeling
the spectral inconsistency in a novel, unified, and two-term factorized
framework, with a spectral skew term independent of the energy threshold, and
an energy-threshold bias analytical characterization term. To solve the
spectral inconsistency, a two-step decomposition algorithm called
energy-threshold bias calculator (ETB-Cal) is derived here, in which the
spectral skew term is grossly determined at a relatively low energy threshold
and only the energy-threshold bias is needed to be calculated as the energy
threshold changes. After the two terms being computed out in calibration stage,
they will be incorporated into our spectral model to generate the spectral
correction vectors as well as the material decomposition vectors if needed, for
PCCT projection data. To validate our method, both numerical simulations
physics experiments were carried out on a tabletop PCCT system. Preliminary
results showed that the spectral inconsistency can be significantly reduced,
for example, with an non-uniformity quantitative indicators decreasing from
26.27 to 5.80 HU for Gammex multi-energy phantom and from 27.88 to 3.16 HU for
kyoto head phantom. The considerable improvements consistently demonstrate a
great potential of the proposed novel physics-model based correction scheme in
practical applications, as computationally efficient, calibration-wise
convenient with high degree of generality, and substantially avoiding the use
of X-ray florescence material in the energy-threshold calibration.",['physics.med-ph'],2501.13053," Boron Neutron Capture Therapy (BNCT) is a form of radiotherapy based on the
irradiation of the tumour with a low energy neutron beam, after the
administration of a selective drug enriched in boron-10. The therapy exploits
the high cross section of thermal neutron capture in boron, generating two
low-range charged particles. The availability of accelerators able to generate
high-intensity neutron beams via proton nuclear interaction is boosting the
construction of new clinical centres. One of these is under development in
Italy, using a 5 MeV, 30 mA proton radiofrequency accelerator coupled to a
beryllium target, funded by the Complementary Plan to the Recovery and
Resilience National Plan, under the project ANTHEM. The present study focuses
on radiation protection aspects of patients undergoing BNCT, specifically on
the activation of their organs and tissues. A criterion to establish the
relevance of such activation after BNCT has been proposed. Based on the current
Italian regulatory framework, the level of patient activation following BNCT
treatment does not pose a significant radiological concern, even shortly after
irradiation. Another aspect is the activation of patient's excretions, which
can impact on the design of the building and requires a process for the
discharge. The described study contributes to the radiation protection study
for the ANTHEM BNCT centre in Italy.",['physics.med-ph'],False,,,,"Energy-Threshold Bias Calculator: A Physics-Model Based Adaptive
  Correction Scheme for Photon-Counting CT","Evaluation of patient activation and dosimetry after Boron Neutron
  Capture Therapy"
neg-d2-580,2025-01-06,,2501.02862," We show that a substantial portion of stochastic calculus can be developed
along similar lines to ordinary calculus, with derivative-based concepts
driving the development. We define a notion of stopping derivative, which is a
form of right derivative with respect to stopping times. Using this, we define
the drift and variance rate of a process as stopping derivatives for
(generalised) conditional expectation and conditional variance respectively.
Applying elementary, derivative-based methods, we derive a calculus of rules
describing how drift and variance rate transform under constructions on
processes, culminating in a version of the multi-dimensional It\^o formula. Our
approach connects with the standard machinery of stochastic calculus via a
theorem establishing that continuous processes with zero drift coincide with
random translations of continuous local martingales. This equivalence allows us
to derive a Fundamental Theorem of Calculus for stopping derivatives, which
relates the quantities of drift and variance rate, defined as stopping
derivatives, to parameters used in the description of a process as a stochastic
integral.",['math.PR'],2502.0256," The automorphism group of a transitive graph defines a weight function on the
vertices through the Haar modulus. Benjamini, Lyons, Peres, and Schramm
introduced the notion of weighted-amenability for a transitive graph, which is
equivalent to the amenability of its automorphism group. We prove that this
property is equivalent to level-amenability, that is, the property that the
collection of vertices of weights in a given finite set always induces an
amenable graph. We then use this to prove a version of Hutchcroft's conjecture
about $p_h<p_u$, relaxed \`a la Pak-Smirnova-Nagnibeda, where $p_h$ is the
critical probability for the regime where clusters of infinite total weight
arise, and $p_u$ is the uniqueness threshold. Further characterizations are
given in terms of the spectral radius and invariant spanning forests. One
consequence is the continuity of the phase transition at $p_h$ for
weighted-nonamenable graphs.",['math.PR'],False,,,,Stochastic Calculus via Stopping Derivatives,Weighted-amenability and percolation
neg-d2-581,2025-03-19,,2503.15451," This paper addresses the challenge of text-conditioned streaming motion
generation, which requires us to predict the next-step human pose based on
variable-length historical motions and incoming texts. Existing methods
struggle to achieve streaming motion generation, e.g., diffusion models are
constrained by pre-defined motion lengths, while GPT-based methods suffer from
delayed response and error accumulation problem due to discretized non-causal
tokenization. To solve these problems, we propose MotionStreamer, a novel
framework that incorporates a continuous causal latent space into a
probabilistic autoregressive model. The continuous latents mitigate information
loss caused by discretization and effectively reduce error accumulation during
long-term autoregressive generation. In addition, by establishing temporal
causal dependencies between current and historical motion latents, our model
fully utilizes the available information to achieve accurate online motion
decoding. Experiments show that our method outperforms existing approaches
while offering more applications, including multi-round generation, long-term
generation, and dynamic motion composition. Project Page:
https://zju3dv.github.io/MotionStreamer/",['cs.CV'],2501.08174," Current Gaussian Splatting approaches are effective for reconstructing entire
scenes but lack the option to target specific objects, making them
computationally expensive and unsuitable for object-specific applications. We
propose a novel approach that leverages object masks to enable targeted
reconstruction, resulting in object-centric models. Additionally, we introduce
an occlusion-aware pruning strategy to minimize the number of Gaussians without
compromising quality. Our method reconstructs compact object models, yielding
object-centric Gaussian and mesh representations that are up to 96\% smaller
and up to 71\% faster to train compared to the baseline while retaining
competitive quality. These representations are immediately usable for
downstream applications such as appearance editing and physics simulation
without additional processing.",['cs.CV'],False,,,,"MotionStreamer: Streaming Motion Generation via Diffusion-based
  Autoregressive Model in Causal Latent Space","Object-Centric 2D Gaussian Splatting: Background Removal and
  Occlusion-Aware Pruning for Compact Object Models"
neg-d2-582,2025-02-19,,2502.13515," Ammonia, a versatile compound that can be used as a fertilizer, chemical or
fuel, has since long been produced through the energy-intensive Haber-Bosch
process. Recently, the electrochemical nitrate reduction reaction (NO3RR) using
electricity generated from renewable sources has attracted widespread
attention. However, the complex reaction pathway of NO3RR leads to the
formation of many undesirable by-products. Herein we successfully prepared a
mixed (FeMo)2C catalyst with good electrocatalytic NO3RR, having a NH3 yield of
14.66 mg h-1 cm-2 and an FE of 94.35 % at low potential -0.3 V vs RHE. DFT
calculations show that the presence of Fe in Mo2C lattice changes the reaction
mechanism, decreasing the potential barrier to be overcome from 1.36 to 0.89
eV. In addition, mixed Fe-Mo carbide facilitates the adsorption of
intermediates and promotes NH3 desorption, facilitating NO3- reduction to NH3.
In addition, (FeMo)2C was used as cathode for Zn-NO3 battery to generate
electricity, producing ammonia at the same time, with a power density of 3.8
mWcm-2 and an NH3 FE of 88 %. This work describes a new synthesis method for
mixed metal carbides and provides a promising strategy for NH3 production.",['cond-mat.mtrl-sci'],2501.02515," Although multiferroics have undergone extensive examination for several
decades, the occurrence of ferroelectricity induced by orbital order is only
scarcely documented. In this study, we propose the existence of spontaneous
ferroelectric states featuring a finite out-of-plane polarization in monolayer
compounds of magnetic transition metal di-halides. Our first principles
analysis reveals that partially occupied d-orbital states within octahedra
exhibit a preference for spatial orbital order within a two-dimensional
lattice. The absence of inversion symmetry, arising from orbital order, serves
as the driving force introducing additional electric polarization along the
out-of-plane direction. Unlike previous reported orbital orders arising from
metal states in lattice, the non-colinear ones we studied in this work relate
to the transition between two insulator states. The resultant asymmetric
Jahn-Teller distortions are accompanied as the consequence producing additional
ionic polarization. Importantly, our findings indicate that this mechanism is
not confined to a specific material but is a shared characteristic among a
series of monolayer transition metal magnetic di-halides, proposing an
innovative form of intrinsic two-dimensional multiferroic physics.",['cond-mat.mtrl-sci'],False,,,,"Mixed Fe-Mo carbide prepared by a sonochemical synthesis as highly
  efficient nitrate reduction electrocatalyst","Orbital Order Triggered Out-of-Plane Ferroelectricity in Magnetic
  Transition Metal di-halide Monolayers"
neg-d2-583,2025-01-26,,2501.15548," This paper examines games with strategic complements or substitutes and
incomplete information, where players are uncertain about the opponents'
parameters. We assume that the players' beliefs about the opponent's parameters
are selected from some given set of beliefs. One extreme is the case where
these sets only contain a single belief, representing a scenario where the
players' actual beliefs about the parameters are commonly known among the
players. Another extreme is the situation where these sets contain all possible
beliefs, representing a scenario where the players have no information about
the opponents' beliefs about parameters. But we also allow for intermediate
cases, where these sets contain some, but not all, possible beliefs about the
parameters. We introduce an assumption of weakly increasing differences that
takes both the choice belief and parameter belief of a player into account.
Under this assumption, we demonstrate that greater choice-parameter beliefs
leads to greater optimal choices. Moreover, we show that the greatest and least
point rationalizable choice of a player is increasing in their parameter, and
these can be determined through an iterative procedure. In each round of the
iterative procedure, the lowest surviving choice is optimal for the lowest
choice-parameter belief, while the greatest surviving choice is optimal for the
highest choice-parameter belief.",['econ.TH'],2502.05342," The appropriate discount rate for evaluating policies is a critical
consideration in economic decision-making. This paper presents a new model for
calculating the derived discount rate for a society that includes different
groups with varying desirable discount rates. The model takes into account
equality in society and is designed to be used by social planners. The derived
discount rate is a useful tool for examining the social planner's approach to
policies related to the future of society. If the discount rate is determined
correctly, it can help determine the amount of growth and equality in society,
as well as the level of attention paid to long-term public projects. The model
can be customized for different distributions of wealth and discount rates,
allowing researchers to extract desired results. Analysis of the model shows
that when equality in society is considered, the derived discount rate is lower
than the result obtained using Hamilton's method. Social planners must consider
that this may increase disagreement in more consuming groups of society at
first.",['econ.TH'],False,,,,Rationalizability and Monotonocity in Games with Incomplete Information,Discounting under inequality and lobbyists disagreement
neg-d2-584,2025-01-06,,2501.03356," An approach is presented to address singularities in general relativity using
a complex Riemannian spacetime extension. We demonstrate how this method can be
applied to both black hole and cosmological singularities, specifically
focusing on the Schwarzschild and Kerr black holes and the
Friedmann-Lema\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending
the relevant coordinates into the complex plane and carefully choosing
integration contours, we show that it is possible to regularize these
singularities, resulting in physically meaningful, singularity-free solutions
when projected back onto real spacetime. The removal of the singularity at the
Big Bang allows for a bounce cosmology. This approach offers a potential bridge
between classical general relativity and quantum gravity effects, suggesting a
way to resolve longstanding issues in gravitational physics without requiring a
full theory of quantum gravity.",['gr-qc'],2501.03356," An approach is presented to address singularities in general relativity using
a complex Riemannian spacetime extension. We demonstrate how this method can be
applied to both black hole and cosmological singularities, specifically
focusing on the Schwarzschild and Kerr black holes and the
Friedmann-Lema\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending
the relevant coordinates into the complex plane and carefully choosing
integration contours, we show that it is possible to regularize these
singularities, resulting in physically meaningful, singularity-free solutions
when projected back onto real spacetime. The removal of the singularity at the
Big Bang allows for a bounce cosmology. This approach offers a potential bridge
between classical general relativity and quantum gravity effects, suggesting a
way to resolve longstanding issues in gravitational physics without requiring a
full theory of quantum gravity.",['gr-qc'],False,,,,"Complex Riemannian spacetime and singularity-free black holes and
  cosmology","Complex Riemannian spacetime and singularity-free black holes and
  cosmology"
neg-d2-585,2025-03-19,,2503.15388," The Perpendicular Shape Anisotropy Spin Transfer Torque Magnetic Random
Access Memory (PSA-STT-MRAM) is a recent concept proposed to maintain the
thermal stability of standard MRAM at small diameters, considering thick
vertical pillars as the free layer. In order to explore the specific physics of
PSA-STT-MRAMs expected in relation with their three-dimensional nature, we have
performed simulations combining a micromagnetic model coupled self-consistently
with spin-dependent transport equations. The 3D shape induces flower states at
the upper and lower surfaces. Besides, the field-like component of STT is found
to be larger than in standard MRAMs, suggesting that it needs to be considered.
The combination of both effects leads to the excitation of high-order 3D
ferromagnetic resonance modes, playing a key role in magnetisation reversal.
These results highlight features of 3D nanomagnetic systems, largely
disregarded so far, which need to be considered to optimise PSA-STT-MRAM to be
a competitive solution for technological implementation.",['cond-mat.mes-hall'],2503.14406," We investigate the influence of a time-periodic drive on three-dimensional
Weyl and multi-Weyl semimetals in planar-Hall/planar-thermal-Hall set-ups. The
drive is modelled here by circularly-polarized electromagnetic fields, whose
effects are incorporated by a combination of the Floquet theorem and the van
Vleck perturbation theory, applicable in the high-frequency limit. We evaluate
the longitudinal and in-plane transverse components of the linear-response
coefficients using the semiclassical Boltzmann formalism. We demonstrate the
explicit expressions of these transport coefficients in certain limits of the
system parameters, where it is possible to derive the explicit analytical
expressions. Our results demonstrate that the topological charges of the
corresponding semimetals etch their trademark signatures in these transport
properties, which can be observed in experiments.",['cond-mat.mes-hall'],False,,,,"Simulation of current-driven magnetisation switching in nanopillars with
  Perpendicular Shape Anisotropy","Effects of time-periodic drive in the linear response for planar-Hall
  set-ups with Weyl and multi-Weyl semimetals"
neg-d2-586,2025-03-03,,2503.01597," A high-repetition-rate pulsed muon source operating at approximately 50\,kHz
holds the potential to improve the sensitivity of various particle physics and
material science experiments involving muons. In this article, we propose
utilizing the high-repetition-rate pulsed electron beam at the SHINE facility
to generate a surface muon beam. Our simulation studies indicate that an
8\,GeV, 100\,pC charge pulsed electron beam impinging on a copper target can
produce up to $2 \times 10^{3}$ muons per pulse. Beamline optimization results
demonstrate that approximately 60 surface muons per electron bunch can be
efficiently transported to the end of the beamline. This translates to a
surface muon rate of $3 \times 10^{6}\,\mu^{+}$/s when the pulsed electron beam
is operated at 50\,kHz, which is comparable to existing muon facilities. This
high-repetition-rate pulsed muon beam, with its ideal time structure,
represents a unique and pioneering effort once constructed. It serves as a
model for building cost-effective muon sources at existing electron machines
with GeV electron energies. The main challenge of positron removal is also
discussed.",['physics.acc-ph'],2503.01597," A high-repetition-rate pulsed muon source operating at approximately 50\,kHz
holds the potential to improve the sensitivity of various particle physics and
material science experiments involving muons. In this article, we propose
utilizing the high-repetition-rate pulsed electron beam at the SHINE facility
to generate a surface muon beam. Our simulation studies indicate that an
8\,GeV, 100\,pC charge pulsed electron beam impinging on a copper target can
produce up to $2 \times 10^{3}$ muons per pulse. Beamline optimization results
demonstrate that approximately 60 surface muons per electron bunch can be
efficiently transported to the end of the beamline. This translates to a
surface muon rate of $3 \times 10^{6}\,\mu^{+}$/s when the pulsed electron beam
is operated at 50\,kHz, which is comparable to existing muon facilities. This
high-repetition-rate pulsed muon beam, with its ideal time structure,
represents a unique and pioneering effort once constructed. It serves as a
model for building cost-effective muon sources at existing electron machines
with GeV electron energies. The main challenge of positron removal is also
discussed.",['physics.acc-ph'],False,,,,"Simulation studies of a high-repetition-rate electron-driven surface
  muon beamline at SHINE","Simulation studies of a high-repetition-rate electron-driven surface
  muon beamline at SHINE"
neg-d2-587,2025-01-22,,2501.12976," In commonly used sub-quadratic complexity modules, linear attention benefits
from simplicity and high parallelism, making it promising for image synthesis
tasks. However, the architectural design and learning strategy for linear
attention remain underexplored in this field. In this paper, we offer a suite
of ready-to-use solutions for efficient linear diffusion Transformers. Our core
contributions include: (1) Simplified Linear Attention using few heads,
observing the free-lunch effect of performance without latency increase. (2)
Weight inheritance from a fully pre-trained diffusion Transformer: initializing
linear Transformer using pre-trained diffusion Transformer and loading all
parameters except for those related to linear attention. (3) Hybrid knowledge
distillation objective: using a pre-trained diffusion Transformer to help the
training of the student linear Transformer, supervising not only the predicted
noise but also the variance of the reverse diffusion process. These guidelines
lead to our proposed Linear Diffusion Transformer (LiT), an efficient
text-to-image Transformer that can be deployed offline on a laptop. Experiments
show that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT
achieves highly competitive FID while reducing training steps by 80% and 77%
compared to DiT. LiT also rivals methods based on Mamba or Gated Linear
Attention. Besides, for text-to-image generation, LiT allows for the rapid
synthesis of up to 1K resolution photorealistic images. Project page:
https://techmonsterwang.github.io/LiT/.",['cs.CV'],2503.1782," Interactive segmentation aims to segment the specified target on the image
with positive and negative clicks from users. Interactive ambiguity is a
crucial issue in this field, which refers to the possibility of multiple
compliant outcomes with the same clicks, such as selecting a part of an object
versus the entire object, a single object versus a combination of multiple
objects, and so on. The existing methods cannot provide intuitive guidance to
the model, which leads to unstable output results and makes it difficult to
meet the large-scale and efficient annotation requirements for specific targets
in some scenarios. To bridge this gap, we introduce RefCut, a reference-based
interactive segmentation framework designed to address part ambiguity and
object ambiguity in segmenting specific targets. Users only need to provide a
reference image and corresponding reference masks, and the model will be
optimized based on them, which greatly reduces the interactive burden on users
when annotating a large number of such targets. In addition, to enrich these
two kinds of ambiguous data, we propose a new Target Disassembly Dataset which
contains two subsets of part disassembly and object disassembly for evaluation.
In the combination evaluation of multiple datasets, our RefCut achieved
state-of-the-art performance. Extensive experiments and visualized results
demonstrate that RefCut advances the field of intuitive and controllable
interactive segmentation. Our code will be publicly available and the demo
video is in https://www.lin-zheng.com/refcut.",['cs.CV'],False,,,,"LiT: Delving into a Simplified Linear Diffusion Transformer for Image
  Generation",RefCut: Interactive Segmentation with Reference Guidance
neg-d2-588,2025-01-08,,2501.04584," This paper proposes a new approach for the calibration of material parameters
in elastoplastic constitutive models. The calibration is posed as a constrained
optimization problem, where the constitutive evolution equations serve as
constraints. The objective function quantifies the mismatch between the stress
predicted by the model and corresponding experimental measurements. To improve
calibration efficiency, a novel direct-adjoint approach is presented to compute
the Hessian of the objective function, which enables the use of second-order
optimization algorithms. Automatic differentiation (AD) is used for gradient
and Hessian computations. Two numerical examples are employed to validate the
Hessian matrices and to demonstrate that the Newton-Raphson algorithm
consistently outperforms gradient-based algorithms such as L-BFGS-B.",['cs.CE'],2501.05162," This paper proposes a novel and efficient key conditional quotient filter
(KCQF) for the estimation of state in the nonlinear system which can be either
Gaussian or non-Gaussian, and either Markovian or non-Markovian. The core idea
of the proposed KCQF is that only the key measurement conditions, rather than
all measurement conditions, should be used to estimate the state. Based on key
measurement conditions, the quotient-form analytical integral expressions for
the conditional probability density function, mean, and variance of state are
derived by using the principle of probability conservation, and are calculated
by using the Monte Carlo method, which thereby constructs the KCQF. Two
nonlinear numerical examples were given to demonstrate the superior estimation
accuracy of KCQF, compared to seven existing filters.",['cs.CE'],False,,,,"A Direct-adjoint Approach for Material Point Model Calibration with
  Application to Plasticity","A Key Conditional Quotient Filter for Nonlinear, non-Gaussian and
  non-Markovian System"
neg-d2-589,2025-02-19,,2502.13929," Formal verification plays a crucial role in making smart contracts safer,
being able to find bugs or to guarantee their absence, as well as checking
whether the business logic is correctly implemented. For Solidity, even though
there already exist several mature verification tools, the semantical quirks of
the language can make verification quite hard in practice. Move, on the other
hand, has been designed with security and verification in mind, and it has been
accompanied since its early stages by a formal verification tool, the Move
Prover. In this paper, we investigate through a comparative analysis: 1) how
the different designs of the two contract languages impact verification, and 2)
what is the state-of-the-art of verification tools for the two languages, and
how do they compare on three paradigmatic use cases. Our investigation is
supported by an open dataset of verification tasks performed in Certora and in
the Aptos Move Prover.",['cs.CR'],2501.03423," Blockchain technology has revolutionized industries by enabling secure and
decentralized transactions. However, the isolated nature of blockchain
ecosystems hinders the seamless transfer of digital assets across different
chains. Cross-chain bridges have emerged as vital web3 infrastructure to
address this challenge by facilitating interoperability between distinct
blockchains. Cross-chain bridges remain vulnerable to various attacks despite
sophisticated designs and security measures. The industry has experienced a
surge in bridge attacks, resulting in significant financial losses. The largest
hack impacted Axie Infinity Ronin Bridge, with a loss of almost \$600 million
USD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and
examines the exploited vulnerabilities. By understanding the attack nature and
underlying weaknesses, the paper aims to enhance bridge security and propose
potential countermeasures. The findings contribute to developing industry-wide
standards for bridge security and operational resilience. Addressing the
vulnerabilities and weaknesses exploited in recent cross-chain bridge hacks
fosters trust and confidence in cross-chain interoperability.",['cs.CR'],False,,,,"Formal verification in Solidity and Move: insights from a comparative
  analysis",SoK: A Review of Cross-Chain Bridge Hacks in 2023
neg-d2-590,2025-01-17,,2501.10042," We present a theoretical model to investigate the dynamics and spectroscopic
properties of a plexciton system consisting of a molecular exciton coupled to a
single short-lived plasmonic mode. The exciton is described as a two-level
system (TLS), while the plasmonic mode is treated as a dissipative harmonic
oscillator. The hierarchical equations of motion method is employed to simulate
energy transfer dynamics, absorption spectra, and two-dimensional electronic
spectra (2DES) of the system across a range of coupling strengths. It is shown
that increasing the exciton-plasmon coupling strength drives a transition in
the absorption spectra from an asymmetric Fano line shape to a Rabi splitting
pattern, while coupling the TLS to intramolecular vibrational modes reduces the
central dip of the absorption spectra and makes the line shape more symmetric.
The simulated 2DES exhibit distinct features compared to those of a coupled
molecular dimer, highlighting the unique nonlinear response of plexciton
systems. In addition, a ""breathing mod"" pattern observed in the strong coupling
regime can serve as a direct evidence of Rabi oscillation.",['physics.chem-ph'],2501.06632," We present the first analytic-derivative-based formulation of vibrational
circular dichroism (VCD) atomic axial tensors for second-order Moller-Plesset
(MP2) perturbation theory. We compare our implementation to our recently
reported finite-difference approach and find close agreement, thus validating
the new formulation. The new approach is dramatically less computationally
expensive than the numerical-derivative method with an overall computational
scaling of $O(N^6)$. In addition, we report the first fully analytic VCD
spectrum for (S)-methyloxirane at the MP2 level of theory.",['physics.chem-ph'],False,,,,A theoretical model for linear and nonlinear spectroscopy of plexcitons,"Analytic Computation of Vibrational Circular Dichroism Spectra Using
  Second-Order M{\o}ller-Plesset Perturbation Theory"
neg-d2-591,2025-02-17,,2502.1248," Soliton microcombs are a cornerstone of integrated frequency comb
technologies, with applications spanning photonic computing, ranging, microwave
synthesis, optical communications, and quantum light generation. In nearly all
such applications, electro-optic (EO) components play a critical role in
generating, monitoring, stabilizing, and modulating the solitons. Towards
building photonic integrated circuits for next-generation applications, that
will simultaneously maximize system performance and minimize size, weight, and
power consumption metrics, achieving soliton microcombs and efficient EO
modulation on a chip is essential. X-cut thin-film lithium niobate (TFLN) has
emerged as a leading photonic platform for the realization of high-performance
integrated EO devices and systems. However, despite extensive research, soliton
microcombs have remained elusive to X-cut TFLN due to its multiple strong
Raman-active modes, in-plane refractive index anisotropy, and photorefractive
effects. Here, we address this long-standing challenge and demonstrate
versatile soliton microcombs on X-cut TFLN, with repetition-rates spanning from
the gigahertz (~26 GHz) up to the millimeter-wave (~0.156 THz) regime. The
combs feature exceptional long-term stability, maintaining a direct
injection-locked state for over 90 minutes (manually terminated), with
repetition-rate phase noise closely tracking that of a high-quality electronic
microwave synthesizer. Our finding broadly advances both the fundamental
science and practical applications of integrated comb sources by enabling
efficient EO modulation and broadband coherent solitons to be monolithically
combined on the same chip.",['physics.optics'],2501.05275," Achieving uniform nanowire size, density, and alignment across a wafer is
challenging, as small variations in growth parameters can impact performance in
energy harvesting devices like solar cells and photodetectors. This study
demonstrates the in-depth characterization of uniformly grown GaAs/AlGaAs
core-shell nanowires on a two-inch Si(111) substrate using Ga-induced
self-catalyzed molecular beam epitaxy. By integrating Scanning Electron
Microscopy and Time Correlated Single-Photon Counting, we establish a detailed
model of structural and optoelectronic properties across wafer and micron
scales. While emission intensity varies by up to 35%, carrier lifetime shows
only 9% variation, indicating stable material quality despite structural
inhomogeneities. These findings indicate that, for the two-inch GaAs/AlGaAs
nanowire wafer, achieving uniform nanowire coverage had a greater impact on
consistent optoelectronic properties than variations in material quality,
highlighting its significance for scalable III-V semiconductor integration on
silicon in advanced optoelectronic devices such as solar cells and
photodetectors.",['physics.optics'],False,,,,"Stable gigahertz- and mmWave-repetition-rate soliton microcombs on X-cut
  lithium niobate","Wafer-scale correlated morphology and optoelectronic properties in
  GaAs/AlGaAs core-shell nanowires"
neg-d2-592,2025-01-03,,2501.01765," As advancements in large language models (LLMs) continue and the demand for
personalized models increases, parameter-efficient fine-tuning (PEFT) methods
(e.g., LoRA) will become essential due to their efficiency in reducing
computation costs. However, recent studies have raised alarming concerns that
LoRA fine-tuning could potentially compromise the safety alignment in LLMs,
posing significant risks for the model owner. In this paper, we first
investigate the underlying mechanism by analyzing the changes in safety
alignment related features before and after fine-tuning. Then, we propose a
fixed safety module calculated by safety data and a task-specific
initialization for trainable parameters in low-rank adaptations, termed
Safety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA
methods and their variants, SaLoRA enables targeted modifications to LLMs
without disrupting their original alignments. Our experiments show that SaLoRA
outperforms various adapters-based approaches across various evaluation metrics
in different fine-tuning tasks.",['cs.LG'],2502.05496," Outlier detection tasks aim at discovering potential issues or opportunities
and are widely used in cybersecurity, financial security, industrial
inspection, etc. To date, thousands of outlier detection algorithms have been
proposed. Clearly, in real-world scenarios, such a large number of algorithms
is unnecessary. In other words, a large number of outlier detection algorithms
are redundant. We believe the root cause of this redundancy lies in the current
highly customized (i.e., non-generic) optimization strategies. Specifically,
when researchers seek to improve the performance of existing outlier detection
algorithms, they have to design separate optimized versions tailored to the
principles of each algorithm, leading to an ever-growing number of outlier
detection algorithms. To address this issue, in this paper, we introduce the
explosion from physics into the outlier detection task and propose a generic
optimization strategy based on feature explosion, called OSD (Optimization
Strategy for outlier Detection algorithms). In the future, when improving the
performance of existing outlier detection algorithms, it will be sufficient to
invoke the OSD plugin without the need to design customized optimized versions
for them. We compared the performances of 14 outlier detection algorithms on 24
datasets before and after invoking the OSD plugin. The experimental results
show that the performances of all outlier detection algorithms are improved on
almost all datasets. In terms of average accuracy, OSD make these outlier
detection algorithms improve by 15% (AUC), 63.7% (AP).",['cs.LG'],False,,,,SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation,"Feature Explosion: a generic optimization strategy for outlier detection
  algorithms"
neg-d2-593,2025-01-13,,2501.0735," Photochemistry in the earth's atmosphere is driven by the sun, continuously
altering the concentration and spatial distribution of pollutants. Precisely
monitoring their atmospheric abundance relies predominantly on optical sensing,
which requires the knowledge of exact absorption cross sections. One key
pollutant which impacts many photochemical reaction-pathways is formaldehyde.
Agreement on formaldehyde absolute absorption cross section remains elusive in
the photochemically-relevant ultraviolet spectral region, hampering sensitive
concentration tracking. Here, we introduce free-running ultraviolet dual comb
spectroscopy, combining high spectral resolution (1 GHz), broad spectral
coverage (12 THz), and fast acquisition speed (500 ms), as a novel method for
absolute absorption cross section determination with unprecedented fidelity.
Within this bandwidth, our method uncovers almost one order of magnitude more
rovibrational transitions than detected before which leads to refined
rotational constants for high-level quantum simulations of molecular
eigenstates. This ultra-resolution method can be generalized to provide a
universal tool for fast electronic fingerprinting of atmospherically-relevant
species, both for sensing applications and to benchmark improvements of
ab-initio quantum theory.",['physics.optics'],2501.06966," Networks of coupled nonlinear optical resonators have emerged as an important
class of systems in ultrafast optical science, enabling richer and more complex
nonlinear dynamics compared to their single-resonator or travelling-wave
counterparts. In recent years, these coupled nonlinear optical resonators have
been applied as application-specific hardware accelerators for computing
applications including combinatorial optimization and artificial intelligence.
In this work, we rigorously prove a fundamental result showing that coupled
nonlinear optical resonators are Turing-complete computers, which endows them
with much greater computational power than previously thought. Furthermore, we
show that the minimum threshold of hardware complexity needed for
Turing-completeness is surprisingly low, which has profound physical
consequences. In particular, we show that several problems of interest in the
study of coupled nonlinear optical resonators are formally undecidable. These
theoretical findings can serve as the foundation for better understanding the
promise of next-generation, ultrafast all-optical computers.",['physics.optics'],False,,,,Ultra-resolution photochemical sensing,"Turing-Completeness and Undecidability in Coupled Nonlinear Optical
  Resonators"
neg-d2-594,2025-02-12,,2502.08779," Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful
societal prejudices, undermining the fairness and equity of AI applications. As
LMMs grow increasingly influential, addressing and mitigating inherent biases
related to stereotypes, harmful generations, and ambiguous assumptions in
real-world scenarios has become essential. However, existing datasets
evaluating stereotype biases in LMMs often lack diversity and rely on synthetic
images, leaving a gap in bias evaluation for real-world visual contexts. To
address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most
comprehensive framework to date for assessing stereotype biases across nine
diverse categories with non-synthetic images. SB-bench rigorously evaluates
LMMs through carefully curated, visually grounded scenarios, challenging them
to reason accurately about visual stereotypes. It offers a robust evaluation
framework featuring real-world visual samples, image variations, and
multiple-choice question formats. By introducing visually grounded queries that
isolate visual biases from textual ones, SB-bench enables a precise and nuanced
assessment of a model's reasoning capabilities across varying levels of
difficulty. Through rigorous testing of state-of-the-art open-source and
closed-source LMMs, SB-bench provides a systematic approach to assessing
stereotype biases in LMMs across key social dimensions. This benchmark
represents a significant step toward fostering fairness in AI systems and
reducing harmful biases, laying the groundwork for more equitable and socially
responsible LMMs. Our code and dataset are publicly available.",['cs.CV'],2501.01197," Layers have become indispensable tools for professional artists, allowing
them to build a hierarchical structure that enables independent control over
individual visual elements. In this paper, we propose LayeringDiff, a novel
pipeline for the synthesis of layered images, which begins by generating a
composite image using an off-the-shelf image generative model, followed by
disassembling the image into its constituent foreground and background layers.
By extracting layers from a composite image, rather than generating them from
scratch, LayeringDiff bypasses the need for large-scale training to develop
generative capabilities for individual layers. Furthermore, by utilizing a
pretrained off-the-shelf generative model, our method can produce diverse
contents and object scales in synthesized layers. For effective layer
decomposition, we adapt a large-scale pretrained generative prior to estimate
foreground and background layers. We also propose high-frequency alignment
modules to refine the fine-details of the estimated layers. Our comprehensive
experiments demonstrate that our approach effectively synthesizes layered
images and supports various practical applications.",['cs.CV'],False,,,,SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models,"LayeringDiff: Layered Image Synthesis via Generation, then Disassembly
  with Generative Knowledge"
neg-d2-595,2025-03-21,,2503.17241," Superconducting electronics represents a promising technology, offering not
only efficient integration with quantum computing systems, but also the
potential for significant power reduction in high-performance computing.
Nonetheless, the lack of superconducting memories better than conventional
metal-oxide semiconductor (CMOS) memories represent a major obstacle towards
the development of computing systems entirely based on superconducting
electronics. In this work, we combine the emerging concept of gate-controlled
supercurrent (GCS) with the well-established mechanism of charge-trapping
memory to demonstrate a novel, highly scalable, voltage-controlled and
non-volatile superconducting memory. GCS denotes the observation that the
supercurrent in a superconducting constriction can be suppressed by applying a
certain gate voltage (VG) to it. Our findings show that charge trapping within
the gate dielectric, here sapphire, influences the voltage threshold needed to
suppress the supercurrent. We demonstrate reliable reading and reversible
writing of two distinct charge-trapping memory states, associated with
different supercurrent values. Based on our memory device demonstrator, we
discuss its integration into a NOT AND (NAND) gate layout, outlining the
significant improvements offered by this novel memory concept over other
existing NAND memory technologies.",['cond-mat.supr-con'],2502.01881," In light of breakthroughs in superconductivity under high pressure, and
considering that record critical temperatures (T$_c$s) across various systems
have been achieved under high pressure, the primary challenge for higher Tc
should no longer solely be to increase T$_c$ under extreme conditions but also
to reduce, or ideally eliminate, the need for applied pressure in retaining
pressure-induced or -enhanced superconductivity. The topological semiconductor
Bi$_{0.5}$Sb$_{1.5}$Te$_3$ (BST) was chosen to demonstrate our approach to
addressing this challenge and exploring its intriguing physics. Under pressures
up to ~ 50 GPa, three superconducting phases (BST-I, -II, and -III) were
observed. A superconducting phase in BST-I appears at ~ 4 GPa, without a
structural transition, suggesting the possible topological nature of this
phase. Using the pressure-quench protocol (PQP) recently developed by us, we
successfully retained this pressure-induced phase at ambient pressure and
revealed the bulk nature of the state. Significantly, this demonstrates
recovery of a pressure-quenched sample from a diamond anvil cell at room
temperature with the pressure-induced phase retained at ambient pressure. Other
superconducting phases were retained in BST-II and -III at ambient pressure and
subjected to thermal and temporal stability testing. Superconductivity was also
found in BST with T$_c$ up to 10.2 K, the record for this compound series.
While PQP maintains superconducting phases in BST at ambient pressure, both
depressurization and PQP enhance its T$_c$, possibly due to microstructures
formed during these processes, offering an added avenue to raise T$_c$. These
findings are supported by our density-functional theory calculations.",['cond-mat.supr-con'],False,,,,"Superconducting non-volatile memory based on charge trapping and
  gate-controlled superconductivity","Creation, stabilization, and study at ambient pressure of
  pressure-induced superconductivity in Bi$_{0.5}$Sb$_{1.5}$Te$_3$"
neg-d2-596,2025-02-13,,2502.09294," Automatic Affect Prediction (AAP) uses computational analysis of input data
such as text, speech, images, and physiological signals to predict various
affective phenomena (e.g., emotions or moods). These models are typically
constructed using supervised machine-learning algorithms, which rely heavily on
labeled training datasets. In this position paper, we posit that all AAP
training data are derived from human Affective Interpretation Processes,
resulting in a form of Affective Meaning. Research on human affect indicates a
form of complexity that is fundamental to such meaning: it can possess what we
refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing
Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of
confidence regarding meanings' correctness), Ambiguity (meaning contains
mutually exclusive concepts) and Vagueness (meaning is situated at different
levels in a nested hierarchy). Failing to appropriately consider QIs leads to
results incapable of meaningful and reliable predictions. Based on this
premise, we argue that a crucial step in adequately addressing indeterminacy in
AAP is the development of data collection practices for modeling corpora that
involve the systematic consideration of 1) a relevant set of QIs and 2) context
for the associated interpretation processes. To this end, we are 1) outlining a
conceptual model of AIPs and the QIs associated with the meaning these produce
and a conceptual structure of relevant context, supporting understanding of its
role. Finally, we use our framework for 2) discussing examples of
context-sensitivity-related challenges for addressing QIs in data collection
setups. We believe our efforts can stimulate a structured discussion of both
the role of aspects of indeterminacy and context in research on AAP, informing
the development of better practices for data collection and analysis.",['cs.AI'],2501.07071," As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning
their values with humans has become imperative for their responsible
development and customized applications. However, there still lack evaluations
of LLMs values that fulfill three desirable goals. (1) Value Clarification: We
expect to clarify the underlying values of LLMs precisely and comprehensively,
while current evaluations focus narrowly on safety risks such as bias and
toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are
prone to data contamination and quickly become obsolete as LLMs evolve.
Additionally, these discriminative evaluations uncover LLMs' knowledge about
values, rather than valid assessments of LLMs' behavioral conformity to values.
(3) Value Pluralism: The pluralistic nature of human values across individuals
and cultures is largely ignored in measuring LLMs value alignment. To address
these challenges, we presents the Value Compass Leaderboard, with three
correspondingly designed modules. It (i) grounds the evaluation on
motivationally distinct \textit{basic values to clarify LLMs' underlying values
from a holistic view; (ii) applies a \textit{generative evolving evaluation
framework with adaptive test items for evolving LLMs and direct value
recognition from behaviors in realistic scenarios; (iii) propose a metric that
quantifies LLMs alignment with a specific value as a weighted sum over multiple
dimensions, with weights determined by pluralistic values.",['cs.AI'],False,,,,"Indeterminacy in Affective Computing: Considering Meaning and Context in
  Data Collection Practices","Value Compass Leaderboard: A Platform for Fundamental and Validated
  Evaluation of LLMs Values"
neg-d2-597,2025-01-26,,2501.15649," The evidence of a Stochastic Gravitational Wave Background (SGWB) in the nHz
frequency range is posed to open a new window on the Universe. A preferred
explanation relies on a supercooled first order phase transition at the 100 MeV
- GeV scale. In this article, we address the feasibility going from the
particle physics model to the production of the gravitational waves. We take a
minimal approach for the dark sector model introducing the fewest ingredients
required, namely a new U(1) gauge group and a dark scalar that dynamically
breaks the symmetry. Supercooling poses challenges in the analysis that put
under question the feasibility of this explanation: we address them, going
beyond previous studies by carefully considering the effects of a vacuum
domination phase and explicitly tracking the phase transition from its onset to
its completion. We find that the proposed model can successfully give origin to
the observed PTA SGWB signal. The strong supercooling imposes a correlation
between the new gauge coupling and the scalar quartic one, leading to a
significant hierarchy between the (heavier) gauge boson and the dark scalar.
Ultimately, information on phase transitions from SGWB observations could
provide a direct probe of the microphysics of the Early Universe and be used to
guide future searches of dark sector in laboratories.",['hep-ph'],2502.02992," We present the results obtained by performing global fits of
two-Higgs-doublet models (2HDMs) using the full Run 1 and Run 2 Higgs datasets
collected at the LHC. Avoiding unwanted tree-level flavor-changing neutral
currents and including the wrong-sign cases, we consider 12 scenarios across
six types of 2HDMs: Inert, type I, type II, type III, type IV, and Aligned
2HDMs. Our main results are presented in Table 3 and Fig. 1. We find that the
type-I 2HDM provides the best fit, while the wrong-sign scenarios of the
type-II and type-IV 2HDMs, where the normalized Yukawa coupling to down-type
quarks is opposite in sign to the Standard Model (SM), are disfavored. We also
observe that the Aligned 2HDM gives the second-best fit when the Yukawa
couplings to down-type quarks take the same sign as in the SM, regardless of
the sign of the Yukawa couplings to the charged leptons.",['hep-ph'],False,,,,Supercooled Dark Scalar Phase Transitions explanation of NANOGrav data,"Higgs boson precision analysis of two Higgs doublet models: Full LHC Run
  1 and Run 2 data"
neg-d2-598,2025-03-09,,2503.06662," This paper studies a distributed algorithm for constrained consensus
optimization that is obtained by fusing the Arrow-Hurwicz-Uzawa primal-dual
gradient method for centralized constrained optimization and the Wang-Elia
method for distributed unconstrained optimization. It is shown that the optimal
primal-dual point is a semiglobally exponentially stable equilibrium for the
algorithm, which implies linear convergence. The analysis is based on the
separation between a slow centralized optimization dynamics describing the
evolution of the average estimate toward the optimum, and a fast dynamics
describing the evolution of the consensus error over the network. These two
dynamics are mutually coupled, and the stability analysis builds on control
theoretic tools such as time-scale separation, Lyapunov theory, and the
small-gain principle. Our analysis approach highlights that the consensus
dynamics can be seen as a fast, parasite one, and that stability of the
distributed algorithm is obtained as a robustness consequence of the semiglobal
exponential stability properties of the centralized method. This perspective
can be used to enable other significant extensions, such as time-varying
networks or delayed communication, that can be seen as ``perturbations"" of the
centralized algorithm.",['math.OC'],2501.13886," The stochastic three points (STP) algorithm is a derivative-free optimization
technique designed for unconstrained optimization problems in $\mathbb{R}^d$.
In this paper, we analyze this algorithm for three classes of functions: smooth
functions that may lack convexity, smooth convex functions, and smooth
functions that are strongly convex. Our work provides the first almost sure
convergence results of the STP algorithm, alongside some convergence results in
expectation. For the class of smooth functions, we establish that the best
gradient iterate of the STP algorithm converges almost surely to zero at a rate
arbitrarily close to $o(\frac{1}{\sqrt{T}})$, where $T$ is the number of
iterations. Furthermore, within the same class of functions, we establish both
almost sure convergence and convergence in expectation of the final gradient
iterate towards zero. For the class of smooth convex functions, we establish
that $f(\theta^T)$ converges to $\inf_{\theta \in \mathbb{R}^d} f(\theta)$
almost surely at a rate arbitrarily close to $o(\frac{1}{T})$, and in
expectation at a rate of $O(\frac{d}{T})$ where $d$ is the dimension of the
space. Finally, for the class of smooth functions that are strongly convex, we
establish that when step sizes are obtained by approximating the directional
derivatives of the function, $f(\theta^T)$ converges to $\inf_{\theta \in
\mathbb{R}^d} f(\theta)$ in expectation at a rate of $O((1-\frac{\mu}{dL})^T)$,
and almost surely at a rate arbitrarily close to $o((1-\frac{\mu}{dL})^T)$,
where $\mu$ and $L$ are the strong convexity and smoothness parameters of the
function.",['math.OC'],False,,,,"An exponentially stable discrete-time primal-dual algorithm for
  distributed constrained optimization",On the Almost Sure Convergence of the Stochastic Three Points Algorithm
neg-d2-599,2025-02-15,,2502.10746," It is difficult to establish an analytical criterion to identify the
boundaries of quantum correlations, even for the simplest Bell scenario. Here,
we briefly reviewed the plausible analytical criterion, and we found a way to
confirm the extremal conditions from another direction. For that purpose, we
analyzed the Navascu\'es-Pironio-Ac\'{\i}n (NPA) hierarchy to study the
algebraic structure and found that the problem could not be simplified using
$1\!+\!AB$ level. However, considering the plausible criterion, the $1\!+\!AB$
and second levels for correlations were equal, and the extremal condition in
the simplest Bell scenario was replaced by that in the $1\!+\!AB$ level. Thus,
the correctness of the plausible criterion was verified, and the results
demonstrated that the plausible criterion held, thereby explaining its
simplicity. It seemed plausible, but now it becomes more certain.",['quant-ph'],2501.12945," Quantum interference is known to become extinct with distinguishing
information, as illustrated by the ubiquitous double-slit experiment or the
two-photon HOM effect. In the former case single particle interference is
destroyed with which-path information while in the latter bunching interference
tails-off as photons become distinguishable. It has been observed that when
more than two particles are involved, these interference patterns are in
general a non monotonic function of the distinguishability. Here we perform a
comprehensive characterization, both theoretically and experimentally, of
four-photon interference by analyzing the corresponding correlation functions,
contemplating several degrees of distinguishability across different
parameters. This study provides all the necessary tools to quantify the impact
of multi-photon interference on precision measurements of parameters such as
phase, frequency, and time difference. We apply these insights to quantify the
precision in the estimation of an interferometric phase in a two-port
interferometer using a four-photon state. Our results reveal that, for certain
phase values, partially distinguishable multi-photon states can achieve higher
Fisher information values compared to the two-photon experiment. These findings
highlight the potential of distinguishable multi-photon states for enhanced
precision in quantum metrology and related applications.",['quant-ph'],False,,,,NPA Hierarchy and Extremal Criterion in the Simplest Bell Scenario,"Unraveling quantum phase estimation: exploring the impact of
  multi-photon interference on the quantum Fisher information"
neg-d2-600,2025-03-17,,2503.13722," The largest prime p that can be the order of an automorphism of a 2-(35,17,8)
design is p=17, and all 2-(35,17,8) designs with an automorphism of order 17
were classified by Tonchev. The symmetric 2-(35,17,8) designs with
automorphisms of odd prime order $p<17$ were also classified. In this paper we
give the classification of all symmetric 2-(35,17,8) designs that admit an
automorphism of order $p=2$. It is shown that there are exactly $11,642,495$
nonisomorphic such designs. Furthermore, it is shown that the number of
nonisomorphic 3-(36,18,8) designs which have at least one derived 2-$(35,17,8)$
design with an automorphism of order 2, is $1,015,225$.",['math.CO'],2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],False,,,,"Symmetric 2-(35,17,8) designs with an automorphism of order 2",Coarse tree-width
neg-d2-601,2025-03-09,,2503.06702," We propose and analyze a sequential quadratic programming algorithm for
minimizing a noisy nonlinear smooth function subject to noisy nonlinear smooth
equality constraints. The algorithm uses a step decomposition strategy and, as
a result, is robust to potential rank-deficiency in the constraints, allows for
two different step size strategies, and has an early stopping mechanism. Under
the linear independence constraint qualification, convergence is established to
a neighborhood of a first-order stationary point, where the radius of the
neighborhood is proportional to the noise levels in the objective function and
constraints. Moreover, in the rank-deficient setting, the merit parameter may
converge to zero, and convergence to a neighborhood of an infeasible stationary
point is established. Numerical experiments demonstrate the efficiency and
robustness of the proposed method.",['math.OC'],2503.06702," We propose and analyze a sequential quadratic programming algorithm for
minimizing a noisy nonlinear smooth function subject to noisy nonlinear smooth
equality constraints. The algorithm uses a step decomposition strategy and, as
a result, is robust to potential rank-deficiency in the constraints, allows for
two different step size strategies, and has an early stopping mechanism. Under
the linear independence constraint qualification, convergence is established to
a neighborhood of a first-order stationary point, where the radius of the
neighborhood is proportional to the noise levels in the objective function and
constraints. Moreover, in the rank-deficient setting, the merit parameter may
converge to zero, and convergence to a neighborhood of an infeasible stationary
point is established. Numerical experiments demonstrate the efficiency and
robustness of the proposed method.",['math.OC'],False,,,,"Optimistic Noise-Aware Sequential Quadratic Programming for Equality
  Constrained Optimization with Rank-Deficient Jacobians","Optimistic Noise-Aware Sequential Quadratic Programming for Equality
  Constrained Optimization with Rank-Deficient Jacobians"
neg-d2-602,2025-01-21,,2501.11974," We investigated wideband pulse generation for underwater acoustic
applications using a parametric array. We fabricated a transducer consisting of
a 3 mm thick 75 mm-by-75 mm square-shaped PZT ceramic plate, which is matched
to water media at the radiating face and terminated by a very low impedance at
the back. All measurements were made in a large test tank. We transmitted
square-root amplitude modulated pulses centered around 855 kHz primary
frequency. We showed that phase-sensitive generation of in-phase and
out-of-phase bursts suitable for coded transmission using a parametric array is
possible. We generated very short duration bursts, as short as half-cycle, at a
10-80 kHz difference frequency range. The definition of the bursts is
excellent, e.g., with a normalized cross-correlation of 0.92 with an ideal
2-cycle square burst, for both in-phase and out-of-phase pulses.",['eess.SP'],2502.11653," Learning-based algorithms have gained great popularity in communications
since they often outperform even carefully engineered solutions by learning
from training samples. In this paper, we show that the selection of appropriate
training examples can be important for the performance of such learning-based
algorithms. In particular, we consider non-linear 1-bit precoding for massive
multi-user MIMO systems using the C2PO algorithm. While previous works have
already shown the advantages of learning critical coefficients of this
algorithm, we demonstrate that straightforward selection of training samples
that follow the channel model distribution does not necessarily lead to the
best result. Instead, we provide a strategy to generate training data based on
the specific properties of the algorithm, which significantly improves its
error floor performance.",['eess.SP'],False,,,,"Wideband Pulse Generation for Underwater Applications Using Parametric
  Array","Training Channel Selection for Learning-based 1-bit Precoding in Massive
  MU-MIMO"
neg-d2-603,2025-03-05,,2503.03742," Investigating whether and how galaxy mergers affect black hole growth can be
determinant for black hole-galaxy evolution models and, in particular, for
understanding how early Universe seed black holes grew to become supermassive.
However, while mergers have been observed to enhance the active galactic
nucleus (AGN) activity, and thus black hole growth in massive galaxies, it is
yet not known how this relation and the role of the environment translates to
dwarf galaxies (the most likely hosts of the early seed black holes), since
there are scarce and mixed results in the literature. We want to assess the
impact of galaxy mergers and the environment on AGN triggering in dwarf
galaxies. We use a sample of 3280 dwarf galaxies with integral-field
spectroscopic data from the MaNGA survey to study the AGN fraction throughout
the merger process and how it is affected by the environment (characterized by
galaxy isolation, being in a void, and group richness). We also compare the
fraction of interacting galaxies in AGN and non-AGN dwarf galaxies. We find
that dwarf galaxy mergers can ignite AGNs at separations below 20 kpc. The AGN
fraction increases notoriously after the first pass and remains enhanced until
the final stage. Despite this, mergers are not the dominant AGN triggering
mechanism. We also find that the environment has a non-negligible impact on AGN
activity in dwarf galaxies, as the AGN fraction increases when moving to lower
density environments. These findings provide the most statistically robust
constraints to date on the effects of dwarf galaxy mergers and environment on
AGN activity and black hole growth.",['astro-ph.GA'],2502.13029," Many spiral galaxies host magnetic fields with energy densities comparable to
those of the turbulent and thermal motions of their interstellar gas. However,
quantitative comparison between magnetic field properties inferred from
observation and those obtained from theoretical modeling has been lacking. In
Paper I we developed a simple, axisymmetric galactic dynamo model that uses
various observational data as input. Here we apply our model to calculate
radial profiles of azimuthally and vertically averaged magnetic field strength
and pitch angle, gas velocity dispersion and scale height, turbulent
correlation time and length, and the sizes of supernova remnants for the
galaxies M31, M33, M51, and NGC 6946, using input data collected from the
literature. Scaling factors are introduced to account for a lack of precision
in both theory and observation. Despite the simplicity of our model, its
outputs agree fairly well with galaxy properties inferred from observation.
Additionally, we find that most of the parameter values are similar between
galaxies. We extend the model to predict the magnetic field pitch angles
arising from a combination of mean-field dynamo action and the winding up of
the random small-scale field owing to the large-scale radial shear. We find
their magnitudes to be much smaller than those of the pitch angles measured in
polarized radio and far infrared emission. This suggests that effects not
included in our model, such as effects associated with spiral arms, are needed
to explain the pitch angle values.",['astro-ph.GA'],False,,,,"MaNGA AGN dwarf galaxies (MAD) -- III. The role of mergers and
  environment in AGN activity in dwarf galaxies",Galactic magnetic fields II. Applying the model to nearby galaxies
neg-d2-604,2025-03-12,,2503.09237," We attempt to take a comprehensive look at the challenges of representing the
spatio-temporal structures and dynamic processes defining a city's overall
characteristics. For the task of urban planning and urban operation, we take
the stance that even if the necessary representations of these structures and
processes can be achieved, the most important representation of the relevant
mindsets of the citizens are, unfortunately, mostly neglected.
  After a review of major ""traditional"" urban models of structures behind urban
scale, form, and dynamics, we turn to major recent modeling approaches
triggered by recent advances in AI that enable multi-modal generative models.
Some of these models can create representations of geometries, networks and
images, and reason flexibly at a human-compatible semantic level. They provide
huge amounts of knowledge extracted from Terabytes of text and image documents
and cover the required rich representation spectrum including geographic
knowledge by different knowledge sources, degrees of granularity and scales.
  We then discuss what these new opportunities mean for the modeling challenges
posed by cities, in particular with regard to the role and impact of citizens
and their interactions within the city infrastructure. We propose to integrate
these possibilities with existing approaches, such as agent-based models, which
opens up new modeling spaces including rich citizen models which are able to
also represent social interactions.
  Finally, we put forward some thoughts about a vision of a ""social AI in a
city ecosystem"" that adds relevant citizen models to state-of-the-art
structural and process models. This extended city representation will enable
urban planners to establish citizen-oriented planning of city infrastructures
for human culture, city resilience and sustainability.",['cs.ET'],2503.09237," We attempt to take a comprehensive look at the challenges of representing the
spatio-temporal structures and dynamic processes defining a city's overall
characteristics. For the task of urban planning and urban operation, we take
the stance that even if the necessary representations of these structures and
processes can be achieved, the most important representation of the relevant
mindsets of the citizens are, unfortunately, mostly neglected.
  After a review of major ""traditional"" urban models of structures behind urban
scale, form, and dynamics, we turn to major recent modeling approaches
triggered by recent advances in AI that enable multi-modal generative models.
Some of these models can create representations of geometries, networks and
images, and reason flexibly at a human-compatible semantic level. They provide
huge amounts of knowledge extracted from Terabytes of text and image documents
and cover the required rich representation spectrum including geographic
knowledge by different knowledge sources, degrees of granularity and scales.
  We then discuss what these new opportunities mean for the modeling challenges
posed by cities, in particular with regard to the role and impact of citizens
and their interactions within the city infrastructure. We propose to integrate
these possibilities with existing approaches, such as agent-based models, which
opens up new modeling spaces including rich citizen models which are able to
also represent social interactions.
  Finally, we put forward some thoughts about a vision of a ""social AI in a
city ecosystem"" that adds relevant citizen models to state-of-the-art
structural and process models. This extended city representation will enable
urban planners to establish citizen-oriented planning of city infrastructures
for human culture, city resilience and sustainability.",['cs.ET'],False,,,,"City Models: Past, Present and Future Prospects","City Models: Past, Present and Future Prospects"
neg-d2-605,2025-02-24,,2502.17538," This paper introduces a novel causal framework for multi-stage
decision-making in natural language action spaces where outcomes are only
observed after a sequence of actions. While recent approaches like Proximal
Policy Optimization (PPO) can handle such delayed-reward settings in
high-dimensional action spaces, they typically require multiple models (policy,
value, and reward) and substantial training data. Our approach employs
Q-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,
enabling data-efficient policy learning via gradient ascent on language
embeddings. A key technical contribution of our approach is a decoding strategy
that translates optimized embeddings back into coherent natural language. We
evaluate our approach on mental health intervention, hate speech countering,
and sentiment transfer tasks, demonstrating significant improvements over
competitive baselines across multiple metrics. Notably, our method achieves
superior transfer strength while maintaining content preservation and fluency,
as validated through human evaluation. Our work provides a practical foundation
for learning optimal policies in complex language tasks where training data is
limited.",['cs.CL'],2502.0808," Decomposition of text into atomic propositions is a flexible framework
allowing for the closer inspection of input and output text. We use atomic
decomposition of hypotheses in two natural language reasoning tasks,
traditional NLI and defeasible NLI, to form atomic sub-problems, or granular
inferences that models must weigh when solving the overall problem. These
atomic sub-problems serve as a tool to further understand the structure of both
NLI and defeasible reasoning, probe a model's consistency and understanding of
different inferences, and measure the diversity of examples in benchmark
datasets. Our results indicate that LLMs still struggle with logical
consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify
critical atomic sub-problems of defeasible NLI examples, or those that most
contribute to the overall label, and propose a method to measure the
inferential consistency of a model, a metric designed to capture the degree to
which a model makes consistently correct or incorrect predictions about the
same fact under different contexts.",['cs.CL'],False,,,,Policy Learning with a Natural Language Action Space: A Causal Approach,NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals
neg-d2-606,2025-03-17,,2503.1301," Foil windings have, due to their layered structure, different properties than
conventional wire windings, which make them advantageous for high frequency
applications. Both electromagnetic and thermal analyses are relevant for foil
windings. These two physical areas are coupled through Joule losses and
temperature dependent material properties. For an efficient simulation of foil
windings, homogenization techniques are used to avoid resolving the single
turns. Therefore, this paper comprises a coupled magneto-thermal simulation
that uses a homogenization method in the electromagnetic and thermal part. A
weak coupling with different time step sizes for both parts is presented. The
method is validated on a simple geometry and showcased for a pot transformer
that uses a foil and a wire winding.",['cs.CE'],2502.00227," This paper presents a single-life reinforcement learning (SLRL) approach to
adaptively select the dimension of the Krylov subspace during the generalized
minimal residual (GMRES) iteration. GMRES is an iterative algorithm for solving
large and sparse linear systems of equations in the form of \(Ax = b\) which
are mainly derived from partial differential equations (PDEs). The proposed
framework uses RL to adjust the Krylov subspace dimension (m) in the GMRES (m)
algorithm. This research demonstrates that altering the dimension of the Krylov
subspace in an online setup using SLRL can accelerate the convergence of the
GMRES algorithm by more than an order of magnitude. A comparison of different
matrix sizes and sparsity levels is performed to demonstrate the effectiveness
of adaptive Krylov subspace exploration using single-life RL (AK-SLRL). We
compare AK-SLRL with constant-restart GMRES by applying the highest restart
value used in AK-SLRL to the GMRES method. The results show that using an
adjustable restart parameter with single-life soft-actor critic (SLSAC) and an
experience replay buffer sized to half the matrix dimension converges
significantly faster than the constant restart GMRES with higher values. Higher
values of the restart parameter are equivalent to a higher number of Arnoldi
iterations to construct an orthonormal basis for the Krylov subspace $ K_m(A,
r_0) $. This process includes constructing $m$ orthonormal vectors and updating
the Hessenberg matrix $H$. Therefore, lower values of $m$ result in reduced
computation needed in GMRES minimization to solve the least-squares problem in
the smaller Hessenberg matrix. The robustness of the result is validated
through a wide range of matrix dimensions and sparsity. This paper contributes
to the series of RL combinations with numerical solvers to achieve accelerated
scientific computing.",['cs.CE'],False,,,,"Magneto-thermally Coupled Field Simulation of Homogenized Foil Winding
  Models","AK-SLRL: Adaptive Krylov Subspace Exploration Using Single-Life
  Reinforcement Learning for Sparse Linear System"
neg-d2-607,2025-01-08,,2501.0442," The movie recommender system typically leverages user feedback to provide
personalized recommendations that align with user preferences and increase
business revenue. This study investigates the impact of gender stereotypes on
such systems through a specific attack scenario. In this scenario, an attacker
determines users' gender, a private attribute, by exploiting gender stereotypes
about movie preferences and analyzing users' feedback data, which is either
publicly available or observed within the system. The study consists of two
phases. In the first phase, a user study involving 630 participants identified
gender stereotypes associated with movie genres, which often influence viewing
choices. In the second phase, four inference algorithms were applied to detect
gender stereotypes by combining the findings from the first phase with users'
feedback data. Results showed that these algorithms performed more effectively
than relying solely on feedback data for gender inference. Additionally, we
quantified the extent of gender stereotypes to evaluate their broader impact on
digital computational science. The latter part of the study utilized two major
movie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental
information is available on our GitHub repository:
https://github.com/fr-iit/GSMRS",['cs.IR'],2502.13763," In session-based recommender systems, predictions are based on the user's
preceding behavior in the session. State-of-the-art sequential recommendation
algorithms either use graph neural networks to model sessions in a graph or
leverage the similarity of sessions by exploiting item features. In this paper,
we combine these two approaches and propose a novel method, Graph Convolutional
Network Extension (GCNext), which incorporates item features directly into the
graph representation via graph convolutional networks. GCNext creates a
feature-rich item co-occurrence graph and learns the corresponding item
embeddings in an unsupervised manner. We show on three datasets that
integrating GCNext into sequential recommendation algorithms significantly
boosts the performance of nearest-neighbor methods as well as neural network
models. Our flexible extension is easy to incorporate in state-of-the-art
methods and increases the MRR@20 by up to 12.79%.",['cs.IR'],False,,,,"A Closer Look on Gender Stereotypes in Movie Recommender Systems and
  Their Implications with Privacy","Unsupervised Graph Embeddings for Session-based Recommendation with Item
  Features"
neg-d2-608,2025-03-21,,2503.17187," Cigler considered certain shifted Hankel determinants of convolution powers
of Catalan numbers and conjectured identities for these determinants. Recently,
Fulmek gave a bijective proof of Cigler's conjecture. Cigler then provided a
computational proof. We extend Cigler's determinant identities to the
convolution of general power series $F(x)$, where $F(x)$ satisfies a certain
type of quadratic equation. As an application, we present the Hankel
determinant identities of convolution powers of Motzkin numbers.",['math.CO'],2503.1286," Given a graph $H$, a graph $G$ is $H$-free if $G$ does not contain $H$ as an
induced subgraph. Shi and Shan conjectured that every $1$-tough $2k$-connected
$(P_2 \cup kP_1)$-free graph is hamiltonian for $k \geq 4$. This conjecture has
been independently confirmed by Xu, Li, and Zhou, as well as by Ota and Sanka.
Inspired by this, we prove that every $2k$-connected $(P_2\cup kP_1)$-free
graph with toughness greater than one is hamiltonian-connected.",['math.CO'],False,,,,"Hankel Determinants for Convolution of Power Series: An Extension of
  Cigler's Results","Every $2k$-connected $(P_2\cup kP_1)$-free graph with toughness greater
  than one is hamiltonian-connected"
neg-d2-609,2025-01-23,,2501.14074," The blazar 3C 279 is well known for its prolific emission of rapid flares. A
particular event occurred on 12/20/2013, exhibiting a large flux increase with
a doubling time scale of a few hours, a very hard gamma-ray spectrum, and a
time-asymmetric light curve with slow decay, but no significant variations
detected in the optical range. We propose a novel scenario to interpret this
flare, based on two emission zones, a stationary blob and a moving plasma blob.
The stationary blob, located within the BLR, accounts for the low-state
emission. The moving blob decouples from the stationary zone, accelerates and
crosses the BLR. The high-energy flare is attributed to the variable external
Compton emission as the blob moves through the BLR, while variations in the
synchrotron emission are negligible. Our interpretation differs from previous
interpretations by attributing the flare to the bulk motion and geometry of the
external photon fields, without invoking varying electron injection.",['astro-ph.HE'],2503.1153," This paper addresses the evolution of an axially symmetric magnetic field in
the core of a neutron star. The matter in the core is modeled as a system of
two fluids, namely neutrons and charged particles, with slightly different
velocity fields, controlled by their mutual collisional friction. This problem
was addressed in our previous work through the so-called ``fictitious
friction'' approach. We study the validity of our previous work and improve it
by comparing the fictitious friction approach to alternatives, making
approximations that allow it to be applied to arbitrary magnetic field
strengths and using realistic equations of state. We assume the neutron star
crust to be perfectly resistive, so its magnetic field reacts instantaneously
to changes in the core, in which we neglect the effects of Cooper pairing. We
explore different approaches to solve the equations to obtain the velocities
and chemical potential perturbations induced by a given, fixed magnetic field
configuration in the core. We also present a new version of our code to perform
time-evolving simulations and discuss the results obtained with it. Our
calculations without fictitious friction further confirm that bulk velocity is
generally much greater than ambipolar velocity, leading to faster evolution.
These findings align with those with fictitious friction, validating this
approach. We also find that, in the long term, the star evolves towards a
barotropic ``Grad-Shafranov equilibrium,'' where the magnetic force is fully
balanced by charged particle fluid forces. Qualitatively, the evolution and the
final equilibrium are independent of the magnetic field strength $B$ and the
equation of state considered. The timescale to reach this equilibrium is
proportional to $B^{-2}$ and becomes shorter for equations of state with a
smaller gradient of the ratio between the densities of protons and neutrons.",['astro-ph.HE'],False,,,,An orphan flare from a plasma blob crossing the broad-line region ?,"Validating and improving two-fluid simulations of the magnetic field
  evolution in neutron star cores"
neg-d2-610,2025-03-13,,2503.10951," This work presents the microscopic calculation of energy rates ({\gamma} ray
heating and (anti)neutrino cooling rates) due to weak decay of selected Fe
isotopes. The isotopes have astrophysical significance during the presupernova
evolution of massive stars. The energy rates are calculated using the pn QRPA
model and compared with the independent particle model (IPM), large scale shell
model (LSSM) and recent shell model calculation (GXPF1J). The reported
(anti)neutrino cooling rates are smaller by up to two orders of magnitude at
low core temperature values than the IPM rates. The two calculations compare
well at T = 30 GK. The comparison of cooling rates with the LSSM is
interesting. The pn QRPA cooling rates due to even even Fe isotopes are smaller
(up to 2 orders of magnitude). For the odd A isotopes, the reported rates are
bigger up to an order of magnitude. The pn QRPA computed cooling rates are, up
to 2 orders of magnitude, bigger when compared with the GXPF1J calculation. The
{\gamma} ray heating rates due to electron capture rates rise with the
temperature and density values of the stellar core. On the other hand, the
{\gamma} ray heating due to \b{eta} decay increases with the core temperature
values but decreases by orders of magnitude when the stellar core stiffens. The
pn QRPA computed {\gamma} heating rates are bigger (up to 3 orders of
magnitude) at high temperatures and densities (for the case of 55 56Fe) when
compared with the recent shell model results. Owing to the importance of energy
rates, this study may contribute to a realistic simulation of presupernova
evolution of massive stars.",['nucl-th'],2503.13889," Pairing of nucleons plays a key role in solving various nuclear physics
problems. We investigate the probable effects of pairing correlations on the
calculated Gamow-Teller (GT) strength distributions and the associated
$\beta$-decay half-lives. Computations are performed for a total of 35 fp-shell
nuclei using the proton-neutron quasiparticle random phase approximation
(pn-QRPA) model. The nuclei were selected because of their importance in
various astrophysical environments. Pairing gaps are one of the key parameters
in the pn-QRPA model to compute GT transitions. We employed three different
values of the pairing gaps obtained from three different empirical formulae in
our calculation. The GT strength distributions changed significantly as the
pairing gap values changed. This in turn resulted in contrasting centroid and
total strength values of the calculated GT distributions and led to differences
in calculated half-lives using the three schemes. The half-life values computed
via the three-term pairing formula, based on separation energies of nucleons,
were in best agreement with the measured data. We conclude that the traditional
choice of pairing gap values, $\Delta_p = \Delta_n = 12/\sqrt{A}$, may not lead
to half-life values in good agreement with measured data. The findings of this
study are interesting but warrant further investigation.",['nucl-th'],False,,,,"Energy rates due to Fe isotopes during presupernova evolution of massive
  stars","Investigation of effects of pairing correlations on calculated
  $\beta$-decay half-lives of fp-shell nuclei"
neg-d2-611,2025-03-10,,2503.07547," In human-robot interactions, human and robot agents maintain internal mental
models of their environment, their shared task, and each other. The accuracy of
these representations depends on each agent's ability to perform theory of
mind, i.e. to understand the knowledge, preferences, and intentions of their
teammate. When mental models diverge to the extent that it affects task
execution, reconciliation becomes necessary to prevent the degradation of
interaction. We propose a framework for bi-directional mental model
reconciliation, leveraging large language models to facilitate alignment
through semi-structured natural language dialogue. Our framework relaxes the
assumption of prior model reconciliation work that either the human or robot
agent begins with a correct model for the other agent to align to. Through our
framework, both humans and robots are able to identify and communicate missing
task-relevant context during interaction, iteratively progressing toward a
shared mental model.",['cs.RO'],2501.18564," Robotic manipulation systems operating in diverse, dynamic environments must
exhibit three critical abilities: multitask interaction, generalization to
unseen scenarios, and spatial memory. While significant progress has been made
in robotic manipulation, existing approaches often fall short in generalization
to complex environmental variations and addressing memory-dependent tasks. To
bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based
policy that leverages multi-resolution upsampling with visual representations
from large-scale foundation model. SAM2Act achieves a state-of-the-art average
success rate of 86.8% across 18 tasks in the RLBench benchmark, and
demonstrates robust generalization on The Colosseum benchmark, with only a 4.3%
performance gap under diverse environmental perturbations. Building on this
foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,
which incorporates a memory bank, an encoder, and an attention mechanism to
enhance spatial memory. To address the need for evaluating memory-dependent
tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial
memory and action recall in robotic manipulation. SAM2Act+ achieves competitive
performance on MemoryBench, significantly outperforming existing approaches and
pushing the boundaries of memory-enabled robotic systems. Project page:
https://sam2act.github.io/",['cs.RO'],False,,,,"Bi-Directional Mental Model Reconciliation for Human-Robot Interaction
  with Large Language Models","SAM2Act: Integrating Visual Foundation Model with A Memory Architecture
  for Robotic Manipulation"
neg-d2-612,2025-03-23,,2503.18251," In recent years, a lot of technological advances in computer science have
aided software programmers to create innovative and real-time user-friendly
software. With the creation of the software and the urging interest of people
to learn to write software, there is a large collection of source codes that
can be found on the web, also known as Big Code, which can be used as a source
of data for driving the machine learning applications tending to solve certain
software engineering problems. In this paper, we present COFO, a dataset
consisting of 809 classes/problems with a total of 369K source codes written in
C, C++, Java, and Python programming languages, along with other metadata such
as code tags, problem specification, and input-output specifications. COFO has
been scraped from the openly available Codeforces website using a
selenium-beautifulsoup-python based scraper. We envision that this dataset can
be useful for solving machine learning-based problems like program
classification/recognition, tagging, predicting program properties, and code
comprehension.",['cs.SE'],2503.09189," Observability of a software system aims at allowing its engineers and
operators to keep the system robust and highly available. With this paper, we
present the Kieker Observability Framework Version 2, the successor of the
Kieker Monitoring Framework.
  In this tool artifact paper, we do not just present the Kieker framework, but
also a demonstration of its application to the TeaStore benchmark, integrated
with the visual analytics tool ExplorViz. This demo is provided both as an
online service and as an artifact to deploy it yourself.",['cs.SE'],False,,,,"COFO: COdeFOrces dataset for Program Classification, Recognition and
  Tagging",The Kieker Observability Framework Version 2
neg-d2-613,2025-03-19,,2503.15807," In the field of video-language pretraining, existing models face numerous
challenges in terms of inference efficiency and multimodal data processing.
This paper proposes a KunLunBaize-VoT-R1 video inference model based on a
long-sequence image encoder, along with its training and application methods.
By integrating image packing technology, the Autonomy-of-Experts (AoE)
architecture, and combining the video of Thought (VoT), a large language model
(LLM) trained with large-scale reinforcement learning, and multiple training
techniques, the efficiency and accuracy of the model in video inference tasks
are effectively improved. Experiments show that this model performs
outstandingly in multiple tests, providing a new solution for video-language
understanding.",['cs.AI'],2503.15807," In the field of video-language pretraining, existing models face numerous
challenges in terms of inference efficiency and multimodal data processing.
This paper proposes a KunLunBaize-VoT-R1 video inference model based on a
long-sequence image encoder, along with its training and application methods.
By integrating image packing technology, the Autonomy-of-Experts (AoE)
architecture, and combining the video of Thought (VoT), a large language model
(LLM) trained with large-scale reinforcement learning, and multiple training
techniques, the efficiency and accuracy of the model in video inference tasks
are effectively improved. Experiments show that this model performs
outstandingly in multiple tests, providing a new solution for video-language
understanding.",['cs.AI'],False,,,,"Video-VoT-R1: An efficient video inference model integrating image
  packing and AoE architecture","Video-VoT-R1: An efficient video inference model integrating image
  packing and AoE architecture"
neg-d2-614,2025-02-03,,2502.01256," With the presence of robots increasing in the society, the need for
interacting with robots is becoming necessary. The field of Human-Robot
Interaction (HRI) has emerged important since more repetitive and tiresome jobs
are being done by robots. In the recent times, the field of soft robotics has
seen a boom in the field of research and commercialization. The Industry 5.0
focuses on human robot collaboration which also spurs the field of soft
robotics. However the HRI for soft robotics is still in the nascent stage. In
this work we review and then discuss how HRI is done for soft robots. We first
discuss the control, design, materials and manufacturing of soft robots. This
will provide an understanding of what is being interacted with. Then we discuss
about the various input and output modalities that are used in HRI. The
applications where the HRI for soft robots are found in the literature are
discussed in detail. Then the limitations of HRI for soft robots and various
research opportunities that exist in this field are discussed in detail. It is
concluded that there is a huge scope for development for HRI for soft robots.",['cs.RO'],2503.04014," Dexterous hand manipulation in real-world scenarios presents considerable
challenges due to its demands for both dexterity and precision. While imitation
learning approaches have thoroughly examined these challenges, they still
require a significant number of expert demonstrations and are limited by a
constrained performance upper bound. In this paper, we propose a novel and
efficient Imitation-Bootstrapped Online Reinforcement Learning (IBORL) method
tailored for robotic dexterous hand manipulation in real-world environments.
Specifically, we pretrain the policy using a limited set of expert
demonstrations and subsequently finetune this policy through direct
reinforcement learning in the real world. To address the catastrophic
forgetting issues that arise from the distribution shift between expert
demonstrations and real-world environments, we design a regularization term
that balances the exploration of novel behaviors with the preservation of the
pretrained policy. Our experiments with real-world tasks demonstrate that our
method significantly outperforms existing approaches, achieving an almost 100%
success rate and a 23% improvement in cycle time. Furthermore, by finetuning
with online reinforcement learning, our method surpasses expert demonstrations
and uncovers superior policies. Our code and empirical results are available in
https://hggforget.github.io/iborl.github.io/.",['cs.RO'],False,,,,Soft is Safe: Human-Robot Interaction for Soft Robots,"Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online
  Reinforcement Learning"
neg-d2-615,2025-02-28,,2502.20927," Efficient video transmission is essential for seamless communication and
collaboration within the visually-driven digital landscape. To achieve low
latency and high-quality video transmission over a bandwidth-constrained noisy
wireless channel, we propose a stable diffusion (SD)-based goal-oriented
semantic communication (GSC) framework. In this framework, we first design a
semantic encoder that effectively identify the keyframes from video and extract
the relevant semantic information (SI) to reduce the transmission data size. We
then develop a semantic decoder to reconstruct the keyframes from the received
SI and further generate the full video from the reconstructed keyframes using
frame interpolation to ensure high-quality reconstruction. Recognizing the
impact of wireless channel noise on SI transmission, we also propose an
SD-based denoiser for GSC (SD-GSC) condition on an instantaneous channel gain
to remove the channel noise from the received noisy SI under a known channel.
For scenarios with an unknown channel, we further propose a parallel SD
denoiser for GSC (PSD-GSC) to jointly learn the distribution of channel gains
and denoise the received SI. It is shown that, with the known channel, our
proposed SD-GSC outperforms state-of-the-art ADJSCC, Latent-Diff DNSC, DeepWiVe
and DVST, improving Peak Signal-to-Noise Ratio (PSNR) by 69%, 58%, 33% and 38%,
reducing mean squared error (MSE) by 52%, 50%, 41% and 45%, and reducing
Fr\'echet Video Distance (FVD) by 38%, 32%, 22% and 24%, respectively. With the
unknown channel, our PSD-GSC achieves a 17% improvement in PSNR, a 29%
reduction in MSE, and a 19% reduction in FVD compared to MMSE
equalizer-enhanced SD-GSC. These significant performance improvements
demonstrate the robustness and superiority of our proposed methods in enhancing
video transmission quality and efficiency under various channel conditions.",['eess.IV'],2501.15246," Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular
environments. Accurate reconstruction of high-resolution volumes is complicated
by the very low signal-to-noise ratio and a restricted range of sample tilts,
creating a missing wedge of Fourier information. Recent self-supervised deep
learning approaches, which post-process initial reconstructions done by
filtered backprojection (FBP), have significantly improved reconstruction
quality, but they are computationally expensive, demand large memory, and
require retraining for each new dataset. End-to-end supervised learning is an
appealing alternative but is impeded by the lack of ground truth and the large
memory demands of high-resolution volumetric data. Training on synthetic data
often leads to overfitting and poor generalization to real data, and, to date,
no general end-to-end deep learning reconstructors exist for cryo-ET. In this
work, we introduce CryoLithe, a local, memory-efficient reconstruction network
that directly estimates the volume from an aligned tilt-series, overcoming the
suboptimal FBP. We demonstrate that leveraging transform-domain locality makes
our network robust to distribution shifts, enabling effective supervised
training and giving excellent results on real data -- without retraining or
fine-tuning.",['eess.IV'],False,,,,"Goal-Oriented Semantic Communication for Wireless Video Transmission via
  Generative AI",End-to-end localized deep learning for Cryo-ET
neg-d2-616,2025-02-14,,2502.10153," We present the MeerKAT discovery and MeerLICHT contemporaneous optical
observations of the Fast Radio Burst (FRB) 20230808F, which was found to have a
dispersion measure of $\mathrm{DM}=653.2\pm0.4\mathrm{\,pc\,cm^{-3}}$. FRB
20230808F has a scattering timescale $\tau_{s}=3.1\pm0.1\,\mathrm{ms}$ at
$1563.6$ MHz, a rotation measure
$\mathrm{RM}=169.4\pm0.2\,\mathrm{rad\,m^{-2}}$, and a radio fluence
$F_{\mathrm{radio}}=1.72\pm0.01\,\mathrm{Jy\,ms}$. We find no optical
counterpart in the time immediately after the FRB, nor in the three months
after the FRB during which we continued to monitor the field of the FRB. We set
an optical upper flux limit in MeerLICHT's $q$-band of $11.7\,\mathrm{\mu Jy}$
for a 60 s exposure which started $\sim3.4$ s after the burst, which
corresponds to an optical fluence, $F_{\mathrm{opt}}$, of
$0.039\,\mathrm{Jy\,ms}$ on a timescale of $\sim3.4$ s. We obtain an estimate
for the $q-$band luminosity limit of $vL_{v}\sim
1.3\times10^{43}\,\mathrm{erg\,s^{-1}}$. We localise the burst to a close
galaxy pair at a redshift of $z_{\mathrm{spec}}=0.3472\pm0.0002$. Our time
delay of $\sim3.4$ s between the FRB arrival time and the start of our optical
exposure is the shortest ever for an as yet non-repeating FRB, and hence the
closest to simultaneous optical follow-up that exists for such an FRB.",['astro-ph.HE'],2501.01736," The Pierre Auger Observatory concluded its first phase of data taking after
seventeen years of operation. The dataset collected by its surface and
fluorescence detectors (FD and SD) provides us with the most precise estimates
of the energy spectrum and mass composition of ultra-high energy cosmic rays
yet available. We present measurements of the depth of shower maximum, the main
quantity used to derive species of primary particles, determined either from
the direct observation of longitudinal profiles of showers by the FD, or
indirectly through the analysis of signals in the SD stations. The energy
spectrum of primaries is also determined from both FD and SD measurements,
where the former exhibits lower systematic uncertainty in the energy
determination while the latter exploits unprecedentedly large exposure. The
data for primaries with energy below 1 EeV are also available thanks to the
high-elevation telescopes of FD and the denser array of SD, making measurements
possible down to 6 PeV and 60 PeV, respectively.",['astro-ph.HE'],False,,,,"Contemporaneous optical-radio observations of a fast radio burst in a
  close galaxy pair","Energy spectrum and mass composition of cosmic rays from Phase I data
  measured using the Pierre Auger Observatory"
neg-d2-617,2025-03-11,,2503.09023," Given that the phase amplification method based on harmonic generation
exhibits significant phase super-resolution capability in interferometric
precision measurement, extending this technology to birefringence
interferometers to achieve super-resolution characterization of birefringent
crystal properties has important research significance and application value.
Here, we achieve a four-fold enhancement in the measurement resolution of the
thermo-optic coefficient of a KTiOPO4 crystal by combining a self-stabilized
birefringence interferometer with cascaded second harmonic generation
processes. We observe the tunable interference beating phenomenon by rotating a
birefringent crystal versus the temperature of the crystal for the fundamental
wave, second harmonic, and fourth harmonic. Furthermore, the fourth harmonic
interference fringes beat 4 times faster than the fundamental wave interference
fringes. This beating effect is used to determine the thermo-optic coefficients
of the two principal refractive axes with a single measurement. This work
provides a feasible, real-time, and robust method for super-resolution
measurements based on birefringence interferometry.",['physics.optics'],2502.03996," The efficient and independent operation of power-over-fiber (PoF) and
distributed acoustic sensing (DAS) has been demonstrated using standard
single-mode fiber (SSMF). A transmission optical power efficiency (OPTE) of
6.67% was achieved over an 11.8 km fiber link, supporting both power delivery
and distributed optical fiber sensing (DOFS). To minimize cross-talk, the
system separates the power and sensing channels by a 40 THz bandwidth. In the
experiment, the power and sensing light wavelengths are 1064 nm (continuous)
and 1550 nm (pulsed), respectively. As the transmitted optical power increased
from 0 W to 2.13 W, the DAS system successfully localized vibration sources and
reconstructed phase information, confirming its ability to operate under high
optical power. The reported scheme verifies the possibility of constructing the
sensing-energy hybrid network based on conventional optical fiber with the
advantages of flexibility and low cost.",['physics.optics'],False,,,,"Super-resolution measurement of thermo-optic coefficient of KTP crystal
  based on phase amplification","Power-over-fiber and distributed acoustic sensing hybridization in
  single fiber channel"
neg-d2-618,2025-03-18,,2503.14264," Deciding the positivity of a sequence defined by a linear recurrence and
initial conditions is, in general, a hard problem. When the coefficients of the
recurrences are constants, decidability has only been proven up to order 5. The
difficulty arises when the characteristic polynomial of the recurrence has
several roots of maximal modulus, called dominant roots of the recurrence. We
study the positivity problem for recurrences with polynomial coefficients,
focusing on sequences of Poincar\'e type, which are perturbations of
constant-coefficient recurrences. The dominant eigenvalues of a recurrence in
this class are the dominant roots of the associated constant-coefficient
recurrence. Previously, we have proved the decidability of positivity for
recurrences having a unique, simple, dominant eigenvalue, under a genericity
assumption. The associated algorithm proves positivity by constructing a
positive cone contracted by the recurrence operator. We extend this cone-based
approach to a larger class of recurrences, where a contracted cone may no
longer exist. The main idea is to construct a sequence of cones. Each cone in
this sequence is mapped by the recurrence operator to the next. This
construction can be applied to prove positivity by induction. For recurrences
with several simple dominant eigenvalues, we provide a condition that ensures
that these successive inclusions hold. Additionally, we demonstrate the
applicability of our method through examples, including recurrences with a
double dominant eigenvalue.",['cs.SC'],2503.14264," Deciding the positivity of a sequence defined by a linear recurrence and
initial conditions is, in general, a hard problem. When the coefficients of the
recurrences are constants, decidability has only been proven up to order 5. The
difficulty arises when the characteristic polynomial of the recurrence has
several roots of maximal modulus, called dominant roots of the recurrence. We
study the positivity problem for recurrences with polynomial coefficients,
focusing on sequences of Poincar\'e type, which are perturbations of
constant-coefficient recurrences. The dominant eigenvalues of a recurrence in
this class are the dominant roots of the associated constant-coefficient
recurrence. Previously, we have proved the decidability of positivity for
recurrences having a unique, simple, dominant eigenvalue, under a genericity
assumption. The associated algorithm proves positivity by constructing a
positive cone contracted by the recurrence operator. We extend this cone-based
approach to a larger class of recurrences, where a contracted cone may no
longer exist. The main idea is to construct a sequence of cones. Each cone in
this sequence is mapped by the recurrence operator to the next. This
construction can be applied to prove positivity by induction. For recurrences
with several simple dominant eigenvalues, we provide a condition that ensures
that these successive inclusions hold. Additionally, we demonstrate the
applicability of our method through examples, including recurrences with a
double dominant eigenvalue.",['cs.SC'],False,,,,"Positivity Proofs for Linear Recurrences with Several Dominant
  Eigenvalues","Positivity Proofs for Linear Recurrences with Several Dominant
  Eigenvalues"
neg-d2-619,2025-02-28,,2502.20756," The aim of this paper is to state and prove existence and uniqueness results
for a general elliptic problem with homogeneous Neumann boundary conditions,
often associated with image processing tasks like denoising. The novelty is
that we surpass the lack of coercivity of the Euler-Lagrange functional with an
innovative technique that has at its core the idea of showing that the minimum
of the energy functional over a subset of the space $W^{1,p(x)}(\Omega)$
coincides with the global minimum. The obtained existence result applies to
multiple-phase elliptic problems under remarkably weak assumptions.",['math.AP'],2501.03963," We prove global well-posedness and scattering for the massive
Dirac-Klein-Gordon system with small and low regularity initial data in
dimension two. To achieve this, we impose a non-resonance condition on the
masses.",['math.AP'],False,,,,"A general quasilinear elliptic problem with variable exponents and
  Neumann boundary conditions for image processing","Global well-posedness and scattering for the massive Dirac-Klein-Gordon
  system in two dimensions"
neg-d2-620,2025-02-21,,2502.15612," State space models (SSMs), such as Mamba, have emerged as an efficient
alternative to transformers for long-context sequence modeling. However,
despite their growing adoption, SSMs lack the interpretability tools that have
been crucial for understanding and improving attention-based architectures.
While recent efforts provide insights into Mamba's internal mechanisms, they do
not explicitly decompose token-wise contributions, leaving gaps in
understanding how Mamba selectively processes sequences across layers. In this
work, we introduce LaTIM, a novel token-level decomposition method for both
Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively
evaluate our method across diverse tasks, including machine translation,
copying, and retrieval-based generation, demonstrating its effectiveness in
revealing Mamba's token-to-token interaction patterns.",['cs.CL'],2501.16748," Large Language Models (LLMs) have shown remarkable advancements but also
raise concerns about cultural bias, often reflecting dominant narratives at the
expense of under-represented subcultures. In this study, we evaluate the
capacity of LLMs to recognize and accurately respond to the Little Traditions
within Indian society, encompassing localized cultural practices and
subcultures such as caste, kinship, marriage, and religion. Through a series of
case studies, we assess whether LLMs can balance the interplay between dominant
Great Traditions and localized Little Traditions. We explore various prompting
strategies and further investigate whether using prompts in regional languages
enhances the models cultural sensitivity and response quality. Our findings
reveal that while LLMs demonstrate an ability to articulate cultural nuances,
they often struggle to apply this understanding in practical, context-specific
scenarios. To the best of our knowledge, this is the first study to analyze
LLMs engagement with Indian subcultures, offering critical insights into the
challenges of embedding cultural diversity in AI systems.",['cs.CL'],False,,,,LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models,"Through the Prism of Culture: Evaluating LLMs' Understanding of Indian
  Subcultures and Traditions"
neg-d2-621,2025-03-23,,2503.18282," Multi-object tracking, player identification, and pose estimation are
fundamental components of sports analytics, essential for analyzing player
movements, performance, and tactical strategies. However, existing datasets and
methodologies primarily target mainstream team sports such as soccer and
conventional 5-on-5 basketball, often overlooking scenarios involving
fixed-camera setups commonly used at amateur levels, less mainstream sports, or
datasets that explicitly incorporate pose annotations. In this paper, we
propose the TrackID3x3 dataset, the first publicly available comprehensive
dataset specifically designed for multi-player tracking, player identification,
and pose estimation in 3x3 basketball scenarios. The dataset comprises three
distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera
footage), capturing diverse full-court camera perspectives and environments. We
also introduce the Track-ID task, a simplified variant of the game state
reconstruction task that excludes field detection and focuses exclusively on
fixed-camera scenarios. To evaluate performance, we propose a baseline
algorithm called Track-ID algorithm, tailored to assess tracking and
identification quality. Furthermore, our benchmark experiments, utilizing
recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose
estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results
and highlight remaining challenges. Our dataset and evaluation benchmarks
provide a solid foundation for advancing automated analytics in 3x3 basketball.
Dataset and code will be available at
https://github.com/open-starlab/TrackID3x3.",['cs.CV'],2501.15201," Training semantic segmenter with synthetic data has been attracting great
attention due to its easy accessibility and huge quantities. Most previous
methods focused on producing large-scale synthetic image-annotation samples and
then training the segmenter with all of them. However, such a solution remains
a main challenge in that the poor-quality samples are unavoidable, and using
them to train the model will damage the training process. In this paper, we
propose a training-free Synthetic Data Selection (SDS) strategy with CLIP to
select high-quality samples for building a reliable synthetic dataset.
Specifically, given massive synthetic image-annotation pairs, we first design a
Perturbation-based CLIP Similarity (PCS) to measure the reliability of
synthetic image, thus removing samples with low-quality images. Then we propose
a class-balance Annotation Similarity Filter (ASF) by comparing the synthetic
annotation with the response of CLIP to remove the samples related to
low-quality annotations. The experimental results show that using our method
significantly reduces the data size by half, while the trained segmenter
achieves higher performance. The code is released at
https://github.com/tanghao2000/SDS.",['cs.CV'],False,,,,"TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with
  Identification and Pose Estimation in 3x3 Basketball Full-court Videos","A Training-free Synthetic Data Selection Method for Semantic
  Segmentation"
neg-d2-622,2025-01-04,,2501.02254," In this paper, we consider a class of structured nonsmooth fractional
minimization, where the first part of the objective is the ratio of a
nonnegative nonsmooth nonconvex function to a nonnegative nonsmooth convex
function, while the second part is the difference of a smooth nonconvex
function and a nonsmooth convex function. This model problem has many important
applications, for example, the scale-invariant sparse signal recovery in signal
processing. However, the existing methods for fractional programs are not
suitable for solving this problem due to its special structure. We first
present a novel nonfractional min-max reformulation for the original fractional
program and show the connections between their global (local) optimal solutions
and stationary points. Based on the reformulation, we propose an alternating
maximization proximal descent algorithm and show its subsequential convergence
towards a critical point of the original fractional program under a mild
assumption. By further assuming the Kurdyka-{\L}ojasiewicz (KL) property of an
auxiliary function, we also establish the convergence of the entire solution
sequence generated by the proposed algorithm. Finally, some numerical
experiments on the scale-invariant sparse signal recovery are conducted to
demonstrate the efficiency of the proposed method.",['math.OC'],2502.04936," We study the problem of controlling the initial condition of a vibrating
beam. The optimal control problem seeks to determine solutions of initial
velocity that assure the approach of the state of the beam to a given target
function in the $L^2-$norm. We prove both the existence and uniqueness of the
optimal solution. Employing identities based on the adjoint and difference
problems, we determine the Fr\'echet derivative of the cost functional. We
further derive the necessary optimality conditions of this control problem.
Finally, we provide a sketch of a gradient-based algorithm, that rests on the
explicit formula of the gradient of the cost functional, to obtain numerical
solutions.",['math.OC'],False,,,,"A min-max reformulation and proximal algorithms for a class of
  structured nonsmooth fractional optimization problems","On the optimal control of initial velocity in a hyperbolic beam equation
  by the variational method"
neg-d2-623,2025-02-17,,2502.11838," The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is
among the early evidences that they contain multiple populations of stars.
Indeed, the location of each star along the HB depends both on its initial
helium content (Y) and on the global average mass loss along the red giant
branch ($\mu$). In most GCs, it is generally straightforward to analyse the
first stellar population (standard Y), and the most extreme one (largest Y),
while it is more tricky to look at the ""intermediate"" populations (mildly
enhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever
possible the helium abundance for each stellar populations is constrained by
using independent measurements present in the literature. We compare population
synthesis models with photometric catalogues from the Hubble Space Telescope
Treasury survey to derive the parameters of these HB stars. We find that the
location of helium enriched stars on the HB is reproduced only by adopting a
higher value of $\mu$ with respect to the first generation stars in all the
analysed stellar populations. We also find that $\mu$ correlates with the
helium enhancement of the populations. This holds for both clusters. This
finding is naturally predicted by the model of ''pre-main sequence disc early
loss'', previously suggested in the literature, and is consistent with the
findings of multiple-populations formation models that foresee the formation of
second generation stars in a cooling flow.",['astro-ph.SR'],2502.19256," Observations of stars other than the Sun are sensitive to oscillations of
only low degree. Many are high-order acoustic modes. Acoustic frequencies of
main-sequence stars, for example, satisfy a well-known pattern, which some
astronomers have adopted even for red-giant stars. That is not wise, because
the internal structures of these stars can be quite different from those on the
Main Sequence, which is populated by stars whose structure is regular. Here I
report on pondering this matter, and point out two fundamental deviations from
the commonly adopted relation. There are aspects of the regular relation that
are connected in a simple way to gross properties of the star, such as the
dependence of the eigenfrequencies on the linear combination
$n+\textstyle{\frac {1}{2}}l$ of the order $n$ and degree $l$, which is
characteristic of a regular spherical acoustic cavity. That is not a feature of
red-giant frequencies, because, as experienced by the waves, red-giant stars
appear to have (phantom) singular centres, which substantially modify the
propagation of waves. That requires a generalization of the eigenfrequency
relation, which I present here. When fitted to the observed frequencies of the
Sun, the outcome is consistent with the Sun being round, with no singularity in
the core. That is hardly novel, but at least it provides some assurance that
our understanding of stellar acoustic wave dynamics is on a sound footing.",['astro-ph.SR'],False,,,,"Mass loss along the red giant branch of the intermediate stellar
  populations in NGC6752 and NGC2808",Some musings on erythrogigantoacoustics
neg-d2-624,2025-02-05,,2502.02991," We consider a generalized Derrida-Retaux model on a Galton-Watson tree with a
geometric offspring distribution. For a class of recursive systems, including
the Derrida-Retaux model with either a geometric or exponential initial
distribution, we characterize the critical curve using an involution-type
equation and prove that the free energy satisfies the Derrida-Retaux
conjecture.",['math.PR'],2503.08837," We study a system of reflecting Brownian motions on the positive half-line in
which each particle has a drift toward the origin determined by the local times
at the origin of all the particles. We show that if this local time drift is
too strong, such systems can exhibit a breakdown in their solutions in that
there is a time beyond which the system cannot be extended. In the finite
particle case we give a complete characterization of the finite time breakdown,
relying on a novel dynamic graph structure. We consider the mean-field limit of
the system in the symmetric case and show that there is a McKean--Vlasov
representation. If the drift is too strong, the solution to the corresponding
Fokker--Planck equation has a blow up in its solution. We also establish the
existence of stationary and self-similar solutions to the McKean--Vlasov
equation in the case where there is no breakdown of the system. This work is
motivated by models for liquidity in financial markets, the supercooled Stefan
problem, and a toy model for cell polarization.",['math.PR'],False,,,,The Derrida-Retaux model on a geometric Galton-Watson tree,"Particle Systems and McKean--Vlasov Dynamics with Singular Interaction
  through Local Times"
neg-d2-625,2025-03-05,,2503.03455," Despite advancements in MLOps and AutoML, ML development still remains
challenging for data scientists. First, there is poor support for and limited
control over optimizing and evolving ML models. Second, there is lack of
efficient mechanisms for continuous evolution of ML models which would leverage
the knowledge gained in previous optimizations of the same or different models.
We propose an experiment-driven MLOps approach which tackles these problems.
Our approach relies on the concept of an experiment, which embodies a fully
controllable optimization process. It introduces full traceability and
repeatability to the optimization process, allows humans to be in full control
of it, and enables continuous improvement of the ML system. Importantly, it
also establishes knowledge, which is carried over and built across a series of
experiments and allows for improving the efficiency of experimentation over
time. We demonstrate our approach through its realization and application in
the ExtremeXP1 project (Horizon Europe).",['cs.SE'],2503.09282," In operating system development, concurrency poses significant challenges. It
is difficult for humans to manually review concurrent behaviors or to write
test cases covering all possible executions, often resulting in critical bugs.
Preemption in schedulers serves as a typical example. This paper proposes a
development method for concurrent software, such as schedulers. Our method
incorporates model checking as an aid for tracing code, simplifying the
analysis of concurrent behavior; we refer to this as model checking-assisted
code review. While this approach aids in tracing behaviors, the accuracy of the
results is limited because of the semantics gap between the modeling language
and the programming language. Therefore, we also introduce runtime verification
to address this limitation in model checking-assisted code review. We applied
our approach to a real-world operating system, Awkernel, as a case study. This
new operating system, currently under development for autonomous driving, is
designed for preemptive task execution using asynchronous functions in Rust.
After implementing our method, we identified several bugs that are difficult to
detect through manual reviews or simple tests.",['cs.SE'],False,,,,Towards Continuous Experiment-driven MLOps,A Case Study on Model Checking and Runtime Verification for Awkernel
neg-d2-626,2025-03-19,,2503.15626," Selecting the combination of security controls that will most effectively
protect a system's assets is a difficult task. If the wrong controls are
selected, the system may be left vulnerable to cyber-attacks that can impact
the confidentiality, integrity, and availability of critical data and services.
In practical settings, as standardized control catalogues can be quite large,
it is not possible to select and implement every control possible. Instead,
considerations, such as budget, effectiveness, and dependencies among various
controls, must be considered to choose a combination of security controls that
best achieve a set of system security objectives. In this paper, we present a
game-theoretic approach for selecting effective combinations of security
controls based on expected attacker profiles and a set budget. The control
selection problem is set up as a two-person zero-sum one-shot game. Valid
control combinations for selection are generated using an algebraic formalism
to account for dependencies among selected controls. Using a software tool, we
apply the approach on a fictional Canadian military system with Canada's
standardized control catalogue, ITSG-33. Through this case study, we
demonstrate the approach's scalability to assist in selecting an effective set
of security controls for large systems. The results illustrate how a security
analyst can use the proposed approach and supporting tool to guide and support
decision-making in the control selection activity when developing secure
systems of all sizes.",['cs.SE'],2502.20812," The rapid growth of Large Language Models (LLMs) and AI-driven applications
has propelled Vector Database Management Systems (VDBMSs) into the spotlight as
a critical infrastructure component. VDBMS specializes in storing, indexing,
and querying dense vector embeddings, enabling advanced LLM capabilities such
as retrieval-augmented generation, long-term memory, and caching mechanisms.
However, the explosive adoption of VDBMS has outpaced the development of
rigorous software testing methodologies tailored for these emerging systems.
Unlike traditional databases optimized for structured data, VDBMS face unique
testing challenges stemming from the high-dimensional nature of vector data,
the fuzzy semantics in vector search, and the need to support dynamic data
scaling and hybrid query processing. In this paper, we begin by conducting an
empirical study of VDBMS defects and identify key challenges in test input
generation, oracle definition, and test evaluation. Drawing from these
insights, we propose the first comprehensive research roadmap for developing
effective testing methodologies tailored to VDBMS. By addressing these
challenges, the software testing community can contribute to the development of
more reliable and trustworthy VDBMS, enabling the full potential of LLMs and
data-intensive AI applications.",['cs.SE'],False,,,,"A Scalable Game-Theoretic Approach for Selecting Security Controls from
  Standardized Catalogues","Towards Reliable Vector Database Management Systems: A Software Testing
  Roadmap for 2030"
neg-d2-627,2025-02-27,,2502.19835," Let $B$ be a bidirected multigraph with signing $\sigma$, let $X$ be a set of
vertices in $B$, and let $k$ be a non-negative integer. For any pair of vertex
sets $S,T\subset V(B)$ satisfying $X\cap S = X\cap T$, we denote by $B_{S,T}$
the multigraph with the same vertex set as $B$ and with edge set consisting of
those edges $e$ of $B$ each of whose endvertices $v$ satisfies $v\notin S\cup
T$ or $v\in S\setminus T$, $\sigma(v,e)=-$ or $v\in T\setminus S$,
$\sigma(v,e)=+$. We prove that $B$ admits a set of $k$ pairwise disjoint
$X$-paths if and only if for any $S,T\subseteq V(B)$ with $X\cap S = X\cap T$,
the inequality $\left\lvert S\cap T \right\rvert +\sum \lfloor \tfrac{1}{2}
\left\lvert V(C)\cap (X\cup S\cup T) \right\rvert \rfloor \geq k$ holds where
the sum is indexed by the components of $B_{S,T}$. This result is a
generalization of a result of Gallai from undirected graphs to bidirected ones.
Furthermore, we will deduce from this a kind of an Erd\H{o}s-P\'osa property
for $X$-paths in bidirected multigraphs.",['math.CO'],2503.14022," As the scale of data centers continues to grow, there is an increasing demand
for interconnection networks to resist malicious attacks. Hence, it is
necessary to evaluate the reliability of networks under various fault patterns.
The family of generalized $K_4$-hypercubes serve as interconnection networks of
data centers, characterized by topological structures with exceptional
properties. The $h$-extra edge-connectivity $\lambda_h$, the $l$-super
edge-connectivity $\lambda^l$, the $l$-average degree edge-connectivity
$\overline{\lambda^l}$, the $l$-embedded edge-connectivity $\eta_l$ and the
cyclic edge-connectivity $\lambda_c$ are vital parameters to accurately assess
the reliability of interconnection networks. Let integer $n\geq3$. This paper
obtains the optimal solution of the edge isoperimetric problem and its explicit
representation, which offers an upper bound of the $h$-extra edge-connectivity
of an $n$-dimensional $K_4$-hypercube $H_n^4$. As an application, we presents
$\lambda_h(H_n^4)$ for $1\leq h\leq 2^{\lceil n/2 \rceil }$. Moreover, for
$2^{\lceil n/2\rceil+t}-g_t \le h\le2^{\lceil n/2\rceil+t}$,
$g_t=\lceil(2^{2t+2+\gamma})/3\rceil$,
  $0\leq t \leq\lfloor n/2\rfloor-1 $, $\gamma=0$ for even $n$ and $\gamma=1$
for odd $n$, $\lambda_h(H_n^4)$ is a constant $(\lfloor n/2\rfloor-t)2^{\lceil
n/2\rceil+t}$. The above lower and upper bounds of the integer $h$ are both
sharp. Furthermore, $\lambda^l(H_n^4)$, $\overline{\lambda^l}(H_n^4)$,
$\lambda_{2^l}(H_n^4)$, and $\eta_l(H_n^4)$ share a common value $(n-l)2^l$ for
$2\leq l\leq n-1$, and we determines the values of $\lambda_c(H_n^4)$.",['math.CO'],False,,,,Disjoint $X$-paths in bidirected graphs,"Reliability Evaluation of Generalized $K_4$-Hypercubes Based on Five
  Link Fault Patterns"
neg-d2-628,2025-01-02,,2501.01557," Surround-View System (SVS) is an essential component in Advanced Driver
Assistance System (ADAS) and requires precise calibrations. However,
conventional offline extrinsic calibration methods are cumbersome and
time-consuming as they rely heavily on physical patterns. Additionally, these
methods primarily focus on short-range areas surrounding the vehicle, resulting
in lower calibration quality in more distant zones. To address these
limitations, we propose Click-Calib, a pattern-free approach for offline SVS
extrinsic calibration. Without requiring any special setup, the user only needs
to click a few keypoints on the ground in natural scenes. Unlike other offline
calibration approaches, Click-Calib optimizes camera poses over a wide range by
minimizing reprojection distance errors of keypoints, thereby achieving
accurate calibrations at both short and long distances. Furthermore,
Click-Calib supports both single-frame and multiple-frame modes, with the
latter offering even better results. Evaluations on our in-house dataset and
the public WoodScape dataset demonstrate its superior accuracy and robustness
compared to baseline methods. Code is available at
https://github.com/lwangvaleo/click_calib.",['cs.CV'],2502.05457," Content-based video retrieval is one of the most challenging tasks in
surveillance systems. In this study, Latent Dirichlet Allocation (LDA) topic
model is used to annotate surveillance videos in an unsupervised manner. In
scene understanding methods, some of the learned patterns are ambiguous and
represents a mixture of atomic actions. To address the ambiguity issue in the
proposed method, feature vectors, and the primary model are processed to obtain
a secondary model which describes the scene with primitive patterns that lack
any ambiguity. Experiments show performance improvement in the retrieval task
compared to other topic model-based methods. In terms of false positive and
true positive responses, the proposed method achieves at least 80\% and 124\%
improvement respectively. Four search strategies are proposed, and users can
define and search for a variety of activities using the proposed query
formulation which is based on topic models. In addition, the lightweight
database in our method occupies much fewer storage which in turn speeds up the
search procedure compared to the methods which are based on low-level features.",['cs.CV'],False,,,,"Click-Calib: A Robust Extrinsic Calibration Method for Surround-View
  Systems","Content-based Video Retrieval in Traffic Videos using Latent Dirichlet
  Allocation Topic Model"
neg-d2-629,2025-02-21,,2502.1522," A robust estimation framework for binary regression models is studied, aiming
to extend traditional approaches like logistic regression models. While
previous studies largely focused on logistic models, we explore a broader class
of models defined by general link functions. We incorporate various loss
functions to improve estimation under model misspecification. Our investigation
addresses robustness against outliers and model misspecifications, leveraging
divergence-based techniques such as the $\beta$-divergence and
$\gamma$-divergence, which generalize the maximum likelihood approach. These
divergences introduce loss functions that mitigate the influence of atypical
data points while retaining Fisher consistency. We establish a theoretical
property of the estimators under both correctly specified and misspecified
models, analyzing their robustness through quantifying the effect of outliers
in linear predictor. Furthermore, we uncover novel relationships between
existing estimators and robust loss functions, identifying previously
unexplored classes of robust estimators. Numerical experiments illustrate the
efficacy of the proposed methods across various contamination scenarios,
demonstrating their potential to enhance reliability in binary classification
tasks. By providing a unified framework, this study highlights the versatility
and robustness of divergence-based methods, offering insights into their
practical application and theoretical underpinnings.",['stat.ME'],2502.11032," Model-assisted estimation combines sample survey data with auxiliary
information to increase precision when estimating finite population quantities.
Accurately estimating the variance of model-assisted estimators is challenging:
the classical approach ignores uncertainty from estimating the working model
for the functional relationship between survey and auxiliary variables. This
approach may be asymptotically valid, but can underestimate variance in
practical settings with limited sample sizes. In this work, we develop a
connection between model-assisted estimation and the theory of U- and
V-statistics. We demonstrate that when predictions from the working model for
the variable of interest can be represented as a U- or V-statistic, the
resulting model-assisted estimator also admits a U- or V-statistic
representation. We exploit this connection to derive an improved estimator of
the exact variance of such model-assisted estimators. The class of working
models for which this strategy can be used is broad, ranging from linear models
to modern ensemble methods. We apply our approach to the model-assisted
estimator constructed with a linear regression working model, commonly referred
to as the generalized regression estimator, show that it can be re-written as a
U-statistic, and propose an estimator of its exact variance. We illustrate our
proposal and compare it against the classical asymptotic variance estimator
using household survey data from the American Community Survey.",['stat.ME'],False,,,,On a class of binary regression models and their robust estimation,"Exact variance estimation for model-assisted survey estimators using U-
  and V-statistics"
neg-d2-630,2025-01-02,,2501.0106," Using the pinch technique, we compute the one-loop vertices of weak
interactions in the B-LSSM and incorporate their pinch contributions into the
gauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$
parameters in the Standard Model based on the $SU(2)_L\otimes U(1)_Y$ group,
the corresponding parameters in the B-LSSM are modified. We provide these
redefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the
results. In the framework of the low-energy effective Lagrangian for weak
interactions, the $S$, $T$, and $U$ parameters can be expressed as functions of
certain parameters in the B-LSSM. The updated experimental and fitting results
constrain the parameter space of the B-LSSM strongly.",['hep-ph'],2502.18931," We investigate axion emission from singlet proton Cooper pairs in neutron
stars, a process that dominates axion emission in young neutron stars in the
KSVZ model. By re-deriving its emissivity, we confirm consistency with most
existing literature, except for a recent study that exhibits a different
dependence on the effective mass. This discrepancy results in more than an
order-of-magnitude deviation in emissivity, significantly impacting constraints
on the KSVZ axion from the cooling observations of the Cassiopeia A neutron
star. Furthermore, we examine uncertainties arising from neutron-star equations
of state and their role in the discrepancy, finding that the large deviation
persists regardless of the choice of equations of state.",['hep-ph'],False,,,,"$S$, $T$, $U$ Parameters in The B-LSSM",Axion Emission from Proton Cooper Pairs in Neutron Stars
neg-d2-631,2025-01-16,,2501.09524," This note is an (exact) copy of the report of Jaak Peetre, ""H-infinity and
Complex Interpolation"". Published as Technical Report, Lund (1981). Some more
recent general references have been added, some references updated though (in
italics) and some misprints corrected.",['math.FA'],2501.09524," This note is an (exact) copy of the report of Jaak Peetre, ""H-infinity and
Complex Interpolation"". Published as Technical Report, Lund (1981). Some more
recent general references have been added, some references updated though (in
italics) and some misprints corrected.",['math.FA'],False,,,,H-infinity and Complex Interpolation,H-infinity and Complex Interpolation
neg-d2-632,2025-02-14,,2502.10542," Drug overdose deaths, including those due to prescription opioids, represent
a critical public health issue in the United States and worldwide. Artificial
intelligence (AI) approaches have been developed and deployed to help
prescribers assess a patient's risk for overdose-related death, but it is
unknown whether public health experts can leverage similar predictions to make
local resource allocation decisions more effectively. In this work, we
evaluated how AI-based overdose risk assessment could be used to inform local
public health decisions using a working prototype system. Experts from three
health departments, of varying locations and sizes with respect to staff and
population served, were receptive to the potential benefits of algorithmic risk
prediction and of using AI-augmented visualization to connect across data
sources. However, they also expressed concerns about whether the risk
prediction model's formulation and underlying data would match the state of the
overdose epidemic as it evolved in their specific locations. Our findings
extend those of other studies on algorithmic systems in the public sector, and
they present opportunities for future human-AI collaborative tools to support
decision-making in local, time-varying contexts.",['cs.HC'],2501.07234," The integration of haptics within Augmented Reality may help to deliver an
enriched experience, while facilitating the performance of specific actions
(e.g. repositioning or resizin ) that are still dependent on the user's skills.
This paper gathers the description of a flexible architecture designed to
deploy haptically-enabled AR applications. The haptic feedback may be generated
through a variety of devices (e.g., wearable, graspable, or mid-air ones), and
the architecture facilitates handling the specificity of each. For this reason,
it is discussed how to generate a haptic representation of a 3D digital object
depending on the application and the target device. Additionally, it is
included an analysis of practical, relevant issues that arise when setting up a
system to work with specific devices like Head-Mounted Displays (e.g.,
HoloLens) and mid-air haptic devices (e.g., Ultrahaptics UHK), such as the
alignment between the real world and the virtual one. The architecture
applicability is demonstrated through the implementation of two applications:
Form Inspector and Simon Game, built for HoloLens and iOS mobile phones for
visualization and for UHK for mid-air haptics delivery. These applications have
been used by nine users to explore the efficiency, meaningfulness, and
usefulness of mid-air haptics for form perception, object resizing, and push
interaction tasks. Results show that, although mobile interaction is preferred
when this option is available, haptics turn out to be more meaningful in
identifying shapes when compared to what users initially expect and in
contributing to the execution of resizing tasks. Moreover, this preliminary
user study reveals that users may be expecting a tailored interface metaphor,
not necessarily inspired in natural interaction.",['cs.HC'],False,,,,"Static Algorithm, Evolving Epidemic: Understanding the Potential of
  Human-AI Risk Assessment to Support Regional Overdose Prevention","Enhancing Interaction with Augmented Reality through Mid-Air Haptic
  Feedback: Architecture Design and User Feedback"
neg-d2-633,2025-01-06,,2501.04893," In this paper, we consider the existence and multiplicity of prescribed mass
solutions to the following nonlinear Schr\""{o}dinger equation with general
nonlinearity: Mass super-critical case: \[\begin{cases} -\Delta u+V(x)u+\lambda
u=g(u),\\ \|u\|_2^2=\int|u|^2\mathrm{d}x=c, \end{cases} \] both on large
bounded smooth star-shaped domain $\Omega\subset\mathbb{R}^N$ and on
$\mathbb{R}^N$, where $V(x)$ is the potential and the nonlinearity $g(\cdot)$
considered here are very general and of mass super-critical. The standard
approach based on the Pohozaev identity to obtain normalized solutions is
invalid as the presence of potential $V(x)$. In addition, our study can be
considered as a complement of Bartsch-Qi-Zou (Math Ann 390, 4813--4859, 2024),
which has addressed an open problem raised in Bartsch et al. (Commun Partial
Differ Equ 46(9):1729--1756, 2021).",['math.AP'],2501.11523," In this work, our interest lies in proving the existence of solutions to the
following Fractional Lane-Emden Hamiltonian system: $$ \begin{cases}
(-\Delta)^s u = H_v(x,u,v) & \text{in }\Omega,\\ (-\Delta)^s v = H_u(x,u,v) &
\text{in }\Omega,\\ u=v=0 & \text{in } \R^n\setminus\Omega. \end{cases} $$ The
method, that can be traced back to the work of De Figueiredo and Felmer
\cite{DF-F}, is flexible enough to deal with more general nonlocal operators
and make use of a combination of fractional order Sobolev spaces together with
functional calculus for self-adjoint operators.",['math.AP'],False,,,,"Normalized Solutions on large smooth domains to the Schr\""{o}dinger
  equation with potential and general nonlinearity: Mass super-critical case",Fractional Lane-Emden Hamiltonian systems
neg-d2-634,2025-02-23,,2502.16626," The non-detection of periodicity related to rotation challenges the magnetar
model for fast radio bursts (FRBs). Moreover, a bimodal distribution of the
burst waiting times is widely observed in hyper-active FRBs, a significant
deviation from the exponential distribution expected from stationary Poisson
processes. By combining the epidemic-type aftershock sequence (ETAS) earthquake
model and the rotating vector model (RVM) involving the rotation of the
magnetar and orientations of the spin and magnetic axes, we find that starquake
events modulated by the rotation of FRB-emitting magnetar can explain the
bimodal distribution of FRB waiting times, as well as the non-detection of
periodicity in active repeating FRBs. We analyze data from multiple FRB
sources, demonstrating that differences in waiting time distributions and
observed energies can be explained by varying parameters related to magnetar
properties and starquake dynamics. Our results suggest that rotation-modulated
starquakes on magnetars can possibly be a unified source for FRBs. Notably, we
find that active repeaters tend to have small magnetic inclination angles in
order to hide their periodicity. We also show that our model can reproduce the
waiting time distribution of a pulsar phase of the galactic magnetar SGR
J1935+2154 with a larger inclination angle than the active repeaters, which
could explain the detection of spin period and the relatively low observed
energy for FRBs from the magnetar. The spin periods of active repeaters are not
well constrained, but most likely fall in the valley region between the two
peaks of the waiting time distributions.",['astro-ph.HE'],2503.08487," Black holes can launch powerful jets through the Blandford-Znajek process.
This relies on enough plasma in the jet funnel to conduct the necessary
current. However, in some low luminosity active galactic nuclei, the plasma
supply near the jet base may be an issue. It has been proposed that spark gaps
-- local regions with unscreened electric field -- can form in the
magnetosphere, accelerating particles to initiate pair cascades, thus filling
the jet funnel with plasma. In this paper, we carry out 2D general relativistic
particle-in-cell (GRPIC) simulations of the gap, including self-consistent
treatment of inverse Compton scattering and pair production. We observe gap
dynamics that is fully consistent with our earlier 1D GRPIC simulations. We
find strong dependence of the gap power on the soft photon spectrum and energy
density, as well as the strength of the horizon magnetic field. We derive
physically motivated scaling relations, and applying to M87, we find that the
gap may be energetically viable for the observed TeV flares. For Sgr A$^*$, the
energy dissipated in the gap may also be sufficient to power the X-ray flares.",['astro-ph.HE'],False,,,,"Hyper-active repeating fast radio bursts from rotation modulated
  starquakes on magnetars","Physics of Pair Producing Gaps in Black Hole Magnetospheres: Two
  Dimensional General Relativistic Particle-in-cell Simulations"
neg-d2-635,2025-01-19,,2501.11077," Duplication-divergence models are a popular model for the evolution of gene
and protein interaction networks. However, existing duplication-divergence
models often neglect realistic features such as loss of interactions. Thus, in
this paper we present two novel models that incorporate random edge deletions
into the duplication-divergence framework. As in protein-protein interaction
networks, with proteins as vertices and interactions as edges, by design
isolated vertices tend to be rare, our main focus is on the number of isolated
vertices; our main result gives lower and upper bounds for the proportion of
isolated vertices, when the network size is large. Using these bounds we
identify the parameter regimes for which almost all vertices are typically
isolated; and also show that there are parameter regimes in which the
proportion of isolated vertices can be bounded away from 0 and 1 with high
probability. In addition, we find regimes in which the proportion of isolated
vertices tends to be small. The proof relies on a standard martingale argument,
which in turn requires a careful analysis of the first two moments of the
expected degree distribution. The theoretical findings are illustrated by
simulations, indicating that as the network size tends to infinity, the
proportion of isolated vertices can converge to a limit that is neither 0 or 1.",['math.PR'],2502.13448," In this paper, we establish three criteria for the asymptotic behavior of
Markov-Feller semigroups. First, we present a criterion for convergence in
total variation to a unique invariant measure, requiring only $TV$-eventual
continuity of the semigroup at a single point. Second, we propose two new
criteria for asymptotic stability that require eventual continuity at a single
point. This localized condition is more practical and easier to check. To
illustrate the advantages of our framework, we provide an explicit example
where verifying eventual continuity at a single point is straightforward,
whereas establishing the corresponding global property is challenging.",['math.PR'],False,,,,"Isolated vertices in two duplication-divergence models with edge
  deletion","Criteria for asymptotic stability of eventually continuous Markov-Feller
  semigroups"
neg-d2-636,2025-02-26,,2503.13464," Research data management (RDM) strategies and practices play a pivotal role
in adhering to the paradigms of reproducibility and transparency by enabling
research sharing in accordance with the principles of Open Science.
Discipline-specificity is an essential factor when understanding RDM
declinations, to tailor a comprehensive support service and to enhance
interdisciplinarity.
  In this paper we present the results of a mapping carried out to gather
information on research data generated and managed within the University of
Bologna (UniBO). The aim is to identify differences and commonalities between
disciplines and potential challenges for institutional support.
  We analyzed the data management plans (DMPs) of European competitive projects
drafted by researchers affiliated with UniBO. We applied descriptive statistics
to the collected variables to answer three main questions: How diverse is the
range of data managed within the University of Bologna? Which trends of
problems and patterns in terms of data management can influence/improve data
stewardship service? Is there an interdisciplinary approach to data production
within the University?
  The research work evidenced many points of contact between different
disciplines in terms of data produced, formats used and modest predilection for
data reuse. Hot topics such as data confidentiality, needed either on privacy
or intellectual property rights (IPR) premises, and long-term preservation pose
challenges to all researchers.
  These results show an increasing attention to RDM while highlighting the
relevance of training and support to face the relatively new challenges posed
by this approach.",['cs.DL'],2502.11923," This editorial explores the significance of research visibility within the
evolving landscape of academic communication, mainly focusing on the role of
search engines as online meta-markets shaping the impact of research. With the
rapid expansion of scientific output and the increasing reliance on
algorithm-driven platforms such as Google and Google Scholar, the online
visibility of scholarly work has become an essential factor in determining its
reach and influence. The need for more rigorous research into academic search
engine optimization (A-SEO), a field still in its infancy despite its growing
relevance, is also discussed, highlighting key challenges in the field,
including the lack of robust research methodologies, the skepticism within the
academic community regarding the commercialization of science, and the need for
standardization in reporting and measurement techniques. This editorial thus
invites a multidisciplinary dialogue on the future of research visibility, with
significant implications for academic publishing, science communication,
research evaluation, and the global scientific ecosystem.",['cs.DL'],False,,,,Mapping Research Data at the University of Bologna,Research on Research Visibility
neg-d2-637,2025-01-28,,2501.16792," Landau levels in certain models are known to protrude into the zero-field
energy gap. These are known as anomalous Landau levels (ALLs). We study whether
ALLs can lead to in-gap quantum oscillation in the absence of a zero-field
Fermi surface. Focusing on two-dimensional multi-band low-energy models of
electrons with continuous rotation symmetry, we show that an effective-band
description, akin to the semiclassical treatment of Landau level problems in
metals, can be used to predict the Landau level spectrum, including possible
ALLs. This description then predicts quantum oscillation for certain insulating
models, which we demonstrate through numerical calculations.",['cond-mat.str-el'],2503.00484," Realizing a quantum critical point (QCP) in clean ferromagnetic (FM) metals
has remained elusive due to the coupling of magnetization to the electronic
soft modes that drive the transition to be of first order. However, by
introducing a suitable amount of quenched disorder, one can still establish a
QCP in ferromagnets. In this study, we ascertain that the itinerant ferromagnet
Ni$_{1-x}$Mo$_{x}$ exhibits a FM QCP at a critical doping of $x_c \simeq
0.125$. Through magnetization and muon-spin relaxation measurements, we
demonstrate that the FM ordering temperature is suppressed continuously to zero
at $x_c$, while the magnetic volume fraction remains $100\%$ up to $x_c$,
indicating a second-order phase transition. The QCP is accompanied by a
non-Fermi liquid behavior, as evidenced by the logarithmic divergence of the
specific heat and the linear temperature dependence of the low-temperature
resistivity. Our findings reveal a minimal effect of disorder on the critical
spin dynamics of Ni$_{1-x}$Mo$_{x}$ at $x_c$, highlighting it as one of the
rare systems to exhibit a clean FM QCP.",['cond-mat.str-el'],False,,,,"Anomalous Landau levels and quantum oscillation in rotation-invariant
  insulators","Magnetic order and spin dynamics across the ferromagnetic quantum
  critical point in Ni\boldmath{$_{1-x}$}Mo\boldmath{$_{x}$}"
neg-d2-638,2025-03-11,,2503.08535," We report on the astrophysical properties of a sample of star clusters in the
Small Magellanic Cloud (SMC). They have been selected with the aim of looking
for the connection between their ages, heliocentric distances and metallicities
with the existence of tidally perturbed/induced outermost SMC regions. We
derived the star cluster fundamental parameters from relatively deep Survey of
the Magellanic Stellar History (SMASH) DR2 color magnitude diagrams, cleaned
from field star contamination, and compared to thousand synthetic CMDs covering
a wide range of heliocentric distances, ages and metal content. Heliocentric
distances for 15 star clusters are derived for the first time, which represents
an increase of 50 per cent of SMC clusters with estimated heliocentric
distances. The analysis of the age-metallicity relationships (AMRs) of cluster
located in outermost regions distributed around the SMC and in the SMC Main
Body reveals that they have followed the overall galaxy chemical enrichment
history. However, since half of the studied clusters are placed in front of or
behind the SMC Main Body, we concluded that they formed in the SMC and have
traveled outward because of the tidal effects from the interaction with the
Large Magellanic Cloud (LMC). Furthermore, metal rich clusters formed recently
in some of these outermost regions from gas that was also dragged by tidal
effects from the inner SMC. This outcome leads to consider the SMC as a galaxy
scarred by the LMC tidal interaction with distance-perturbed and newly induced
outermost stellar substructures.",['astro-ph.GA'],2502.06933," Non-ideal MHD effects are thought to be a crucial component of the
star-formation process. Numerically, several complications render the study of
non-ideal MHD effects in 3D simulations extremely challenging and hinder our
efforts of exploring a large parameter space. We aim to overcome such
challenges by proposing a novel, physically-motivated empirical approximation
to model non-ideal MHD effects. We perform a number of 2D axisymmetric 3-fluid
non-ideal MHD simulations of collapsing prestellar cores and clouds with
non-equilibrium chemistry and leverage upon previously-published results. We
utilize these simulations to develop a multivariate interpolating function to
predict the ionization fraction in each region of the cloud depending on the
local physical conditions. We subsequently use analytically-derived, simplified
expressions to calculate the resistivities of the cloud in each grid cell.
Therefore, in our new approach the resistivities are calculated without the use
of a chemical network. We benchmark our method against additional 2D
axisymmetric non-ideal MHD simulations with random initial conditions and a 3D
non-ideal MHD simulation with non-equilibrium chemistry. We find excellent
quantitative and qualitative agreement between our approach and the ""full""
non-ideal MHD simulations both in terms of the spatial structure of the
simulated clouds and regarding their time evolution. We achieve a factor of
100-1000 increase in computational speed. Given that we ignore the contribution
of grains, our approximation is valid up to number densities of 10^6 cm^(-3)
and is therefore suitable for pc-scale simulations of molecular clouds. The
tabulated data required for integrating our method in hydrodynamical codes,
along with a fortran implementation of the interpolating function are publicly
available at https://github.com/manosagian/Non-Ideal-MHD-Approximate-Code.",['astro-ph.GA'],False,,,,"Astrophysical properties of star clusters projected toward tidally
  perturbed SMC regions","A fast and robust recipe for modeling non-ideal MHD effects in
  star-formation simulations"
neg-d2-639,2025-01-06,,2501.03387," Controlled modulation of electronic band structure in two-dimensional (2D)
materials via doping is crucial for devices fabrication. For instance doped
graphene has been envisaged for various applications like sensors,
super-capacitors, transistors, p-n junctions, photo-detectors, etc. Many
different techniques have been developed to achieve desired doping in 2D
materials, like chemical doping, electrostatic doping, substrate doping, etc.
Here, we have combined space charge doping with space and angle resolved
photoemission (nano-ARPES), in order to directly observe the Fermi level
modulation on micron-sized flakes of monolayer and bilayer graphene. The doping
level can be tuned in a controlled manner, which allows us to directly observe
the Fermi level tuning. In our experiment we successfully doped the graphene
with p- and n-type carriers (holes/electrons) which are directly observed
through band shift in ARPES measurements. The observed band shift is $\sim$250
meV for bilayer and $\sim$500 meV for monolayer graphene. The results from our
experiment promote the space charge doping technique and nano-ARPES into other
materials such as 2D semiconductors and superconductors, in order to directly
observe the physical phenomena such as band gap transition and phase transition
as function of carrier doping.",['cond-mat.mtrl-sci'],2502.07104," The use of high-dimensional regression techniques from machine learning has
significantly improved the quantitative accuracy of interatomic potentials.
Atomic simulations can now plausibly target quantitative predictions in a
variety of settings, which has brought renewed interest in robust means to
quantify uncertainties on simulation results. In many practical settings,
encompassing both classical and a large class of machine learning potentials,
the dominant form of uncertainty is currently not due to lack of training data
but to misspecification, namely the inability of any one choice of model
parameters to exactly match all ab initio training data. However, Bayesian
inference, the most common formal tool used to quantify uncertainty, is known
to ignore misspecification and thus significantly underestimates parameter
uncertainties. Here, we employ a recent misspecification-aware regression
technique to quantify parameter uncertainties, which is then propagated to a
broad range of phase and defect properties in tungsten via brute force
resampling or implicit differentiation. The propagated misspecification
uncertainties robustly envelope errors to direct \textit{ab initio} calculation
of material properties outside of the training dataset, an essential
requirement for any quantitative multi-scale modeling scheme. Finally, we
demonstrate application to recent foundational machine learning interatomic
potentials, accurately predicting and bounding errors in MACE-MPA-0 energy
predictions across the diverse materials project database. Perspectives for the
approach in multiscale simulation workflows are discussed.",['cond-mat.mtrl-sci'],False,,,,"Space Charge Doping Induced Band Modulation in Mono- and Bi-layer
  Graphene: a nano-ARPES study","Uncertainty Quantification for Misspecified Machine Learned Interatomic
  Potentials"
neg-d2-640,2025-03-19,,2503.15345," We use the observed vertical velocity field, of various young tracers of the
gas kinematics, obtained by Li and Chen (2022), Konietzka et al. (2024) and Zhu
et al. (2024), in order to test for the existence of turbulence. We do so by
computing the power spectrum and the structure function of the vertical
velocity field. The latter suggest the existence of compressible, Burgers,
turbulence.
  The turbulence timescale on the largest spatial scale is about 500 Myr ,
implying that the turbulence has been generated 500 Myr ago. The turbulence
region depth in a direction perpendicular to the Radcliffe wave direction is
  about 400 pc.",['astro-ph.GA'],2501.09791," Effectively finding and identifying active galactic nuclei (AGNs) in dwarf
galaxies is an important step in studying black hole formation and evolution.
In this work, we examine four mid-IR-selected AGN candidates in dwarf galaxies
with stellar masses between $M_\star \sim 10^8 - 10^9 M_\odot$ , and find that
the galaxies are host to nuclear star clusters (NSCs) that are notably rare in
how young and massive they are. We perform photometric measurements on the
central star clusters in our target galaxies galaxies using Hubble Space
Telescope optical and near-IR imaging and compare their observed properties to
models of stellar population evolution. We find that these galaxies are host to
very massive ($\sim10^7 M_\odot$), extremely young ($\lesssim 8$ Myr), dusty
($0.6 \lesssim \mathrm{A_v} \lesssim 1.8$) nuclear star clusters. Our results
indicate that these galactic nuclei have ongoing star-formation, are still at
least partially obscured by clouds of gas and dust, and are most likely
producing the extremely red AGN-like mid-IR colors. Moreover, prior work has
shown that these galaxies do not exhibit X-ray or optical AGN signatures.
Therefore, we recommend caution when using mid-IR color-color diagnostics for
AGN selection in dwarf galaxies, since, as directly exemplified in this sample,
they can be contaminated by massive star clusters with ongoing star formation.",['astro-ph.GA'],False,,,,Is the Radcliffe wave turbulent?,"Star-Forming Nuclear Clusters in Dwarf Galaxies Mimicking AGN Signatures
  in the Mid-Infrared"
neg-d2-641,2025-03-05,,2503.03457," For a minimal Anosov $\mathbb R^{\kappa}$-action on a closed manifold, we
study the measure of maximal entropy constructed by Carrasco and
Rodriguez-Hertz in \cite{CarHer} and show that it fits into the theory of
Ruelle-Taylor resonances introduced by Guedes Bonthonneau, Guillarmou, Hilgert,
and Weich in \cite{GBGHW}. More precisely, we show that the topological entropy
corresponds to the first Ruelle-Taylor resonance for the action on a certain
bundle of forms and that the measure of maximal entropy can be retrieved as the
distributional product of the corresponding resonant and co-resonant states. As
a consequence, we prove a Bowen-type formula for the measure of maximal entropy
and a counting result on the number of periodic torii.",['math.DS'],2501.11359," We consider two types of dynamical systems namely non-autonomous discrete
dynamical systems(NDDS) and generic dynamical systems(GDS). In both of them, we
study various notions of transitivity. We give many equivalent conditions for
each of these notions and present the implications among these in NDDS and GDS.
For a given NDDS, we associate a GDS and discuss whether if the given NDDS has
a particular variation of transitivity then the associated GDS also has such a
variation and vice versa.",['math.DS'],False,,,,Measure of maximal entropy for minimal Anosov actions,"Various notions of topological transitivity in non-autonomous and
  generic dynamical systems"
neg-d2-642,2025-02-17,,2502.12516," Frame-semantic parsing is a critical task in natural language understanding,
yet the ability of large language models (LLMs) to extract frame-semantic
arguments remains underexplored. This paper presents a comprehensive evaluation
of LLMs on frame-semantic argument identification, analyzing the impact of
input representation formats, model architectures, and generalization to unseen
and out-of-domain samples. Our experiments, spanning models from 0.5B to 78B
parameters, reveal that JSON-based representations significantly enhance
performance, and while larger models generally perform better, smaller models
can achieve competitive results through fine-tuning. We also introduce a novel
approach to frame identification leveraging predicted frame elements, achieving
state-of-the-art performance on ambiguous targets. Despite strong
generalization capabilities, our analysis finds that LLMs still struggle with
out-of-domain data.",['cs.CL'],2502.15612," State space models (SSMs), such as Mamba, have emerged as an efficient
alternative to transformers for long-context sequence modeling. However,
despite their growing adoption, SSMs lack the interpretability tools that have
been crucial for understanding and improving attention-based architectures.
While recent efforts provide insights into Mamba's internal mechanisms, they do
not explicitly decompose token-wise contributions, leaving gaps in
understanding how Mamba selectively processes sequences across layers. In this
work, we introduce LaTIM, a novel token-level decomposition method for both
Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively
evaluate our method across diverse tasks, including machine translation,
copying, and retrieval-based generation, demonstrating its effectiveness in
revealing Mamba's token-to-token interaction patterns.",['cs.CL'],False,,,,Can LLMs Extract Frame-Semantic Arguments?,LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models
neg-d2-643,2025-01-28,,2501.16904," The study of adversarial defense still struggles to combat with advanced
adversarial attacks. In contrast to most prior studies that rely on the
diffusion model for test-time defense to remarkably increase the inference
time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked
AutoEncoder (MAE) into an adversarial purifier framework for test-time
purification. While MAEP achieves promising adversarial robustness, it
particularly features model defense transferability and attack generalization
without relying on using additional data that is different from the training
dataset. To our knowledge, MAEP is the first study of adversarial purifier
based on MAE. Extensive experimental results demonstrate that our method can
not only maintain clear accuracy with only a slight drop but also exhibit a
close gap between the clean and robust accuracy. Notably, MAEP trained on
CIFAR10 achieves state-of-the-art performance even when tested directly on
ImageNet, outperforming existing diffusion-based models trained specifically on
ImageNet.",['cs.CV'],2502.05457," Content-based video retrieval is one of the most challenging tasks in
surveillance systems. In this study, Latent Dirichlet Allocation (LDA) topic
model is used to annotate surveillance videos in an unsupervised manner. In
scene understanding methods, some of the learned patterns are ambiguous and
represents a mixture of atomic actions. To address the ambiguity issue in the
proposed method, feature vectors, and the primary model are processed to obtain
a secondary model which describes the scene with primitive patterns that lack
any ambiguity. Experiments show performance improvement in the retrieval task
compared to other topic model-based methods. In terms of false positive and
true positive responses, the proposed method achieves at least 80\% and 124\%
improvement respectively. Four search strategies are proposed, and users can
define and search for a variety of activities using the proposed query
formulation which is based on topic models. In addition, the lightweight
database in our method occupies much fewer storage which in turn speeds up the
search procedure compared to the methods which are based on low-level features.",['cs.CV'],False,,,,Adversarial Masked Autoencoder Purifier with Defense Transferability,"Content-based Video Retrieval in Traffic Videos using Latent Dirichlet
  Allocation Topic Model"
neg-d2-644,2025-02-28,,2503.00241," We study the retrieval accuracy and capacity of modern Hopfield networks of
with two-state (Ising) spins interacting via modified Hebbian $n$-spin
interactions. In particular, we consider systems where the interactions deviate
from the Hebb rule through additive or multiplicative noise or through clipping
or deleting interactions. We find that the capacity scales as $N^{n-1}$ with
the number of spins $N$ in all cases, but with a prefactor reduced compared to
the Hebbian case. For $n=2$ our results agree with the previously known results
for the conventional $n = 2$ Hopfield network.",['cond-mat.dis-nn'],2503.00241," We study the retrieval accuracy and capacity of modern Hopfield networks of
with two-state (Ising) spins interacting via modified Hebbian $n$-spin
interactions. In particular, we consider systems where the interactions deviate
from the Hebb rule through additive or multiplicative noise or through clipping
or deleting interactions. We find that the capacity scales as $N^{n-1}$ with
the number of spins $N$ in all cases, but with a prefactor reduced compared to
the Hebbian case. For $n=2$ our results agree with the previously known results
for the conventional $n = 2$ Hopfield network.",['cond-mat.dis-nn'],False,,,,Accuracy and capacity of Modern Hopfield networks with synaptic noise,Accuracy and capacity of Modern Hopfield networks with synaptic noise
neg-d2-645,2025-01-03,,2501.01685," Image segmentation is a vital task for providing human assistance and
enhancing autonomy in our daily lives. In particular, RGB-D
segmentation-leveraging both visual and depth cues-has attracted increasing
attention as it promises richer scene understanding than RGB-only methods.
However, most existing efforts have primarily focused on semantic segmentation
and thus leave a critical gap. There is a relative scarcity of instance-level
RGB-D segmentation datasets, which restricts current methods to broad category
distinctions rather than fully capturing the fine-grained details required for
recognizing individual objects. To bridge this gap, we introduce three RGB-D
instance segmentation benchmarks, distinguished at the instance level. These
datasets are versatile, supporting a wide range of applications from indoor
navigation to robotic manipulation. In addition, we present an extensive
evaluation of various baseline models on these benchmarks. This comprehensive
analysis identifies both their strengths and shortcomings, guiding future work
toward more robust, generalizable solutions. Finally, we propose a simple yet
effective method for RGB-D data integration. Extensive evaluations affirm the
effectiveness of our approach, offering a robust framework for advancing toward
more nuanced scene understanding.",['cs.CV'],2503.1612," Multi-species animal pose estimation has emerged as a challenging yet
critical task, hindered by substantial visual diversity and uncertainty. This
paper challenges the problem by efficient prompt learning for Vision-Language
Pretrained (VLP) models, \textit{e.g.} CLIP, aiming to resolve the
cross-species generalization problem. At the core of the solution lies in the
prompt designing, probabilistic prompt modeling and cross-modal adaptation,
thereby enabling prompts to compensate for cross-modal information and
effectively overcome large data variances under unbalanced data distribution.
To this end, we propose a novel probabilistic prompting approach to fully
explore textual descriptions, which could alleviate the diversity issues caused
by long-tail property and increase the adaptability of prompts on unseen
category instance. Specifically, we first introduce a set of learnable prompts
and propose a diversity loss to maintain distinctiveness among prompts, thus
representing diverse image attributes. Diverse textual probabilistic
representations are sampled and used as the guidance for the pose estimation.
Subsequently, we explore three different cross-modal fusion strategies at
spatial level to alleviate the adverse impacts of visual uncertainty. Extensive
experiments on multi-species animal pose benchmarks show that our method
achieves the state-of-the-art performance under both supervised and zero-shot
settings. The code is available at https://github.com/Raojiyong/PPAP.",['cs.CV'],False,,,,IAM: Enhancing RGB-D Instance Segmentation with New Benchmarks,Probabilistic Prompt Distribution Learning for Animal Pose Estimation
neg-d2-646,2025-03-03,,2503.02204," The gas-phase abundances of deuterium (D) in the local interstellar medium
(ISM) exhibit considerable regional variations. Particularly, in some regions
the gas-phase D abundances are substantially lower than the primordial D
abundance generated in the Big Bang, after subtracting the astration reduction
caused by the Galactic chemical evolution. Deuterated polycyclic aromatic
hydrocarbon (PAH) molecules have been suggested as a potential reservoir of the
D atoms missing from the gas-phase. Recent observations from the James Webb
Space Telescope's Near Infrared Spectrograph have revealed the widespread of
deuterated PAHs in the Orion Bar through their aliphatic C--D emission at
4.65${\,{\rm \mu m}}$ and possibly aromatic C--D emission at 4.4${\,{\rm \mu
m}}$ as well. To examine the viability of deuterated PAHs as the D reservoir,
we model the infrared (IR) emission spectra of small PAH molecules containing
various aromatic and aliphatic D atoms in the Orion Bar. We find that small
deuterated PAHs exhibit a noticeable emission band at 4.4 or 4.65${\,{\rm \mu
m}}$ even if they contain only one aromatic or aliphatic D atom. We derive
${{N_{\rm D,ali}}}/{N_{\rm H}}\approx3.4\%$, the deuteration degree of PAHs
measured as the number of aliphatic D atoms (relative to H), from the observed
intensity ratios of the 4.65${\,{\rm \mu m}}$ band to the 3.3${\,{\rm \mu m}}$
aromatic C--H band. The deuteration degree for aromatically-deuterated PAHs is
less certain as C--N stretch also contributes to the observed emission around
4.4${\,{\rm \mu m}}$. If we attribute it exclusively to aromatic C--D, we
derive an upper limit of $\approx14\%$ on the deuteration degree, which is
capable of accounting for an appreciable fraction of the missing D budget.",['astro-ph.GA'],2503.15345," We use the observed vertical velocity field, of various young tracers of the
gas kinematics, obtained by Li and Chen (2022), Konietzka et al. (2024) and Zhu
et al. (2024), in order to test for the existence of turbulence. We do so by
computing the power spectrum and the structure function of the vertical
velocity field. The latter suggest the existence of compressible, Burgers,
turbulence.
  The turbulence timescale on the largest spatial scale is about 500 Myr ,
implying that the turbulence has been generated 500 Myr ago. The turbulence
region depth in a direction perpendicular to the Radcliffe wave direction is
  about 400 pc.",['astro-ph.GA'],False,,,,"Deuterated Polycyclic Aromatic Hydrocarbons in the Interstellar Medium:
  Constraints from the Orion Bar as Observed by the James Webb Space Telescope",Is the Radcliffe wave turbulent?
neg-d2-647,2025-02-27,,2502.2008," Recently synthesized Porous 12-Atom-Wide Armchair Graphene Nanoribbons Nano
Lett. 2024, 24, 10718-10723 exhibit tunable properties through periodic
porosity, enabling precise control over their electronic, optical, thermal, and
mechanical behavior. This work presents a comprehensive theoretical
characterization of pristine and porous 12-AGNRs based on density functional
theory (DFT) and molecular dynamics (MD) simulations. DFT calculations reveal
substantial electronic modifications, including band gap widening and the
emergence of localized states. Analyzed within the Bethe-Salpeter equation
(BSE) framework, optical properties highlight strong excitonic effects and
significant absorption shifts. Thermal transport simulations indicate a
pronounced reduction in conductivity due to enhanced phonon scattering at
nanopores. At the same time, MD-based mechanical analysis shows decreased
stiffness and strength while maintaining structural integrity. Despite these
modifications, porous 12-AGNRs remain mechanically and thermally stable. These
findings establish porosity engineering as a powerful strategy for tailoring
graphene nanoribbons' functional properties, reinforcing their potential for
nanoelectronic, optoelectronic, and thermal management applications.",['cond-mat.mtrl-sci'],2502.09882," The interplay among heat, spin, and charge is the central focus in spin
caloritronic research. While the longitudinal heat-to-spin conversion via the
spin Seebeck effect has been intensively studied, the transverse heat-to-spin
conversion via the spin Nernst effect (SNE) has not been equally explored. One
major challenge is the minuscule signals generated by the SNE, which are often
mixed with the background noises. In this work, we overcome this difficulty by
studying the thin films of Ni70Cu30 alloy with not only a sizable spin Hall
angle but also a large Seebeck coefficient. We observe in the Ni70Cu30 alloy a
large spin Nernst effect with an estimated spin Nernst angle ranging from -28%
to -72%. In comparison, the spin Nernst angle for Pt is -8.2%. Our ab initio
calculation reveals that the large spin Nernst conductivity in Ni70Cu30 is
caused by the Fermi energy shift to the steepest slope of the spin Hall
conductivity curve due to electron doping from 30% Cu. Our study provides
critical directions in searching for materials with a large spin Nernst effect.",['cond-mat.mtrl-sci'],False,,,,"Computational Characterization of the Recently Synthesized Pristine and
  Porous 12-Atom-Wide Armchair Graphene Nanoribbon",Large Spin Nernst Effect in Ni70Cu30 Alloy
neg-d2-648,2025-01-30,,2501.18564," Robotic manipulation systems operating in diverse, dynamic environments must
exhibit three critical abilities: multitask interaction, generalization to
unseen scenarios, and spatial memory. While significant progress has been made
in robotic manipulation, existing approaches often fall short in generalization
to complex environmental variations and addressing memory-dependent tasks. To
bridge this gap, we introduce SAM2Act, a multi-view robotic transformer-based
policy that leverages multi-resolution upsampling with visual representations
from large-scale foundation model. SAM2Act achieves a state-of-the-art average
success rate of 86.8% across 18 tasks in the RLBench benchmark, and
demonstrates robust generalization on The Colosseum benchmark, with only a 4.3%
performance gap under diverse environmental perturbations. Building on this
foundation, we propose SAM2Act+, a memory-based architecture inspired by SAM2,
which incorporates a memory bank, an encoder, and an attention mechanism to
enhance spatial memory. To address the need for evaluating memory-dependent
tasks, we introduce MemoryBench, a novel benchmark designed to assess spatial
memory and action recall in robotic manipulation. SAM2Act+ achieves competitive
performance on MemoryBench, significantly outperforming existing approaches and
pushing the boundaries of memory-enabled robotic systems. Project page:
https://sam2act.github.io/",['cs.RO'],2503.05623," There is now a large body of techniques, many based on formal methods, for
describing and realizing complex robotics tasks, including those involving a
variety of rich goals and time-extended behavior. This paper explores the
limits of what sorts of tasks are specifiable, examining how the precise
grounding of specifications, that is, whether the specification is given in
terms of the robot's states, its actions and observations, its knowledge, or
some other information,is crucial to whether a given task can be specified.
While prior work included some description of particular choices for this
grounding, our contribution treats this aspect as a first-class citizen: we
introduce notation to deal with a large class of problems, and examine how the
grounding affects what tasks can be posed. The results demonstrate that certain
classes of tasks are specifiable under different combinations of groundings.",['cs.RO'],False,,,,"SAM2Act: Integrating Visual Foundation Model with A Memory Architecture
  for Robotic Manipulation",Limits of specifiability for sensor-based robotic planning tasks
neg-d2-649,2025-01-28,,2501.16993," When dealing with a multi-objective optimization problem, obtaining a
comprehensive representation of the Pareto front can be computationally
expensive. Furthermore, identifying the most representative Pareto solutions
can be difficult and sometimes ambiguous. A popular selection are the so-called
Pareto knee solutions, where a small improvement in any objective leads to a
large deterioration in at least one other objective. In this paper, using
Pareto sensitivity, we show how to compute Pareto knee solutions according to
their verbal definition of least maximal change. We refer to the resulting
approach as the sensitivity knee (snee) approach, and we apply it to
unconstrained and constrained problems. Pareto sensitivity can also be used to
compute the most-changing Pareto sub-fronts around a Pareto solution, where the
points are distributed along directions of maximum change, which could be of
interest in a decision-making process if one is willing to explore solutions
around a current one. Our approach is still restricted to scalarized methods,
in particular to the weighted-sum or epsilon-constrained methods, and require
the computation or approximations of first- and second-order derivatives. We
include numerical results from synthetic problems that illustrate the benefits
of our approach.",['math.OC'],2501.0339," The Pseudo-Boolean problem deals with linear or polynomial constraints with
integer coefficients over Boolean variables. The objective lies in optimizing a
linear objective function, or finding a feasible solution, or finding a
solution that satisfies as many constraints as possible. In the 2024
Pseudo-Boolean competition, solvers incorporating the SCIP framework won five
out of six categories it was competing in. From a total of 1,207 instances,
SCIP successfully solved 759, while its parallel version FiberSCIP solved 776.
Based on the results from the competition, we further enhanced SCIP's
Pseudo-Boolean capabilities. This article discusses the results and presents
the winning algorithmic ideas.",['math.OC'],False,,,,"Pareto sensitivity, most-changing sub-fronts, and knee solutions",State-of-the-art Methods for Pseudo-Boolean Solving with SCIP
neg-d2-650,2025-01-06,,2501.03382," The main aim of the paper is to give a full classification (up to isometry)
of all metric spaces X with the following two properties: X contains a compact
set with non-empty interior; and for any three distinct points a, b and c of X
there exists a (bijective) dilation on X that fixes a and sends b to c. As a
consequence, we obtain a new characterisation of the Euclidean spaces: these
are (up to isometry) precisely all metric spaces that have the above two
properties, and (in addition) contain three distinct points x, y, z that are
metrically collinear (that is, for which d(x,z) = d(x,y)+d(y,z)).",['math.MG'],2503.18343," A horoboundary is one of the attempts to compactify metric spaces, and is
constructed using continuous functions on metric spaces. It is a concept that
includes global information of metric spaces, and its correspondence with an
ideal boundary constructed using geodesics has been studied in nonpositive
curvature spaces such as CAT(0) spaces and geodesic Gromov hyperbolic spaces.
We will introduce a certain correspondence between the horoboundary and the
ideal boundary of coarsely convex spaces, which can be regarded as a
generalization of spaces of nonpositive curvature.",['math.MG'],False,,,,Two-point dilation-homogeneous metric spaces,Horoboundaries of coarsely convex spaces
neg-d2-651,2025-01-20,,2501.11797," This paper investigates the exit-time problem for time-inhomogeneous
diffusion processes. The focus is on the small-noise behavior of the exit time
from a bounded positively invariant domain. We demonstrate that, when the drift
and diffusion terms are uniformly close to some time-independent functions, the
exit time grows exponentially both in probability and in $L_1$ as a parameter
that controls the noise tends to zero. We also characterize the exit position
of the time-inhomogeneous process. Additionally, we investigate the impact of
relaxing the uniform closeness condition on the exit-time behavior. As an
application, we extend these results to the McKean-Vlasov process. Our findings
improve upon existing results in the literature for the exit-time problem for
this class of processes.",['math.PR'],2502.13448," In this paper, we establish three criteria for the asymptotic behavior of
Markov-Feller semigroups. First, we present a criterion for convergence in
total variation to a unique invariant measure, requiring only $TV$-eventual
continuity of the semigroup at a single point. Second, we propose two new
criteria for asymptotic stability that require eventual continuity at a single
point. This localized condition is more practical and easier to check. To
illustrate the advantages of our framework, we provide an explicit example
where verifying eventual continuity at a single point is straightforward,
whereas establishing the corresponding global property is challenging.",['math.PR'],False,,,,"Freidlin-Wentzell type exit-time estimates for time-inhomogeneous
  diffusions and their applications","Criteria for asymptotic stability of eventually continuous Markov-Feller
  semigroups"
neg-d2-652,2025-02-26,,2502.19032," The non-fungible tokens (NFTs) market has evolved over the past decade, with
NFTs serving as unique digital identifiers on a blockchain that certify
ownership and authenticity. However, their high value also attracts attackers
who exploit vulnerabilities in NFT smart contracts for illegal profits, thereby
harming the NFT ecosystem. One notable vulnerability in NFT smart contracts is
sleepminting, which allows attackers to illegally transfer others' tokens.
Although some research has been conducted on sleepminting, these studies are
basically qualitative analyses or based on historical transaction data. There
is a lack of understanding from the contract code perspective, which is crucial
for identifying such issues and preventing attacks before they occur. To
address this gap, in this paper, we categoriz four distinct types of
sleepminting in NFT smart contracts. Each type is accompanied by a
comprehensive definition and illustrative code examples to provide how these
vulnerabilities manifest within the contract code. Furthermore, to help detect
the defined defects before the sleepminting problem occurrence, we propose a
tool named WakeMint, which is built on a symbolic execution framework and is
designed to be compatible with both high and low versions of Solidity. The tool
also employs a pruning strategy to shorten the detection period. Additionally,
WakeMint gathers some key information, such as the owner of an NFT and
emissions of events related to the transfer of the NFT's ownership during
symbolic execution. Then, it analyzes the features of the transfer function
based on this information so that it can judge the existence of sleepminting.
We ran WakeMint on 11,161 real-world NFT smart contracts and evaluated the
results. We found 115 instances of sleepminting issues in total, and the
precision of our tool is 87.8%.",['cs.SE'],2503.03455," Despite advancements in MLOps and AutoML, ML development still remains
challenging for data scientists. First, there is poor support for and limited
control over optimizing and evolving ML models. Second, there is lack of
efficient mechanisms for continuous evolution of ML models which would leverage
the knowledge gained in previous optimizations of the same or different models.
We propose an experiment-driven MLOps approach which tackles these problems.
Our approach relies on the concept of an experiment, which embodies a fully
controllable optimization process. It introduces full traceability and
repeatability to the optimization process, allows humans to be in full control
of it, and enables continuous improvement of the ML system. Importantly, it
also establishes knowledge, which is carried over and built across a series of
experiments and allows for improving the efficiency of experimentation over
time. We demonstrate our approach through its realization and application in
the ExtremeXP1 project (Horizon Europe).",['cs.SE'],False,,,,WakeMint: Detecting Sleepminting Vulnerabilities in NFT Smart Contracts,Towards Continuous Experiment-driven MLOps
neg-d2-653,2025-03-05,,2503.03675," The ratio between the stellar mass of a galaxy, $M_{*}$, and that of its
central supermassive black hole (SMBH), $M_\bullet$, the ``Magorrian''
relationship, traces their coevolution. JWST observations have suggested
significant evolution in $M_\bullet/M_{*}$ relative to local scaling
relationships both in low-mass galaxies and in quasars at z $\ge$ 4. We test
this possibility by (1) determining the preferred $M_\bullet/M_{*}$ scaling
relation among those proposed locally; and (2) providing uniform host galaxy
stellar mass estimates. These steps reduce the prominence of the reported
evolution. We then apply Monte Carlo simulations to account for observational
biases. We still find a significant increase over the local scaling relation in
$M_\bullet/M_{*}$ for z $\ge$ 4 SMBHs in very low-mass galaxies
($\log(M_*/M_{\odot})<10$). However, similarly high values of $M_\bullet/M_{*}$
are also found in low mass galaxies at $z \sim$ 0.5 to 3 that may be common at
cosmic noon. Nonetheless, galaxies with similar behavior are rare locally and
not accounted for in the local scaling relations. In contrast, z $\sim$ 6
quasars can have $M_\bullet/M_{*}$ well above the local relation value, but
they can be explained as extreme cases still within the scaling relation for
their higher mass host galaxies. Black holes in some of them and in the
low-mass systems may be undergoing very high accretion episodes that result in
high $M_\bullet/M_{*}$ but that will be followed by quiescent periods when
growth of the host drives the systems toward more typical $M_\bullet/M_{*}$
values.",['astro-ph.GA'],2503.16607," The baryonic Tully Fisher relation (bTFR) provides an empirical connection
between baryonic mass and dynamical mass (measured by the maximum rotation
velocity) for galaxies. Due to the impact of baryonic feedback in the shallower
potential wells of dwarf galaxies, the bTFR is predicted to turn down at low
masses from the extrapolated power-law relation at high masses. The low-mass
end of the bTFR is poorly constrained due to small samples and difficulty in
connecting the galaxy's gas kinematics to its dark matter halo. Simulations can
help us understand this connection and interpret observations. We measure the
bTFR with 66 dwarf galaxies from the Marvel-ous and Marvelous Massive Dwarfs
hydrodynamic simulations. Our sample has M$_\star = 10^6-10^9$ M$_\odot$, and
is mostly gas dominated. We compare five velocity methods: V$_\text{out,circ}$
(spatially resolved mass-enclosed), V$_\text{out,mid}$ (spatially resolved
midplane gravitational potential), and unresolved HI linewidths at different
percentages of the peak flux (W$_\text{10}$, W$_\text{20}$, and W$_\text{50}$).
We find an intrinsic turndown in the bTFR for maximum halo speeds $\lesssim 50$
km s$^{-1}$ (or total baryonic mass, M$_\text{bary}\lesssim 10^{8.5}$
M$_\odot$). We find that observing HI in lower-mass galaxies to the
conventional surface density limit of 1M$_\odot$pc$^{-2}$ is not enough to
detect a turndown in the bTFR; none of the HI velocity methods (spatially
resolved or unresolved) recover the turndown, and we find bTFR slopes
consistent with observations of higher-mass galaxies. However, we predict that
the turndown can be recovered by resolved rotation curves if the HI limit is
$\lesssim 0.08$ M$_\odot$ pc$^{-2}$, which is within the sensitivity of current
HI surveys like FEASTS and MHONGOOSE.",['astro-ph.GA'],False,,,,"The $M_{*}-M_{\rm BH}$ Relation Evolution from z $\sim$ 6 to the Present
  Epoch","Predictions for Detecting a Turndown in the Baryonic Tully Fisher
  Relation"
neg-d2-654,2025-01-09,,2501.05437," We show that the complete Sp(2)-invariant expanding solitons for Bryant's
Laplacian flow on the anti-self-dual bundle of the 4-sphere form a 1-parameter
family, and that they are all asymptotically conical (AC). We determine their
asymptotic cones, and prove that this cone determines the complete expander (up
to scale). Neither the unique Sp(2)-invariant torsion-free G_2-cone nor the
asymptotic cone of the explicit AC Sp(2)-invariant shrinker from
arxiv:2112.09095 occurs as the asymptotic cone of a complete AC Sp(2)-invariant
expander.
  We determine all possible end behaviours of Sp(2)-invariant solitons,
identifying novel forward-complete end solutions for both expanders and
shrinkers with faster-than-Euclidean volume growth. We conjecture that there
exists a 1-parameter family of complete SU(3)-invariant expanders on the
anti-self-dual bundle of the complex projective plane CP^2 with such asymptotic
behaviour.
  We also conjecture that, in contrast to the Sp(2)-invariant case, there exist
complete SU(3)-invariant AC expanders with asymptotic cone matching that of the
explicit AC SU(3)-invariant shrinker from arxiv:2112.09095. The latter
conjecture suggests that Laplacian flow may naturally implement a type of
surgery in which a CP^2 shrinks to a conically singular point, but after which
the flow can be continued smoothly, expanding a topologically different CP^2
from the singularity.",['math.DG'],2501.06068," We report on a few interrelations between bi-Hermitian metrics and locally
conformally K\""ahler metrics on complex surfaces.",['math.DG'],False,,,,Sp(2)-invariant expanders and shrinkers in Laplacian flow,"Bi-Hermitian and locally conformally K\""ahler surfaces"
neg-d2-655,2025-03-19,,2503.15084," The discovery of the kilonova (KN) AT 2017gfo, accompanying the gravitational
wave event GW170817, provides crucial insight into the synthesis of heavy
elements during binary neutron star (BNS) mergers. Following this landmark
event, another KN was detected in association with the second-brightest
gamma-ray burst (GRB) observed to date, GRB 230307A, and subsequently confirmed
by observations of the James Webb Space Telescope (JWST). In this work, we
conduct an end-to-end simulation to analyze the temporal evolution of the KN AT
2023vfi associated with GRB 230307A, and constrain the abundances of superheavy
elements produced. We find that the temporal evolution of AT 2023vfi is similar
to AT 2017gfo in the first week post-burst. Additionally, the
\textit{r}-process nuclide abundances of lanthanide-rich ejecta, derived from
numerical relativity simulations of BNS mergers, can also successfully
interpret the temporal evolution of the KN with the lanthanide-rich ejecta mass
of $0.02 M_\odot$, which is consistent with the mass range of dynamical ejecta
from numerical simulations in literature. Both findings strongly suggest the
hypothesis that GRB 230307A originated from a BNS merger, similar to AT
2017gfo. Based on the first time observation of the KN for JWST, we are able to
constrain the superheavy elements of another KN following AT 2017gfo. The
pre-radioactive-decay abundances of the superheavy nuclides: $^{222}$Rn,
$^{223}$Ra, $^{224}$Ra and $^{225}$Ac, are estimated to be at least on the
order of $1 \times 10^{-5}$. These abundance estimates provide valuable insight
into the synthesis of superheavy elements in BNS mergers, contributing to our
understanding of astrophysical \textit{r}-process nucleosynthesis.",['astro-ph.HE'],2502.12283," A recent study shows that if the power spectra (PS) of accreting compact
objects consist of a combination of Lorentzian functions that are coherent in
different energy bands but incoherent with each other, the same is true for the
Real and Imaginary parts of the cross spectrum (CS). Using this idea, we
discovered imaginary quasi-periodic oscillations (QPOs) in NICER observations
of the black hole candidate MAXI J1820+070. The imaginary QPOs appear as narrow
features with a small Real and large Imaginary part in the CS but are not
significantly detected in the PS when they overlap in frequency with other
variability components. The coherence function drops and the phase lags
increase abruptly at the frequency of the imaginary QPO. We show that the
multi-Lorentzian model that fits the PS and CS of the source in two energy
bands correctly reproduces the lags and the coherence, and that the narrow drop
of the coherence is caused by the interaction of the imaginary QPO with other
variability components. The imaginary QPO appears only in the decay of the
outburst, during the transition from the high-soft to the low-hard state of
MAXI J1820+070, and its frequency decreases from approximately 5 Hz to around 1
Hz as the source spectrum hardens. We also analysed the earlier observations of
the transition, where no narrow features were seen, and we identified a QPO in
the PS that appears to evolve into the imaginary QPO as the source hardens. As
for the type-B and C QPOs in this source, the rms spectrum of the imaginary QPO
increases with energy. The lags of the imaginary QPO are similar to those of
the type-B and C QPOs above 2 keV but differ from the lags of those other QPOs
below that energy. While the properties of this imaginary QPO resemble those of
type-C QPOs, we cannot rule out that it is a new type of QPO.",['astro-ph.HE'],False,,,,A constraint on superheavy elements of the GRB-kilonova AT 2023vfi,"The nature of an imaginary quasi-periodic oscillation in the
  soft-to-hard transition of MAXI J1820+070"
neg-d2-656,2025-01-11,,2501.06684," In this paper, we investigate the phenomenon of coming down from infinity for
(sub)critical cooperative branching processes with pairwise interactions (BPI
processes for short) under appropriate conditions. BPI processes are
continuous-time Markov chains that extend pure branching dynamics by
incorporating additional mechanisms that allow both competition and cooperation
events between pairs of individuals.
  Specifically, we focus on characterising the speed at which BPI processes
evolve when starting from a very large initial population in the subcritical
and critical cooperative regimes. Further, in the subcritical cooperative
regime, we analyse their second-order fluctuations.",['math.PR'],2503.16912," The purpose of this paper is to introduce the construction of a stochastic
process called ``diffusion house-moving'' and to explore its properties. We
study the weak convergence of diffusion bridges conditioned to stay between two
curves, and we refer to this limit as diffusion house-moving. Applying this
weak convergence result, we give the sample path properties of diffusion
house-moving.",['math.PR'],False,,,,"On the speed of coming down from infinity for (sub)critical branching
  processes with pairwise interactions","Construction and sample path properties of diffusion house-moving
  between two curves"
neg-d2-657,2025-01-07,,2501.03963," We prove global well-posedness and scattering for the massive
Dirac-Klein-Gordon system with small and low regularity initial data in
dimension two. To achieve this, we impose a non-resonance condition on the
masses.",['math.AP'],2501.06544," In the mean-field regime, a gas of quantum particles with Boltzmann
statistics can be described by the Hartree-Fock equation. This dynamics becomes
trivial if the initial distribution of particle is invariant by translation.
However, the first correction is given on time of order $O(N)$ by the quantum
Lenard--Balescu equation. In the first part of the present article, we justify
this equation until time of order $O((\log N)^{1-\delta})$ (for any
$\delta\in(0,1)$).
  A similar phenomenon exists in the classical setting (with a similar validity
time obtained by Duerinckx \cite{Duerinckx}). In a second time, we prove the
convergence for dimension $d\geq 2$ of the solutions of the quantum
Lenard--Balescu equation to the solutions of its classical counterpart in the
semi-classical limit. This problem can be interpreted as a grazing collision
limit: the quantum Lenard--Balescu equation looks like a cut-off Boltzmann
equation, when the classical one looks like the Landau equation.",['math.AP'],False,,,,"Global well-posedness and scattering for the massive Dirac-Klein-Gordon
  system in two dimensions",Around the Quantum Lenard-Balescu equation
neg-d2-658,2025-03-10,,2503.07342," In this work, we introduce a novel variant of the multivariate quadratic
problem, which is at the core of one of the most promising post-quantum
alternatives: multivariate cryptography. In this variant, the solution of a
given multivariate quadratic system must also be regular, i.e. if it is split
into multiple blocks of consecutive entries with the same fixed length, then
each block has only one nonzero entry. We prove the NP-completeness of this
variant and show similarities and differences with other computational problems
used in cryptography. Then we analyze its hardness by reviewing the most common
solvers for polynomial systems over finite fields, derive asymptotic formulas
for the corresponding complexities and compare the different approaches.",['cs.SC'],2503.07342," In this work, we introduce a novel variant of the multivariate quadratic
problem, which is at the core of one of the most promising post-quantum
alternatives: multivariate cryptography. In this variant, the solution of a
given multivariate quadratic system must also be regular, i.e. if it is split
into multiple blocks of consecutive entries with the same fixed length, then
each block has only one nonzero entry. We prove the NP-completeness of this
variant and show similarities and differences with other computational problems
used in cryptography. Then we analyze its hardness by reviewing the most common
solvers for polynomial systems over finite fields, derive asymptotic formulas
for the corresponding complexities and compare the different approaches.",['cs.SC'],False,,,,The regular multivariate quadratic problem,The regular multivariate quadratic problem
neg-d2-659,2025-02-17,,2502.11485," The incompressible Navier-Stokes equations and static Euler equations are
considered. We find that there exist infinite non-trivial regular solutions of
incompressible static Euler equations with given boundary conditions. Moreover
there exist random solutions of incompressible static Euler equations. Provided
Reynolds number is large enough and time variable $t$ goes to infinity, these
random solutions of static Euler equations are the path limits of corresponding
Navier-Stokes flows. But the double limits of these Navier-Stokes flows do not
exist. These phenomena reveal randomness and turbulence of incompressible
fluids. Therefore these solutions are called turbulent solutions. Here some
typing models without Prandtl layer are given.",['math.AP'],2503.18717," In this work, we study the existence and nonexistence of nonnegative
solutions to a class of nonlocal elliptic systems set in a bounded open subset
of $\mathbb{R}^N$. The diffusion operators are of type $u_i\mapsto
d_i(-\Delta)^{s_i}u_i$ where $0<s_1\neq s_2<1$, and the gradients of the
unknowns act as source terms. Existence results are obtained by proving some
fine estimates when data belong to weighted Lebesgue spaces. Those estimates
are new and interesting in themselves.",['math.AP'],False,,,,"Non-uniqueness of Regular Solutions for Incompressible Static Euler
  Equations with Given Boundary Conditions and Turbulent Global Solutions of
  Incompressible Navier-Stokes Equations","Fractional elliptic reaction-diffusion systems with coupled gradient
  terms and different diffusion"
neg-d2-660,2025-01-07,,2501.04099," Imbalanced multiclass datasets pose challenges for machine learning
algorithms. These datasets often contain minority classes that are important
for accurate prediction. Existing methods still suffer from sparse data and may
not accurately represent the original data patterns, leading to noise and poor
model performance. A hybrid method called Neighbor Displacement-based Enhanced
Synthetic Oversampling (NDESO) is proposed in this paper. This approach uses a
displacement strategy for noisy data points, computing the average distance to
their neighbors and moving them closer to their centroids. Random oversampling
is then performed to achieve dataset balance. Extensive evaluations compare 14
alternatives on nine classifiers across synthetic and 20 real-world datasets
with varying imbalance ratios. The results show that our method outperforms its
competitors regarding average G-mean score and achieves the lowest statistical
mean rank. This highlights its superiority and suitability for addressing data
imbalance in practical applications.",['cs.LG'],2501.16186," This paper studies the uplink and downlink power allocation for interactive
augmented reality (AR) services, where live video captured by an AR device is
uploaded to the network edge and then the augmented video is subsequently
downloaded. By modeling the AR transmission process as a tandem queuing system,
we derive an upper bound for the probabilistic quality of service (QoS)
requirement concerning end-to-end latency and reliability. The resource
allocation with the QoS constraints results in a functional optimization
problem. To address it, we design a deep neural network to learn the power
allocation policy, leveraging the structure of optimal power allocation to
enhance learning performance. Simulation results demonstrate that the proposed
method effectively reduces transmit powers while meeting the QoS requirement.",['cs.LG'],False,,,,"Neighbor displacement-based enhanced synthetic oversampling for
  multiclass imbalanced data",Learn to Optimize Resource Allocation under QoS Constraint of AR
neg-d2-661,2025-01-16,,2501.09543," We introduce and study a multiparameter Poisson process (MPP). In a
particular case, it is observed that the MPP has a unique representation. Its
subordination with the multivariate subordinator and inverse subordinator are
studied in detail. Also, we consider a multivariate multiparameter Poisson
process and establish its connection with the MPP. An integral of the MPP is
defined, and its asymptotic distribution is obtained. Later, we study some
properties of the multiparameter martingales. Moreover, the multiparameter
martingale characterizations for the MPP and its subordinated variants are
derived.",['math.PR'],2503.08837," We study a system of reflecting Brownian motions on the positive half-line in
which each particle has a drift toward the origin determined by the local times
at the origin of all the particles. We show that if this local time drift is
too strong, such systems can exhibit a breakdown in their solutions in that
there is a time beyond which the system cannot be extended. In the finite
particle case we give a complete characterization of the finite time breakdown,
relying on a novel dynamic graph structure. We consider the mean-field limit of
the system in the symmetric case and show that there is a McKean--Vlasov
representation. If the drift is too strong, the solution to the corresponding
Fokker--Planck equation has a blow up in its solution. We also establish the
existence of stationary and self-similar solutions to the McKean--Vlasov
equation in the case where there is no breakdown of the system. This work is
motivated by models for liquidity in financial markets, the supercooled Stefan
problem, and a toy model for cell polarization.",['math.PR'],False,,,,Multiparameter Poisson Processes and Martingales,"Particle Systems and McKean--Vlasov Dynamics with Singular Interaction
  through Local Times"
neg-d2-662,2025-02-17,,2502.11923," This editorial explores the significance of research visibility within the
evolving landscape of academic communication, mainly focusing on the role of
search engines as online meta-markets shaping the impact of research. With the
rapid expansion of scientific output and the increasing reliance on
algorithm-driven platforms such as Google and Google Scholar, the online
visibility of scholarly work has become an essential factor in determining its
reach and influence. The need for more rigorous research into academic search
engine optimization (A-SEO), a field still in its infancy despite its growing
relevance, is also discussed, highlighting key challenges in the field,
including the lack of robust research methodologies, the skepticism within the
academic community regarding the commercialization of science, and the need for
standardization in reporting and measurement techniques. This editorial thus
invites a multidisciplinary dialogue on the future of research visibility, with
significant implications for academic publishing, science communication,
research evaluation, and the global scientific ecosystem.",['cs.DL'],2503.13464," Research data management (RDM) strategies and practices play a pivotal role
in adhering to the paradigms of reproducibility and transparency by enabling
research sharing in accordance with the principles of Open Science.
Discipline-specificity is an essential factor when understanding RDM
declinations, to tailor a comprehensive support service and to enhance
interdisciplinarity.
  In this paper we present the results of a mapping carried out to gather
information on research data generated and managed within the University of
Bologna (UniBO). The aim is to identify differences and commonalities between
disciplines and potential challenges for institutional support.
  We analyzed the data management plans (DMPs) of European competitive projects
drafted by researchers affiliated with UniBO. We applied descriptive statistics
to the collected variables to answer three main questions: How diverse is the
range of data managed within the University of Bologna? Which trends of
problems and patterns in terms of data management can influence/improve data
stewardship service? Is there an interdisciplinary approach to data production
within the University?
  The research work evidenced many points of contact between different
disciplines in terms of data produced, formats used and modest predilection for
data reuse. Hot topics such as data confidentiality, needed either on privacy
or intellectual property rights (IPR) premises, and long-term preservation pose
challenges to all researchers.
  These results show an increasing attention to RDM while highlighting the
relevance of training and support to face the relatively new challenges posed
by this approach.",['cs.DL'],False,,,,Research on Research Visibility,Mapping Research Data at the University of Bologna
neg-d2-663,2025-03-07,,2503.07659," This article presents an educational proposal based on the computational
implementation of a model of interaction between particles of different types,
as a tool for the development of STEM (Science, Technology, Engineering, and
Mathematics) skills. The mathematical formulation of the model, the equations
governing particle interaction, the interaction matrix that governs collective
behavior, and a complete pseudocode of the simulation algorithm are detailed.
The article discusses how this interdisciplinary project enables students to
apply and connect knowledge from physics, mathematics, programming, and systems
thinking, fostering skills such as computational thinking, mathematical
modeling, and understanding of complex systems.",['physics.ed-ph'],2503.07659," This article presents an educational proposal based on the computational
implementation of a model of interaction between particles of different types,
as a tool for the development of STEM (Science, Technology, Engineering, and
Mathematics) skills. The mathematical formulation of the model, the equations
governing particle interaction, the interaction matrix that governs collective
behavior, and a complete pseudocode of the simulation algorithm are detailed.
The article discusses how this interdisciplinary project enables students to
apply and connect knowledge from physics, mathematics, programming, and systems
thinking, fostering skills such as computational thinking, mathematical
modeling, and understanding of complex systems.",['physics.ed-ph'],False,,,,"Desarrollo de competencias STEM mediante la programaci\'on de modelos de
  auto-organizaci\'on","Desarrollo de competencias STEM mediante la programaci\'on de modelos de
  auto-organizaci\'on"
neg-d2-664,2025-03-06,,2503.05079," This work studies the alignment of large language models with preference data
from an imitation learning perspective. We establish a close theoretical
connection between reinforcement learning from human feedback RLHF and
imitation learning (IL), revealing that RLHF implicitly performs imitation
learning on the preference data distribution. Building on this connection, we
propose DIL, a principled framework that directly optimizes the imitation
learning objective. DIL provides a unified imitation learning perspective on
alignment, encompassing existing alignment algorithms as special cases while
naturally introducing new variants. By bridging IL and RLHF, DIL offers new
insights into alignment with RLHF. Extensive experiments demonstrate that DIL
outperforms existing methods on various challenging benchmarks.",['cs.LG'],2502.00716," Graph-structured datasets often suffer from class imbalance, which
complicates node classification tasks. In this work, we address this issue by
first providing an upper bound on population risk for imbalanced transductive
node classification. We then propose a simple and novel algorithm,
Uncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels
assigned to unlabeled nodes to mitigate the adverse effects of imbalance on
classification accuracy. Furthermore, the UPL algorithm enhances the accuracy
of pseudo-labeling by reducing training noise of pseudo-labels through a novel
uncertainty-aware approach. We comprehensively evaluate the UPL algorithm
across various benchmark datasets, demonstrating its superior performance
compared to existing state-of-the-art methods.",['cs.LG'],False,,,,On a Connection Between Imitation Learning and RLHF,"UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node
  Classification"
neg-d2-665,2025-01-22,,2501.12739," Stochastic Gradient Descent (SGD) is the foundation of modern deep learning
optimization but becomes increasingly inefficient when training convolutional
neural networks (CNNs) on high-resolution data. This paper introduces
Multiscale Stochastic Gradient Descent (Multiscale-SGD), a novel optimization
approach that exploits coarse-to-fine training strategies to estimate the
gradient at a fraction of the cost, improving the computational efficiency of
SGD type methods while preserving model accuracy. We derive theoretical
criteria for Multiscale-SGD to be effective, and show that while standard
convolutions can be used, they can be suboptimal for noisy data. This leads us
to introduce a new class of learnable, scale-independent Mesh-Free Convolutions
(MFCs) that ensure consistent gradient behavior across resolutions, making them
well-suited for multiscale training. Through extensive empirical validation, we
demonstrate that in practice, (i) our Multiscale-SGD approach can be used to
train various architectures for a variety of tasks, and (ii) when the noise is
not significant, standard convolutions benefit from our multiscale training
framework. Our results establish a new paradigm for the efficient training of
deep networks, enabling practical scalability in high-resolution and multiscale
learning tasks.",['cs.LG'],2503.09986," Motivated by the remarkable success of artificial intelligence (AI) across
diverse fields, the application of AI to solve scientific problems-often
formulated as partial differential equations (PDEs)-has garnered increasing
attention. While most existing research concentrates on theoretical properties
(such as well-posedness, regularity, and continuity) of the solutions,
alongside direct AI-driven methods for solving PDEs, the challenge of
uncovering symbolic relationships within these equations remains largely
unexplored. In this paper, we propose leveraging large language models (LLMs)
to learn such symbolic relationships. Our results demonstrate that LLMs can
effectively predict the operators involved in PDE solutions by utilizing the
symbolic information in the PDEs. Furthermore, we show that discovering these
symbolic relationships can substantially improve both the efficiency and
accuracy of the finite expression method for finding analytical approximation
of PDE solutions, delivering a fully interpretable solution pipeline. This work
opens new avenues for understanding the symbolic structure of scientific
problems and advancing their solution processes.",['cs.LG'],False,,,,"Multiscale Stochastic Gradient Descent: Efficiently Training
  Convolutional Neural Networks","From Equations to Insights: Unraveling Symbolic Structures in PDEs with
  LLMs"
neg-d2-666,2025-01-30,,2501.18165," We discuss the vacuum structure of the SU(6) model, a chiral gauge theory,
from the perspective of anomaly matching. To this end, we first identify all
possible 't Hooft anomalies in the UV theory using the Stora-Zumino procedure.
Subsequently, we construct an effective theory by applying the idea of the
Wess-Zumino-Witten action to derive the topological terms that encode the 't
Hooft anomalies. As a result, we demonstrate that a low-energy effective theory
reproducing one of the anomalies, namely the mixed anomaly, is described by a
Z3-valued scalar field. On the other hand, the effective theory that accounts
for the discrete chiral self-anomaly is significantly more intricate, and
elucidating its structure remains an ongoing challenge.",['hep-lat'],2501.18165," We discuss the vacuum structure of the SU(6) model, a chiral gauge theory,
from the perspective of anomaly matching. To this end, we first identify all
possible 't Hooft anomalies in the UV theory using the Stora-Zumino procedure.
Subsequently, we construct an effective theory by applying the idea of the
Wess-Zumino-Witten action to derive the topological terms that encode the 't
Hooft anomalies. As a result, we demonstrate that a low-energy effective theory
reproducing one of the anomalies, namely the mixed anomaly, is described by a
Z3-valued scalar field. On the other hand, the effective theory that accounts
for the discrete chiral self-anomaly is significantly more intricate, and
elucidating its structure remains an ongoing challenge.",['hep-lat'],False,,,,SU(6) model revisited,SU(6) model revisited
neg-d2-667,2025-01-12,,2502.03472," The rapid advancement of Large Language Models (LLMs) has created a critical
gap in consumer protection due to the lack of standardized certification
processes for LLM-powered Artificial Intelligence (AI) systems. This paper
argues that current regulatory approaches, which focus on compute-level
thresholds and generalized model evaluations, are insufficient to ensure the
safety and effectiveness of specific LLM-based user experiences. We propose a
shift towards a certification process centered on actual user-facing
experiences and the curation of high-quality datasets for evaluation. This
approach offers several benefits: it drives consumer confidence in AI system
performance, enables businesses to demonstrate the credibility of their
products, and allows regulators to focus on direct consumer protection. The
paper outlines a potential certification workflow, emphasizing the importance
of domain-specific datasets and expert evaluation. By repositioning data as the
strategic center of regulatory efforts, this framework aims to address the
challenges posed by the probabilistic nature of AI systems and the rapid pace
of technological advancement. This shift in regulatory focus has the potential
to foster innovation while ensuring responsible AI development, ultimately
benefiting consumers, businesses, and government entities alike.",['cs.CY'],2501.12544," Systems interacting with humans, such as assistive robots or chatbots, are
increasingly integrated into our society. To prevent these systems from causing
social, legal, ethical, empathetic, or cultural (SLEEC) harms, normative
requirements specify the permissible range of their behaviors. These
requirements encompass both functional and non-functional aspects and are
defined with respect to time. Typically, these requirements are specified by
stakeholders from a broad range of fields, such as lawyers, ethicists, or
philosophers, who may lack technical expertise. Because such stakeholders often
have different goals, responsibilities, and objectives, ensuring that these
requirements are well-formed is crucial. SLEEC DSL, a domain-specific language
resembling natural language, has been developed to formalize these requirements
as SLEEC rules. In this paper, we present LEGOS-SLEEC, a tool designed to
support interdisciplinary stakeholders in specifying normative requirements as
SLEEC rules, and in analyzing and debugging their well-formedness. LEGOS-SLEEC
is built using four previously published components, which have been shown to
be effective and usable across nine case studies. Reflecting on this
experience, we have significantly improved the user interface of LEGOS-SLEEC
and its diagnostic support, and demonstrate the effectiveness of these
improvements using four interdisciplinary stakeholders. Showcase video URL is:
https://youtu.be/LLaBLGxSi8A",['cs.CY'],False,,,,"Powering LLM Regulation through Data: Bridging the Gap from Compute
  Thresholds to Customer Experiences",LEGOS-SLEEC: Tool for Formalizing and Analyzing Normative Requirements
neg-d2-668,2025-03-18,,2503.14673," We derive a manifestly superconformally covariant unfolded formulation of the
free (2,0) tensor multiplet. The unfolded system consists of an abelian
two-form and an infinite-dimensional, chiral Weyl zero-form realized using
superoscillators. The construction of the cocycle gluing these forms on a
general superconformal background goes one step beyond previous results in
super-Poincar\'e backgrounds.",['hep-th'],2503.14673," We derive a manifestly superconformally covariant unfolded formulation of the
free (2,0) tensor multiplet. The unfolded system consists of an abelian
two-form and an infinite-dimensional, chiral Weyl zero-form realized using
superoscillators. The construction of the cocycle gluing these forms on a
general superconformal background goes one step beyond previous results in
super-Poincar\'e backgrounds.",['hep-th'],False,,,,Unfolding the Six-Dimensional Tensor Multiplet,Unfolding the Six-Dimensional Tensor Multiplet
neg-d2-669,2025-03-17,,2503.13743," We tackle the problem of monocular 3D object detection across different
sensors, environments, and camera setups. In this paper, we introduce a novel
unsupervised domain adaptation approach, MonoCT, that generates highly accurate
pseudo labels for self-supervision. Inspired by our observation that accurate
depth estimation is critical to mitigating domain shifts, MonoCT introduces a
novel Generalized Depth Enhancement (GDE) module with an ensemble concept to
improve depth estimation accuracy. Moreover, we introduce a novel Pseudo Label
Scoring (PLS) module by exploring inner-model consistency measurement and a
Diversity Maximization (DM) strategy to further generate high-quality pseudo
labels for self-training. Extensive experiments on six benchmarks show that
MonoCT outperforms existing SOTA domain adaptation methods by large margins
(~21% minimum for AP Mod.) and generalizes well to car, traffic camera and
drone views.",['cs.CV'],2503.15871," In this work, we tackle action-scene hallucination in Video Large Language
Models (Video-LLMs), where models incorrectly predict actions based on the
scene context or scenes based on observed actions. We observe that existing
Video-LLMs often suffer from action-scene hallucination due to two main
factors. First, existing Video-LLMs intermingle spatial and temporal features
by applying an attention operation across all tokens. Second, they use the
standard Rotary Position Embedding (RoPE), which causes the text tokens to
overemphasize certain types of tokens depending on their sequential orders. To
address these issues, we introduce MASH-VLM, Mitigating Action-Scene
Hallucination in Video-LLMs through disentangled spatial-temporal
representations. Our approach includes two key innovations: (1) DST-attention,
a novel attention mechanism that disentangles the spatial and temporal tokens
within the LLM by using masked attention to restrict direct interactions
between the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the
dimensionality of the positional IDs, allowing the spatial and temporal tokens
to maintain balanced positions relative to the text tokens. To evaluate the
action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark
with 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that
MASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as
on existing video understanding benchmarks.",['cs.CV'],False,,,,"MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent
  Teacher Models","MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through
  Disentangled Spatial-Temporal Representations"
neg-d2-670,2025-02-03,,2502.01759," A generalization of the Majumdar-Papapetrou multi-black hole spacetime was
recently constructed by Teo and Wan [1], describing charged and spinning
(extremal) balanced black holes in asymptotically flat spacetime. We explore
the dynamics of null geodesics on this geometry, focusing on the two-center
solution. Using the topological charge formalism, we show that various light
ring arrangements arise from different choices of individual angular momenta:
light rings with opposite topological charges can merge and annihilate each
other, resulting in configurations with a total of 4, 6, or 8 light rings.
Using backward ray-tracing, we obtained the shadow and lensing of these
spacetimes. The former, in particular, closely resembles those for the
double-Kerr metric.",['gr-qc'],2501.03125," We revise the role that ultraviolet divergences of quantum fields play in
slow-roll inflation, and discuss the renormalization of cosmological
observables from a space-time perspective, namely the angular power spectrum.
We first derive an explicit expression for the multipole coefficients
$C_{\ell}$ in the Sachs-Wolfe regime in terms of the two-point function of
primordial perturbations. We then analyze the ultraviolet behavior, and point
out that the standard result in the literature is equivalent to a
renormalization of $C_{\ell}$ at zero ``adiabatic'' order. We further argue
that renormalization at second ``adiabatic'' order may be more appropriate from
the viewpoint of standard quantum field theory. This may change significantly
the predictions for $C_{\ell}$, while maintaining scale invariance.",['gr-qc'],False,,,,"Spinning generalizations of Majumdar-Papapetrou multi-black hole
  spacetimes: light rings, lensing and shadows","On the renormalization of ultraviolet divergences in the inflationary
  angular power spectrum"
neg-d2-671,2025-02-21,,2502.15232," In-situ control over band mass inversion is crucial for developing materials
with topologically protected edge modes. In this Letter, we report the direct
observation of displacement field $D$ control of band mass and Berry phase in
Bernal stacked trilayer graphene (TLG) in the region where trigonal warping
distorts the quadratic band into off-center Dirac points, referred to as `Dirac
Gullies.' Using Shubnikov-de-Haas (SdH) oscillations, we map the Fermi surface
contours of the Dirac gullies and the $D$-dependent band structure. With
increasing $D$-field, the Berry phase undergoes multiple transitions from
$\Phi_B=2\pi$ $\rightarrow$ $\pi$ $\rightarrow$ $2\pi$ as $D$ is varied.
Concurrently, measurement of the effective mass reveals a series of transitions
between massless and massive bands, signaling the closure and reopening
(accompanied by a possible band inversion) of the band gap at a critical value
of $D$. Interestingly, the expected Dirac-like behavior of the Dirac gullies
($\Phi_B=\pi$) persists only over a narrow range of $D$. Our study directly
confirms recent predictions of $ D$-field-induced band inversion in the
low-energy regions of TLG. It is a significant step towards achieving control
over pure valley transport in multilayer graphene.",['cond-mat.mes-hall'],2502.12931," We continue our explorations of the transport characteristics in
junction-configurations comprising semimetals with quadratic band-crossings,
observed in the bandstructures of both two- and three-dimensional materials.
Here, we consider short potential barriers/wells modelled by delta-function
potentials. We also generalize our analysis by incorporating tilts in the
dispersion. Due to the parabolic nature of the spectra, caused by
quadratic-in-momentum dependence, there exist evanescent waves, which decay
exponentially as we move away from the junction represented by the location of
the delta-function potential. Investigating the possibility of the appearance
of bound states, we find that their energies appear as pairs of $\pm |E_b |$,
reflecting the presence of the imaginary-valued wavevectors at both positive
and negative values of energies of the propagating quasiparticles.",['cond-mat.mes-hall'],False,,,,Gate tunable Dirac mass and Berry phase in Trilayer graphene,"Delta-function-potential junctions with quasiparticles occupying tilted
  bands with quadratic-in-momentum dispersion"
neg-d2-672,2025-01-31,,2501.19314," Despite the rise of recent neural networks in machine translation, those
networks do not work well if the training data is insufficient. In this paper,
we proposed an approach for machine translation in low-resource languages such
as Vietnamese-Chinese. Our proposed method leveraged the power of the
multilingual pre-trained language model (mBART) and both Vietnamese and Chinese
monolingual corpus. Firstly, we built an early bird machine translation model
using the bilingual training dataset. Secondly, we used TF-IDF technique to
select sentences from the monolingual corpus which are the most related to
domains of the parallel dataset. Finally, the first model was used to
synthesize the augmented training data from the selected monolingual corpus for
the translation model. Our proposed scheme showed that it outperformed 8%
compared to the transformer model. The augmented dataset also pushed the model
performance.",['cs.CL'],2501.19314," Despite the rise of recent neural networks in machine translation, those
networks do not work well if the training data is insufficient. In this paper,
we proposed an approach for machine translation in low-resource languages such
as Vietnamese-Chinese. Our proposed method leveraged the power of the
multilingual pre-trained language model (mBART) and both Vietnamese and Chinese
monolingual corpus. Firstly, we built an early bird machine translation model
using the bilingual training dataset. Secondly, we used TF-IDF technique to
select sentences from the monolingual corpus which are the most related to
domains of the parallel dataset. Finally, the first model was used to
synthesize the augmented training data from the selected monolingual corpus for
the translation model. Our proposed scheme showed that it outperformed 8%
compared to the transformer model. The augmented dataset also pushed the model
performance.",['cs.CL'],False,,,,"An Efficient Approach for Machine Translation on Low-resource Languages:
  A Case Study in Vietnamese-Chinese","An Efficient Approach for Machine Translation on Low-resource Languages:
  A Case Study in Vietnamese-Chinese"
neg-d2-673,2025-03-14,,2503.11187," Video Large Language Models have shown impressive capabilities in video
comprehension, yet their practical deployment is hindered by substantial
inference costs caused by redundant video tokens. Existing pruning techniques
fail to fully exploit the spatiotemporal redundancy inherent in video data. To
bridge this gap, we perform a systematic analysis of video redundancy from two
perspectives: temporal context and visual context. Leveraging this insight, we
propose Dynamic Density Pruning for Fast Video LLMs termed FastVID.
Specifically, FastVID dynamically partitions videos into temporally ordered
segments to preserve temporal structure and applies a density-based token
pruning strategy to maintain essential visual information. Our method
significantly reduces computational overhead while maintaining temporal and
visual integrity. Extensive evaluations show that FastVID achieves
state-of-the-art performance across various short- and long-video benchmarks on
leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID
effectively prunes 90% of video tokens while retaining 98.0% of
LLaVA-OneVision's original performance. The code is available at
https://github.com/LunarShen/FastVID.",['cs.CV'],2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],False,,,,FastVID: Dynamic Density Pruning for Fast Video Large Language Models,Interpretable Failure Detection with Human-Level Concepts
neg-d2-674,2025-02-13,,2502.09509," Latent generative models have emerged as a leading approach for high-quality
image synthesis. These models rely on an autoencoder to compress images into a
latent space, followed by a generative model to learn the latent distribution.
We identify that existing autoencoders lack equivariance to semantic-preserving
transformations like scaling and rotation, resulting in complex latent spaces
that hinder generative performance. To address this, we propose EQ-VAE, a
simple regularization approach that enforces equivariance in the latent space,
reducing its complexity without degrading reconstruction quality. By finetuning
pre-trained autoencoders with EQ-VAE, we enhance the performance of several
state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,
achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.
EQ-VAE is compatible with both continuous and discrete autoencoders, thus
offering a versatile enhancement for a wide range of latent generative models.
Project page and code: https://eq-vae.github.io/.",['cs.LG'],2501.14588," With the development of the digital economy, data is increasingly recognized
as an essential resource for both work and life. However, due to privacy
concerns, data owners tend to maximize the value of data through the
circulation of information rather than direct data transfer. Federated learning
(FL) provides an effective approach to collaborative training models while
preserving privacy. However, as model parameters and training data grow, there
are not only real differences in data resources between different data owners,
but also mismatches between data and computing resources. These challenges lead
to inadequate collaboration among data owners, compute centers, and model
owners, reducing the global utility of the three parties and the effectiveness
of data assetization. In this work, we first propose a framework for
resource-decoupled FL involving three parties. Then, we design a Tripartite
Stackelberg Model and theoretically analyze the Stackelberg-Nash equilibrium
(SNE) for participants to optimize global utility. Next, we propose the
Quality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we
derive and solve the optimal strategies of all parties to achieve SNE using
backward induction. We also design a dynamic optimization mechanism to improve
the optimal strategy profile by evaluating the contribution of data quality
from data owners to the global model during real training. Finally, our
extensive experiments demonstrate that our method effectively encourages the
linkage of the three parties involved, maximizing the global utility and value
of data assets.",['cs.LG'],False,,,,"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative
  Image Modeling",Data Assetization via Resources-decoupled Federated Learning
neg-d2-675,2025-02-07,,2502.04998," We propose and study a planning problem we call Sequential Fault-Intolerant
Process Planning (SFIPP). SFIPP captures a reward structure common in many
sequential multi-stage decision problems where the planning is deemed
successful only if all stages succeed. Such reward structures are different
from classic additive reward structures and arise in important applications
such as drug/material discovery, security, and quality-critical product design.
We design provably tight online algorithms for settings in which we need to
pick between different actions with unknown success chances at each stage. We
do so both for the foundational case in which the behavior of actions is
deterministic, and the case of probabilistic action outcomes, where we
effectively balance exploration for learning and exploitation for planning
through the usage of multi-armed bandit algorithms. In our empirical
evaluations, we demonstrate that the specialized algorithms we develop, which
leverage additional information about the structure of the SFIPP instance,
outperform our more general algorithm.",['cs.AI'],2503.15807," In the field of video-language pretraining, existing models face numerous
challenges in terms of inference efficiency and multimodal data processing.
This paper proposes a KunLunBaize-VoT-R1 video inference model based on a
long-sequence image encoder, along with its training and application methods.
By integrating image packing technology, the Autonomy-of-Experts (AoE)
architecture, and combining the video of Thought (VoT), a large language model
(LLM) trained with large-scale reinforcement learning, and multiple training
techniques, the efficiency and accuracy of the model in video inference tasks
are effectively improved. Experiments show that this model performs
outstandingly in multiple tests, providing a new solution for video-language
understanding.",['cs.AI'],False,,,,On Sequential Fault-Intolerant Process Planning,"Video-VoT-R1: An efficient video inference model integrating image
  packing and AoE architecture"
neg-d2-676,2025-03-08,,2503.06389," Network estimation has been a critical component of single-cell
transcriptomic data analysis, which can provide crucial insights into the
complex interplay among genes, facilitating uncovering the biological basis of
human life at single-cell resolution. Despite notable achievements, existing
methodologies often falter in their practicality, primarily due to their narrow
focus on simplistic linear relationships and inadequate handling of cellular
heterogeneity. To bridge these gaps, we propose a joint regularized deep neural
network method incorporating a Mahalanobis distance-based K-means clustering
(JRDNN-KM) to estimate multiple networks for various cell subgroups
simultaneously, accounting for both unknown cellular heterogeneity and
zero-inflation and, more importantly, complex nonlinear relationships among
genes. We innovatively introduce a selection layer for network construction and
develop homogeneous and heterogeneous hidden layers to accommodate commonality
and specificity across multiple networks. Through simulations and applications
to real single-cell transcriptomic data for multiple tissues and species, we
show that JRDNN-KM constructs networks with more accuracy and biological
interpretability and, meanwhile, identifies more accurate cell subgroups
compared to the state-of-the-art methods in the literature. Building on the
network construction, we further find hub genes with important biological
implications and modules with statistical enrichment of biological processes.",['stat.AP'],2502.16362," Epidemiologic studies often evaluate the association between an exposure and
an event risk. When time-varying, exposure updates usually occur at discrete
visits although changes are in continuous time and survival models require
values to be constantly known. Moreover, exposures are likely measured with
error, and their observation truncated at the event time. We aimed to quantify
in a Cox regression the bias in the association resulting from intermittent
measurements of an error-prone exposure. Using simulations under various
scenarios, we compared five methods: last observation carried-forward (LOCF),
classical two-stage regression-calibration using measurements up to the event
(RC) or also after (PE-RC), multiple imputation (MI) and joint modeling of the
exposure and the event (JM). The LOCF, and to a lesser extent the classical RC,
showed substantial bias in almost all 43 scenarios. The RC bias was avoided
when considering post-event information. The MI performed relatively well, as
did the JM. Illustrations exploring the association of Body Mass Index and
Executive Functioning with dementia risk showed consistent conclusions.
Accounting for measurement error and discrete updates is critical when studying
time-varying exposures. MI and JM techniques may be applied in this context,
while classical RC should be avoided due to the informative truncation.",['stat.AP'],False,,,,"Heterogeneous network estimation for single-cell transcriptomic data via
  a joint regularized deep neural network","Including an infrequently measured time-dependent error-prone covariate
  in survival analyses: a simulation-based comparison of methods"
neg-d2-677,2025-02-17,,2502.11653," Learning-based algorithms have gained great popularity in communications
since they often outperform even carefully engineered solutions by learning
from training samples. In this paper, we show that the selection of appropriate
training examples can be important for the performance of such learning-based
algorithms. In particular, we consider non-linear 1-bit precoding for massive
multi-user MIMO systems using the C2PO algorithm. While previous works have
already shown the advantages of learning critical coefficients of this
algorithm, we demonstrate that straightforward selection of training samples
that follow the channel model distribution does not necessarily lead to the
best result. Instead, we provide a strategy to generate training data based on
the specific properties of the algorithm, which significantly improves its
error floor performance.",['eess.SP'],2501.11974," We investigated wideband pulse generation for underwater acoustic
applications using a parametric array. We fabricated a transducer consisting of
a 3 mm thick 75 mm-by-75 mm square-shaped PZT ceramic plate, which is matched
to water media at the radiating face and terminated by a very low impedance at
the back. All measurements were made in a large test tank. We transmitted
square-root amplitude modulated pulses centered around 855 kHz primary
frequency. We showed that phase-sensitive generation of in-phase and
out-of-phase bursts suitable for coded transmission using a parametric array is
possible. We generated very short duration bursts, as short as half-cycle, at a
10-80 kHz difference frequency range. The definition of the bursts is
excellent, e.g., with a normalized cross-correlation of 0.92 with an ideal
2-cycle square burst, for both in-phase and out-of-phase pulses.",['eess.SP'],False,,,,"Training Channel Selection for Learning-based 1-bit Precoding in Massive
  MU-MIMO","Wideband Pulse Generation for Underwater Applications Using Parametric
  Array"
neg-d2-678,2025-02-20,,2502.14722," We propose a novel method for model-based time super-sampling of turbulent
flow fields. The key enabler is the identification of an empirical Galerkin
model from the projection of the Navier-Stokes equations on a data-tailored
basis. The basis is obtained from a Proper Orthogonal Decomposition (POD) of
the measured fields. Time super-sampling is thus achieved by a time-marching
integration of the identified dynamical system, taking the original snapshots
as initial conditions. Temporal continuity of the reconstructed velocity fields
is achieved through a forward-backwards integration between consecutive
measured Particle Image Velocimetry measurements of a turbulent jet flow. The
results are compared with the interpolation of the POD temporal coefficients
and the low-order reconstruction of data measured at a higher sampling rate. In
both cases, the results obtained show the ability of the method to reconstruct
the dynamics of the flow with small errors during several flow characteristic
times.",['physics.flu-dyn'],2503.05964," Proper-orthogonal decomposition (POD) based reduced-order models (ROM) of
structurally dominant fluid flow can support a wide range of engineering
applications. Yet, although they perform well for unsteady laminar flows, their
straightforward extension to turbulent flows fails to capture the effects of
small scale eddies and often leads to divergent solutions. Several approaches
to mimic nonlinear closure terms modeling techniques within ROM frameworks have
been employed to include the effect of higher modes that are often neglected.
Recent success of neural network based models show promising results in
modeling the effects of turbulence. In this study, we augment POD-ROM with a
recurrent neural network (RNN) to develop ROM for turbulent flows. We simulate
a three dimensional flow past a circular cylinder at Reynolds number of 1000.
We first compute the POD modes and project the Navier-Stokes equations onto the
limited number of modes in a Galerkin approach to develop a conventional ROM
and LES-inspired ROM for comparison. We then develop a hybrid model by
integrating the output of Galerkin projection ROM and long short-term memory
(LSTM) RNN and term it as a physics-guided machine learning (PGML) model. The
novelty of this study is to introduce a hybrid model that integrates LES
inspired ROM and RNN to achieve more accurate and reliable predictions of
turbulent flows. The results demonstrate that PGML for higher temporal
coefficients outperforms the conventional and LES-inspired ROM.",['physics.flu-dyn'],False,,,,Model-based time super-sampling of turbulent flow field sequences,"Hybrid Reduced-Order Models for Turbulent Flows Using Recurrent Neural
  Architectures"
neg-d2-679,2025-02-02,,2502.00716," Graph-structured datasets often suffer from class imbalance, which
complicates node classification tasks. In this work, we address this issue by
first providing an upper bound on population risk for imbalanced transductive
node classification. We then propose a simple and novel algorithm,
Uncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels
assigned to unlabeled nodes to mitigate the adverse effects of imbalance on
classification accuracy. Furthermore, the UPL algorithm enhances the accuracy
of pseudo-labeling by reducing training noise of pseudo-labels through a novel
uncertainty-aware approach. We comprehensively evaluate the UPL algorithm
across various benchmark datasets, demonstrating its superior performance
compared to existing state-of-the-art methods.",['cs.LG'],2501.1858," This study focuses on the application of deep geometric models to solve the
3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and
defining distance as the model's optimization objective. The distance
approximation task is reformulated as a node classification problem,
effectively addressed using Graph Neural Networks (GNNs). After training the
model on a random subgraph, the predicted classes are used to construct a
heuristic for $A^*$ search. We conclude with experiments comparing our
heuristic to that of the DeepCubeA model.",['cs.LG'],False,,,,"UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node
  Classification",Node Classification and Search on the Rubik's Cube Graph with GNNs
neg-d2-680,2025-03-11,,2503.08792," The work describes a numerical method to study the nature of compressive
failure of porous ceramics in relation to its volume fraction. The
microstructure of an alumina-based foam material manufactured by mechanical
stirring of a slurry is studied here. A finite element-based compressive
failure simulation of a real microstructure obtained from microtomography
(micro-CT) scans is conducted. A recently developed microstructure
reconstruction algorithm is utilized to generate artificial microstructures
statistically equivalent to the real one obtained from micro-CT. The accuracy
of the reconstruction procedure is established by comparing the simulated
compression behavior of the reconstructed microstructure with that of the real
one along with the experimentally measured results. The effect of sample size
on the simulated compression behavior is studied by computing compression
stress-strain behavior for varying sizes of the reconstructed microstructures.
Further, artificial microstructures of the porous ceramic with different volume
fractions are reconstructed along with computing compression stress-strain
behavior to establish relationship between ceramic content (volume fraction)
and compressive strength of this material. The nature of the compressive
failure for microstructures with different volume fractions is studied and the
results are compared with the analytical models and the experimental
observations available in the literature.",['physics.app-ph'],2502.21302," We investigate the impact of quantum well (QW) thickness on efficiency loss
in c-plane InGaN/GaN LEDs using a small-signal electroluminescence (SSEL)
technique. Multiple mechanisms related to efficiency loss are independently
examined, including injection efficiency, carrier density vs. current density
relationship, phase space filling (PSF), quantum confined stark effect (QCSE),
and Coulomb enhancement. An optimal QW thickness of around 2.7 nm in these
InGaN/GaN LEDs was determined for quantum wells having constant In composition.
Despite better control of deep-level defects and lower carrier density at a
given current density, LEDs with thin QWs still suffer from an imbalance of
enhancement effects on the radiative and intrinsic Auger-Meitner recombination
coefficients. The imbalance of enhancement effects results in a decline in
internal quantum efficiency (IQE) and radiative efficiency with decreasing QW
thickness at low current density in LEDs with QW thicknesses below 2.7 nm. We
also investigate how LED modulation bandwidth varies with quantum well
thickness, identifying the key trends and their implications for device
performance.",['physics.app-ph'],False,,,,"Compression failure of porous ceramics: A computational study about the
  effect of volume fraction on damage evolution and failure","Impact of Quantum Well Thickness on Efficiency Loss in InGaN/GaN LEDs:
  Challenges for Thin-Well Designs"
neg-d2-681,2025-01-09,,2501.05524," This mentoring resource is a guide to establishing and running near-peer
mentorship programs. It is based on the working knowledge and best practices
developed by the Access Network, a collection of nine student-led communities
at universities across the country working towards a vision of a more diverse,
equitable, inclusive, and accessible STEM environment. Many of these
communities, also referred to as sites, include a near-peer mentoring program
that is developed to best support their local context. The format of these
programs vary, ranging from structured classes with peer mentoring groups to
student clubs supporting 1-on-1 relationships. To further support program
participants as both students and as whole people, sites often run additional
events such as lecture series, workshops, and social activities guided tailored
to each student community's needs. Through this process, student leaders have
generated and honed best practices for all aspects of running their sites. This
guide is an attempt to synthesize those efforts, offering practical advice for
student leaders setting up near-peer mentorship programs in their own
departments. It has been written through the lens of undergraduate near-peer
mentorship programs, although our framework could easily be extended to other
demographics (e.g. high schoolers, graduate students, etc.). Our experience is
with STEM mentorship specifically, though these practices can extend to any
discipline. In this document, we outline best practices for designing, running,
and sustaining near-peer mentorship programs. We provide template resources to
assist with this work, and lesson plans to run mentor and mentee training
sessions. We hope you find this guide useful in designing, implementing, and
re-evaluating community oriented near-peer mentoring programs.",['physics.ed-ph'],2503.0385," Although physics has become increasingly computational, with computing even
being considered the third pillar of physics [1], it is still not well
integrated into physics education [2]. Research suggests that integrating
Computational Thinking (CT) into physics enhances conceptual understanding and
strengthens students ability to model and analyze phenomena [3]. Building on
this, we designed a didactic sequence for K9 students to foster specific CT
practices while reinforcing fundamental kinematics concepts. Assessments
highlight student's ability to apply CT skills to analyze accelerated motion.
This activity can be seamlessly integrated into introductory kinematics
courses.",['physics.ed-ph'],False,,,,"A Starter Kit for Diversity-Oriented Communities for Undergraduates:
  Near-Peer Mentorship Programs","Code in Motion: Integrating Computational Thinking with Kinematics
  Exploration"
neg-d2-682,2025-01-28,,2501.17247," Recent research suggests that the use of Generative AI tools may result in
diminished critical thinking during knowledge work. We study the effect on
knowledge work of provocations: brief textual prompts that offer critiques for
and propose alternatives to AI suggestions. We conduct a between-subjects study
(n=24) in which participants completed AI-assisted shortlisting tasks with and
without provocations. We find that provocations can induce critical and
metacognitive thinking. We derive five dimensions that impact the user
experience of provocations: task urgency, task importance, user expertise,
provocation actionability, and user responsibility. We connect our findings to
related work on design frictions, microboundaries, and distributed cognition.
We draw design implications for critical thinking interventions in AI-assisted
knowledge work.",['cs.HC'],2502.16895," Teaching scientific concepts is essential but challenging, and analogies help
students connect new concepts to familiar ideas. Advancements in large language
models (LLMs) enable generating analogies, yet their effectiveness in education
remains underexplored. In this paper, we first conducted a two-stage study
involving high school students and teachers to assess the effectiveness of
LLM-generated analogies in biology and physics through a controlled in-class
test and a classroom field study. Test results suggested that LLM-generated
analogies could enhance student understanding particularly in biology, but
require teachers' guidance to prevent over-reliance and overconfidence.
Classroom experiments suggested that teachers could refine LLM-generated
analogies to their satisfaction and inspire new analogies from generated ones,
encouraged by positive classroom feedback and homework performance boosts.
Based on findings, we developed and evaluated a practical system to help
teachers generate and refine teaching analogies. We discussed future directions
for developing and evaluating LLM-supported teaching and learning by analogy.",['cs.HC'],False,,,,"""It makes you think"": Provocations Help Restore Critical Thinking to
  AI-Assisted Knowledge Work","Unlocking Scientific Concepts: How Effective Are LLM-Generated Analogies
  for Student Understanding and Classroom Practice?"
neg-d2-683,2025-01-02,,2501.01217," Movable antennas (MAs) enhance flexibility in beamforming gain and
interference suppression by adjusting position within certain areas of the
transceivers. In this paper, we propose an MA-assisted integrated sensing and
communication framework, wherein MAs are deployed for reconfiguring the channel
array responses at both the receiver and transmitter of a base station. Then,
we develop an optimization framework aimed at maximizing the sensing
signal-to-interference-plus-noise-ratio (SINR) by jointly optimizing the
receive beamforming vector, the transmit beamforming matrix, and the positions
of MAs while meeting the minimum SINR requirement for each user. To address
this nonconvex problem involving complex coupled variables, we devise an
alternating optimization-based algorithm that incorporates techniques including
the Charnes-Cooper transform, second-order Taylor expansion, and successive
convex approximation (SCA). Specifically, the closed form of the received
vector and the optimal transmit matrix can be first obtained in each iteration.
Subsequently, the solutions for the positions of the transmit and receive MAs
are obtained using the SCA method based on the second-order Taylor expansion.
The simulation results show that the proposed scheme has significant advantages
over the other baseline schemes. In particular, the proposed scheme has the
ability to match the performance of the fixed position antenna scheme while
utilizing fewer resources.",['eess.SP'],2501.0582," Modular Arrays (MAs) are a promising architecture to enable multi-user
communications in next-generation multiple-input multiple-output (MIMO) systems
based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off an
improved spatial resolution with characteristic interference patterns
associated to grating lobes. In this work, we analyze whether MAs can
outperform conventional collocated deployments, in terms of achievable sum-rate
and served users in a multi-user downlink set-up. First, we provide a rigorous
analytical characterization of the inter-user interference for modular gMIMO
systems operating in the near field. Then, we leverage these results to
optimize the user selection and precoding mechanisms, designing two algorithms
that largely outperform existing alternatives in the literature, with different
algorithmic complexities. Results show that the proposed algorithms yield over
70% improvements in achievable sum-spectral efficiencies compared to the state
of the art. We also illustrate how MAs allow to serve a larger number of users
thanks to their improved spatial resolution, compared to the collocated
counterpart.",['eess.SP'],False,,,,Movable Antenna-Assisted Integrated Sensing and Communication Systems,User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays
neg-d2-684,2025-01-24,,2501.14876," The muon spin rotation ($\mu$SR) technique has been applied to determine the
behavior of the in-plane magnetic penetration depth ($\lambda_{ab}$) in the
vortex state of the unconventional superconductor Sr$_2$RuO$_4$ as a means of
gaining insight into its still unknown superconducting order parameter. A
recent $\mu$SR study of Sr$_2$RuO$_4$ reported a $T$-linear temperature
dependence for $\lambda_{ab}$ at low temperatures that was not identified in an
earlier $\mu$SR study. Here we show that there is no significant difference
between the data in the early and recent $\mu$SR studies and both are
compatible with the limiting low-temperature $\lambda_{ab} \sim T^2$ dependence
expected from measurements of the change in $\lambda_{ab}(T)$ in the Meissner
state by other techniques. However, we argue that at this time there is no
valid theoretical model for reliably determining the absolute value of
$\lambda_{ab}$ in Sr$_2$RuO$_4$ from $\mu$SR measurements. Instead, we identify
the formation of an unusual square vortex lattice that introduces a new
constraint on candidate superconducting order parameters for Sr$_2$RuO$_4$.",['cond-mat.supr-con'],2503.1265," Motivated by experiments on rhombohedral tetralayer graphene showing signs of
superconductivity emerging from a valley-polarized normal state, we here
analyze theoretically how scanning tunneling spectroscopy can be used to probe
the superconducting order parameter of the system. To describe different
pairing scenarios on equal footing, we develop a microscopic tunneling approach
that can capture arbitrary, including finite-momentum, superconducting order
parameters and low-symmetry normal-state Hamiltonians. Our analysis shows that
the broken time-reversal symmetry in a single valley leads to unique features
in the weak-tunneling regime that are different for commensurate and
incommensurate Cooper pair momenta. We further uncover an unconventional
spatial dependence of the Andreev conductance, allowing to distinguish between
three topologically distinct classes of single-$\mathbf{q}$ pairing states in
the system, and compute the signatures of a competing translational-symmetry
breaking three-$\mathbf{q}$ ''moir\'e superconductor''.",['cond-mat.supr-con'],False,,,,"Atypical vortex lattice and the magnetic penetration depth in
  superconducting Sr$_2$RuO$_4$ deduced by $\mu$SR","Probing superconductivity with tunneling spectroscopy in rhombohedral
  graphene"
neg-d2-685,2025-01-16,,2501.09717," Implications of general properties of quantum field theory, such as
causality, unitarity, and locality include constraints on the couplings of the
effective field theory (EFT) coefficients. These constraints follow from the
connections between the infrared (IR) and ultraviolet (UV) theory imposed by
dispersion relations for four-particle amplitudes which formally allow us to
express EFT couplings through the moments of positive-definite functions
(imaginary parts of partial wave amplitudes) forming the EFT-hedron geometry.
Previous studies of these positivity bounds were mainly focused on the weakly
coupled EFTs, limiting the analysis to tree-level amplitudes of the IR theory.
In this work, we extend the scope of positivity bounds including one-loop
amplitudes, which is essential especially for the loops of massless particles.
Examining a single scalar theory we found that the presence of massless loops
cannot be reduced only to the running of EFT couplings because loops modify the
crossing symmetry relations (null constraints). Our results demonstrate that
while for small coupling constants, the one-loop bounds are in good agreement
with the tree-level results, the allowed EFT parameter ranges can be
significantly modified if a weak coupling assumption is not additionally
imposed. In particular, we found that an upper bound on dimension-8 coupling
becomes significantly stronger, and the dimension-12 coupling can be slightly
negative which was thought to be forbidden by tree-level positivity bounds.",['hep-th'],2502.05065," We consider the reduction of four-derivative heterotic supergravity on a
torus and construct two-charge multicenter BPS black hole solutions. In $d=5$,
the three-form field can be dualized to a gauge field and we correspondingly
construct three-charge multicenter BPS black hole solutions to the dualized
Bergshoeff-de Roo action. This makes precise the embedding of known solutions
into five-dimensional $\alpha'$-corrected STU supergravity.",['hep-th'],False,,,,Running EFT-hedron with null constraints at loop level,Multicenter higher-derivative BPS black holes
neg-d2-686,2025-01-21,,2501.12195," We present a method based on optimal transport to remove arbitrage
opportunities within a finite set of option prices. The method is notably
intended for regulatory stress-tests, which impose to apply important local
distortions to implied volatility surfaces. The resulting stressed option
prices are naturally associated to a family of signed marginal measures: we
formulate the process of removing arbitrage as a projection onto the subset of
martingale measures with respect to a Wasserstein metric in the space of signed
measures. We show how this projection problem can be recast as an optimal
transport problem; in view of the numerical solution, we apply an entropic
regularization technique. For the regularized problem, we derive a strong
duality formula, show convergence results as the regularization parameter
approaches zero, and formulate a multi-constrained Sinkhorn algorithm, where
each iteration involves, at worse, finding the root of an explicit scalar
function. The convergence of this algorithm is also established. We compare our
method with the existing approach by [Cohen, Reisinger and Wang, Appl.\ Math.\
Fin.\ 2020] across various scenarios and test cases.",['q-fin.MF'],2501.12195," We present a method based on optimal transport to remove arbitrage
opportunities within a finite set of option prices. The method is notably
intended for regulatory stress-tests, which impose to apply important local
distortions to implied volatility surfaces. The resulting stressed option
prices are naturally associated to a family of signed marginal measures: we
formulate the process of removing arbitrage as a projection onto the subset of
martingale measures with respect to a Wasserstein metric in the space of signed
measures. We show how this projection problem can be recast as an optimal
transport problem; in view of the numerical solution, we apply an entropic
regularization technique. For the regularized problem, we derive a strong
duality formula, show convergence results as the regularization parameter
approaches zero, and formulate a multi-constrained Sinkhorn algorithm, where
each iteration involves, at worse, finding the root of an explicit scalar
function. The convergence of this algorithm is also established. We compare our
method with the existing approach by [Cohen, Reisinger and Wang, Appl.\ Math.\
Fin.\ 2020] across various scenarios and test cases.",['q-fin.MF'],False,,,,"An Optimal Transport approach to arbitrage correction: Application to
  volatility Stress-Tests","An Optimal Transport approach to arbitrage correction: Application to
  volatility Stress-Tests"
neg-d2-687,2025-01-03,,2501.01901," Simplicial complexes arising from real-world settings may not be directly
observable. Hence, for an unknown simplicial complex in Euclidean space, we
want to efficiently reconstruct it by querying local structure. In particular,
we are interested in queries for the indegree of a simplex $\sigma$ in some
direction: the number of cofacets of $\sigma$ contained in some halfspace
""below"" $\sigma$. Fasy et al. proposed a method that, given the vertex set of a
simplicial complex, uses indegree queries to reconstruct the set of edges. In
particular, they use a sweep algorithm through the vertex set, identifying
edges adjacent to and above each vertex in the sweeping order. The algorithm
relies on a natural but crucial property of the sweeping order: at a given
vertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have
another endpoint that appeared earlier in the order.
  The edge reconstruction algorithm does not immediately extend to
higher-dimensional simplex reconstruction. In particular, it is not possible to
sweep through a set of $i$-simplices in a fixed direction and maintain that all
$(i+1)$-cofacets of a given simplex $\sigma$ that come below $\sigma$ are
known. We circumvent this by defining a sweeping order on a set of
$i$-simplices, that additionally pairs each $i$-simplex $\sigma$ with a
direction perpendicular to $\sigma$. Analogous to Fasy et al., our order has
the crucial property that, at any $i$-simplex $\sigma$ paired with direction
$s$, each $(i+1)$-dimensional coface of $\sigma$ that lies in the halfspace
below $\sigma$ with respect to the direction $s$ has an $i$-dimensional face
that appeared earlier in the order. We show how to compute such an order and
use it to extend the edge reconstruction algorithm of Fasy et al. to simplicial
complex reconstruction. Our algorithm can reconstruct arbitrary embedded
simplicial complexes.",['cs.CG'],2502.20909," Pseudoline arrangements are fundamental objects in discrete and computational
geometry, and different works have tackled the problem of improving the known
bounds on the number of simple arrangements of $n$ pseudolines over the past
decades. The lower bound in particular has seen two successive improvements in
recent years (Dumitrescu and Mandal in 2020 and Cort\'es K\""uhnast et al. in
2024). Here we focus on the upper bound, and show that for large enough $n$,
there are at most $2^{0.6496n^2}$ different simple arrangements of $n$
pseudolines. This follows a series of incremental improvements starting with
work by Knuth in 1992 showing a bound of roughly $2^{0.7925n^2},$ then a bound
of $2^{0.6975n^2}$ by Felsner in 1997, and finally the previous best known
bound of $2^{0.6572n^2}$ by Felsner and Valtr in 2011. The improved bound
presented here follows from a simple argument to combine the approach of this
latter work with the use of the Zone Theorem.",['cs.CG'],False,,,,Sweeping Orders for Simplicial Complex Reconstruction,"Improved Bound on the Number of Pseudoline Arrangements via the Zone
  Theorem"
neg-d2-688,2025-02-12,,2502.08249," This paper introduces the conceptual metaphor of 'digital plastic' as a
framework for understanding the implications of Generative Artificial
Intelligence (GenAI) content through a multiliteracies lens, drawing parallels
with the properties of physical plastic. Similar to its physical counterpart,
GenAI content offers possibilities for content creation and accessibility while
potentially contributing to digital pollution and ecosystem degradation.
Drawing on multiliteracies theory and Conceptual Metaphor Theory, we argue that
Critical Artificial Intelligence Literacy (CAIL) must be integrated into
educational frameworks to help learners navigate this synthetic media
landscape.
  We examine how GenAI can simultaneously lower the barriers to creative and
academic production while threatening to degrade digital ecosystems through
misinformation, bias, and algorithmic homogenization. The digital plastic
metaphor provides a theoretical foundation for understanding both the
affordances and challenges of GenAI, particularly in educational contexts,
where issues of equity and access remain paramount. Our analysis concludes that
cultivating CAIL through a multiliteracies lens is vital for ensuring the
equitable development of critical competencies across geographical and cultural
contexts, especially for those disproportionately vulnerable to GenAI's
increasingly disruptive effects worldwide.",['cs.CY'],2501.12544," Systems interacting with humans, such as assistive robots or chatbots, are
increasingly integrated into our society. To prevent these systems from causing
social, legal, ethical, empathetic, or cultural (SLEEC) harms, normative
requirements specify the permissible range of their behaviors. These
requirements encompass both functional and non-functional aspects and are
defined with respect to time. Typically, these requirements are specified by
stakeholders from a broad range of fields, such as lawyers, ethicists, or
philosophers, who may lack technical expertise. Because such stakeholders often
have different goals, responsibilities, and objectives, ensuring that these
requirements are well-formed is crucial. SLEEC DSL, a domain-specific language
resembling natural language, has been developed to formalize these requirements
as SLEEC rules. In this paper, we present LEGOS-SLEEC, a tool designed to
support interdisciplinary stakeholders in specifying normative requirements as
SLEEC rules, and in analyzing and debugging their well-formedness. LEGOS-SLEEC
is built using four previously published components, which have been shown to
be effective and usable across nine case studies. Reflecting on this
experience, we have significantly improved the user interface of LEGOS-SLEEC
and its diagnostic support, and demonstrate the effectiveness of these
improvements using four interdisciplinary stakeholders. Showcase video URL is:
https://youtu.be/LLaBLGxSi8A",['cs.CY'],False,,,,"GenAI as Digital Plastic: Understanding Synthetic Media Through Critical
  AI Literacy",LEGOS-SLEEC: Tool for Formalizing and Analyzing Normative Requirements
neg-d2-689,2025-02-26,,2502.19699," Although efficient extraction of discriminative spatial-spectral features is
critical for hyperspectral images classification (HSIC), it is difficult to
achieve these features due to factors such as the spatial-spectral
heterogeneity and noise effect. This paper presents a Spatial-Spectral
Diffusion Contrastive Representation Network (DiffCRN), based on denoising
diffusion probabilistic model (DDPM) combined with contrastive learning (CL)
for HSIC, with the following characteristics. First,to improve spatial-spectral
feature representation, instead of adopting the UNets-like structure which is
widely used for DDPM, we design a novel staged architecture with spatial
self-attention denoising module (SSAD) and spectral group self-attention
denoising module (SGSAD) in DiffCRN with improved efficiency for
spectral-spatial feature learning. Second, to improve unsupervised feature
learning efficiency, we design new DDPM model with logarithmic absolute error
(LAE) loss and CL that improve the loss function effectiveness and increase the
instance-level and inter-class discriminability. Third, to improve feature
selection, we design a learnable approach based on pixel-level spectral angle
mapping (SAM) for the selection of time steps in the proposed DDPM model in an
adaptive and automatic manner. Last, to improve feature integration and
classification, we design an Adaptive weighted addition modul (AWAM) and Cross
time step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wise
features and perform classification. Experiments conducted on widely used four
HSI datasets demonstrate the improved performance of the proposed DiffCRN over
the classical backbone models and state-of-the-art GAN, transformer models and
other pretrained methods. The source code and pre-trained model will be made
available publicly.",['cs.CV'],2501.08837," Long-term dense action anticipation is very challenging since it requires
predicting actions and their durations several minutes into the future based on
provided video observations. To model the uncertainty of future outcomes,
stochastic models predict several potential future action sequences for the
same observation. Recent work has further proposed to incorporate uncertainty
modelling for observed frames by simultaneously predicting per-frame past and
future actions in a unified manner. While such joint modelling of actions is
beneficial, it requires long-range temporal capabilities to connect events
across distant past and future time points. However, the previous work
struggles to achieve such a long-range understanding due to its limited and/or
sparse receptive field. To alleviate this issue, we propose a novel MANTA
(MAmba for ANTicipation) network. Our model enables effective long-term
temporal modelling even for very long sequences while maintaining linear
complexity in sequence length. We demonstrate that our approach achieves
state-of-the-art results on three datasets - Breakfast, 50Salads, and
Assembly101 - while also significantly improving computational and memory
efficiency. Our code is available at https://github.com/olga-zats/DIFF_MANTA .",['cs.CV'],False,,,,"Spatial-Spectral Diffusion Contrastive Representation Network for
  Hyperspectral Image Classification","MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term
  Dense Anticipation"
neg-d2-690,2025-02-18,,2502.12644," Envy-freeness is one of the most prominent fairness concepts in the
allocation of indivisible goods. Even though trivial envy-free allocations
always exist, rich literature shows this is not true when one additionally
requires some efficiency concept (e.g., completeness, Pareto-efficiency, or
social welfare maximization). In fact, in such case even deciding the existence
of an efficient envy-free allocation is notoriously computationally hard. In
this paper, we explore the limits of efficient computability by relaxing
standard efficiency concepts and analyzing how this impacts the computational
complexity of the respective problems. Specifically, we allow partial
allocations (where not all goods are allocated) and impose only very mild
efficiency constraints, such as ensuring each agent receives a bundle with
positive utility. Surprisingly, even such seemingly weak efficiency
requirements lead to a diverse computational complexity landscape. We identify
several polynomial-time solvable or fixed-parameter tractable cases for binary
utilities, yet we also find NP-hardness in very restricted scenarios involving
ternary utilities.",['cs.GT'],2502.12644," Envy-freeness is one of the most prominent fairness concepts in the
allocation of indivisible goods. Even though trivial envy-free allocations
always exist, rich literature shows this is not true when one additionally
requires some efficiency concept (e.g., completeness, Pareto-efficiency, or
social welfare maximization). In fact, in such case even deciding the existence
of an efficient envy-free allocation is notoriously computationally hard. In
this paper, we explore the limits of efficient computability by relaxing
standard efficiency concepts and analyzing how this impacts the computational
complexity of the respective problems. Specifically, we allow partial
allocations (where not all goods are allocated) and impose only very mild
efficiency constraints, such as ensuring each agent receives a bundle with
positive utility. Surprisingly, even such seemingly weak efficiency
requirements lead to a diverse computational complexity landscape. We identify
several polynomial-time solvable or fixed-parameter tractable cases for binary
utilities, yet we also find NP-hardness in very restricted scenarios involving
ternary utilities.",['cs.GT'],False,,,,Computing Efficient Envy-Free Partial Allocations of Indivisible Goods,Computing Efficient Envy-Free Partial Allocations of Indivisible Goods
neg-d2-691,2025-03-17,,2503.129," Neutron three-axis spectrometry has been used to determine the interplanar
magnetic exchange parameter in the magnetic van der Waals compound CoPS$_3$.
The exchange is found to be small and antiferromagnetic, estimated to be 0.020
$\pm$ 0.001 meV, which is surprising considering that the magnetic structure is
correlated ferromagnetically between the ab planes. A possible explanation,
involving a small anisotropy in the exchanges, is proposed. The results are
discussed with reference to the other members of the transition metal-PS3
compounds.",['cond-mat.str-el'],2503.129," Neutron three-axis spectrometry has been used to determine the interplanar
magnetic exchange parameter in the magnetic van der Waals compound CoPS$_3$.
The exchange is found to be small and antiferromagnetic, estimated to be 0.020
$\pm$ 0.001 meV, which is surprising considering that the magnetic structure is
correlated ferromagnetically between the ab planes. A possible explanation,
involving a small anisotropy in the exchanges, is proposed. The results are
discussed with reference to the other members of the transition metal-PS3
compounds.",['cond-mat.str-el'],False,,,,Interplanar magnetic exchange in CoPS$_3$,Interplanar magnetic exchange in CoPS$_3$
neg-d2-692,2025-02-18,,2502.13029," Many spiral galaxies host magnetic fields with energy densities comparable to
those of the turbulent and thermal motions of their interstellar gas. However,
quantitative comparison between magnetic field properties inferred from
observation and those obtained from theoretical modeling has been lacking. In
Paper I we developed a simple, axisymmetric galactic dynamo model that uses
various observational data as input. Here we apply our model to calculate
radial profiles of azimuthally and vertically averaged magnetic field strength
and pitch angle, gas velocity dispersion and scale height, turbulent
correlation time and length, and the sizes of supernova remnants for the
galaxies M31, M33, M51, and NGC 6946, using input data collected from the
literature. Scaling factors are introduced to account for a lack of precision
in both theory and observation. Despite the simplicity of our model, its
outputs agree fairly well with galaxy properties inferred from observation.
Additionally, we find that most of the parameter values are similar between
galaxies. We extend the model to predict the magnetic field pitch angles
arising from a combination of mean-field dynamo action and the winding up of
the random small-scale field owing to the large-scale radial shear. We find
their magnitudes to be much smaller than those of the pitch angles measured in
polarized radio and far infrared emission. This suggests that effects not
included in our model, such as effects associated with spiral arms, are needed
to explain the pitch angle values.",['astro-ph.GA'],2501.01613," The cold and hot interstellar medium (ISM) in star forming galaxies resembles
the reservoir for star formation and associated heating by stellar winds and
explosions during stellar evolution, respectively. We utilize data from deep
$Chandra$ observations and archival millimeter surveys to study the
interconnection between these two phases and the relation to star formation
activities in M51 on kiloparsec scales. A sharp radial decrease is present in
the hot gas surface brightness profile within the inner 2 kpc of M51. The ratio
between the total infrared luminosity ($L_{\rm IR}$) and the hot gas luminosity
($L_{\rm 0.5 - 2\,keV}^{\rm gas}$) shows a positive correlation with the
galactic radius in the central region. For the entire galaxy, a twofold
correlation is revealed in the $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm
IR}$ diagram, where $L_{\rm 0.5 - 2\,keV}^{\rm gas}$ sharply increases with
$L_{\rm IR}$ in the center but varies more slowly in the disk. The best fit
gives a steep relation of ${\rm log}(L_{\rm 0.5-2\,keV}^{\rm gas} /{\rm
erg\,s^{-1}})=1.82\,{\rm log}(L_{\rm IR} /{L_{\rm \odot}})+22.26$ for the
center of M51. The similar twofold correlations are also found in the $L_{\rm
0.5 - 2\,keV}^{\rm gas}$${-}$molecular line luminosity ($L^\prime_{\rm gas}$)
relations for the four molecular emission lines CO(1-0), CO(2-1), HCN(1-0), and
HCO$^+$(1-0). We demonstrate that the core-collapse supernovae (SNe) are the
primary source of energy for heating gas in the galactic center of M51, leading
to the observed steep $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm IR}$ and
$L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L^\prime_{\rm gas}$ relations, as their
X-ray radiation efficiencies ($\eta$ $\equiv$ $L_{\rm 0.5 - 2\,keV}^{\rm
gas}$/$\dot{E}_\mathrm{SN}$) increase with the star formation rate surface
densities, where $\dot{E}_\mathrm{SN}$ is the SN mechanical energy input rate.",['astro-ph.GA'],False,,,,Galactic magnetic fields II. Applying the model to nearby galaxies,"Fire and Ice in the Whirlpool: Spatially Resolved Scaling Relations
  between X-ray Emitting Hot Gas and Cold Molecular Gas in M51"
neg-d2-693,2025-02-08,,2502.05482," Implicit Neural Representations (INRs) employ neural networks to represent
continuous functions by mapping coordinates to the corresponding values of the
target function, with applications e.g., inverse graphics. However, INRs face a
challenge known as spectral bias when dealing with scenes containing varying
frequencies. To overcome spectral bias, the most common approach is the Fourier
features-based methods such as positional encoding. However, Fourier
features-based methods will introduce noise to output, which degrades their
performances when applied to downstream tasks. In response, this paper
initially hypothesizes that combining multi-layer perceptrons (MLPs) with
Fourier feature embeddings mutually enhances their strengths, yet
simultaneously introduces limitations inherent in Fourier feature embeddings.
By presenting a simple theorem, we validate our hypothesis, which serves as a
foundation for the design of our solution. Leveraging these insights, we
propose the use of multi-layer perceptrons (MLPs) without additive",['cs.CV'],2502.06427," Hyperspectral image (HSI) classification plays a pivotal role in domains such
as environmental monitoring, agriculture, and urban planning. However, it faces
significant challenges due to the high-dimensional nature of the data and the
complex spectral-spatial relationships inherent in HSI. Traditional methods,
including conventional machine learning and convolutional neural networks
(CNNs), often struggle to effectively capture these intricate spectral-spatial
features and global contextual information. Transformer-based models, while
powerful in capturing long-range dependencies, often demand substantial
computational resources, posing challenges in scenarios where labeled datasets
are limited, as is commonly seen in HSI applications. To overcome these
challenges, this work proposes GraphMamba, a hybrid model that combines
spectral-spatial token generation, graph-based token prioritization, and
cross-attention mechanisms. The model introduces a novel hybridization of
state-space modeling and Gated Recurrent Units (GRU), capturing both linear and
nonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model
complex spatial-spectral relationships while maintaining scalability and
computational efficiency across diverse HSI datasets. Through comprehensive
experiments, we demonstrate that GraphMamba outperforms existing
state-of-the-art models, offering a scalable and robust solution for complex
HSI classification tasks.",['cs.CV'],False,,,,"Robustifying Fourier Features Embeddings for Implicit Neural
  Representations","Hybrid State-Space and GRU-based Graph Tokenization Mamba for
  Hyperspectral Image Classification"
neg-d2-694,2025-02-26,,2502.19061," We introduce a new approach to measure the magnetic pitch angle profile in
tokamak plasmas with Doppler backscattering (DBS), a technique traditionally
used for measuring flows and density fluctuations. The DBS signal is maximised
when its probe beam's wavevector is perpendicular to the magnetic field at the
cutoff location, independent of the density fluctuations. Hence, if one could
isolate this effect, DBS would then yield information about the magnetic pitch
angle. By varying the toroidal launch angle, the DBS beam reaches cutoff with
different angles with respect to the magnetic field, but with other properties
remaining similar. Hence, the toroidal launch angle which gives maximum
backscattered power is thus that which is matched to the pitch angle at the
cutoff location, enabling inference of the magnetic pitch angle. We performed
systematic scans of the DBS toroidal launch angle for repeated DIII-D tokamak
discharges. Experimental DBS data from this scan were analysed and combined
with Gaussian beam-tracing simulations using the Scotty code. The pitch-angle
inferred from DBS is consistent with that from magnetics-only and
motional-Stark-effect-constrained (MSE) equilibrium reconstruction in the edge.
In the core, the pitch angles from DBS and magnetics-only reconstructions
differ by one to two degrees, while simultaneous MSE measurements were not
available. The uncertainty in these measurements was under a degree; we show
that this uncertainty is primarily due to the error in toroidal steering, the
number of toroidally separated measurements, and shot-to-shot repeatability. We
find that the error of pitch-angle measurements can be reduced by optimising
the poloidal launch angle and initial beam properties.",['physics.plasm-ph'],2502.17386," Modelling of plasma dynamics is fundamental to ensure appropriate diverter
and core performance, and is desirable for both interpreting the current
generation of experiments and informing the next generation devices like ITER
\cite{Loarte2007Chapter4P,Eich2013ScalingOT}. Yet the computational expense of
many plasma simulations makes them unsuitable for real-time applications or
iterative design workflows. Neural operator surrogate models of JOREK
\cite{Hoelzl_2021} and STORM \cite{Walkden2016-ys} are evaluated, investigating
their capability to replicate plasma dynamics accurately whilst reducing
computational cost. It is found that the accuracy of the surrogate models will
degrade for long term predictions, and that physics considerations are
important in assessing the performance of the surrogates. Surrogates trained on
one dataset can be effectively fine tuned with only a few simulations from a
target domain. This is particularly effective where the source domain is a low
fidelity physics model and the target domain is a high fidelity model, with an
order of magnitude improvement in performance for a small dataset and a short
rollout.",['physics.plasm-ph'],False,,,,"Conceptual study on using Doppler backscattering to measure magnetic
  pitch angle in tokamak plasmas","Data efficiency and long-term prediction capabilities for neural
  operator surrogate models of edge plasma simulations"
neg-d2-695,2025-03-19,,2503.1529," In the field of robotics many different approaches ranging from classical
planning over optimal control to reinforcement learning (RL) are developed and
borrowed from other fields to achieve reliable control in diverse tasks. In
order to get a clear understanding of their individual strengths and weaknesses
and their applicability in real world robotic scenarios is it important to
benchmark and compare their performances not only in a simulation but also on
real hardware. The '2nd AI Olympics with RealAIGym' competition was held at the
IROS 2024 conference to contribute to this cause and evaluate different
controllers according to their ability to solve a dynamic control problem on an
underactuated double pendulum system with chaotic dynamics. This paper
describes the four different RL methods submitted by the participating teams,
presents their performance in the swing-up task on a real double pendulum,
measured against various criteria, and discusses their transferability from
simulation to real hardware and their robustness to external disturbances.",['cs.RO'],2501.07216," Soft pneumatic fingers are of great research interest. However, their
significant potential is limited as most of them can generate only one motion,
mostly bending. The conventional design of soft fingers does not allow them to
switch to another motion mode. In this paper, we developed a novel multi-modal
and single-actuated soft finger where its motion mode is switched by changing
the finger's temperature. Our soft finger is capable of switching between three
distinctive motion modes: bending, twisting, and extension-in approximately
five seconds. We carried out a detailed experimental study of the soft finger
and evaluated its repeatability and range of motion. It exhibited repeatability
of around one millimeter and a fifty percent larger range of motion than a
standard bending actuator. We developed an analytical model for a
fiber-reinforced soft actuator for twisting motion. This helped us relate the
input pressure to the output twist radius of the twisting motion. This model
was validated by experimental verification. Further, a soft robotic gripper
with multiple grasp modes was developed using three actuators. This gripper can
adapt to and grasp objects of a large range of size, shape, and stiffness. We
showcased its grasping capabilities by successfully grasping a small berry, a
large roll, and a delicate tofu cube.",['cs.RO'],False,,,,"Reinforcement Learning for Robust Athletic Intelligence: Lessons from
  the 2nd 'AI Olympics with RealAIGym' Competition",Temperature Driven Multi-modal/Single-actuated Soft Finger
neg-d2-696,2025-03-06,,2503.04894," We explore the evolution of galaxy sizes at high redshift ($3 < z < 13$)
using the high-resolution THESAN-ZOOM radiation-hydrodynamics simulations,
focusing on the mass range of $10^6\,\mathrm{M}_{\odot} < \mathrm{M}_{\ast} <
10^{10}\,\mathrm{M}_{\odot}$. Our analysis reveals that galaxy size growth is
tightly coupled to bursty star formation. Galaxies above the star-forming main
sequence experience rapid central compaction during starbursts, followed by
inside-out quenching and spatially extended star formation that leads to
expansion, causing oscillatory behavior around the size-mass relation. Notably,
we find a positive intrinsic size-mass relation at high redshift, consistent
with observations but in tension with large-volume simulations. We attribute
this discrepancy to the bursty star formation captured by our multi-phase
interstellar medium framework, but missing from simulations using the effective
equation-of-state approach with hydrodynamically decoupled feedback. We also
find that the normalization of the size-mass relation follows a double power
law as a function of redshift, with a break at $z\approx6$, because the
majority of galaxies at $z > 6$ show rising star-formation histories, and
therefore are in a compaction phase. We demonstrate that H$\alpha$ emission is
systematically extended relative to the UV continuum by a median factor of 1.7,
consistent with recent JWST studies. However, in contrast to previous
interpretations that link extended H$\alpha$ sizes to inside-out growth, we
find that Lyman-continuum (LyC) emission is spatially disconnected from
H$\alpha$. Instead, a simple Str\""{o}mgren sphere argument reproduces observed
trends, suggesting that extreme LyC production during central starbursts is the
primary driver of extended nebular emission.",['astro-ph.GA'],2502.08712," We present an analysis of deep $\textit{JWST}$/NIRSpec spectra of
star-forming galaxies at $z\simeq1.4-10$, observed as part of the AURORA
survey. We infer median low-ionization electron densities of
$268_{-49}^{+45}~\rm cm^{-3}$, $350_{-76}^{+140}~\rm cm^{-3}$, and
$480_{-310}^{+390}~\rm cm^{-3}$ at redshifts z$=2.3$, $z=3.2$, and $z=5.3$,
respectively, revealing an evolutionary trend following $(1+z)^{1.5\pm0.6}$. We
identify weak positive correlations between electron density and star formation
rate (SFR) as well as SFR surface density, but no significant trends with
stellar mass or specific SFR. Correlations with rest-optical emission line
ratios show densities increasing with $\rm [NeIII]\lambda3869/[OII]\lambda3727$
and, potentially, $\rm [OIII]\lambda5007/[OII]\lambda3727$, although variations
in dust attenuation complicate the latter. Additionally, electron density is
more strongly correlated with distance from the local BPT sequence than can be
explained by simple photoionization models. We further derive electron
densities from the CIII] doublet probing higher-ionization gas, and find a
median value of $1.4_{-0.5}^{+0.7}\times10^4~\rm cm^{-3}$, $\sim30$ times
higher than densities inferred from [SII]. This comparison suggests a
consistent HII region structure across cosmic time with dense, high-ionization
interiors surrounded by less dense, low-ionization gas. We compare measurements
of AURORA galaxies to predictions from the SPHINX galaxy formations,
highlighting the interplay between residual molecular cloud pressure in young
galaxies and feedback from stellar winds and supernovae as galaxies mature.",['astro-ph.GA'],False,,,,"The THESAN-ZOOM project: central starbursts and inside-out quenching
  govern galaxy sizes in the early Universe","The AURORA Survey: The Evolution of Multi-phase Electron Densities at
  High Redshift"
neg-d2-697,2025-03-19,,2503.14981," This work is concerned with convex analysis of so-called spectral functions
of matrices that only depend on eigenvalues of the matrix. An abstract
framework of spectral decomposition systems is proposed that covers a wide
range of previously studied settings, including eigenvalue decomposition of
Hermitian matrices and singular value decomposition of rectangular matrices and
allows deriving new results in more general settings such as Euclidean Jordan
algebras. The main results characterize convexity, lower semicontinuity,
Fenchel conjugates, convex subdifferentials, and Bregman proximity operators of
spectral functions in terms of the reduced functions. As a byproduct, a
generalization of the Ky Fan majorization theorem is obtained.",['math.OC'],2503.06662," This paper studies a distributed algorithm for constrained consensus
optimization that is obtained by fusing the Arrow-Hurwicz-Uzawa primal-dual
gradient method for centralized constrained optimization and the Wang-Elia
method for distributed unconstrained optimization. It is shown that the optimal
primal-dual point is a semiglobally exponentially stable equilibrium for the
algorithm, which implies linear convergence. The analysis is based on the
separation between a slow centralized optimization dynamics describing the
evolution of the average estimate toward the optimum, and a fast dynamics
describing the evolution of the consensus error over the network. These two
dynamics are mutually coupled, and the stability analysis builds on control
theoretic tools such as time-scale separation, Lyapunov theory, and the
small-gain principle. Our analysis approach highlights that the consensus
dynamics can be seen as a fast, parasite one, and that stability of the
distributed algorithm is obtained as a robustness consequence of the semiglobal
exponential stability properties of the centralized method. This perspective
can be used to enable other significant extensions, such as time-varying
networks or delayed communication, that can be seen as ``perturbations"" of the
centralized algorithm.",['math.OC'],False,,,,Convex Analysis in Spectral Decomposition Systems,"An exponentially stable discrete-time primal-dual algorithm for
  distributed constrained optimization"
neg-d2-698,2025-02-21,,2502.15456," For a graph $F$, let ${\rm EX}(n,F)$ be the set of $F$-free graphs of order
$n$ with the maximum number of edges. The graph $F$ is called vertex-critical,
if the deletion of its some vertex induces a graph with smaller chromatic
number. For example, an odd wheel (obtained by connecting a vertex to a cycle
of even length) is a vertex-critical graph with chromatic number 3. For
$h\geq2$, let $F_{1},F_{2},...,F_{h}$ be vertex-critical graphs with the same
chromatic number. Let $\cup_{1\leq i\leq h}F_{i}$ be the disjoint union of
them. In this paper, we characterize the graphs in ${\rm EX}(n,\cup_{1\leq
i\leq h}F_{i})$, when there is a proper order among the graphs
$F_{1},F_{2},...,F_{h}$. This solves a conjecture (on extremal problem for
disjoint union of odd wheels) proposed by Xiao and Zamora \cite{XZ}.",['math.CO'],2502.15647," We introduce two new families of permutation group polynomials over finite
fields of arbitrary characteristic, which are special types of bivariate local
permutation polynomials. For each family, we explicitly construct their
companions. Furthermore, we precisely determine the total number of permutation
group polynomials equivalent to the proposed families. Moreover, we resolve the
problem of enumerating permutation group polynomials that are equivalent to
$e$-Klenian polynomials over finite fields for $e\geq 1$, a problem previously
noted as nontrivial by Gutierrez and Urroz (2023).",['math.CO'],False,,,,Extremal graphs for disjoint union of vertex-critical graphs,"Bivariate local permutation polynomials, their companions, and related
  enumeration results"
neg-d2-699,2025-02-03,,2502.01751," We consider unified dark sector models in which the fluid can collapse and
cluster into halos, allowing for hierarchical structure formation to proceed as
in standard cosmology. We show that both background evolution and linear
perturbations tend towards those in $\LCDM$ as the clustered fraction $f
\rightarrow 1$. We confront such models with various observational datasets,
with emphasis on the relatively well motivated standard Chaplygin gas. We show
that the strongest constraints come from secondary anisotropies in the CMB
spectrum, which prefer models with $f \rightarrow 1$. However, as a larger
Hubble constant is allowed for smaller $f$, values of $f \simeq 0.99$ (rather
than tending to exact unity) are favored when late universe expansion data is
included, with $f \simeq 0.97$ and $H_0 \simeq 70 {\rm km/s/Mpc}$ allowed at
the 2-$\sigma$ level. Such values of $f$ imply extremely efficient clustering
into nonlinear structures. They may nevertheless be compatible with clustered
fractions in warm dark matter based cosmologies, which have similar minimal
halo mass scales as the models considered here. Tight CMB constraints on $f$
also apply to the generalized Chaplygin gas, except for models that are already
quite close to $\LCDM$, in which case all values of $0 \le f \le 1$ are
allowed. In contrast to the CMB, large scale structure data, which were
initially used to rule out unclustered unified dark matter models, are far less
constraining. Indeed, late universe data, including the large scale galaxy
distribution, prefer models that are far from $\LCDM$. But these are in tension
with the CMB data.",['astro-ph.CO'],2503.16355," In recent years, the study of secondary anisotropies in the Cosmic Microwave
Background has become a fundamental instrument to test our understanding of
Cosmology and Astrophysics. Using a set of lightcones produced with the ``Dark
Energy and Massive Neutrino Universe'' $N$-body simulations we study how
different dark energy models and neutrino masses impact the properties of the
Sunyaev-Zel'dovich (SZ) effects, focusing on the signal arising from galaxy
clusters and groups. We analyse the distribution of values, Compton-$y$
parameter for the thermal SZ effect and $\Delta T/T$ for the kinematic SZ
effect, and study their angular power spectra. We find that the distribution of
logarithmic Compton parameter can be fitted with a skewed Gaussian, with a mean
that, at fixed dark energy model, decreases linearly with an approximate slope
of $10 f_\nu$. Regarding the power spectrum of the thermal SZ effect, we find
that an increase in $\sum {m_\nu}$ is observed as a power-law scaling with
respect to $\sigma_8^{\mathrm{cb}}$, with exponents ranging from 7.2 to 8.2. We
also find that four cosmological models, one with $\sum {m_\nu} = 0.16$ eV and
three with $\sum {m_\nu} = 0.32$ eV, fit equally well the Planck data for the
Compton-$y$. For all the \texttt{DEMNUni} models we forecast the cumulative
signal-to-noise for thermal SZ observations with the LAT instrument of Simons
Observatory; furthermore, we compute a tailored $\chi_\mathrm{SNR}^2$ estimator
to infer if they can be distinguished from the reference $\Lambda$CDM. We also
provide estimates for the power spectrum of the cluster component of the
kinematic SZ effect, in all the different cosmological scenarios.",['astro-ph.CO'],False,,,,"Clustered unified dark sector cosmology: Background evolution and linear
  perturbations in light of observations","DEMNUni: the Sunyaev-Zel'dovich effect in the presence of massive
  neutrinos and dynamical dark energy"
neg-d2-700,2025-01-14,,2501.08525," In this paper we classify a kind of special Calabi hypersurfaces with
negative constant sectional curvature in Calabi affine geometry. Meanwhile, we
find a class of new Euclidean complete and Calabi complete affine
hypersurfaces, which satisfy the affine maximal type equation and the Abreu
equation with negative constant scalar curvatures.",['math.DG'],2501.17877," In this paper, we carry out in-depth research centering around the $(p,
q)$-Sobolev inequality and Nash inequality on forward complete Finsler metric
measure manifolds under the condition that ${\rm Ric}_{\infty} \geq -K$ for
some $K \geq 0$. We first obtain a global $p$-Poincar\'{e} inequality on such
Finsler manifolds. Based on this, we can derive a $(p, q)$-Sobolev inequality.
Furthermore, we establish a global optimal $(p, q)$-Sobolev inequality with a
sharp Sobolev constant. Finally, as an application of the $p$-Poincar\'{e}
inequality, we prove a Nash inequality.",['math.DG'],False,,,,A class of new complete affine maximal type hypersurfaces,"$(p, q)$-Sobolev inequality and Nash inequality on forward complete
  Finsler metric measure manifolds"
neg-d2-701,2025-03-18,,2503.147," In brain network analysis using resting-state fMRI, there is growing interest
in modeling higher-order interactions beyond simple pairwise connectivity via
persistent homology. Despite the promise of these advanced topological tools,
robust and consistently observed higher-order interactions over time remain
elusive. In this study, we investigate why conventional analyses often fail to
reveal complex higher-order structures - such as interactions involving four or
more nodes - and explore whether such interactions truly exist in functional
brain networks. We utilize a simplicial complex framework often used in
persistent homology to address this question.",['q-bio.NC'],2503.147," In brain network analysis using resting-state fMRI, there is growing interest
in modeling higher-order interactions beyond simple pairwise connectivity via
persistent homology. Despite the promise of these advanced topological tools,
robust and consistently observed higher-order interactions over time remain
elusive. In this study, we investigate why conventional analyses often fail to
reveal complex higher-order structures - such as interactions involving four or
more nodes - and explore whether such interactions truly exist in functional
brain networks. We utilize a simplicial complex framework often used in
persistent homology to address this question.",['q-bio.NC'],False,,,,"From Density to Void: Why Brain Networks Fail to Reveal Complex
  Higher-Order Structures","From Density to Void: Why Brain Networks Fail to Reveal Complex
  Higher-Order Structures"
neg-d2-702,2025-02-07,,2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],2503.08668," Vector Quantization (VQ) has emerged as a prominent weight compression
technique, showcasing substantially lower quantization errors than uniform
quantization across diverse models, particularly in extreme compression
scenarios. However, its efficacy during fine-tuning is limited by the
constraint of the compression format, where weight vectors assigned to the same
codeword are restricted to updates in the same direction. Consequently, many
quantized weights are compelled to move in directions contrary to their local
gradient information. To mitigate this issue, we introduce a novel VQ paradigm,
Sign-Splitting VQ (SSVQ), which decouples the sign bit of weights from the
codebook. Our approach involves extracting the sign bits of uncompressed
weights and performing clustering and compression on all-positive weights. We
then introduce latent variables for the sign bit and jointly optimize both the
signs and the codebook. Additionally, we implement a progressive freezing
strategy for the learnable sign to ensure training stability. Extensive
experiments on various modern models and tasks demonstrate that SSVQ achieves a
significantly superior compression-accuracy trade-off compared to conventional
VQ. Furthermore, we validate our algorithm on a hardware accelerator, showing
that SSVQ achieves a 3$\times$ speedup over the 8-bit compressed model by
reducing memory access.",['cs.CV'],False,,,,Interpretable Failure Detection with Human-Level Concepts,"SSVQ: Unleashing the Potential of Vector Quantization with
  Sign-Splitting"
neg-d2-703,2025-01-25,,2501.151," The rapid development of programmable network devices and the widespread use
of machine learning (ML) in networking have facilitated efficient research into
intelligent data plane (IDP). Offloading ML to programmable data plane (PDP)
enables quick analysis and responses to network traffic dynamics, and efficient
management of network links. However, PDP hardware pipeline has significant
resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in
each stage, and lacks support for multiplication, division and floating-point
operations. These constraints significantly hinder the development of IDP. This
paper presents \quark, a framework that fully offloads convolutional neural
network (CNN) inference onto PDP. \quark employs model pruning to simplify the
CNN model, and uses quantization to support floating-point operations.
Additionally, \quark divides the CNN into smaller units to improve resource
utilization on the PDP. We have implemented a testbed prototype of \quark on
both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2).
Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy
in anomaly detection task while using only 22.7\% of the SRAM resources on the
Intel Tofino ASIC switch, completing inference tasks at line rate with an
average latency of 42.66$\mu s$.",['cs.NI'],2501.08229," This research proposes a system as a solution for the challenges faced by Sri
Lanka' s historic railway system, such as scheduling delays, overcrowding,
manual ticketing, and management inefficiencies. It proposes a multi-subsystem
approach, incorporating GPS tracking, RFID-based e-ticketing, seat reservation,
and vision-based people counting. The GPS based real time train tracking system
performs accurately within 24 meters, with the MQTT protocol showing twice the
speed of the HTTP-based system. All subsystems use the MQTT protocol to enhance
efficiency, reliability, and passenger experience. The study's data and
methodology demonstrate the effectiveness of these innovations in improving
scheduling, passenger flow, and overall system performance, offering promising
solutions for modernizing Sri Lanka's railway infrastructure.",['cs.NI'],False,,,,"Quark: Implementing Convolutional Neural Networks Entirely on
  Programmable Data Plane","Enhancing Train Transportation in Sri Lanka: A Smart IOT based
  Multi-Subsystem Approach using MQTT"
neg-d2-704,2025-02-13,,2502.0989," Diffusion models are powerful tools for capturing complex distributions, but
modeling data with inherent symmetries, such as molecular structures, remains
challenging. Equivariant denoisers are commonly used to address this, but they
introduce architectural complexity and optimization challenges, including noisy
gradients and convergence issues. We propose a novel approach that enforces
equivariance through a symmetrized loss function, which applies a
time-dependent weighted averaging operation over group actions to the model's
prediction target. This ensures equivariance without explicit architectural
constraints and reduces gradient variance, leading to more stable and efficient
optimization. Our method uses Monte Carlo sampling to estimate the average,
incurring minimal computational overhead. We provide theoretical guarantees of
equivariance for the minimizer of our loss function and demonstrate its
effectiveness on synthetic datasets and the molecular conformation generation
task using the GEOM-QM9 dataset. Experiments show improved sample quality
compared to existing methods, highlighting the potential of our approach to
enhance the scalability and practicality of equivariant diffusion models in
generative tasks.",['cs.LG'],2501.10049," To take the esports scene to the next level, we introduce PandaSkill, a
framework for assessing player performance and skill rating. Traditional rating
systems like Elo and TrueSkill often overlook individual contributions and face
challenges in professional esports due to limited game data and fragmented
competitive scenes. PandaSkill leverages machine learning to estimate in-game
player performance from individual player statistics. Each in-game role is
modeled independently, ensuring a fair comparison between them. Then, using
these performance scores, PandaSkill updates the player skill ratings using the
Bayesian framework OpenSkill in a free-for-all setting. In this setting, skill
ratings are updated solely based on performance scores rather than game
outcomes, hightlighting individual contributions. To address the challenge of
isolated rating pools that hinder cross-regional comparisons, PandaSkill
introduces a dual-rating system that combines players' regional ratings with a
meta-rating representing each region's overall skill level. Applying PandaSkill
to five years of professional League of Legends matches worldwide, we show that
our method produces skill ratings that better predict game outcomes and align
more closely with expert opinions compared to existing methods.",['cs.LG'],False,,,,Symmetry-Preserving Diffusion Models via Target Symmetrization,"PandaSkill - Player Performance and Skill Rating in Esports: Application
  to League of Legends"
neg-d2-705,2025-02-12,,2502.17467," If H is a strongly regular hypergroup, we show that the set of regular
relations on H and the set of subhypergroups containing $0_{H}$ are two
lattices that are isomorphic to each other. In the next step, we introduce and
study the properties of functors that are constructed by a sequence of strongly
regular relations. This helps us to define a specific type of free objects and
tensor products on the category of regular hypergroups.",['math.GM'],2502.17467," If H is a strongly regular hypergroup, we show that the set of regular
relations on H and the set of subhypergroups containing $0_{H}$ are two
lattices that are isomorphic to each other. In the next step, we introduce and
study the properties of functors that are constructed by a sequence of strongly
regular relations. This helps us to define a specific type of free objects and
tensor products on the category of regular hypergroups.",['math.GM'],False,,,,Functors Associated to Relations on Hypergroups and Hypermodules,Functors Associated to Relations on Hypergroups and Hypermodules
neg-d2-706,2025-03-13,,2503.10239," Super-apps have emerged as comprehensive platforms integrating various
mini-apps to provide diverse services. While super-apps offer convenience and
enriched functionality, they can introduce new privacy risks. This paper
reveals a new privacy leakage source in super-apps: mini-app interaction
history, including mini-app usage history (Mini-H) and operation history
(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as
their frequency and categories. Op-H captures user interactions within
mini-apps, including button clicks, bar drags, and image views. Super-apps can
naturally collect these data without instrumentation due to the web-based
feature of mini-apps. We identify these data types as novel and unexplored
privacy risks through a literature review of 30 papers and an empirical
analysis of 31 super-apps. We design a mini-app interaction history-oriented
inference attack (THEFT), to exploit this new vulnerability. Using THEFT, the
insider threats within the low-privilege business department of the super-app
vendor acting as the adversary can achieve more than 95.5% accuracy in
inferring privacy attributes of over 16.1% of users. THEFT only requires a
small training dataset of 200 users from public breached databases on the
Internet. We also engage with super-app vendors and a standards association to
increase industry awareness and commitment to protect this data. Our
contributions are significant in identifying overlooked privacy risks,
demonstrating the effectiveness of a new attack, and influencing industry
practices toward better privacy protection in the super-app ecosystem.",['cs.CR'],2501.12034," Like most computer systems, a manycore can also be the target of security
attacks. It is essential to ensure the security of the NoC since all
information travels through its channels, and any interference in the traffic
of messages can reflect on the entire chip, causing communication problems.
Among the possible attacks on NoC, Denial of Service (DoS) attacks are the most
cited in the literature. The state of the art shows a lack of work that can
detect such attacks through learning techniques. On the other hand, these
techniques are widely explored in computer network security via an Intrusion
Detection System (IDS). In this context, the main goal of this document is to
present the progress of a work that explores an IDS technique using machine
learning and temporal series for detecting DoS attacks in NoC-based manycore
systems. To fulfill this goal, it is necessary to extract traffic data from a
manycore NoC and execute the learning techniques in the extracted data.
However, while low-level platforms offer precision and slow execution,
high-level platforms offer higher speed and data incompatible with reality.
Therefore, a platform is being developed using the OVP tool, which has a higher
level of abstraction. To solve the low precision problem, the developed
platform will have its data validated with a low-level platform.",['cs.CR'],False,,,,"I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app
  Interaction History in Super-apps","Application of Machine Learning Techniques for Secure Traffic in
  NoC-based Manycores"
neg-d2-707,2025-02-24,,2502.1719," We develop a theory of type semigroups for arbitrary twisted, not necessarily
Hausdorff \'etale groupoids. The type semigroup is a dynamical version of the
Cuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable
finiteness of the reduced and essential C*-algebras. If the reduced C*-algebra
of a twisted groupoid is simple and the type semigroup satisfies a weak version
of almost unperforation, then the C*-algebra is either stably finite or purely
infinite. We apply our theory to Cartan inclusions. We calculate the type
semigroup for the possibly non-Hausdorff groupoids associated to self-similar
group actions on graphs and deduce a dichotomy for the resulting Exel-Pardo
algebras.",['math.OA'],2502.01975," We define a new class of regular inclusions, the pseudo-Cartan inclusions. We
show this class coincides with the class of regular inclusions having a Cartan
envelope and also with the class of regular inclusions with the faithful unique
pseudo-expectation property. We describe the twisted groupoid associated with
the Cartan envelope of a pseudo-Cartan inclusion. These results significantly
extend previous results obtained for the unital setting.
  We explore properties of pseudo-Cartan inclusions and the relationship
between a pseudo-Cartan inclusion and its Cartan envelope. For example, if $D
\subseteq C$ is a pseudo-Cartan inclusion with Cartan envelope $B \subseteq A$,
then $C$ is simple if and only if $A$ is simple. We show how to construct
pseudo-Cartan inclusions from a given Cartan inclusion, that the inductive
limit of pseudo-Cartan inclusions with suitable connecting maps is a
pseudo-Cartan inclusion, and the minimal tensor product of pseudo-Cartan
inclusions is a pseudo-Cartan inclusion. Further, we describe the Cartan
envelope of pseudo-Cartan inclusions arising from these constructions. We give
some applications and conclude with a few open questions.",['math.OA'],False,,,,"Type semigroups for twisted groupoids and a dichotomy for groupoid
  C*-algebras",Pseudo-Cartan Inclusions
neg-d2-708,2025-03-19,,2503.15625," Surficial geologic mapping is essential for understanding Earth surface
processes, addressing modern challenges such as climate change and national
security, and supporting common applications in engineering and resource
management. However, traditional mapping methods are labor-intensive, limiting
spatial coverage and introducing potential biases. To address these
limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset
specifically designed for surficial geologic mapping and Earth surface
analysis. EarthScape integrates high-resolution aerial RGB and near-infrared
(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain
features, and hydrologic and infrastructure vector data. The dataset provides
detailed annotations for seven distinct surficial geologic classes encompassing
various geological processes. We present a comprehensive data processing
pipeline using open-sourced raw data and establish baseline benchmarks using
different spatial modalities to demonstrate the utility of EarthScape. As a
living dataset with a vision for expansion, EarthScape bridges the gap between
computer vision and Earth sciences, offering a valuable resource for advancing
research in multimodal learning, geospatial analysis, and geological mapping.
Our code is available at https://github.com/masseygeo/earthscape.",['cs.CV'],2502.12723," This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.",['cs.CV'],False,,,,"EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and
  Earth Surface Analysis","myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  Dataset"
neg-d2-709,2025-02-18,,2502.12611," The rise of Large Language Models (LLMs) necessitates accurate AI-generated
text detection. However, current approaches largely overlook the influence of
author characteristics. We investigate how sociolinguistic attributes-gender,
CEFR proficiency, academic field, and language environment-impact
state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored
texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous
evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our
results reveal significant biases: CEFR proficiency and language environment
consistently affected detector accuracy, while gender and academic field showed
detector-dependent effects. These findings highlight the crucial need for
socially aware AI text detection to avoid unfairly penalizing specific
demographic groups. We offer novel empirical evidence, a robust statistical
framework, and actionable insights for developing more equitable and reliable
detection systems in real-world, out-of-domain contexts. This work paves the
way for future research on bias mitigation, inclusive evaluation benchmarks,
and socially responsible LLM detectors.",['cs.CL'],2502.17538," This paper introduces a novel causal framework for multi-stage
decision-making in natural language action spaces where outcomes are only
observed after a sequence of actions. While recent approaches like Proximal
Policy Optimization (PPO) can handle such delayed-reward settings in
high-dimensional action spaces, they typically require multiple models (policy,
value, and reward) and substantial training data. Our approach employs
Q-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,
enabling data-efficient policy learning via gradient ascent on language
embeddings. A key technical contribution of our approach is a decoding strategy
that translates optimized embeddings back into coherent natural language. We
evaluate our approach on mental health intervention, hate speech countering,
and sentiment transfer tasks, demonstrating significant improvements over
competitive baselines across multiple metrics. Notably, our method achieves
superior transfer strength while maintaining content preservation and fluency,
as validated through human evaluation. Our work provides a practical foundation
for learning optimal policies in complex language tasks where training data is
limited.",['cs.CL'],False,,,,"Who Writes What: Unveiling the Impact of Author Roles on AI-generated
  Text Detection",Policy Learning with a Natural Language Action Space: A Causal Approach
neg-d2-710,2025-03-03,,2503.09612," Technology can pose signicant risks to a wide array of vulnerable
populations. However, by addressing the challenges and opportunities in
technology design, research, and deployment, we can create systems that benet
everyone, fostering a society where even the most vulnerable are empowered and
supported.",['cs.CY'],2502.11889," As artificial intelligence (AI) systems become increasingly integrated into
critical domains, ensuring their responsible design and continuous development
is imperative. Effective AI quality management (QM) requires tools and
methodologies that address the complexities of the AI lifecycle. In this paper,
we propose an approach for AI lifecycle planning that bridges the gap between
generic guidelines and use case-specific requirements (MQG4AI). Our work aims
to contribute to the development of practical tools for implementing
Responsible AI (RAI) by aligning lifecycle planning with technical, ethical and
regulatory demands. Central to our approach is the introduction of a flexible
and customizable Methodology based on Quality Gates, whose building blocks
incorporate RAI knowledge through information linking along the AI lifecycle in
a continuous manner, addressing AIs evolutionary character. For our present
contribution, we put a particular emphasis on the Explanation stage during
model development, and illustrate how to align a guideline to evaluate the
quality of explanations with MQG4AI, contributing to overall Transparency.",['cs.CY'],False,,,,"Prioritizing Computing Research to Empower and Protect Vulnerable
  Populations","MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency
  Focusing on Explainability Techniques"
neg-d2-711,2025-01-15,,2501.08644," In future wireless communication systems, millimeter waves (mmWaves) will
play an important role in meeting high data rates. However, due to their short
wavelengths, these mmWaves present high propagation losses and are highly
attenuated by blocking. In this chapter, we seek to increase the indoor radio
coverage at 60 GHz in non line-of-sight (NLOS) environments. Firstly, a
metallic passive reflector is used in an L-shaped corridor. Secondly, an array
of grooved metallic antennas of size 20 cm x 20 cm (corresponding to 80
grooves) is used in a T-shaped corridor. Next, the study focuses on the
blockage losses caused by the human body. The results obtained in these
different configurations show that it is possible to use beamforming to exploit
a reflected path when the direct path is blocked.",['cs.NI'],2501.151," The rapid development of programmable network devices and the widespread use
of machine learning (ML) in networking have facilitated efficient research into
intelligent data plane (IDP). Offloading ML to programmable data plane (PDP)
enables quick analysis and responses to network traffic dynamics, and efficient
management of network links. However, PDP hardware pipeline has significant
resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in
each stage, and lacks support for multiplication, division and floating-point
operations. These constraints significantly hinder the development of IDP. This
paper presents \quark, a framework that fully offloads convolutional neural
network (CNN) inference onto PDP. \quark employs model pruning to simplify the
CNN model, and uses quantization to support floating-point operations.
Additionally, \quark divides the CNN into smaller units to improve resource
utilization on the PDP. We have implemented a testbed prototype of \quark on
both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2).
Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy
in anomaly detection task while using only 22.7\% of the SRAM resources on the
Intel Tofino ASIC switch, completing inference tasks at line rate with an
average latency of 42.66$\mu s$.",['cs.NI'],False,,,,"Extension of indoor mmW link radio coverage in non line-of-sight
  conditions","Quark: Implementing Convolutional Neural Networks Entirely on
  Programmable Data Plane"
neg-d2-712,2025-02-12,,2502.08608," We introduce the notion of admissible injective envelope for a locally
C*-algebra and show that each object in the category whose objects are unital
Fr\'{e}chet locally C*-algebras and whose morphisms are unital admissible local
completely positive maps has a unique admissible injective envelope. The
concept of admissible injectivity is stronger than that of injectivity. As a
consequence, we show that a unital Fr\'{e}chet locally W*-algebras is injective
if and only if the C*-algebras from its Arens-Michael decomposition are
injective.",['math.OA'],2502.01975," We define a new class of regular inclusions, the pseudo-Cartan inclusions. We
show this class coincides with the class of regular inclusions having a Cartan
envelope and also with the class of regular inclusions with the faithful unique
pseudo-expectation property. We describe the twisted groupoid associated with
the Cartan envelope of a pseudo-Cartan inclusion. These results significantly
extend previous results obtained for the unital setting.
  We explore properties of pseudo-Cartan inclusions and the relationship
between a pseudo-Cartan inclusion and its Cartan envelope. For example, if $D
\subseteq C$ is a pseudo-Cartan inclusion with Cartan envelope $B \subseteq A$,
then $C$ is simple if and only if $A$ is simple. We show how to construct
pseudo-Cartan inclusions from a given Cartan inclusion, that the inductive
limit of pseudo-Cartan inclusions with suitable connecting maps is a
pseudo-Cartan inclusion, and the minimal tensor product of pseudo-Cartan
inclusions is a pseudo-Cartan inclusion. Further, we describe the Cartan
envelope of pseudo-Cartan inclusions arising from these constructions. We give
some applications and conclude with a few open questions.",['math.OA'],False,,,,Injective envelopes for locally C*-algebras,Pseudo-Cartan Inclusions
neg-d2-713,2025-03-07,,2503.05547," This paper presents a frequency synthesis that achieves exceptional stability
by transferring optical signals to the radio frequency (RF) domain at 100 MHz.
We describe and characterize two synthesis chains composed of a cryogenic
silicon cavity-stabilized laser at 1542 nm and an ultra-low expansion (ULE)
glass cavity at 1157 nm, both converted to 10 GHz signals via Ti:Sapphire and
Er/Yb:glass optical frequency combs (OFCs). The 10 GHz microwave outputs are
further divided down to 100 MHz using a commercial microwave prescaler, which
exhibits a residual frequency instability of $\sigma_y(1~\text{s})<10^{-15}$
and low $10^{-18}$ level at a few thousand seconds. Measurements are performed
using a newly developed custom ultra-low-noise digital measurement system and
are compared to the carrier-suppression technique. The new system enables
high-sensitivity evaluation across the entire synthesis chain, from the optical
and microwave heterodynes as well as the direct RF signals. Results show an
absolute instability of $\sigma_y(1~\text{s})~\approx~4.7\times10^{-16}$ at 100
MHz. This represents the first demonstration of such low instability at 100
MHz, corresponding to a phase noise of -140 dBc/Hz at a 1 Hz offset and
significantly surpassing earlier systems. These advancements open new
opportunities for precision metrology and timing systems.",['physics.optics'],2501.0735," Photochemistry in the earth's atmosphere is driven by the sun, continuously
altering the concentration and spatial distribution of pollutants. Precisely
monitoring their atmospheric abundance relies predominantly on optical sensing,
which requires the knowledge of exact absorption cross sections. One key
pollutant which impacts many photochemical reaction-pathways is formaldehyde.
Agreement on formaldehyde absolute absorption cross section remains elusive in
the photochemically-relevant ultraviolet spectral region, hampering sensitive
concentration tracking. Here, we introduce free-running ultraviolet dual comb
spectroscopy, combining high spectral resolution (1 GHz), broad spectral
coverage (12 THz), and fast acquisition speed (500 ms), as a novel method for
absolute absorption cross section determination with unprecedented fidelity.
Within this bandwidth, our method uncovers almost one order of magnitude more
rovibrational transitions than detected before which leads to refined
rotational constants for high-level quantum simulations of molecular
eigenstates. This ultra-resolution method can be generalized to provide a
universal tool for fast electronic fingerprinting of atmospherically-relevant
species, both for sensing applications and to benchmark improvements of
ab-initio quantum theory.",['physics.optics'],False,,,,"Radio Frequency from Optical with Instabilities below $10^{-15}$-
  Generation and Measurement",Ultra-resolution photochemical sensing
neg-d2-714,2025-02-22,,2502.16256," The cold-start problem remains a significant challenge in recommendation
systems based on generative models. Current methods primarily focus on
enriching embeddings or inputs by gathering more data, often overlooking the
effectiveness of how existing training knowledge is utilized. This inefficiency
can lead to missed opportunities for improving cold-start recommendations. To
address this, we propose the use of epistemic uncertainty, which reflects a
lack of certainty about the optimal model, as a tool to measure and enhance the
efficiency with which a recommendation system leverages available knowledge. By
considering epistemic uncertainty as a reducible component of overall
uncertainty, we introduce a new approach to refine model performance. The
effectiveness of this approach is validated through extensive offline
experiments on publicly available datasets, demonstrating its superior
performance and robustness in tackling the cold-start problem.",['cs.IR'],2502.16924," Large Language Model (LLM)-based cold-start recommendation systems continue
to face significant computational challenges in billion-scale scenarios, as
they follow a ""Text-to-Judgment"" paradigm. This approach processes user-item
content pairs as input and evaluates each pair iteratively. To maintain
efficiency, existing methods rely on pre-filtering a small candidate pool of
user-item pairs. However, this severely limits the inferential capabilities of
LLMs by reducing their scope to only a few hundred pre-filtered candidates. To
overcome this limitation, we propose a novel ""Text-to-Distribution"" paradigm,
which predicts an item's interaction probability distribution for the entire
user set in a single inference. Specifically, we present FilterLLM, a framework
that extends the next-word prediction capabilities of LLMs to billion-scale
filtering tasks. FilterLLM first introduces a tailored distribution prediction
and cold-start framework. Next, FilterLLM incorporates an efficient
user-vocabulary structure to train and store the embeddings of billion-scale
users. Finally, we detail the training objectives for both distribution
prediction and user-vocabulary construction. The proposed framework has been
deployed on the Alibaba platform, where it has been serving cold-start
recommendations for two months, processing over one billion cold items.
Extensive experiments demonstrate that FilterLLM significantly outperforms
state-of-the-art methods in cold-start recommendation tasks, achieving over 30
times higher efficiency. Furthermore, an online A/B test validates its
effectiveness in billion-scale recommendation systems.",['cs.IR'],False,,,,Exploiting Epistemic Uncertainty in Cold-Start Recommendation Systems,"FilterLLM: Text-To-Distribution LLM for Billion-Scale Cold-Start
  Recommendation"
neg-d2-715,2025-01-14,,2501.08305," Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.",['cs.LG'],2503.01048," Personalizing large language models (LLMs) is essential for delivering
tailored interactions that improve user experience. Many existing
personalization methods require fine-tuning LLMs for each user, rendering them
prohibitively expensive for widespread adoption. Although retrieval-based
approaches offer a more compute-efficient alternative, they still depend on
large, high-quality datasets that are not consistently available for all users.
To address this challenge, we propose CHAMELEON, a scalable and efficient
personalization approach that uses (1) self-generated personal preference data
and (2) representation editing to enable quick and cost-effective
personalization. Our experiments on various tasks, including those from the
LaMP personalization benchmark, show that CHAMELEON efficiently adapts models
to personal preferences, improving instruction-tuned models and outperforms two
personalization baselines by an average of 40% across two model architectures.",['cs.LG'],False,,,,"Benchmarking Graph Representations and Graph Neural Networks for
  Multivariate Time Series Classification",Personalize Your LLM: Fake it then Align it
neg-d2-716,2025-03-15,,2503.12303," Despite their impressive capabilities, Multimodal Large Language Models
(MLLMs) face challenges with fine-grained perception and complex reasoning.
Prevalent multimodal pre-training approaches in MLLM construction focus on
enhancing perception by training on high-quality image captions. While
leveraging advanced MLLMs for caption generation enhances scalability, their
outputs often lack comprehensiveness and accuracy. In this paper, we introduce
Self-Improving cognition (SIcog), a self-learning framework designed to
construct next-generation foundation MLLMs by enhancing their systematic
cognitive capabilities through multimodal pre-training with self-generated
data. Specifically, we propose Chain-of-Description (CoD), an approach that
improves an MLLM's systematic perception by enabling step-by-step visual
understanding. CoD sequentially focuses on salient content, fine-grained
details, relational attributes, and peripheral context, before generating a
coherent description, ensuring greater accuracy and comprehensiveness.
Additionally, we adopt a structured chain-of-thought (CoT) reasoning technique
to enable MLLMs to integrate in-depth multimodal reasoning. To construct a
next-generation foundation MLLM with self-improved cognition, SIcog first
equips an MLLM with systematic perception and reasoning abilities using minimal
external annotations. The enhanced models then generate detailed captions and
CoT reasoning data, which are further curated through self-consistency. This
curated data is ultimately used for multimodal pre-training to develop
next-generation foundation models. Extensive experiments on both low- and
high-resolution MLLMs across diverse benchmarks demonstrate that, SIcog
produces next-generation foundation MLLMs with significantly improved
cognition, achieving benchmark-leading performance compared to prevalent
pre-training approaches.",['cs.CV'],2503.07456," Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding
clinicians and radiologists by automatically retrieving relevant patient cases
in the database given the query image and/or report, for more efficient
clinical diagnosis and treatment, especially for rare diseases. However
conventional ITR systems typically only rely on global image or text
representations for measuring patient image/report similarities, which overlook
local distinctiveness across patient cases. This often results in suboptimal
retrieval performance. In this paper, we propose an Anatomical
Location-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a
query image and the associated suspicious anatomical region(s), aims to
retrieve similar patient cases exhibiting the same disease or symptoms in the
same anatomical region. To perform location-conditioned multimodal retrieval,
we learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with
semantic global-level and region-/word-level alignment to produce
generalizable, well-aligned multi-modal representations. Additionally, we
perform location-conditioned contrastive learning to further utilize cross-pair
region-level contrastiveness for improved multi-modal retrieval. We show that
our proposed RRA-VL achieves state-of-the-art localization performance in
phase-grounding tasks, and satisfying multi-modal retrieval performance with or
without location conditioning. Finally, we thoroughly investigate the
generalizability and explainability of our proposed ALC-ITR system in providing
explanations and preliminary diagnosis reports given retrieved patient cases
(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.",['cs.CV'],False,,,,"Towards Self-Improving Systematic Cognition for Next-Generation
  Foundation MLLMs",Anatomy-Aware Conditional Image-Text Retrieval
neg-d2-717,2025-02-13,,2502.09887," In this paper, we combine the concepts of the fibered Burnside ring and the
character ring, viewing them as fibered biset functors, into what we call the
global representation fibered ring of a finite group. We compute all ring
homomorphisms from this ring to the complex numbers, determine its spectrum and
its connected components, and identify the primitive idempotents of this ring
tensor with $\mathbb{Q}$ and its conductors.",['math.RT'],2502.09887," In this paper, we combine the concepts of the fibered Burnside ring and the
character ring, viewing them as fibered biset functors, into what we call the
global representation fibered ring of a finite group. We compute all ring
homomorphisms from this ring to the complex numbers, determine its spectrum and
its connected components, and identify the primitive idempotents of this ring
tensor with $\mathbb{Q}$ and its conductors.",['math.RT'],False,,,,The global representation fibered ring,The global representation fibered ring
neg-d2-718,2025-01-13,,2501.07071," As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning
their values with humans has become imperative for their responsible
development and customized applications. However, there still lack evaluations
of LLMs values that fulfill three desirable goals. (1) Value Clarification: We
expect to clarify the underlying values of LLMs precisely and comprehensively,
while current evaluations focus narrowly on safety risks such as bias and
toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are
prone to data contamination and quickly become obsolete as LLMs evolve.
Additionally, these discriminative evaluations uncover LLMs' knowledge about
values, rather than valid assessments of LLMs' behavioral conformity to values.
(3) Value Pluralism: The pluralistic nature of human values across individuals
and cultures is largely ignored in measuring LLMs value alignment. To address
these challenges, we presents the Value Compass Leaderboard, with three
correspondingly designed modules. It (i) grounds the evaluation on
motivationally distinct \textit{basic values to clarify LLMs' underlying values
from a holistic view; (ii) applies a \textit{generative evolving evaluation
framework with adaptive test items for evolving LLMs and direct value
recognition from behaviors in realistic scenarios; (iii) propose a metric that
quantifies LLMs alignment with a specific value as a weighted sum over multiple
dimensions, with weights determined by pluralistic values.",['cs.AI'],2502.09294," Automatic Affect Prediction (AAP) uses computational analysis of input data
such as text, speech, images, and physiological signals to predict various
affective phenomena (e.g., emotions or moods). These models are typically
constructed using supervised machine-learning algorithms, which rely heavily on
labeled training datasets. In this position paper, we posit that all AAP
training data are derived from human Affective Interpretation Processes,
resulting in a form of Affective Meaning. Research on human affect indicates a
form of complexity that is fundamental to such meaning: it can possess what we
refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing
Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of
confidence regarding meanings' correctness), Ambiguity (meaning contains
mutually exclusive concepts) and Vagueness (meaning is situated at different
levels in a nested hierarchy). Failing to appropriately consider QIs leads to
results incapable of meaningful and reliable predictions. Based on this
premise, we argue that a crucial step in adequately addressing indeterminacy in
AAP is the development of data collection practices for modeling corpora that
involve the systematic consideration of 1) a relevant set of QIs and 2) context
for the associated interpretation processes. To this end, we are 1) outlining a
conceptual model of AIPs and the QIs associated with the meaning these produce
and a conceptual structure of relevant context, supporting understanding of its
role. Finally, we use our framework for 2) discussing examples of
context-sensitivity-related challenges for addressing QIs in data collection
setups. We believe our efforts can stimulate a structured discussion of both
the role of aspects of indeterminacy and context in research on AAP, informing
the development of better practices for data collection and analysis.",['cs.AI'],False,,,,"Value Compass Leaderboard: A Platform for Fundamental and Validated
  Evaluation of LLMs Values","Indeterminacy in Affective Computing: Considering Meaning and Context in
  Data Collection Practices"
neg-d2-719,2025-02-07,,2502.05065," We consider the reduction of four-derivative heterotic supergravity on a
torus and construct two-charge multicenter BPS black hole solutions. In $d=5$,
the three-form field can be dualized to a gauge field and we correspondingly
construct three-charge multicenter BPS black hole solutions to the dualized
Bergshoeff-de Roo action. This makes precise the embedding of known solutions
into five-dimensional $\alpha'$-corrected STU supergravity.",['hep-th'],2502.13501," We consider four-dimensional Euclidean Yang-Mills theories quantized in the
maximal Abelian and linear covariant gauges at finite temperature.
Non-perturbatively, the Faddeev-Popov procedure must be improved to take into
account the existence of the so-called Gribov copies. Tapping on previous
results about the elimination of infinitesimal Gribov copies in maximal Abelian
and linear covariant gauges at zero temperature, we explore the interplay
between finite temperature effects and the removal of gauge copies. We focus in
a hybrid approach where the thermal masses are derived through perturbative
propagators as a stepping stone for a self-consistent treatment. The resulting
action collects the effects of the elimination of infinitesimal Gribov copies
as well as the thermal masses. We verify the existence of three different
phases for the gluonic degrees of freedom; one of complete confinement at low
temperatures, an intermediate one of partial confinement, and one of complete
deconfinement at high temperatures.",['hep-th'],False,,,,Multicenter higher-derivative BPS black holes,"Yang-Mills theories at finite temperature quantized in linear covariant
  gauges: gauge copies and semi-non-perturbative effects"
neg-d2-720,2025-02-27,,2502.20053," We present a computationally efficient strategy that allows to simulate
magnetization switching driven by spin-transfer torque in magnetic tunnel
junctions within a micromagnetic model coupled with a matrix-based
non-equilibrium Green's function algorithm. Exemplary simulation for a
realistic set of parameters are carried out and show switching times below 4 ns
for voltages above 300 mV or around 2*10^{10} A m^{-2} for the P to AP
(parallel to anti-parallel) direction. For AP to P switching, a trend-reversal
in the switching time is seen i.e. the time for magnetization reversal first
decreases with increasing bias voltage but then starts to rise again.",['physics.comp-ph'],2502.20053," We present a computationally efficient strategy that allows to simulate
magnetization switching driven by spin-transfer torque in magnetic tunnel
junctions within a micromagnetic model coupled with a matrix-based
non-equilibrium Green's function algorithm. Exemplary simulation for a
realistic set of parameters are carried out and show switching times below 4 ns
for voltages above 300 mV or around 2*10^{10} A m^{-2} for the P to AP
(parallel to anti-parallel) direction. For AP to P switching, a trend-reversal
in the switching time is seen i.e. the time for magnetization reversal first
decreases with increasing bias voltage but then starts to rise again.",['physics.comp-ph'],False,,,,"Efficient solution strategy to couple micromagnetic simulations with
  ballistic transport in magnetic tunnel junctions","Efficient solution strategy to couple micromagnetic simulations with
  ballistic transport in magnetic tunnel junctions"
neg-d2-721,2025-03-21,,2503.17026," Climate change is one of the most critical challenges of the twenty-first
century. Public understanding of climate issues and of the goals regarding the
climate transition is essential to translate awareness into concrete actions.
Social media platforms play a crucial role in disseminating information about
climate change and climate policy. In this context, we propose a model that
analyses the Supply and Demand of information to better understand information
circulation and information voids within the Italian climate-transition
discourse. We conceptualise information supply as the production of content on
Facebook and Instagram while leveraging Google searches to capture information
demand. Our findings highlight the persistence of information voids, which can
hinder informed decision-making and collective action. Furthermore, we observe
that the dynamics of information supply and demand on climate-related topics
tend to intensify in response to significant external events, shaping public
attention and social media discourse.",['cs.SI'],2501.1621," Despite extensive research and development of tools and technologies for
misinformation tracking and detection, we often find ourselves largely on the
losing side of the battle against misinformation. In an era where
misinformation poses a substantial threat to public discourse, trust in
information sources, and societal and political stability, it is imperative
that we regularly revisit and reorient our work strategies. While we have made
significant strides in understanding how and why misinformation spreads, we
must now broaden our focus and explore how technology can help realise new
approaches to address this complex challenge more efficiently.",['cs.SI'],False,,,,"Modelling the Climate Change Debate in Italy through Information Supply
  and Demand",New Frontiers in Fighting Misinformation
neg-d2-722,2025-02-21,,2502.16064," Single Domain Generalization (SDG) remains a formidable challenge in the
field of machine learning, particularly when models are deployed in
environments that differ significantly from their training domains. In this
paper, we propose a novel data augmentation approach, named as Model-aware
Parametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM
deploys adversarial queries generated with stochastic gradient Langevin
dynamics, and produces model-aware augmenting instances with a parametric
batch-wise mixup generator network that is carefully designed through an
innovative attention mechanism. By exploiting inter-feature correlations, the
parameterized mixup generator introduces additional versatility in combining
features across a batch of instances, thereby enhancing the capacity to
generate highly adaptive and informative synthetic instances for specific
queries. The synthetic data produced by this adaptable generator network,
guided by informative queries, is expected to significantly enrich the
representation space covered by the original training dataset and subsequently
enhance the prediction model's generalizability across diverse and previously
unseen domains. To prevent excessive deviation from the training data, we
further incorporate a real-data alignment-based adversarial loss into the
learning process of MPBM, regularizing any tendencies toward undesirable
expansions. We conduct extensive experiments on several benchmark datasets. The
empirical results demonstrate that by augmenting the training set with
informative synthesis data, our proposed MPBM method achieves the
state-of-the-art performance for single domain generalization.",['cs.LG'],2503.00485," Graph spectra are an important class of structural features on graphs that
have shown promising results in enhancing Graph Neural Networks (GNNs). Despite
their widespread practical use, the theoretical understanding of the power of
spectral invariants -- particularly their contribution to GNNs -- remains
incomplete. In this paper, we address this fundamental question through the
lens of homomorphism expressivity, providing a comprehensive and quantitative
analysis of the expressive power of spectral invariants. Specifically, we prove
that spectral invariant GNNs can homomorphism-count exactly a class of specific
tree-like graphs which we refer to as parallel trees. We highlight the
significance of this result in various contexts, including establishing a
quantitative expressiveness hierarchy across different architectural variants,
offering insights into the impact of GNN depth, and understanding the subgraph
counting capabilities of spectral invariant GNNs. In particular, our results
significantly extend Arvind et al. (2024) and settle their open questions.
Finally, we generalize our analysis to higher-order GNNs and answer an open
question raised by Zhang et al. (2024).",['cs.LG'],False,,,,"Single Domain Generalization with Model-aware Parametric Batch-wise
  Mixup",Homomorphism Expressivity of Spectral Invariant Graph Neural Networks
neg-d2-723,2025-01-08,,2501.04907," Generation and propagation of optical skyrmions provide a versatile plalform
for topologically nontrivial optical informatics and light-matter interactions,
but their acceleration along curved trajectories is to be studied. In this
study, we experimentally demonstrate the first accelerating skyrmion lattices
conveyed by Airy structured light, characterized by topologically stable
skyrmion textures with self-acceleration along parabolic trajectories. We show
that the skyrmion unit cell can maintain a Skyrme number $|N_\text{sk}|>0.9$
within a propagation range of $\pm1.22\ z_R$ upon parabolic acceleration.
Notably, the meron structure remains $|N_\text{sk}|$ stable within $0.5\pm0.02$
over a significantly extended range of $\pm3.06\ z_R$. Our work provides a new
potential carrier for topologically robust information distribution, particle
sorting and manipulation.",['physics.optics'],2502.07801," In recent years, the large electric field enhancement and tight spatial
confinement supported by the so-called epsilon near-zero (ENZ) mode has
attracted significant attention for the realization of efficient nonlinear
optical devices. Here, we experimentally demonstrate ENZ photonic gap antennas
(PGAs), which consist of a dielectric pillar within which a thin slab of indium
tin oxide (ITO) material is embedded. In ENZ PGAs, hybrid dielectric-ENZ modes
emerge from strong coupling between the dielectric antenna modes and the ENZ
bulk plasmon resonance. These hybrid modes efficiently couple to free space and
allow for large enhancements of the incident electric field over nearly an
octave bandwidth, without the stringent lateral nanofabrication requirements of
conventional plasmonic or dielectric nanoantennas. To understand the modal
features, we probe the linear response of single ENZ PGAs with dark field
scattering and interpret the results in terms of a simple coupled oscillator
framework. Third harmonic generation (THG) is used to probe the ITO local
fields and large enhancements are observed in the THG efficiency over a broad
spectral range. Surprisingly, sharp peaks emerge on top of the nonlinear
response, which were not predicted by full wave calculations. These peaks are
attributed to the ENZ material's nonlocal response, which once included using a
hydrodynamic model for the ITO permittivity improves the agreement of our
calculations for both the linear and nonlinear response. This proof of concept
demonstrates the potential of ENZ PGAs, which we have previously shown can
support electric field enhancements of up to 100--200X, and the importance of
including nonlocal effects when describing the response of thin ENZ layers.
Importantly, inclusion of the ITO nonlocality leads to increases in the
predicted field enhancement, as compared to the local calculation.",['physics.optics'],False,,,,Optical skyrmion lattices accelerating in free space,"Field-enhancement and nonlocal effects in epsilon-near-zero photonic gap
  antennas"
neg-d2-724,2025-03-05,,2503.03204," This paper presents an innovative approach that enables the user to find
matching faces based on the user-selected face parameters. Through gradio-based
user interface, the users can interactively select the face parameters they
want in their desired partner. These user-selected face parameters are
transformed into a text prompt which is used by the Text-To-Image generation
model to generate a realistic face image. Further, the generated image along
with the images downloaded from the Jeevansathi.com are processed through face
detection and feature extraction model, which results in high dimensional
vector embedding of 512 dimensions. The vector embeddings generated from the
downloaded images are stored into vector database. Now, the similarity search
is carried out between the vector embedding of generated image and the stored
vector embeddings. As a result, it displays the top five similar faces based on
the user-selected face parameters. This contribution holds a significant
potential to turn into a high-quality personalized face matching tool.",['cs.CV'],2502.07302," Multi-class cell segmentation in high-resolution gigapixel whole slide images
(WSIs) is crucial for various clinical applications. However, training such
models typically requires labor-intensive, pixel-wise annotations by domain
experts. Recent efforts have democratized this process by involving lay
annotators without medical expertise. However, conventional non-corrective
approaches struggle to handle annotation noise adaptively because they lack
mechanisms to mitigate false positives (FP) and false negatives (FN) at both
the image-feature and pixel levels. In this paper, we propose a consensus-aware
self-corrective AI agent that leverages the Consensus Matrix to guide its
learning process. The Consensus Matrix defines regions where both the AI and
annotators agree on cell and non-cell annotations, which are prioritized with
stronger supervision. Conversely, areas of disagreement are adaptively weighted
based on their feature similarity to high-confidence consensus regions, with
more similar regions receiving greater attention. Additionally, contrastive
learning is employed to separate features of noisy regions from those of
reliable consensus regions by maximizing their dissimilarity. This paradigm
enables the model to iteratively refine noisy labels, enhancing its robustness.
Validated on one real-world lay-annotated cell dataset and two reasoning-guided
simulated noisy datasets, our method demonstrates improved segmentation
performance, effectively correcting FP and FN errors and showcasing its
potential for training robust models on noisy datasets. The official
implementation and cell annotations are publicly available at
https://github.com/ddrrnn123/CASC-AI.",['cs.CV'],False,,,,Find Matching Faces Based On Face Parameters,"CASC-AI: Consensus-aware Self-corrective Learning for Noise Cell
  Segmentation"
neg-d2-725,2025-03-07,,2503.05648," This work presents a physics-based machine learning framework to predict and
analyze oxides of nitrogen (NOx) emissions from compression-ignition
engine-powered vehicles using on-board diagnostics (OBD) data as input.
Accurate NOx prediction from OBD datasets is difficult because NOx formation
inside an engine combustion chamber is governed by complex processes occurring
on timescales much shorter than the data collection rate. Thus, emissions
generally cannot be predicted accurately using simple empirically derived
physics models. Black box models like genetic algorithms or neural networks can
be more accurate, but have poor interpretability. The transparent model
presented in this paper has both high accuracy and can explain potential
sources of high emissions. The proposed framework consists of two major steps:
a physics-based NOx prediction model combined with a novel Divergent Window
Co-occurrence (DWC) Pattern detection algorithm to analyze operating conditions
that are not adequately addressed by the physics-based model. The proposed
framework is validated for generalizability with a second vehicle OBD dataset,
a sensitivity analysis is performed, and model predictions are compared with
that from a deep neural network. The results show that NOx emissions
predictions using the proposed model has around 55% better root mean square
error, and around 60% higher mean absolute error compared to the baseline NOx
prediction model from previously published work. The DWC Pattern Detection
Algorithm identified low engine power conditions to have high statistical
significance, indicating an operating regime where the model can be improved.
This work shows that the physics-based machine learning framework is a viable
method for predicting NOx emissions from engines that do not incorporate NOx
sensing.",['cs.LG'],2503.05079," This work studies the alignment of large language models with preference data
from an imitation learning perspective. We establish a close theoretical
connection between reinforcement learning from human feedback RLHF and
imitation learning (IL), revealing that RLHF implicitly performs imitation
learning on the preference data distribution. Building on this connection, we
propose DIL, a principled framework that directly optimizes the imitation
learning objective. DIL provides a unified imitation learning perspective on
alignment, encompassing existing alignment algorithms as special cases while
naturally introducing new variants. By bridging IL and RLHF, DIL offers new
insights into alignment with RLHF. Extensive experiments demonstrate that DIL
outperforms existing methods on various challenging benchmarks.",['cs.LG'],False,,,,"Physics-based machine learning framework for predicting NOx emissions
  from compression ignition engines using on-board diagnostics data",On a Connection Between Imitation Learning and RLHF
neg-d2-726,2025-01-22,,2501.12707, We construct a weak Hilbert space that is a twisted Hilbert space.,['math.FA'],2502.15486," In this note, we present an alternative proof of a quantified Tauberian
theorem for vector-valued sequences first proved in \cite{Sei15_Tauberian}. The
theorem relates the decay rate of a bounded sequence with properties of a
certain boundary function. We present a slightly strengthened version of this
result, and illustrate how it can be used to obtain quantified versions of the
Katznelson--Tzafriri theorem as well as results on Ritt operators.",['math.FA'],False,,,,A weak Hilbert space that is a twisted HIlbert space,Tauberian theorems for sequences and the Katznelson--Tzafriri theorem
neg-d2-727,2025-01-31,,2501.19294," Many ethical issues in machine learning are connected to the training data.
Online data markets are an important source of training data, facilitating both
production and distribution. Recently, a trend has emerged of for-profit
""ethical"" participants in online data markets. This trend raises a fascinating
question: Can online data markets sustainably and efficiently address ethical
issues in the broader machine-learning economy?
  In this work, we study this question in a stylized model of an online data
market. We investigate the effects of intervening in the data market to achieve
balanced training-data production. The model reveals the crucial role of market
conditions. In small and emerging markets, an intervention can drive the data
producers out of the market, so that the cost of fairness is maximal. Yet, in
large and established markets, the cost of fairness can vanish (as a fraction
of overall welfare) as the market grows.
  Our results suggest that ""ethical"" online data markets can be economically
feasible under favorable market conditions, and motivate more models to
consider the role of data production and distribution in mediating the impacts
of ethical interventions.",['cs.GT'],2501.19294," Many ethical issues in machine learning are connected to the training data.
Online data markets are an important source of training data, facilitating both
production and distribution. Recently, a trend has emerged of for-profit
""ethical"" participants in online data markets. This trend raises a fascinating
question: Can online data markets sustainably and efficiently address ethical
issues in the broader machine-learning economy?
  In this work, we study this question in a stylized model of an online data
market. We investigate the effects of intervening in the data market to achieve
balanced training-data production. The model reveals the crucial role of market
conditions. In small and emerging markets, an intervention can drive the data
producers out of the market, so that the cost of fairness is maximal. Yet, in
large and established markets, the cost of fairness can vanish (as a fraction
of overall welfare) as the market grows.
  Our results suggest that ""ethical"" online data markets can be economically
feasible under favorable market conditions, and motivate more models to
consider the role of data production and distribution in mediating the impacts
of ethical interventions.",['cs.GT'],False,,,,The Cost of Balanced Training-Data Production in an Online Data Market,The Cost of Balanced Training-Data Production in an Online Data Market
neg-d2-728,2025-01-21,,2501.12544," Systems interacting with humans, such as assistive robots or chatbots, are
increasingly integrated into our society. To prevent these systems from causing
social, legal, ethical, empathetic, or cultural (SLEEC) harms, normative
requirements specify the permissible range of their behaviors. These
requirements encompass both functional and non-functional aspects and are
defined with respect to time. Typically, these requirements are specified by
stakeholders from a broad range of fields, such as lawyers, ethicists, or
philosophers, who may lack technical expertise. Because such stakeholders often
have different goals, responsibilities, and objectives, ensuring that these
requirements are well-formed is crucial. SLEEC DSL, a domain-specific language
resembling natural language, has been developed to formalize these requirements
as SLEEC rules. In this paper, we present LEGOS-SLEEC, a tool designed to
support interdisciplinary stakeholders in specifying normative requirements as
SLEEC rules, and in analyzing and debugging their well-formedness. LEGOS-SLEEC
is built using four previously published components, which have been shown to
be effective and usable across nine case studies. Reflecting on this
experience, we have significantly improved the user interface of LEGOS-SLEEC
and its diagnostic support, and demonstrate the effectiveness of these
improvements using four interdisciplinary stakeholders. Showcase video URL is:
https://youtu.be/LLaBLGxSi8A",['cs.CY'],2502.03472," The rapid advancement of Large Language Models (LLMs) has created a critical
gap in consumer protection due to the lack of standardized certification
processes for LLM-powered Artificial Intelligence (AI) systems. This paper
argues that current regulatory approaches, which focus on compute-level
thresholds and generalized model evaluations, are insufficient to ensure the
safety and effectiveness of specific LLM-based user experiences. We propose a
shift towards a certification process centered on actual user-facing
experiences and the curation of high-quality datasets for evaluation. This
approach offers several benefits: it drives consumer confidence in AI system
performance, enables businesses to demonstrate the credibility of their
products, and allows regulators to focus on direct consumer protection. The
paper outlines a potential certification workflow, emphasizing the importance
of domain-specific datasets and expert evaluation. By repositioning data as the
strategic center of regulatory efforts, this framework aims to address the
challenges posed by the probabilistic nature of AI systems and the rapid pace
of technological advancement. This shift in regulatory focus has the potential
to foster innovation while ensuring responsible AI development, ultimately
benefiting consumers, businesses, and government entities alike.",['cs.CY'],False,,,,LEGOS-SLEEC: Tool for Formalizing and Analyzing Normative Requirements,"Powering LLM Regulation through Data: Bridging the Gap from Compute
  Thresholds to Customer Experiences"
neg-d2-729,2025-03-02,,2503.01153," The symmetry group of a (discrete) Painlev\'e equation provides crucial
information on the properties of the equation. In this paper we argue against
the commonly-held belief that the symmetry group of a given equation is solely
determined by its surface type as given in the famous Sakai classification. We
will dispel this misconception on a specific example of a
d-${\text{P}_{\mathrm{II}}}$ equation which corresponds to a half-translation
on the root lattice dual to its surface-type root lattice, but which becomes a
genuine translation on a sub-lattice thereof that corresponds to its real
symmetry group. The latter fact is shown in two different ways: first by a
brute force calculation and second through the use of normalizer theory, which
we believe to be an extremely useful tool for this purpose. We finish the paper
with the analysis of a sub-case of our main example which arises in the study
of gap probabilities for Freud unitary ensembles, and the symmetry group of
which is even further restricted due to the appearance of a nodal curve on the
surface on which the equation is regularized.",['nlin.SI'],2503.01153," The symmetry group of a (discrete) Painlev\'e equation provides crucial
information on the properties of the equation. In this paper we argue against
the commonly-held belief that the symmetry group of a given equation is solely
determined by its surface type as given in the famous Sakai classification. We
will dispel this misconception on a specific example of a
d-${\text{P}_{\mathrm{II}}}$ equation which corresponds to a half-translation
on the root lattice dual to its surface-type root lattice, but which becomes a
genuine translation on a sub-lattice thereof that corresponds to its real
symmetry group. The latter fact is shown in two different ways: first by a
brute force calculation and second through the use of normalizer theory, which
we believe to be an extremely useful tool for this purpose. We finish the paper
with the analysis of a sub-case of our main example which arises in the study
of gap probabilities for Freud unitary ensembles, and the symmetry group of
which is even further restricted due to the appearance of a nodal curve on the
surface on which the equation is regularized.",['nlin.SI'],False,,,,"What is the symmetry group of a d-${\text{P}_{\mathrm{II}}}$ discrete
  Painlev\'e equation?","What is the symmetry group of a d-${\text{P}_{\mathrm{II}}}$ discrete
  Painlev\'e equation?"
neg-d2-730,2025-02-13,,2502.10481," The recent increase in morbidity is primarily due to chronic diseases
including Diabetes, Heart disease, Lung cancer, and brain tumours. The results
for patients can be improved, and the financial burden on the healthcare system
can be lessened, through the early detection and prevention of certain
disorders. In this study, we built a machine-learning model for predicting the
existence of numerous diseases utilising datasets from various sources,
including Kaggle, Dataworld, and the UCI repository, that are relevant to each
of the diseases we intended to predict.
  Following the acquisition of the datasets, we used feature engineering to
extract pertinent features from the information, after which the model was
trained on a training set and improved using a validation set. A test set was
then used to assess the correctness of the final model. We provide an
easy-to-use interface where users may enter the parameters for the selected
ailment. Once the right model has been run, it will indicate whether the user
has a certain ailment and offer suggestions for how to treat or prevent it.",['cs.LG'],2502.21187," AI models for lung cancer screening are limited by data scarcity, impacting
generalizability and clinical applicability. Generative models address this
issue but are constrained by training data variability. We introduce SYN-LUNGS,
a framework for generating high-quality 3D CT images with detailed annotations.
SYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for
nodule simulation (varying size, location, and appearance), and DukeSim for CT
image formation with vendor and parameter variability. The dataset includes
3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174
digital twins. Models trained on clinical + simulated data outperform clinical
only models, achieving 10% improvement in detection, 2-9% in segmentation and
classification, and enhanced synthesis.By incorporating anatomy-informed
simulations, SYN-LUNGS provides a scalable approach for AI model development,
particularly in rare disease representation and improving model reliability.",['cs.LG'],False,,,,Chronic Diseases Prediction Using ML,"SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital
  Twins for AI Training"
neg-d2-731,2025-03-21,,2503.17453," This article presents our results for the eighth Affective Behavior Analysis
in-the-wild (ABAW) competition.Multimodal emotion recognition (ER) has
important applications in affective computing and human-computer interaction.
However, in the real world, compound emotion recognition faces greater issues
of uncertainty and modal conflicts. For the Compound Expression (CE)
Recognition Challenge,this paper proposes a multimodal emotion recognition
method that fuses the features of Vision Transformer (ViT) and Residual Network
(ResNet). We conducted experiments on the C-EXPR-DB and MELD datasets. The
results show that in scenarios with complex visual and audio cues (such as
C-EXPR-DB), the model that fuses the features of ViT and ResNet exhibits
superior performance.Our code are avalible on
https://github.com/MyGitHub-ax/8th_ABAW",['cs.CV'],2501.17821," Scene flow enables an understanding of the motion characteristics of the
environment in the 3D world. It gains particular significance in the
long-range, where object-based perception methods might fail due to sparse
observations far away. Although significant advancements have been made in
scene flow pipelines to handle large-scale point clouds, a gap remains in
scalability with respect to long-range. We attribute this limitation to the
common design choice of using dense feature grids, which scale quadratically
with range. In this paper, we propose Sparse Scene Flow (SSF), a general
pipeline for long-range scene flow, adopting a sparse convolution based
backbone for feature extraction. This approach introduces a new challenge: a
mismatch in size and ordering of sparse feature maps between time-sequential
point scans. To address this, we propose a sparse feature fusion scheme, that
augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater
importance to faraway points. Our method, SSF, achieves state-of-the-art
results on the Argoverse2 dataset, demonstrating strong performance in
long-range scene flow estimation. Our code will be released at
https://github.com/KTH-RPL/SSF.git.",['cs.CV'],False,,,,"Feature-Based Dual Visual Feature Extraction Model for Compound
  Multimodal Emotion Recognition",SSF: Sparse Long-Range Scene Flow for Autonomous Driving
neg-d2-732,2025-03-11,,2503.08668," Vector Quantization (VQ) has emerged as a prominent weight compression
technique, showcasing substantially lower quantization errors than uniform
quantization across diverse models, particularly in extreme compression
scenarios. However, its efficacy during fine-tuning is limited by the
constraint of the compression format, where weight vectors assigned to the same
codeword are restricted to updates in the same direction. Consequently, many
quantized weights are compelled to move in directions contrary to their local
gradient information. To mitigate this issue, we introduce a novel VQ paradigm,
Sign-Splitting VQ (SSVQ), which decouples the sign bit of weights from the
codebook. Our approach involves extracting the sign bits of uncompressed
weights and performing clustering and compression on all-positive weights. We
then introduce latent variables for the sign bit and jointly optimize both the
signs and the codebook. Additionally, we implement a progressive freezing
strategy for the learnable sign to ensure training stability. Extensive
experiments on various modern models and tasks demonstrate that SSVQ achieves a
significantly superior compression-accuracy trade-off compared to conventional
VQ. Furthermore, we validate our algorithm on a hardware accelerator, showing
that SSVQ achieves a 3$\times$ speedup over the 8-bit compressed model by
reducing memory access.",['cs.CV'],2503.04478," General-purpose AI models, particularly those designed for text and vision,
demonstrate impressive versatility across a wide range of deep-learning tasks.
However, they often underperform in specialised domains like medical imaging,
where domain-specific solutions or alternative knowledge transfer approaches
are typically required. Recent studies have noted that general-purpose models
can exhibit similar latent spaces when processing semantically related data,
although this alignment does not occur naturally. Building on this insight, it
has been shown that applying a simple transformation - at most affine -
estimated from a subset of semantically corresponding samples, known as
anchors, enables model stitching across diverse training paradigms,
architectures, and modalities. In this paper, we explore how semantic alignment
- estimating transformations between anchors - can bridge general-purpose AI
with specialised medical knowledge. Using multiple public chest X-ray datasets,
we demonstrate that model stitching across model architectures allows general
models to integrate domain-specific knowledge without additional training,
leading to improved performance on medical tasks. Furthermore, we introduce a
novel zero-shot classification approach for unimodal vision encoders that
leverages semantic alignment across modalities. Our results show that our
method not only outperforms general multimodal models but also approaches the
performance levels of fully trained, medical-specific multimodal solutions",['cs.CV'],False,,,,"SSVQ: Unleashing the Potential of Vector Quantization with
  Sign-Splitting",Semantic Alignment of Unimodal Medical Text and Vision Representations
neg-d2-733,2025-01-02,,2501.01355," We report on the electron spin resonance (ESR), heat capacity, magnetization,
nuclear magnetic resonance (NMR), magnetic circular and linear dichroism (XMCD,
XMLD), as well as the electrical resistivity of EuMn$_{2}$P$_{2}$ single
crystals. Antiferromagnetic order of Eu was observed in several quantities at
$T^{\rm Eu}_{\rm N}\,=\,18\,\rm K$. The temperature dependencies of ESR
linewidth and resonance shift show, when approaching the Eu-ordered state, a
divergence towards $T^{\rm Eu}_{\rm N}$, indicating the growing importance of
magnetic correlations and the build-up of internal magnetic fields. An
additional temperature scale of $\approx 47\,\rm K$ has considerable impact on
linewidth, resonance field and intensity. This points to the presence of weak
Mn-based ordering. The observed ESR line is interpreted as an Eu$^{2+}$
resonance, which probes the weak magnetic background of the Mn subsystem. Such
picture is suggested by the lineshape which keeps to be Lorentzian across the
$47\,\rm K$ scale and by the ESR intensity which can be described by the same
Curie-Weiss temperature above and below $47\,\rm K$. In the same temperature
range anomalies were observed at $48.5\,\rm K$ and $51\,\rm K$ in the heat
capacity data as well as a pronounced broadening of the NMR signal of the
EuMn$_{2}$P$_{2}$ samples. In XMCD and XMLD measurements, this weak magnetic
order could not be detected in the same temperature range which might be due to
the small magnetic moment, with a potential $c$-component or frustration.",['cond-mat.str-el'],2503.08," According to Faraday's law in classical physics, a varying magnetic field
stimulates an electric eddy field. Intuitively, when a classical field is
constant and imposed on a lattice, the Wannier-Stark ladders (WSL) can be
established, resulting in Bloch oscillations. In this work, we investigate the
dynamics of an interacting system on a (generalized) ring lattice threaded by a
varying magnetic flux. Based on the rigorious results, we demonstrate that
there exist many invariant subspaces in which the dynamics is periodic when the
flux varies linearly over time. Nevertheless, for a given initial state, the
evolved state differs from that driven by a linear field. However, the
probability distributions of the two states are identical, referred to as the
quantum analogue of Faraday's law. Our results are ubiquitous for a wide
variety of interacting systems. We demonstrate these results through numerical
simulations in an extended fermi-Hubbard model.",['cond-mat.str-el'],False,,,,Magnetic frustration and weak Mn magnetic ordering in EuMn$_2$P$_2$,"Bloch oscillations in interacting systems driven by a time-dependent
  magnetic field"
neg-d2-734,2025-03-20,,2503.16255," We investigate limit linear series on chains of elliptic curves, giving a
simple proof of a conjecture of Farkas stating the existence of curves with a
theta-characteristic with a given number of sections for the expected range of
genera. Using the additional structure afforded by considering limit linear
series on chains of elliptic curves, we find examples of reducible
Brill-Noether loci, admitting at least two components, with and without a
theta-characteristic respectively. This allows us to display reducible Hilbert
schemes for $r\ge 3$ and the largest possible value of $d$, namely $d=g-1$. We
also give examples of Brill-Noether loci with three components. On the positive
side, we provide optimal bounds on the degree under which Brill-Noether loci
are irreducible when $r=2$.",['math.AG'],2503.17273," We study the subrank of real order-three tensors and give an upper bound to
the subrank of a real tensor given its complex subrank. Using similar arguments
to those used by Bernardi-Blekherman-Ottaviani, we show that all subranks
between the minimal typical subrank and the maximal typical subrank, which
equals the generic subrank, are also typical. We then study small tensor
formats with more than one typical subrank. In particular, we construct a $3
\times 3 \times 5$-tensor with subrank $2$ and show that the subrank of the $4
\times 4 \times 4$-quaternion multiplication tensor is $2$. Finally, we
consider the tensor associated to componentwise complex multiplication in
$\mathbb{C}^n$ and show that this tensor has real subrank $n$ - informally, no
more than $n$ real scalar multiplications can be carried out using a device
that does $n$ complex scalar multiplications. We also prove a version of this
result for other real division algebras.",['math.AG'],False,,,,Some reducible and irreducible Brill-Noether loci,Real subrank of order-three tensors
neg-d2-735,2025-03-16,,2503.12598," In this paper, we present new characterizations of normal and positive
operators in terms of their powers. Among other things, we show that if $T^2$
is normal, $\mathcal{W}(T^{2k+1})$ lies on one side of a line passing through
the origin (possibly including some points on the line) for some
$k\in\mathbb{N}$, and $\mathrm{asc\,}(T)= 1$ (or $\mathrm{dsc\,}(T)=1$), then
$T$ must be normal. This complements the previous result due to Putnam [28].
Furthermore, we prove that $T$ is normal (positive) if and only if
$\mathrm{asc\,}(T)= 1$ and there exist coprime numbers $p,q\geq 2$ such that
$T^p$ and $T^q$ are normal (positive). Finally, we also show that $T$ is
positive if and only if $T^k$ is accretive for all $k\in\mathbb{N}$, which
answers the question from [22] in the affirmative.",['math.FA'],2503.03415," In 1954, I. Kaplansky proposed three test problems for deciding the strength
of structural understanding of a class of mathematical objects in his treatise
""Infinite abelian groups"", which can be formulated for very general
mathematical systems. In this paper, we focus on Kaplansky's second test
problem in a context of complex geometry. Let $H^2_{\beta}$ be a weighted Hardy
space. The Cowen-Douglas operator theory tells us that each
$h\in\textrm{Hol}(\overline{\mathbb{D}})$ induces a Hermitian holomorphic
vector bundle on $H^2_{\beta}$, denoted by $E_{h(S_\beta)}(\Omega)$, where
$\Omega$ is a domain. We show that the vector bundle $E_{h(S_\beta)}$ is a
push-forwards Hermitian holomorphic vector bundle and study the similarity
deformation problems. Our main theorem is that if $H^2_{\beta}$ is a weighted
Hardy space of polynomial growth, then for any $f\in
\textrm{Hol}(\overline{\mathbb{D}})$, there exists a unique positive integer
$m$ and an function $h\in\textrm{Hol}(\overline{\mathbb{D}})$ inducing an
indecomposable vector bundle $E_{h(S_{\beta})}$, such that $E_{f(S_\beta)}$ is
similar to $\bigoplus_1^m E_{h(S_\beta)}$, where $h$ is unique in the sense of
analytic automorphism group action. That could be seemed as a Jordan
decomposition theorem for the push-forwards Hermitian holomorphic vector
bundles. Furthermore, we give the similarity classification of those
push-forwards Hermitian holomorphic vector bundles induced by analytic
functions, and give an affirmative answer to Kaplansky's second test problem
for those objects. We also give an affirmative answer to the geometric version
and generalized version of a problem proposed by R. Douglas in 2007, and obtain
the $K_0$-group of the commutant algebra of a multiplication operator on a
weighted Hardy space of polynomial growth. In addition, we give an example to
show the setting of polynomial growth condition is necessary.",['math.FA'],False,,,,Characterizations of positive operators via their powers,"The Jordan decomposition and Kaplansky's second test problem for
  Hermitian holomorphic vector bundles"
neg-d2-736,2025-01-27,,2501.16616," Hallucination detection in text generation remains an ongoing struggle for
natural language processing (NLP) systems, frequently resulting in unreliable
outputs in applications such as machine translation and definition modeling.
Existing methods struggle with data scarcity and the limitations of unlabeled
datasets, as highlighted by the SHROOM shared task at SemEval-2024. In this
work, we propose a novel framework to address these challenges, introducing
DeepSeek Few-shot optimization to enhance weak label generation through
iterative prompt engineering. We achieved high-quality annotations that
considerably enhanced the performance of downstream models by restructuring
data to align with instruct generative models. We further fine-tuned the
Mistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to
accurately detect hallucinations in resource-limited settings. Combining this
fine-tuned model with ensemble learning strategies, our approach achieved 85.5%
accuracy on the test set, setting a new benchmark for the SHROOM task. This
study demonstrates the effectiveness of data restructuring, few-shot
optimization, and fine-tuning in building scalable and robust hallucination
detection frameworks for resource-constrained NLP systems.",['cs.CL'],2502.17927," Alignment techniques enable Large Language Models (LLMs) to generate outputs
that align with human preferences and play a crucial role in their
effectiveness. However, their impact often diminishes when applied to Small
Language Models (SLMs), likely due to the limited capacity of these models.
Instead of directly applying existing alignment techniques to SLMs, we propose
to utilize a well-aligned teacher LLM to guide the alignment process for these
models, thereby facilitating the transfer of the teacher's knowledge of human
preferences to the student model. To achieve this, we first explore a
straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that
employs knowledge distillation with two KL-divergence constraints from the
aligned teacher to the unaligned student. To further enhance the student's
ability to distinguish between preferred and dispreferred responses, we then
propose Advantage-Guided Distillation for Preference Alignment (ADPA), which
leverages an advantage function from the aligned teacher to deliver more
nuanced, distribution-level reward signals for the student's alignment. Our
experimental results show that these two approaches appreciably improve the
alignment of SLMs and narrow the performance gap with larger counterparts.
Among them, ADPA demonstrates superior performance and achieves even greater
effectiveness when integrated with DCKD. Our code is available at
https://github.com/SLIT-AI/ADPA.",['cs.CL'],False,,,,"Few-Shot Optimized Framework for Hallucination Detection in
  Resource-Limited NLP Systems","Advantage-Guided Distillation for Preference Alignment in Small Language
  Models"
neg-d2-737,2025-02-24,,2502.17679," Adverse childhood experiences (ACEs) have been linked to a wide range of
negative health outcomes in adulthood. However, few studies have investigated
what specific combinations of ACEs most substantially impact mental health. In
this article, we provide the protocol for our observational study of the
effects of combinations of ACEs on adult depression. We use data from the 2023
Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We
will evaluate the replicability of our findings by splitting the sample into
two discrete subpopulations of individuals. We employ data turnover for this
analysis, enabling a single team of statisticians and domain experts to
collaboratively evaluate the strength of evidence, and also integrating both
qualitative and quantitative insights from exploratory data analysis. We
outline our analysis plan using this method and conclude with a brief
discussion of several specifics for our study.",['stat.AP'],2502.09248," Traditional Phase-Linking (PL) algorithms are known for their high cost,
especially with the huge volume of Synthetic Aperture Radar (SAR) images
generated by Sentinel-1 SAR missions. Recently, a COvariance Fitting
Interferometric Phase Linking (COFI-PL) approach has been proposed, which can
be seen as a generic framework for existing PL methods. Although this method is
less computationally expensive than traditional PL approaches, COFI-PL exploits
the entire covariance matrix, which poses a challenge with the increasing time
series of SAR images. However, COFI-PL, like traditional PL approaches, cannot
accommodate the efficient inclusion of newly acquired SAR images. This paper
overcomes this drawback by introducing a sequential integration of a block of
newly acquired SAR images. Specifically, we propose a method for effectively
addressing optimization problems associated with phase-only complex vectors on
the torus based on the Majorization-Minimization framework.",['stat.AP'],False,,,,"Protocol For An Observational Study On The Effects Of Combinations Of
  Adverse Childhood Experiences On Adult Depression",Sequential Covariance Fitting for InSAR Phase Linking
neg-d2-738,2025-03-02,,2503.01048," Personalizing large language models (LLMs) is essential for delivering
tailored interactions that improve user experience. Many existing
personalization methods require fine-tuning LLMs for each user, rendering them
prohibitively expensive for widespread adoption. Although retrieval-based
approaches offer a more compute-efficient alternative, they still depend on
large, high-quality datasets that are not consistently available for all users.
To address this challenge, we propose CHAMELEON, a scalable and efficient
personalization approach that uses (1) self-generated personal preference data
and (2) representation editing to enable quick and cost-effective
personalization. Our experiments on various tasks, including those from the
LaMP personalization benchmark, show that CHAMELEON efficiently adapts models
to personal preferences, improving instruction-tuned models and outperforms two
personalization baselines by an average of 40% across two model architectures.",['cs.LG'],2501.08305," Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.",['cs.LG'],False,,,,Personalize Your LLM: Fake it then Align it,"Benchmarking Graph Representations and Graph Neural Networks for
  Multivariate Time Series Classification"
neg-d2-739,2025-02-24,,2502.1692," Recent Multi-Party Conversation (MPC) models typically rely on graph-based
approaches to capture dialogue structures. However, these methods have
limitations, such as information loss during the projection of utterances into
structural embeddings and constraints in leveraging pre-trained language models
directly. In this paper, we propose \textbf{SS-MPC}, a response generation
model for MPC that eliminates the need for explicit graph structures. Unlike
existing models that depend on graphs to analyze conversation structures,
SS-MPC internally encodes the dialogue structure as a sequential input,
enabling direct utilization of pre-trained language models. Experimental
results show that \textbf{SS-MPC} achieves \textbf{15.60\% BLEU-1} and
\textbf{12.44\% ROUGE-L} score, outperforming the current state-of-the-art MPC
response generation model by \textbf{3.91\%p} in \textbf{BLEU-1} and
\textbf{0.62\%p} in \textbf{ROUGE-L}. Additionally, human evaluation confirms
that SS-MPC generates more fluent and accurate responses compared to existing
MPC models.",['cs.CL'],2502.13646," In-context learning (ICL) has demonstrated significant potential in enhancing
the capabilities of large language models (LLMs) during inference. It's
well-established that ICL heavily relies on selecting effective demonstrations
to generate outputs that better align with the expected results. As for
demonstration selection, previous approaches have typically relied on intuitive
metrics to evaluate the effectiveness of demonstrations, which often results in
limited robustness and poor cross-model generalization capabilities. To tackle
these challenges, we propose a novel method, \textbf{D}emonstration
\textbf{VA}lidation (\textbf{D.Va}), which integrates a demonstration
validation perspective into this field. By introducing the demonstration
validation mechanism, our method effectively identifies demonstrations that are
both effective and highly generalizable. \textbf{D.Va} surpasses all existing
demonstration selection techniques across both natural language understanding
(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate
the robustness and generalizability of our approach across various language
models with different retrieval models.",['cs.CL'],False,,,,SS-MPC: A Sequence-Structured Multi-Party Conversation System,D.Va: Validate Your Demonstration First Before You Use It
neg-d2-740,2025-01-05,,2501.02557," We construct and study new generalisations to rooted trees and forests of
some properties of shuffles of words. First, we build a coproduct on rooted
trees which, together with their shuffle, endow them with bialgebra structure.
We then caracterize the coproduct dual to the shuffle product of rooted forests
and build a product on rooted trees to obtain the bialgebra dual to the shuffle
bialgebra. We then characterize and enumerate primitive trees for the dual
coproduct. Finally, using modified shuffles of rooted forests, we prove a
property in the category of Rota-Baxter algebras.",['math.CO'],2501.11334," It is known that there are many notions of largeness in a semigroup that own
rich combinatorial properties. In this paper, we focus on partition and almost
disjoint properties of these notions. One of the most remarkable results with
respect to this topic is that in an infinite very weakly cancellative semigroup
of size \kappa, every central set can be split into \kappa disjoint central
subsets. Moreover, if \kappa contains \lambda almost disjoint subsets, then
every central set contains a family of \lambda almost disjoint central subsets.
And many other combinatorial notions are found successively to have analogous
properties, among these are thick sets, piecewise syndetic sets, J-sets and
C-sets. In this paper, we mainly study four other notions: IP sets,
combinatorially rich sets, Cp-sets and PP-rich sets. Where the latter two are
known in (N, +), related to the polynomial extension of the central sets
theorem. We lift them up to commutative cancellative semigroups and obtain an
uncountable version of the polynomial extension of the central sets theorem
incidentally. And we finally find that the infinite partition and almost
disjoint properties hold for Cp-sets in commutative cancellative semigroups and
for other three notions in (N, +).",['math.CO'],False,,,,"Coalgebras, bialgebras and Rota-Baxter algebras from shuffles of rooted
  forests",On partition and almost disjoint properties of combinatorial notions
neg-d2-741,2025-01-20,,2501.1184," Systematic reviews are time-consuming endeavors. Historically speaking,
knowledgeable humans have had to screen and extract data from studies before it
can be analyzed. However, large language models (LLMs) hold promise to greatly
accelerate this process. After a pilot study which showed great promise, we
investigated the use of freely available LLMs for extracting data for
systematic reviews. Using three different LLMs, we extracted 24 types of data,
9 explicitly stated variables and 15 derived categorical variables, from 112
studies that were included in a published scoping review. Overall we found that
Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably
well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with
human coding, respectively. While promising, these results highlight the dire
need for a human-in-the-loop (HIL) process for AI-assisted data extraction. As
a result, we present a free, open-source program we developed (AIDE) to
facilitate user-friendly, HIL data extraction with LLMs.",['cs.HC'],2502.06197," Large Language Models (LLMs) have been widely used to support ideation in the
writing process. However, whether generating ideas with the help of LLMs leads
to idea fixation or idea expansion is unclear. This study examines how
different timings of LLM usage - either at the beginning or after independent
ideation - affect people's perceptions and ideation outcomes in a writing task.
In a controlled experiment with 60 participants, we found that using LLMs from
the beginning reduced the number of original ideas and lowered creative
self-efficacy and self-credit, mediated by changes in autonomy and ownership.
We discuss the challenges and opportunities associated with using LLMs to
assist in idea generation. We propose delaying the use of LLMs to support
ideation while considering users' self-efficacy, autonomy, and ownership of the
ideation outcomes.",['cs.HC'],False,,,,"Large Language Models with Human-In-The-Loop Validation for Systematic
  Review Data Extraction","Timing Matters: How Using LLMs at Different Timings Influences Writers'
  Perceptions and Ideation Outcomes in AI-Assisted Ideation"
neg-d2-742,2025-03-05,,2503.03415," In 1954, I. Kaplansky proposed three test problems for deciding the strength
of structural understanding of a class of mathematical objects in his treatise
""Infinite abelian groups"", which can be formulated for very general
mathematical systems. In this paper, we focus on Kaplansky's second test
problem in a context of complex geometry. Let $H^2_{\beta}$ be a weighted Hardy
space. The Cowen-Douglas operator theory tells us that each
$h\in\textrm{Hol}(\overline{\mathbb{D}})$ induces a Hermitian holomorphic
vector bundle on $H^2_{\beta}$, denoted by $E_{h(S_\beta)}(\Omega)$, where
$\Omega$ is a domain. We show that the vector bundle $E_{h(S_\beta)}$ is a
push-forwards Hermitian holomorphic vector bundle and study the similarity
deformation problems. Our main theorem is that if $H^2_{\beta}$ is a weighted
Hardy space of polynomial growth, then for any $f\in
\textrm{Hol}(\overline{\mathbb{D}})$, there exists a unique positive integer
$m$ and an function $h\in\textrm{Hol}(\overline{\mathbb{D}})$ inducing an
indecomposable vector bundle $E_{h(S_{\beta})}$, such that $E_{f(S_\beta)}$ is
similar to $\bigoplus_1^m E_{h(S_\beta)}$, where $h$ is unique in the sense of
analytic automorphism group action. That could be seemed as a Jordan
decomposition theorem for the push-forwards Hermitian holomorphic vector
bundles. Furthermore, we give the similarity classification of those
push-forwards Hermitian holomorphic vector bundles induced by analytic
functions, and give an affirmative answer to Kaplansky's second test problem
for those objects. We also give an affirmative answer to the geometric version
and generalized version of a problem proposed by R. Douglas in 2007, and obtain
the $K_0$-group of the commutant algebra of a multiplication operator on a
weighted Hardy space of polynomial growth. In addition, we give an example to
show the setting of polynomial growth condition is necessary.",['math.FA'],2503.03415," In 1954, I. Kaplansky proposed three test problems for deciding the strength
of structural understanding of a class of mathematical objects in his treatise
""Infinite abelian groups"", which can be formulated for very general
mathematical systems. In this paper, we focus on Kaplansky's second test
problem in a context of complex geometry. Let $H^2_{\beta}$ be a weighted Hardy
space. The Cowen-Douglas operator theory tells us that each
$h\in\textrm{Hol}(\overline{\mathbb{D}})$ induces a Hermitian holomorphic
vector bundle on $H^2_{\beta}$, denoted by $E_{h(S_\beta)}(\Omega)$, where
$\Omega$ is a domain. We show that the vector bundle $E_{h(S_\beta)}$ is a
push-forwards Hermitian holomorphic vector bundle and study the similarity
deformation problems. Our main theorem is that if $H^2_{\beta}$ is a weighted
Hardy space of polynomial growth, then for any $f\in
\textrm{Hol}(\overline{\mathbb{D}})$, there exists a unique positive integer
$m$ and an function $h\in\textrm{Hol}(\overline{\mathbb{D}})$ inducing an
indecomposable vector bundle $E_{h(S_{\beta})}$, such that $E_{f(S_\beta)}$ is
similar to $\bigoplus_1^m E_{h(S_\beta)}$, where $h$ is unique in the sense of
analytic automorphism group action. That could be seemed as a Jordan
decomposition theorem for the push-forwards Hermitian holomorphic vector
bundles. Furthermore, we give the similarity classification of those
push-forwards Hermitian holomorphic vector bundles induced by analytic
functions, and give an affirmative answer to Kaplansky's second test problem
for those objects. We also give an affirmative answer to the geometric version
and generalized version of a problem proposed by R. Douglas in 2007, and obtain
the $K_0$-group of the commutant algebra of a multiplication operator on a
weighted Hardy space of polynomial growth. In addition, we give an example to
show the setting of polynomial growth condition is necessary.",['math.FA'],False,,,,"The Jordan decomposition and Kaplansky's second test problem for
  Hermitian holomorphic vector bundles","The Jordan decomposition and Kaplansky's second test problem for
  Hermitian holomorphic vector bundles"
neg-d2-743,2025-01-19,,2501.11164," Recorded option pricing datasets are not always freely available.
Additionally, these datasets often contain numerous prices which are either
higher or lower than can reasonably be expected. Various reasons for these
unexpected observations are possible, including human error in the recording of
the details associated with the option in question. In order for the analyses
performed on these datasets to be reliable, it is necessary to identify and
remove these options from the dataset. In this paper, we list three distinct
problems often found in recorded option price datasets alongside means of
addressing these. The methods used are justified using sound statistical
reasoning and remove option prices violating the standard assumption of no
arbitrage. An attractive aspect of the proposed technique is that no option
pricing model-based assumptions are used. Although the discussion is restricted
to European options, the procedure is easily modified for use with exotic
options as well. As a final contribution, the paper contains a link to six
option pricing datasets which have already been cleaned using the proposed
methods and can be freely used by researchers.",['q-fin.CP'],2501.11164," Recorded option pricing datasets are not always freely available.
Additionally, these datasets often contain numerous prices which are either
higher or lower than can reasonably be expected. Various reasons for these
unexpected observations are possible, including human error in the recording of
the details associated with the option in question. In order for the analyses
performed on these datasets to be reliable, it is necessary to identify and
remove these options from the dataset. In this paper, we list three distinct
problems often found in recorded option price datasets alongside means of
addressing these. The methods used are justified using sound statistical
reasoning and remove option prices violating the standard assumption of no
arbitrage. An attractive aspect of the proposed technique is that no option
pricing model-based assumptions are used. Although the discussion is restricted
to European options, the procedure is easily modified for use with exotic
options as well. As a final contribution, the paper contains a link to six
option pricing datasets which have already been cleaned using the proposed
methods and can be freely used by researchers.",['q-fin.CP'],False,,,,A statistical technique for cleaning option price data,A statistical technique for cleaning option price data
neg-d2-744,2025-02-05,,2502.03003," Spherically symmetric effective dust collapse inspired by effective loop
quantum cosmology predicts a bounce when the stellar energy density becomes
planckian, which in turn inevitably leads to shell-crossing singularity
formation. An extension of the spacetime beyond such singularities is possible
through weak solutions of the equations of motion in integral form, leading to
the shockwave model. In this work, we show explicitly that such an extension is
not unique, and that relevant features like the black hole life-time strongly
depend on the choice of the integral form of the equation of motion.",['gr-qc'],2501.03356," An approach is presented to address singularities in general relativity using
a complex Riemannian spacetime extension. We demonstrate how this method can be
applied to both black hole and cosmological singularities, specifically
focusing on the Schwarzschild and Kerr black holes and the
Friedmann-Lema\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending
the relevant coordinates into the complex plane and carefully choosing
integration contours, we show that it is possible to regularize these
singularities, resulting in physically meaningful, singularity-free solutions
when projected back onto real spacetime. The removal of the singularity at the
Big Bang allows for a bounce cosmology. This approach offers a potential bridge
between classical general relativity and quantum gravity effects, suggesting a
way to resolve longstanding issues in gravitational physics without requiring a
full theory of quantum gravity.",['gr-qc'],False,,,,"Non-uniqueness of the shockwave dynamics in effective loop quantum
  gravity","Complex Riemannian spacetime and singularity-free black holes and
  cosmology"
neg-d2-745,2025-01-07,,2501.04089," We present deep optical observations of the stellar halo of NGC 300, an
LMC-mass galaxy, acquired with the DEEP sub-component of the DECam Local Volume
Exploration survey (DELVE) using the 4 m Blanco Telescope. Our resolved star
analysis reveals a large, low surface brightness stellar stream
($M_{V}\sim-8.5$; [Fe/H] $= -1.4\pm0.15$) extending more than 40 kpc north from
the galaxy's center. We also find other halo structures, including potentially
an additional stream wrap to the south, which may be associated with the main
stream. The morphology and derived low metallicities of the streams and shells
discovered surrounding NGC 300 are highly suggestive of a past accretion event.
Assuming a single progenitor, the accreted system is approximately Fornax-like
in luminosity, with an inferred mass ratio to NGC 300 of approximately $1:15$.
We also present the discovery of a metal-poor globular cluster
($R_{\rm{proj}}=23.3$~kpc; $M_{V}=-8.99\pm0.16$; [Fe/H] $\approx-1.6\pm0.6$) in
the halo of NGC 300, the furthest identified globular cluster associated with
NGC 300. The stellar structures around NGC 300 represent the richest features
observed in a Magellanic Cloud analog to date, strongly supporting the idea
that accretion and subsequent disruption is an important mechanism in the
assembly of dwarf galaxy stellar halos.",['astro-ph.GA'],2501.05288," We present the confirmation of a compact galaxy group candidate, CGG-z4, at
$z=4.3$ in the COSMOS field. This structure was identified by two
spectroscopically confirmed $z=4.3$ $K_s$-dropout galaxies with ALMA $870\rm\,
\mu m$ and 3 mm continuum detections, surrounded by an overdensity of
NIR-detected galaxies with consistent photometric redshifts of $4.0<z<4.6$. The
two ALMA sources, CGG-z4.a and CGG-z4.b, are detected with both CO(4-3) and
CO(5-4) lines. [CI](1-0) is detected on CGG-z4.a, and
H$_{2}$O($1_{1,0}-1_{0,1}$) absorption is detected on CGG-z4.b. We model an
integrated spectral energy distribution by combining the FIR-to-radio
photometry of this group and estimate a total star formation rate of
$\rm\sim2000\, M_{\odot}$ yr$^{-1}$, making it one of the most star-forming
groups known at $z>4$. Their high CO(5-4)/CO(4-3) ratios indicate that the
inter-stellar mediums (ISMs) are close to thermalization, suggesting either
high gas temperatures, densities, and/or pressure, while the low
[CI](1-0)/CO(4-3) line ratios indicate high star formation efficiencies. With
[CI]-derived gas masses we found the two galaxies have extremely short gas
depletion times of $99$ Myr and $<63$ Myr respectively, suggesting the onset of
quenching. With an estimated halo mass of $\rm log (M_{\rm
halo}[M_{\odot}])\sim12.8$, we suggest that this structure is likely in the
process of forming a massive galaxy cluster.",['astro-ph.GA'],False,,,,"Streams, Shells, and Substructures in the Accretion-Built Stellar Halo
  of NGC 300","Revealing the hidden cosmic feast: A z=4.3 galaxy group hosting two
  optically dark, efficiently star-forming galaxies"
neg-d2-746,2025-03-13,,2503.10525," This paper explores the potential of affine frequency division multiplexing
(AFDM) to mitigate the multiuser interference (MUI) problem by employing
time-domain precoding in extremely-large-scale multiple-input multiple-output
(XL-MIMO) systems. In XL-MIMO systems, user mobility significantly improves
network capacity and transmission quality. Meanwhile, the robustness of AFDM to
Doppler shift is enhanced in user mobility scenarios, which further improves
the system performance. However, the multicarrier nature of AFDM has attracted
much attention, and it leads to a significant increase in precoding complexity.
However, the serious problem is that the multicarrier use of AFDM leads to a
sharp increase in precoding complexity. Therefore, we employ efficient
precoding randomized Kaczmarz (rKA) to reduce the complexity overhead. Through
simulation analysis, we compare the performance of XL-MIMO-AFDM and XL-MIMO
orthogonal frequency division multiplexing (XL-MIMO-OFDM) in mobile scenarios,
and the results show that our proposed AFDM-based XL-MIMO precoding design can
be more efficient.",['cs.DC'],2503.02356," Long context fine-tuning of large language models(LLMs) involves training on
datasets that are predominantly composed of short sequences and a small
proportion of longer sequences. However, existing approaches overlook this
long-tail distribution and employ training strategies designed specifically for
long sequences. Moreover, these approaches also fail to address the challenges
posed by variable sequence lengths during distributed training, such as load
imbalance in data parallelism and severe pipeline bubbles in pipeline
parallelism. These issues lead to suboptimal training performance and poor GPU
resource utilization. To tackle these problems, we propose a chunk-centric
training method named ChunkFlow. ChunkFlow reorganizes input sequences into
uniformly sized chunks by consolidating short sequences and splitting longer
ones. This approach achieves optimal computational efficiency and balance among
training inputs. Additionally, ChunkFlow incorporates a state-aware chunk
scheduling mechanism to ensure that the peak memory usage during training is
primarily determined by the chunk size rather than the maximum sequence length
in the dataset. Integrating this scheduling mechanism with existing pipeline
scheduling algorithms further enhances the performance of distributed training.
Experimental results demonstrate that, compared with Megatron-LM, ChunkFlow can
be up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we
believe that ChunkFlow serves as an effective solution for a broader range of
scenarios, such as long context continual pre-training, where datasets contain
variable-length sequences.",['cs.DC'],False,,,,Efficient Precoding in XL-MIMO-AFDM System,Efficient Long Context Fine-tuning with Chunk Flow
neg-d2-747,2025-03-18,,2503.1476," This work aims to discuss the current landscape of kinematic analysis tools,
ranging from the state-of-the-art in sports biomechanics such as inertial
measurement units (IMUs) and retroreflective marker-based optical motion
capture (MoCap) to more novel approaches from the field of computing such as
human pose estimation and human mesh recovery. Primarily, this comparative
analysis aims to validate the use of marker-less MoCap techniques in a clinical
setting by showing that these marker-less techniques are within a reasonable
range for kinematics analysis compared to the more cumbersome and less portable
state-of-the-art tools. Not only does marker-less motion capture using human
pose estimation produce results in-line with the results of both the IMU and
MoCap kinematics but also benefits from a reduced set-up time and reduced
practical knowledge and expertise to set up. Overall, while there is still room
for improvement when it comes to the quality of the data produced, we believe
that this compromise is within the room of error that these low-speed actions
that are used in small clinical tests.",['cs.CV'],2503.11187," Video Large Language Models have shown impressive capabilities in video
comprehension, yet their practical deployment is hindered by substantial
inference costs caused by redundant video tokens. Existing pruning techniques
fail to fully exploit the spatiotemporal redundancy inherent in video data. To
bridge this gap, we perform a systematic analysis of video redundancy from two
perspectives: temporal context and visual context. Leveraging this insight, we
propose Dynamic Density Pruning for Fast Video LLMs termed FastVID.
Specifically, FastVID dynamically partitions videos into temporally ordered
segments to preserve temporal structure and applies a density-based token
pruning strategy to maintain essential visual information. Our method
significantly reduces computational overhead while maintaining temporal and
visual integrity. Extensive evaluations show that FastVID achieves
state-of-the-art performance across various short- and long-video benchmarks on
leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID
effectively prunes 90% of video tokens while retaining 98.0% of
LLaVA-OneVision's original performance. The code is available at
https://github.com/LunarShen/FastVID.",['cs.CV'],False,,,,"Validation of Human Pose Estimation and Human Mesh Recovery for
  Extracting Clinically Relevant Motion Data from Videos",FastVID: Dynamic Density Pruning for Fast Video Large Language Models
neg-d2-748,2025-01-08,,2501.04381," In the present work, we collect solar irradiance and atmospheric condition
data from several products, obtained from both numerical models (ERA5 and
NORA3) and satellite observations (CMSAF-SARAH3). We then train simple
supervised Machine Learning (ML) data fusion models, using these products as
predictors and direct in-situ Global Horizontal Irradiance (GHI) measurements
over Norway as ground-truth. We show that combining these products by applying
our trained ML models provides a GHI estimate that is significantly more
accurate than that obtained from any product taken individually. Using the
trained models, we generate a 30-year ML-corrected map of GHI over Norway,
which we release as a new open data product. Our ML-based data fusion
methodology could be applied, after suitable training and input data selection,
to any geographic area on Earth.",['physics.ao-ph'],2502.00276," Paleoclimate records reveal a fuller range of natural climate variability
than modern records and are essential for better understanding the modern
climate change. However, most paleoclimate records are point-based proxies and
lack the temporal resolution needed to analyze spatiotemporal changes in
destructive extremes like tropical cyclones (TCs). Here we show that historical
records by pre-industrial Chinese intellectuals help investigate long-term
variability of TC landfalls in East Asia. Despite inherent limitations, these
records show a landfalling TC climatology resembling modern observations in
spatial-temporal distributions. Comparisons between the pre-industrial records
(1776-1850), modern observations (1946-2020), and climate simulations reveal an
earlier seasonal occurrence of modern TCs. However, the variations of
seasonally aggregated landfall time show pronounced multi-century variations.
The modern changes and multi-decade trends appear moderate compared to
long-term variability in pre-industrial TC records, suggesting that an
overreliance on modern data may lead to an underestimation of the full range of
TC activity potentially arising from natural variability alone. Analyses of
newly available climate data reveal associations between past landfalling TC
activity and the large-scale climate variability of tropical ocean and
extratropical land. These findings demonstrate the value of paleoclimate data
for exploring natural variability in TC activity and inform the development of
effective adaptation strategies for future climate change.",['physics.ao-ph'],False,,,,"Data fusion of complementary data sources using Machine Learning enables
  higher accuracy Solar Resource Maps","Chinese Historical Documents Reveal Multi-Century Seasonal Shifts in
  Tropical Cyclone Landfalls"
neg-d2-749,2025-02-10,,2502.06738," Recent work showed that small changes in benchmark questions can reduce LLMs'
reasoning and recall. We explore two such changes: pairing questions and adding
more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We
find that for more capable models, these predictably reduce performance,
essentially heightening the performance ceiling of a benchmark and unsaturating
it again. We suggest this approach can resurrect old benchmarks.",['cs.LG'],2503.10115," The purpose of partial multi-label feature selection is to select the most
representative feature subset, where the data comes from partial multi-label
datasets that have label ambiguity issues. For label disambiguation, previous
methods mainly focus on utilizing the information inside the labels and the
relationship between the labels and features. However, the information existing
in the feature space is rarely considered, especially in partial multi-label
scenarios where the noises is considered to be concentrated in the label space
while the feature information is correct. This paper proposes a method based on
latent space alignment, which uses the information mined in feature space to
disambiguate in latent space through the structural consistency between labels
and features. In addition, previous methods overestimate the consistency of
features and labels in the latent space after convergence. We comprehensively
consider the similarity of latent space projections to feature space and label
space, and propose new feature selection term. This method also significantly
improves the positive label identification ability of the selected features.
Comprehensive experiments demonstrate the superiority of the proposed method.",['cs.LG'],False,,,,Resurrecting saturated LLM benchmarks with adversarial encoding,"Reconsidering Feature Structure Information and Latent Space Alignment
  in Partial Multi-label Feature Selection"
neg-d2-750,2025-01-25,,2501.153," In this article, we present a modified variant of the Dai-Liao spectral
conjugate gradient method, developed through an analysis of eigenvalues and
inspired by a modified secant condition. We show that the proposed method is
globally convergent for general nonlinear functions under standard assumptions.
By incorporating the new secant condition and a quasi-Newton direction, we
introduce updated spectral parameters. These changes ensure that the resulting
search direction satisfies the sufficient descent property without relying on
any line search. Numerical experiments show that the proposed algorithm
performs better than several existing methods in terms of convergence speed and
computational efficiency. Its effectiveness is further demonstrated through an
application in signal processing.",['math.OC'],2503.14981," This work is concerned with convex analysis of so-called spectral functions
of matrices that only depend on eigenvalues of the matrix. An abstract
framework of spectral decomposition systems is proposed that covers a wide
range of previously studied settings, including eigenvalue decomposition of
Hermitian matrices and singular value decomposition of rectangular matrices and
allows deriving new results in more general settings such as Euclidean Jordan
algebras. The main results characterize convexity, lower semicontinuity,
Fenchel conjugates, convex subdifferentials, and Bregman proximity operators of
spectral functions in terms of the reduced functions. As a byproduct, a
generalization of the Ky Fan majorization theorem is obtained.",['math.OC'],False,,,,"Modified Dai-Liao Spectral Conjugate Gradient Method with Application to
  Signal Processing",Convex Analysis in Spectral Decomposition Systems
neg-d2-751,2025-02-10,,2502.06973," This paper presents a novel application for directly estimating indoor light
and heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.
In our image-based rendering method, the indoor panorama is used to estimate
the 3D room layout, while the corresponding outdoor panorama serves as an
environment map to infer spatially-varying light and material properties. We
establish a connection between indoor light transport and heat transport and
implement transient heat simulation to generate indoor heat panoramas. The
sensitivity analysis of various thermal parameters is conducted, and the
resulting heat maps are compared with the images captured by the thermal camera
in real-world scenarios. This digital application enables automatic indoor
light and heat estimation without manual inputs and cumbersome field
measurements.",['cs.CV'],2503.04983," In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.",['cs.CV'],False,,,,Indoor Light and Heat Estimation from a Single Panorama,"Leveraging Large Language Models For Scalable Vector Graphics
  Processing: A Review"
neg-d2-752,2025-02-04,,2502.02239," The first SRG/eROSITA all-sky X-ray survey, eRASS1, resulted in a catalogue
of over twelve thousand optically-confirmed galaxy groups and clusters in the
western Galactic hemisphere. Using the eROSITA images of these objects, we
measure and study their morphological properties, including their
concentration, central density and slope, ellipticity, power ratios, photon
asymmetry, centroid shift and Gini coefficient. We also introduce new
forward-modelled parameters which take account of the instrument point spread
function (PSF), which are slosh, which measures how asymmetric the surface
brightness distribution is, and multipole magnitudes, which are analogues to
power ratios. Using simulations, we find some non forward-modelled parameters
are strongly biased due to PSF and data quality. For the same clusters, we find
similar values of concentration and central density compared to results by
ourselves using Chandra and previous results from XMM-Newton. The population as
a whole has log concentrations which are typically around 0.3 dex larger than
South Pole Telescope or Planck-selected samples and the deeper eFEDS sample.
The exposure time, detection likelihood threshold, extension likelihood
threshold and number of counts affect the concentration distribution, but
generally not enough to reduce the concentration to match the other samples.
The concentration of clusters in the survey strongly affects whether they are
detected as a function of redshift and luminosity. We introduce a combined
disturbance score based on a Gaussian mixture model fit to several of the
parameters. For brighter clusters, around 1/4 of objects are classified as
disturbed using this score, which may be due to our sensitivity to concentrated
objects.",['astro-ph.CO'],2501.17564," We present the first results of the HI intensity mapping power spectrum
analysis with the MeerKAT International GigaHertz Tiered Extragalactic
Exploration (MIGHTEE) survey. We use data covering $\sim$ 4 square degrees in
the COSMOS field using a frequency range 962.5 MHz to 1008.42 MHz, equivalent
to HI emission in $0.4<z<0.48$. The data consists of 15 pointings with a total
of 94.2 hours on-source. We verify the suitability of the MIGHTEE data for HI
intensity mapping by testing for residual systematics across frequency,
baselines and pointings. We also vary the window used for HI signal
measurements and find no significant improvement using stringent Fourier mode
cuts. Averaging in the power spectrum domain, i.e. using incoherent averaging,
we calculate the first upper limits from MIGHTEE on the HI power spectrum at
scales $0.5 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$. We obtain the best
1$\sigma$ upper limit of 28.6 mK$^{2}$Mpc${^3}$ on $k\sim$2 Mpc$^{-1}$. Our
results are consistent with the power spectrum detected with observations in
the DEEP2 field with MeerKAT. The data we use here constitutes a small fraction
of the MIGHTEE survey and demonstrates that combined analysis of the full
MIGHTEE survey can potentially detect the HI power spectrum at $z\lesssim0.5$
in the range $0.1 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$ or quasi-linear
scales.",['astro-ph.CO'],False,,,,"The SRG/eROSITA all-sky survey: The morphologies of clusters of galaxies
  I: A catalogue of morphological parameters","HI Intensity Mapping with the MIGHTEE Survey: First Results of the HI
  Power Spectrum"
neg-d2-753,2025-02-13,,2502.09442," In this paper we prove that the Diophantine problem in iterated restricted
wreath products $G$ of arbitrary non-trivial free abelian groups $A_1,\ldots,
A_k$, $k>1$ of finite ranks is undecidable, i.e., there is no algorithm that
given a finite system of group equations with coefficients in $G$ decides
whether or not the system has a solution in $G$.",['math.GR'],2502.09442," In this paper we prove that the Diophantine problem in iterated restricted
wreath products $G$ of arbitrary non-trivial free abelian groups $A_1,\ldots,
A_k$, $k>1$ of finite ranks is undecidable, i.e., there is no algorithm that
given a finite system of group equations with coefficients in $G$ decides
whether or not the system has a solution in $G$.",['math.GR'],False,,,,"The Diophantine problem in iterated wreath products of free abelian
  groups is undecidable","The Diophantine problem in iterated wreath products of free abelian
  groups is undecidable"
neg-d2-754,2025-01-02,,2501.01063," The FAPL-DM-BC solution is a new FL-based privacy, security, and scalability
solution for the Internet of Vehicles (IoV). It leverages Federated Adaptive
Privacy-Aware Learning (FAPL) and Dynamic Masking (DM) to learn and adaptively
change privacy policies in response to changing data sensitivity and state in
real-time, for the optimal privacy-utility tradeoff. Secure Logging and
Verification, Blockchain-based provenance and decentralized validation, and
Cloud Microservices Secure Aggregation using FedAvg (Federated Averaging) and
Secure Multi-Party Computation (SMPC). Two-model feedback, driven by
Model-Agnostic Explainable AI (XAI), certifies local predictions and
explanations to drive it to the next level of efficiency. Combining local
feedback with world knowledge through a weighted mean computation, FAPL-DM-BC
assures federated learning that is secure, scalable, and interpretable.
Self-driving cars, traffic management, and forecasting, vehicular network
cybersecurity in real-time, and smart cities are a few possible applications of
this integrated, privacy-safe, and high-performance IoV platform.",['cs.CR'],2503.02441," Security researchers grapple with the surge of malicious files, necessitating
swift identification and classification of malware strains for effective
protection. Visual classifiers and in particular Convolutional Neural Networks
(CNNs) have emerged as vital tools for this task. However, issues of robustness
and explainability, common in other high risk domain like medicine and
autonomous vehicles, remain understudied in current literature. Although deep
learning visualization classifiers presented in research obtain great results
without the need for expert feature extraction, they have not been properly
studied in terms of their replicability. Additionally, the literature is not
clear on how these types of classifiers arrive to their answers. Our study
addresses these gaps by replicating six CNN models and exploring their
pitfalls. We employ Class Activation Maps (CAMs), like GradCAM and HiResCAM, to
assess model explainability. We evaluate the CNNs' performance and
interpretability on two standard datasets, MalImg and Big2015, and a newly
created called VX-Zoo. We employ these different CAM techniques to gauge the
explainability of each of the models. With these tools, we investigate the
underlying factors contributing to different interpretations of inputs across
the different models, empowering human researchers to discern patterns crucial
for identifying distinct malware families and explain why CNN models arrive at
their conclusions. Other then highlighting the patterns found in the
interpretability study, we employ the extracted heatmpas to enhance Visual
Transformers classifiers' performance and explanation quality. This approach
yields substantial improvements in F1 score, ranging from 2% to 8%, across the
datasets compared to benchmark values.",['cs.CR'],False,,,,"FAPL-DM-BC: A Secure and Scalable FL Framework with Adaptive Privacy and
  Dynamic Masking, Blockchain, and XAI for the IoVs","Through the Static: Demystifying Malware Visualization via
  Explainability"
neg-d2-755,2025-02-25,,2502.18417," While the task of face swapping has recently gained attention in the research
community, a related problem of head swapping remains largely unexplored. In
addition to skin color transfer, head swap poses extra challenges, such as the
need to preserve structural information of the whole head during synthesis and
inpaint gaps between swapped head and background. In this paper, we address
these concerns with GHOST 2.0, which consists of two problem-specific modules.
First, we introduce enhanced Aligner model for head reenactment, which
preserves identity information at multiple scales and is robust to extreme pose
variations. Secondly, we use a Blender module that seamlessly integrates the
reenacted head into the target background by transferring skin color and
inpainting mismatched regions. Both modules outperform the baselines on the
corresponding tasks, allowing to achieve state of the art results in head
swapping. We also tackle complex cases, such as large difference in hair styles
of source and target. Code is available at
https://github.com/ai-forever/ghost-2.0",['cs.CV'],2502.06145," Recent character image animation methods based on diffusion models, such as
Animate Anyone, have made significant progress in generating consistent and
generalizable character animations. However, these approaches fail to produce
reasonable associations between characters and their environments. To address
this limitation, we introduce Animate Anyone 2, aiming to animate characters
with environment affordance. Beyond extracting motion signals from source
video, we additionally capture environmental representations as conditional
inputs. The environment is formulated as the region with the exclusion of
characters and our model generates characters to populate these regions while
maintaining coherence with the environmental context. We propose a
shape-agnostic mask strategy that more effectively characterizes the
relationship between character and environment. Furthermore, to enhance the
fidelity of object interactions, we leverage an object guider to extract
features of interacting objects and employ spatial blending for feature
injection. We also introduce a pose modulation strategy that enables the model
to handle more diverse motion patterns. Experimental results demonstrate the
superior performance of the proposed method.",['cs.CV'],False,,,,GHOST 2.0: generative high-fidelity one shot transfer of heads,"Animate Anyone 2: High-Fidelity Character Image Animation with
  Environment Affordance"
neg-d2-756,2025-03-18,,2503.14042," These lectures provide a concise introduction to flavor physics, within and
beyond the Standard Model, with main focus on B-physics phenomenology and some
recent developments. The first lecture is an introduction to the flavor sector
of the Standard Model. The second lecture is devoted to B-meson mixing and rare
B decays. The last lecture contains a general discussion about flavor physics
beyond the Standard Model, with a highlighting of recent developments related
to the idea of flavor deconstruction.",['hep-ph'],2501.0106," Using the pinch technique, we compute the one-loop vertices of weak
interactions in the B-LSSM and incorporate their pinch contributions into the
gauge boson self-energies. Compared to the definitions of the $S$, $T$, and $U$
parameters in the Standard Model based on the $SU(2)_L\otimes U(1)_Y$ group,
the corresponding parameters in the B-LSSM are modified. We provide these
redefined $S$, $T$, and $U$ parameters and demonstrate the convergence of the
results. In the framework of the low-energy effective Lagrangian for weak
interactions, the $S$, $T$, and $U$ parameters can be expressed as functions of
certain parameters in the B-LSSM. The updated experimental and fitting results
constrain the parameter space of the B-LSSM strongly.",['hep-ph'],False,,,,Flavour Physics and CP Violation,"$S$, $T$, $U$ Parameters in The B-LSSM"
neg-d2-757,2025-01-03,,2501.01942," We employ deep learning (DL) to improve photometric redshifts (photo-$z$s) in
the Kilo-Degree Survey Data Release 4 Bright galaxy sample (KiDS-Bright DR4).
This dataset, used as a foreground for KiDS lensing and clustering studies, is
flux-limited to $r<20$ mag with mean $z=0.23$ and covers 1000 deg$^2$. Its
photo-$z$s were previously derived with artificial neural networks from the
ANNz2 package, trained on the Galaxy And Mass Assembly (GAMA) spectroscopy.
Here we considerably improve over these previous redshift estimations by
building a DL model, Hybrid-z, which combines four-band KiDS images with
nine-band magnitudes from KiDS+VIKING. The Hybrid-z framework provides
photo-$z$s for KiDS-Bright, with negligible mean residuals of O($10^{-4}$) and
scatter at the level of $0.014(1+z)$ -- reduction by 20% over the previous
nine-band derivations with ANNz2. We check our photo-$z$ model performance on
test data drawn from GAMA, as well as from other KiDS-overlapping wide-angle
spectroscopic surveys, namely SDSS, 2dFLenS, and 2dFGRS. We find stable
behavior and consistent improvement over ANNz2 throughout. We finally apply
Hybrid-z trained on GAMA to the entire KiDS-Bright DR4 sample of 1.2 million
galaxies. For these final predictions, we design a method of smoothing the
input redshift distribution of the training set, to avoid propagation of
features present in GAMA, related to its small sky area and large-scale
structure imprint in its fields. Our work paves the way towards the
best-possible photo-$z$s achievable with machine learning for any galaxy type
both for the final KiDS-Bright DR5 data and for future deeper imaging, such as
from the Legacy Survey of Space and Time.",['astro-ph.CO'],2501.08915," In this work we investigate, through a Bayesian study, the ability of a local
low matter density $\Omega_{\rm M}$, in discrepancy with the value usually
inferred from the CMB angular power spectrum, to accommodate observations from
local probes without being in tension with the local values of the Hubble
constant $H_0$ or the matter fluctuation $\sigma_8$ parameters. For that, we
combine multiple local probes, with the criteria that they either can constrain
the matter density parameter independently from the CMB constraints, or can
help in doing so after making their relevant observations more model
independent by relaxing their relevant calibration parameters. We assume
however, either a dynamical dark energy model, or the standard $\Lambda$CDM
model, when computing the corresponding theoretical observables. We also add,
in almost all of our Monte Carlo runs, the latest Baryonic acoustic
oscillations (BAO) measurements from the DESI year one release to our core
group. We found that, within $\Lambda$CDM model, for different combinations of
our probes, we can accommodate a low matter density along with the $H_0$ and
$\sigma_8$ values usually obtained from local probes, providing we promote the
sound drag $r_s$ component in BAO calculations to a free parameter, and that
even if we combine with the Pantheon+ Supernova sample. Assuming $w_0w_a$CDM,
we also found that relaxing $r_s$ allow us to accommodate $\Omega_{\rm M}$,
$H_0$ and $\sigma_8$ within their local values, with still however a preference
for $w_0w_a$ values far from $\Lambda$CDM. However, when including Pantheon+
Supernova sample, we found that the latter preference for high matter density
pushes $\sigma_8$ to much smaller values, mitigating by then a low matter
density solution to the two common tensions. We conclude that a low matter
density value, helps in preserving the concordance within $\Lambda$CDM model.
(abridged)",['astro-ph.CO'],False,,,,"Hybrid-z: Enhancing Kilo-Degree Survey bright galaxy sample photometric
  redshifts with deep learning","The case for a low dark matter density in dynamical dark energy model
  from local probes"
neg-d2-758,2025-01-09,,2501.056," Mentorship in open source software (OSS) is a vital, multifaceted process
that includes onboarding newcomers, fostering skill development, and enhancing
community building. This study examines task-focused mentoring strategies that
help mentees complete their tasks and the ideal personal qualities and outcomes
of good mentorship in OSS communities. We conducted two surveys to gather
contributor perceptions: the first survey, with 70 mentors, mapped 17 mentoring
challenges to 21 strategies that help support mentees. The second survey, with
85 contributors, assessed the importance of personal qualities and ideal
mentorship outcomes. Our findings not only provide actionable strategies to
help mentees overcome challenges and become successful contributors but also
guide current and future mentors and OSS communities in understanding the
personal qualities that are the cornerstone of good mentorship and the outcomes
that mentor-mentee pairs should aspire to achieve.",['cs.HC'],2503.14096," In 3D design, specifying design objectives and visualizing complex shapes
through text alone proves to be a significant challenge. Although advancements
in 3D GenAI have significantly enhanced part assembly and the creation of
high-quality 3D designs, many systems still to dynamically generate and edit
design elements based on the shape parameters. To bridge this gap, we propose
GenPara, an interactive 3D design editing system that leverages
text-conditional shape parameters of part-aware 3D designs and visualizes
design space within the Exploration Map and Design Versioning Tree.
Additionally, among the various shape parameters generated by LLM, the system
extracts and provides design outcomes within the user's regions of interest
based on Bayesian inference. A user study N = 16 revealed that \textit{GenPara}
enhanced the comprehension and management of designers with text-conditional
shape parameters, streamlining design exploration and concretization. This
improvement boosted efficiency and creativity of the 3D design process.",['cs.HC'],False,,,,"The Multifaceted Nature of Mentoring in OSS: Strategies, Qualities, and
  Ideal Outcomes","GenPara: Enhancing the 3D Design Editing Process by Inferring Users'
  Regions of Interest with Text-Conditional Shape Parameters"
neg-d2-759,2025-02-28,,2502.21027," In the context of the Horizon Europe project, METASAT, a hardware platform
was developed as a prototype of future space systems. The platform is based on
a multiprocessor NOEL-V, an established space-grade processor, which is
integrated with the SPARROW AI accelerator and connected to a GPU, Vortex. Both
processing systems follow the RISC-V specification. This is a novel hardware
architecture for the space domain as the use of massive parallel processing
units, such as GPUs, is starting to be considered for upcoming space missions
due to the increased performance required to future space-related workloads, in
particular, related to AI. However, such solutions are only currently adopted
for New Space, since their limitations come not only from the hardware, but
also from the software, which needs to be qualified before being deployed on an
institutional mission. For this reason, the METASAT platform is one of the
first endeavors towards enabling the use of high performance hardware in a
qualifiable environment for safety critical systems. The software stack is
based on baremetal, RTEMS and the XtratuM hypervisor, providing different
options for applications of various degrees of criticality.",['cs.AR'],2501.0733," ML and HPC applications increasingly combine dense and sparse memory access
computations to maximize storage efficiency. However, existing CPUs and GPUs
struggle to flexibly handle these heterogeneous workloads with consistently
high compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,
dual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical
interconnect and in-core streaming units (SUs) designed to accelerate dense and
sparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets
in 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense
linear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On
stencil codes, Occamy reaches an FPU utilization of 83% and a
technology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading
state-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On
sparse-dense linear algebra (LA), it achieves 42% FPU utilization and a
normalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x
and 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up
to 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.
Finally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and
graph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available
under a permissive open-source license.",['cs.AR'],False,,,,"A RISC-V Multicore and GPU SoC Platform with a Qualifiable Software
  Stack for Safety Critical Systems","Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System
  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET"
neg-d2-760,2025-03-12,,2503.09953," In this digital age, ensuring the security of digital data, especially the
image data is critically important. Image encryption plays an important role in
securing the online transmission/storage of images from unauthorized access. In
this regard, this paper presents a novel diffusion-confusion-based image
encryption algorithm named as X-CROSS. The diffusion phase involves a
dual-layer block permutation. It involves a bit-level permutation termed
Inter-Bit Transference (IBT) using a Bit-Extraction key, and pixel permutation
with a unique X-crosspermutation algorithm to effectively scramble the pixels
within an image. The proposed algorithm utilizes a resilient 2D chaotic map
with non-linear dynamical behavior, assisting in generating complex Extraction
Keys. After the permutation phase, the confusion phase proceeds with a dynamic
substitution technique on the permuted images, establishing the final
encryption layer. This combination of novel permutation and confusion results
in the removal of the image's inherent patterns and increases its resistance to
cyber-attacks. The close to ideal statistical security results for information
entropy, correlation, homogeneity, contrast, and energy validate the proposed
scheme's effectiveness in hiding the information within the image.",['cs.CR'],2503.10239," Super-apps have emerged as comprehensive platforms integrating various
mini-apps to provide diverse services. While super-apps offer convenience and
enriched functionality, they can introduce new privacy risks. This paper
reveals a new privacy leakage source in super-apps: mini-app interaction
history, including mini-app usage history (Mini-H) and operation history
(Op-H). Mini-H refers to the history of mini-apps accessed by users, such as
their frequency and categories. Op-H captures user interactions within
mini-apps, including button clicks, bar drags, and image views. Super-apps can
naturally collect these data without instrumentation due to the web-based
feature of mini-apps. We identify these data types as novel and unexplored
privacy risks through a literature review of 30 papers and an empirical
analysis of 31 super-apps. We design a mini-app interaction history-oriented
inference attack (THEFT), to exploit this new vulnerability. Using THEFT, the
insider threats within the low-privilege business department of the super-app
vendor acting as the adversary can achieve more than 95.5% accuracy in
inferring privacy attributes of over 16.1% of users. THEFT only requires a
small training dataset of 200 users from public breached databases on the
Internet. We also engage with super-app vendors and a standards association to
increase industry awareness and commitment to protect this data. Our
contributions are significant in identifying overlooked privacy risks,
demonstrating the effectiveness of a new attack, and influencing industry
practices toward better privacy protection in the super-app ecosystem.",['cs.CR'],False,,,,"X-Cross: Image Encryption Featuring Novel Dual-Layer Block Permutation
  and Dynamic Substitution Techniques","I Can Tell Your Secrets: Inferring Privacy Attributes from Mini-app
  Interaction History in Super-apps"
neg-d2-761,2025-03-17,,2503.1286," Given a graph $H$, a graph $G$ is $H$-free if $G$ does not contain $H$ as an
induced subgraph. Shi and Shan conjectured that every $1$-tough $2k$-connected
$(P_2 \cup kP_1)$-free graph is hamiltonian for $k \geq 4$. This conjecture has
been independently confirmed by Xu, Li, and Zhou, as well as by Ota and Sanka.
Inspired by this, we prove that every $2k$-connected $(P_2\cup kP_1)$-free
graph with toughness greater than one is hamiltonian-connected.",['math.CO'],2501.10921," A digraph is semicomplete multipartite if its underlying graph is a complete
multipartite graph. As a special case of semicomplete multipartite digraphs,
J{\o}rgensen et al. \cite{JG14} initiated the study of doubly regular team
tournaments. As a natural extension, we introduce doubly regular team
semicomplete multipartite digraphs and show that such digraphs fall into three
types. Furthermore, we give a characterization of all semicomplete multipartite
commutative weakly distance-regular digraphs.",['math.CO'],False,,,,"Every $2k$-connected $(P_2\cup kP_1)$-free graph with toughness greater
  than one is hamiltonian-connected",Semicomplete multipartite weakly distance-regular digraphs
neg-d2-762,2025-02-26,,2502.19259," Quantum spin liquid represents an intriguing state where electron spins are
highly entangled yet spin fluctuation persists even at 0 K. Recently, the
hexaaluminates \textit{R}MgAl$_{11}$O$_{19}$ (\textit{R} = rare earth) have
been proposed to be a platform for realizing the quantum spin liquid state with
dominant Ising anisotropic correlations. Here, we report detailed
low-temperature magnetic susceptibility, muon spin relaxation, and
thermodynamic studies on the CeMgAl$_{11}$O$_{19}$ single crystal. Ising
anisotropy is revealed by magnetic susceptibility measurements. Muon spin
relaxation and ac susceptibility measurements rule out any long-range magnetic
ordering or spin freezing down to 50 mK despite the onset of spin correlations
below $\sim$0.8 K. Instead, the spins keep fluctuating at a rate of 1.0(2) MHz
at 50 mK. Specific heat results indicate a gapless excitation with a power-law
dependence on temperature, $C_m(T) \propto T^{\alpha}$. The quasi-quadratic
temperature dependence with $\alpha$ = 2.28(4) in zero field and linear
temperature dependence in 0.25 T support the possible realization of the U(1)
Dirac quantum spin liquid state.",['cond-mat.str-el'],2501.07843," Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$
possesses novel triple-$\mathbf{q}$ magnetic order instead of conventional
single-$\mathbf{q}$ zigzag order. Here we present dedicated experiments in
search for distinct properties expected of the triple-$\mathbf{q}$ order,
namely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking
fields and fictitious magnetic fields generated by the spin vorticity. In
structurally pristine single crystals, we show that $C_3$ symmetry-breaking
in-plane uniaxial strains do not affect the order's magnetic neutron
diffraction signals. We further show that $\mathbf{c}$-axis propagating light
exhibits large Faraday rotations in the ordered state due to the spin
vorticity, the sign of which can be trained via the system's ferrimagnetic
moment. These results are in favor of the triple-$\mathbf{q}$ order in
Na$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",['cond-mat.str-el'],False,,,,"U(1) Dirac quantum spin liquid candidate in triangular-lattice
  antiferromagnet CeMgAl$_{11}$O$_{19}$","Robust triple-q magnetic order with trainable spin vorticity in
  Na$_2$Co$_2$TeO$_6$"
neg-d2-763,2025-02-21,,2502.15943," HeH$^+$ belongs to the class of ""reactive"" ions that can be destroyed so
quickly that chemical formation and destruction rates may compete with
inelastic rates and should be considered when solving the statistical
equilibrium equations. This so-called chemical ""pumping"" or ""excitation"" effect
is investigated here for the first time in HeH$^+$. The chemical evolution of
HeH$^+$ in NGC 7027 is modeled with the CLOUDY photoionization code using
updated reaction rate coefficients. The non-LTE analysis of the three observed
HeH$^+$ emission lines is then performed with the CLOUDY and RADEX codes using
an extensive set of spectroscopic and inelastic collisional data suitable for
the specific high-temperature environment of NGC 7027. In a second approach,
chemical formation and destruction rates of HeH$^+$ are implemented in RADEX.
This code is combined with MCMC sampling (performed on the RADEX-parameters
space) to extract the best-fit HeH$^+$ column density and physical conditions
from the observed line fluxes. The CLOUDY and RADEX non-LTE results are found
to be in good agreement, and the $\upsilon=1-0 \ P(2)/P(1)$ line ratio is
better than 20%. Agreement to better than a factor of 2.3 is obtained when
including the reaction between He($2^3S$) and H as an additional source of
HeH$^+$. The RADEX/MCMC model with chemical pumping is found to reproduce both
the observed line fluxes and the line ratio to 20%. Our results suggest that
additional HeH$^+$ lines must be detected in NGC 7027 to better constrain the
physical conditions via non-LTE models. Uncertainties in collisional (reactive
and inelastic) data of HeH$^+$ have been largely reduced in this work. The
three observed lines are not sensitive to chemical pumping while excited
""short-lived"" levels are significantly overpopulated with respect to a non-LTE
model neglecting chemical excitation.",['astro-ph.GA'],2503.04894," We explore the evolution of galaxy sizes at high redshift ($3 < z < 13$)
using the high-resolution THESAN-ZOOM radiation-hydrodynamics simulations,
focusing on the mass range of $10^6\,\mathrm{M}_{\odot} < \mathrm{M}_{\ast} <
10^{10}\,\mathrm{M}_{\odot}$. Our analysis reveals that galaxy size growth is
tightly coupled to bursty star formation. Galaxies above the star-forming main
sequence experience rapid central compaction during starbursts, followed by
inside-out quenching and spatially extended star formation that leads to
expansion, causing oscillatory behavior around the size-mass relation. Notably,
we find a positive intrinsic size-mass relation at high redshift, consistent
with observations but in tension with large-volume simulations. We attribute
this discrepancy to the bursty star formation captured by our multi-phase
interstellar medium framework, but missing from simulations using the effective
equation-of-state approach with hydrodynamically decoupled feedback. We also
find that the normalization of the size-mass relation follows a double power
law as a function of redshift, with a break at $z\approx6$, because the
majority of galaxies at $z > 6$ show rising star-formation histories, and
therefore are in a compaction phase. We demonstrate that H$\alpha$ emission is
systematically extended relative to the UV continuum by a median factor of 1.7,
consistent with recent JWST studies. However, in contrast to previous
interpretations that link extended H$\alpha$ sizes to inside-out growth, we
find that Lyman-continuum (LyC) emission is spatially disconnected from
H$\alpha$. Instead, a simple Str\""{o}mgren sphere argument reproduces observed
trends, suggesting that extreme LyC production during central starbursts is the
primary driver of extended nebular emission.",['astro-ph.GA'],False,,,,"Chemistry and ro-vibrational excitation of HeH$^+$ in the Planetary
  Nebula NGC 7027","The THESAN-ZOOM project: central starbursts and inside-out quenching
  govern galaxy sizes in the early Universe"
neg-d2-764,2025-01-23,,2501.13886," The stochastic three points (STP) algorithm is a derivative-free optimization
technique designed for unconstrained optimization problems in $\mathbb{R}^d$.
In this paper, we analyze this algorithm for three classes of functions: smooth
functions that may lack convexity, smooth convex functions, and smooth
functions that are strongly convex. Our work provides the first almost sure
convergence results of the STP algorithm, alongside some convergence results in
expectation. For the class of smooth functions, we establish that the best
gradient iterate of the STP algorithm converges almost surely to zero at a rate
arbitrarily close to $o(\frac{1}{\sqrt{T}})$, where $T$ is the number of
iterations. Furthermore, within the same class of functions, we establish both
almost sure convergence and convergence in expectation of the final gradient
iterate towards zero. For the class of smooth convex functions, we establish
that $f(\theta^T)$ converges to $\inf_{\theta \in \mathbb{R}^d} f(\theta)$
almost surely at a rate arbitrarily close to $o(\frac{1}{T})$, and in
expectation at a rate of $O(\frac{d}{T})$ where $d$ is the dimension of the
space. Finally, for the class of smooth functions that are strongly convex, we
establish that when step sizes are obtained by approximating the directional
derivatives of the function, $f(\theta^T)$ converges to $\inf_{\theta \in
\mathbb{R}^d} f(\theta)$ in expectation at a rate of $O((1-\frac{\mu}{dL})^T)$,
and almost surely at a rate arbitrarily close to $o((1-\frac{\mu}{dL})^T)$,
where $\mu$ and $L$ are the strong convexity and smoothness parameters of the
function.",['math.OC'],2501.13886," The stochastic three points (STP) algorithm is a derivative-free optimization
technique designed for unconstrained optimization problems in $\mathbb{R}^d$.
In this paper, we analyze this algorithm for three classes of functions: smooth
functions that may lack convexity, smooth convex functions, and smooth
functions that are strongly convex. Our work provides the first almost sure
convergence results of the STP algorithm, alongside some convergence results in
expectation. For the class of smooth functions, we establish that the best
gradient iterate of the STP algorithm converges almost surely to zero at a rate
arbitrarily close to $o(\frac{1}{\sqrt{T}})$, where $T$ is the number of
iterations. Furthermore, within the same class of functions, we establish both
almost sure convergence and convergence in expectation of the final gradient
iterate towards zero. For the class of smooth convex functions, we establish
that $f(\theta^T)$ converges to $\inf_{\theta \in \mathbb{R}^d} f(\theta)$
almost surely at a rate arbitrarily close to $o(\frac{1}{T})$, and in
expectation at a rate of $O(\frac{d}{T})$ where $d$ is the dimension of the
space. Finally, for the class of smooth functions that are strongly convex, we
establish that when step sizes are obtained by approximating the directional
derivatives of the function, $f(\theta^T)$ converges to $\inf_{\theta \in
\mathbb{R}^d} f(\theta)$ in expectation at a rate of $O((1-\frac{\mu}{dL})^T)$,
and almost surely at a rate arbitrarily close to $o((1-\frac{\mu}{dL})^T)$,
where $\mu$ and $L$ are the strong convexity and smoothness parameters of the
function.",['math.OC'],False,,,,On the Almost Sure Convergence of the Stochastic Three Points Algorithm,On the Almost Sure Convergence of the Stochastic Three Points Algorithm
neg-d2-765,2025-03-06,,2503.0418," The transition to a sustainable, low-carbon energy future requires
transformative advancements in energy and environmental technologies. Carbon
capture and sequestration, underground hydrogen storage, and nuclear waste
geological disposal will be central aspects of a sustainable energy future,
both for mitigating CO2 emissions and providing green energy. A comprehensive
understanding of multiphase flow through porous media, along with reactive
transport and microbial activities, is essential for assessing the feasibility
and managing the risks of these technologies. Microfluidic porous media
platforms have emerged as powerful tools for the direct visualization of
multiphase reactive flow in porous media and eventually optimizing these
multiple physicochemical and biological processes. This review highlights
critical scientific challenges associated with these sustainable energy
solutions and summarizes the state-of-the-art microfluidic techniques for
studying the interplay between multiphase flow, reactive transport, and
biological effects in porous media. We provide a comprehensive overview of how
these microfluidic approaches enhance the understanding of fundamental
pore-scale dynamics and bridge the gap between pore-scale events and
large-scale processes. This review is expected to promote both experimental and
theoretical understanding of multiphase reactive flow in porous media, thereby
informing material design, process optimization, and predictive modeling for
scalable implementation. By fostering interdisciplinary collaboration across
microfluidics, fluid mechanics, geophysics, materials science, and subsurface
engineering, we hope to accelerate innovation and advance sustainable energy
solutions.",['physics.flu-dyn'],2502.19875," Numerical simulations are a valuable research and layout tool for fluid flow
problems, yet repeated evaluations of parametrized problems, necessary to solve
optimization problems, can be very costly. One option to speed up this process
is to replace the costly CFD model with a cheaper one. These surrogate models
can be either data-driven or they can also rely on reduced basis (RB) methods
to speed up the calculations. In contrast to data-driven surrogate models, the
latter are not based on regression techniques but are still aimed at explicitly
solving the conservation equations. Their speed-up comes from a strong
reduction of the solution space, which results in much smaller algebraic
systems that need to be solved. Within this work, an RB model, suited for
slightly compressible flow, is presented and tested on different flow
configurations. The model is stabilized using a Petrov-Galerkin method with
trial and test function spaces of different dimensionality to generate stable
results for a wide range of Reynolds numbers. The presented model applies to
geometrically and physically parametrized flow problems. Finally, a data-driven
approach was used to extend it to turbulent flows.",['physics.flu-dyn'],False,,,,Advancing sustainable energy solutions with microfluidic porous media,Reduced Basis Model for Compressible Flow
neg-d2-766,2025-02-14,,2502.10186," The radius distribution of close-in planets has been observed to have a
bimodal distribution with a dearth of planets around ~1.5-2.0 $R_\oplus$
commonly referred to as the ''radius valley''. The origin of the valley is
normally attributed to mass-loss process such as photoevaporation or
core-powered mass loss. Recent work, however, has suggested that the radius
valley may instead arise as a consequence of gas accretion by low-mass planets.
In this work we therefore aim to investigate the formation of a primordial
radius valley from the formation of planet cores through pebble accretion up
until the dissipation of the protoplanetary disc and subsequent contraction of
accreted atmospheres. The goal of this work is to explore the conditions for
forming a primordial radius valley from first principles of planet formation
theory, rather than attempting to explain the detailed structure of the
observed valley. We use an analytical model with minimal assumptions to
estimate the contraction rate of atmospheres and, indeed, find the formation of
a primordial radius valley. The planets smaller than the valley did not reach
the pebble isolation mass, which is required for the planets to cool down
sufficiently to be able to accrete a significant amount of gas. We also
estimate the slopes of the radius gap as a function of orbital period for the
intrinsic population as well as for planets with orbital periods <100 days. For
the intrinsic population, the radius gap follows the pebble isolation mass and
increases with increasing orbital period, while for close-in planets the
direction of the slope reverses and decreases with increasing orbital period.
We find that planets smaller than the radius valley are predominantly rocky
while the population of planets larger than the valley consists of a mixture of
rocky and water-rich planets.",['astro-ph.EP'],2501.0752," We present the results of combined hydrodynamic and particle tracking
post-processing modeling to study the transport of small dust in a
protoplanetary disk containing an embedded embryo in 3D. We use a suite of
FARGO3D hydrodynamic simulations of disks containing a planetary embryo varying
in mass up to 300 $M_\oplus$ on a fixed orbit in both high and low viscosity
disks. We then simulate solid particles through the disk as a post-processing
step using a Monte Carlo integration, allowing us to track the trajectories of
individual particles as they travel throughout the disk. We find that gas
advection onto the planet can carry small, well-coupled solids across the gap
opened in the disk by the embedded planet for planetary masses above the pebble
isolation mass. This mixing between the inner and outer disk can occur in both
directions, with solids in the inner disk mixing to the outer disk as well.
Additionally, in low viscosity disks, multiple pile-ups in the outer disk may
preserve isotopic heterogeneities, possibly providing an outermost tertiary
isotopic reservoir. Throughout Jupiter's growth, the extent of mixing between
isotopic reservoirs varied depending on dust size, gas turbulence, and the
Jovian embryo mass.",['astro-ph.EP'],False,,,,A primordial radius valley as a consequence of planet formation,"Three-dimensional transport of solids in a protoplanetary disk
  containing a growing giant planet"
neg-d2-767,2025-03-06,,2503.04522," Assessing the quality of automatic image segmentation is crucial in clinical
practice, but often very challenging due to the limited availability of ground
truth annotations. In this paper, we introduce In-Context Reverse
Classification Accuracy (In-Context RCA), a novel framework for automatically
estimating segmentation quality in the absence of ground-truth annotations. By
leveraging recent in-context learning segmentation models and incorporating
retrieval-augmentation techniques to select the most relevant reference images,
our approach enables efficient quality estimation with minimal reference data.
Validated across diverse medical imaging modalities, our method demonstrates
robust performance and computational efficiency, offering a promising solution
for automated quality control in clinical workflows, where fast and reliable
segmentation assessment is essential. The code is available at
https://github.com/mcosarinsky/In-Context-RCA.",['cs.CV'],2502.07302," Multi-class cell segmentation in high-resolution gigapixel whole slide images
(WSIs) is crucial for various clinical applications. However, training such
models typically requires labor-intensive, pixel-wise annotations by domain
experts. Recent efforts have democratized this process by involving lay
annotators without medical expertise. However, conventional non-corrective
approaches struggle to handle annotation noise adaptively because they lack
mechanisms to mitigate false positives (FP) and false negatives (FN) at both
the image-feature and pixel levels. In this paper, we propose a consensus-aware
self-corrective AI agent that leverages the Consensus Matrix to guide its
learning process. The Consensus Matrix defines regions where both the AI and
annotators agree on cell and non-cell annotations, which are prioritized with
stronger supervision. Conversely, areas of disagreement are adaptively weighted
based on their feature similarity to high-confidence consensus regions, with
more similar regions receiving greater attention. Additionally, contrastive
learning is employed to separate features of noisy regions from those of
reliable consensus regions by maximizing their dissimilarity. This paradigm
enables the model to iteratively refine noisy labels, enhancing its robustness.
Validated on one real-world lay-annotated cell dataset and two reasoning-guided
simulated noisy datasets, our method demonstrates improved segmentation
performance, effectively correcting FP and FN errors and showcasing its
potential for training robust models on noisy datasets. The official
implementation and cell annotations are publicly available at
https://github.com/ddrrnn123/CASC-AI.",['cs.CV'],False,,,,"In-Context Reverse Classification Accuracy: Efficient Estimation of
  Segmentation Quality without Ground-Truth","CASC-AI: Consensus-aware Self-corrective Learning for Noise Cell
  Segmentation"
neg-d2-768,2025-03-17,,2503.13045," Image retrieval is the task of finding images in a database that are most
similar to a given query image. The performance of an image retrieval pipeline
depends on many training-time factors, including the embedding model
architecture, loss function, data sampler, mining function, learning rate(s),
and batch size. In this work, we run tens of thousands of training runs to
understand the effect each of these factors has on retrieval accuracy. We also
discover best practices that hold across multiple datasets. The code is
available at https://github.com/gmberton/image-retrieval",['cs.CV'],2503.07456," Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding
clinicians and radiologists by automatically retrieving relevant patient cases
in the database given the query image and/or report, for more efficient
clinical diagnosis and treatment, especially for rare diseases. However
conventional ITR systems typically only rely on global image or text
representations for measuring patient image/report similarities, which overlook
local distinctiveness across patient cases. This often results in suboptimal
retrieval performance. In this paper, we propose an Anatomical
Location-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a
query image and the associated suspicious anatomical region(s), aims to
retrieve similar patient cases exhibiting the same disease or symptoms in the
same anatomical region. To perform location-conditioned multimodal retrieval,
we learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with
semantic global-level and region-/word-level alignment to produce
generalizable, well-aligned multi-modal representations. Additionally, we
perform location-conditioned contrastive learning to further utilize cross-pair
region-level contrastiveness for improved multi-modal retrieval. We show that
our proposed RRA-VL achieves state-of-the-art localization performance in
phase-grounding tasks, and satisfying multi-modal retrieval performance with or
without location conditioning. Finally, we thoroughly investigate the
generalizability and explainability of our proposed ALC-ITR system in providing
explanations and preliminary diagnosis reports given retrieved patient cases
(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.",['cs.CV'],False,,,,All You Need to Know About Training Image Retrieval Models,Anatomy-Aware Conditional Image-Text Retrieval
neg-d2-769,2025-02-12,,2502.08377," Recently, the generation of dynamic 3D objects from a video has shown
impressive results. Existing methods directly optimize Gaussians using whole
information in frames. However, when dynamic regions are interwoven with static
regions within frames, particularly if the static regions account for a large
proportion, existing methods often overlook information in dynamic regions and
are prone to overfitting on static regions. This leads to producing results
with blurry textures. We consider that decoupling dynamic-static features to
enhance dynamic representations can alleviate this issue. Thus, we propose a
dynamic-static feature decoupling module (DSFD). Along temporal axes, it
regards the regions of current frame features that possess significant
differences relative to reference frame features as dynamic features.
Conversely, the remaining parts are the static features. Then, we acquire
decoupled features driven by dynamic features and current frame features.
Moreover, to further enhance the dynamic representation of decoupled features
from different viewpoints and ensure accurate motion prediction, we design a
temporal-spatial similarity fusion module (TSSF). Along spatial axes, it
adaptively selects similar information of dynamic regions. Hinging on the
above, we construct a novel approach, DS4D. Experimental results verify our
method achieves state-of-the-art (SOTA) results in video-to-4D. In addition,
the experiments on a real-world scenario dataset demonstrate its effectiveness
on the 4D scene. Our code will be publicly available.",['cs.CV'],2502.05275," Reliable failure detection holds paramount importance in safety-critical
applications. Yet, neural networks are known to produce overconfident
predictions for misclassified samples. As a result, it remains a problematic
matter as existing confidence score functions rely on category-level signals,
the logits, to detect failures. This research introduces an innovative
strategy, leveraging human-level concepts for a dual purpose: to reliably
detect when a model fails and to transparently interpret why. By integrating a
nuanced array of signals for each category, our method enables a finer-grained
assessment of the model's confidence. We present a simple yet highly effective
approach based on the ordinal ranking of concept activation to the input image.
Without bells and whistles, our method significantly reduce the false positive
rate across diverse real-world image classification benchmarks, specifically by
3.7% on ImageNet and 9% on EuroSAT.",['cs.CV'],False,,,,"Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling
  Dynamic-Static Features",Interpretable Failure Detection with Human-Level Concepts
neg-d2-770,2025-01-23,,2501.1391," The $\frac12$-BPS indices of $\mathcal{N}=4$ Super Yang-Mills theory with
unitary, orthogonal, and symplectic groups all admit $q$-expansions suggesting
an interpretation in terms of D-branes in the dual bulk AdS$_5$ string
theories. We present a derivation of these expansions in the corresponding bulk
duals by quantizing the moduli space of $\frac12$-BPS giant gravitons using
supersymmetric localization, extending and clarifying our study in
arxiv:2312.14921. We perform a detailed analysis of the one-loop fluctuations
around the maximal giants (the fixed points), and show how the Hamiltonian
analysis is recovered from the functional integral for the equivariant index.
We show that the analytic continuation for these giant graviton expansions
observed in the literature maps precisely to a wall-crossing phenomenon for the
index. In the case of orthogonal and symplectic gauge groups, the
$\mathbb{Z}_2$ quotient in the bulk leads to a corresponding projection in the
$q$-expansion. Additional terms in the expansion related to the Pfaffian
operator arise from topologically stable branes in the bulk dual on AdS$_5
\times \mathbb{RP}^5$.",['hep-th'],2503.14673," We derive a manifestly superconformally covariant unfolded formulation of the
free (2,0) tensor multiplet. The unfolded system consists of an abelian
two-form and an infinite-dimensional, chiral Weyl zero-form realized using
superoscillators. The construction of the cocycle gluing these forms on a
general superconformal background goes one step beyond previous results in
super-Poincar\'e backgrounds.",['hep-th'],False,,,,Localization and wall-crossing of giant graviton expansions in AdS$_5$,Unfolding the Six-Dimensional Tensor Multiplet
neg-d2-771,2025-02-10,,2502.06451," The population of black holes observed via gravitational waves currently
covers the local universe up to a redshift $z\lesssim 1$, for the most massive
merging binaries, or $z\lesssim 0.25$ for low-mass BH binaries (BBH). Evolution
of the BBH mass spectrum over cosmic time will be a significant probe of
formation channels and environments. We demonstrate a reconstruction of the BBH
merger rate, allowing for general dependence on binary masses and luminosity
distance or redshift and accounting for selection effects, via iterative kernel
density estimation (KDE) with optimized multidimensional bandwidths. Performing
such reconstructions under a range of detailed assumptions, we see no
significant evidence for the evolution of BBH masses with redshift, over the
range where detected events are available; at most, a possible trend towards
increasing merger rate with redshift for primary masses $m_1\gtrsim
50\,M_\odot$ is supported. We compare these findings with previous
investigations and caution against over-interpreting the current, sparse, data.
Significantly upgraded detectors and/or facilities, and longer observing times,
are required to harness any correlations of the BBH mass distribution with
redshift.",['gr-qc'],2501.03356," An approach is presented to address singularities in general relativity using
a complex Riemannian spacetime extension. We demonstrate how this method can be
applied to both black hole and cosmological singularities, specifically
focusing on the Schwarzschild and Kerr black holes and the
Friedmann-Lema\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending
the relevant coordinates into the complex plane and carefully choosing
integration contours, we show that it is possible to regularize these
singularities, resulting in physically meaningful, singularity-free solutions
when projected back onto real spacetime. The removal of the singularity at the
Big Bang allows for a bounce cosmology. This approach offers a potential bridge
between classical general relativity and quantum gravity effects, suggesting a
way to resolve longstanding issues in gravitational physics without requiring a
full theory of quantum gravity.",['gr-qc'],False,,,,"Looking To The Horizon: Probing Evolution in the Black Hole Spectrum
  With GW Catalogs","Complex Riemannian spacetime and singularity-free black holes and
  cosmology"
neg-d2-772,2025-01-15,,2501.08582," Existing low-rank adaptation (LoRA) methods face challenges on sparse large
language models (LLMs) due to the inability to maintain sparsity. Recent works
introduced methods that maintain sparsity by augmenting LoRA techniques with
additional masking mechanisms. Despite these successes, such approaches suffer
from an increased memory and computation overhead, which affects efficiency of
LoRA methods. In response to this limitation, we introduce LoRS, an innovative
method designed to achieve both memory and computation efficiency when
fine-tuning sparse LLMs. To mitigate the substantial memory and computation
demands associated with preserving sparsity, our approach incorporates
strategies of weight recompute and computational graph rearrangement. In
addition, we also improve the effectiveness of LoRS through better adapter
initialization. These innovations lead to a notable reduction in memory and
computation consumption during the fine-tuning phase, all while achieving
performance levels that outperform existing LoRA approaches.",['cs.CL'],2502.0808," Decomposition of text into atomic propositions is a flexible framework
allowing for the closer inspection of input and output text. We use atomic
decomposition of hypotheses in two natural language reasoning tasks,
traditional NLI and defeasible NLI, to form atomic sub-problems, or granular
inferences that models must weigh when solving the overall problem. These
atomic sub-problems serve as a tool to further understand the structure of both
NLI and defeasible reasoning, probe a model's consistency and understanding of
different inferences, and measure the diversity of examples in benchmark
datasets. Our results indicate that LLMs still struggle with logical
consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify
critical atomic sub-problems of defeasible NLI examples, or those that most
contribute to the overall label, and propose a method to measure the
inferential consistency of a model, a metric designed to capture the degree to
which a model makes consistently correct or incorrect predictions about the
same fact under different contexts.",['cs.CL'],False,,,,LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model,NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals
neg-d2-773,2025-02-10,,2502.06974," We characterise when a rank $n$ generalised Baumslag-Solitar group is CAT(0)
and when it is biautomatic.",['math.GR'],2501.04112," In this work, we provide the first example of an infinite family of branch
groups in the class of non-contracting self-similar groups. We show that these
groups are very strongly fractal, not regular branch, and of exponential
growth. Further, we prove that these groups do not have the congruence subgroup
property by explicitly calculating the structure of their rigid kernels. This
class of groups is also the first example of branch groups with non-torsion
rigid kernels. As a consequence of these results, we also determine the
Hausdorff dimension of these groups.",['math.GR'],False,,,,Higher-rank GBS groups: non-positive curvature and biautomaticity,A Class of Non-Contracting Branch Groups with Non-Torsion Rigid Kernels
neg-d2-774,2025-03-16,,2503.12476," A debate persists regarding the correlation between the DIBs 9577 and 9632
\r{A}, and whether they share a common molecular carrier (i.e., C$_{60}^{+}$).
A robust high correlation determination emerges after bridging the baseline
across an order of magnitude ($\simeq 50 - 700$ m\r{A}, $r=0.93\pm0.02$), and
nearly doubling the important higher equivalent width domain by adding new Mg
II-corrected sightlines. Moreover, additional evidence is presented of possible
DIB linkages to fullerenes, whereby attention is drawn to DIBs at 7470.38,
7558.44, and 7581.47 \r{A}, which match the Campbell experimental results for
C$_{70}^{+}$ within 1 \r{A}, and the same is true of 6926.48 and 7030.26 \r{A}
for C$_{70}^{2+}$. Yet their current correlation uncertainties are
unsatisfactory and exacerbated by expectedly low EWs (e.g., $\overline{EW}=4$
m\r{A} for 6926.48 \r{A}), and thus further observations are required to assess
whether they represent a bona fide connection or numerical coincidence.",['astro-ph.GA'],2502.01728," Gravitational microlensing is a unique probe of the stellar content in strong
lens galaxies. Flux ratio anomalies from gravitationally lensed supernovae
(glSNe), just like lensed quasars, can be used to constrain the stellar mass
fractions at the image positions. Type Ia supernovae are of particular interest
as knowledge of the intrinsic source brightness helps constrain the amount of
(de)magnification from the macromodel predictions that might be due to
microlensing. In addition, the presence or absence of caustic crossings in the
light curves of glSNe can be used to constrain the mass of the microlenses. We
find that a sample of 50 well-modeled glSNe Ia systems with single epoch
observations at peak intrinsic supernova luminosity should be able to constrain
a stellar mass-to-light ratio to within $\sim 15\%$. A set of systems with
light curve level information providing the location (or absence) of caustic
crossing events can also constrain the mass of the microlenses to within $\sim
50\%$. Much work is needed to make such a measurement in practice, but our
results demonstrate the feasibility of microlensing to place constraints on
astrophysical parameters related to the initial mass function of lensing
galaxies without any prior assumptions on the stellar mass.",['astro-ph.GA'],False,,,,"Strengthening the Link Between Fullerenes and a Subset of Diffuse
  Interstellar Bands","Stars as cosmic scales: measuring stellar mass with microlensed
  supernovae"
neg-d2-775,2025-01-16,,2501.09874," Inspired by previous studies and pioneers of the field, we present new
results on an extensive EPR-Bell experiment using photons generated by
parametric down conversion, where one of the photons is deliberately
phase-shifted. Our experiments show some surprising results for particular
angles of this phase shift.",['quant-ph'],2502.04996," The Tilloy-Di\'osi (TD) prescription allows to turn any Markovian spontaneous
collapse model that can be formally regarded as a continuous measurement
process of the mass density into a hybrid classical-quantum theory in which the
gravitational Newtonian field enters as a classical field. We study the
application of a similar idea to the Poissonian Spontaneous Localization (PSL)
model. As in the TD case, the Newtonian classical field is again recovered upon
averaging, and additional decoherence appears due to the gravitational
back-reaction. We study general features of this model and investigate the
dynamics of a single particle and a rigid spherical body. With respect to the
TD models, the PSL model presents some notable differences such as the absence
of long-range decoherence due to the gravitational back-reaction noise and the
absence of negative mass measurements.",['quant-ph'],False,,,,Phase-Shifted Bell States,Newtonian Gravity from a Poissonian Spontaneous Collapse Model
neg-d2-776,2025-03-21,,2503.1718," Aims. We study the behaviour of Cl abundance and its ratios with respect to
O, S and Ar abundances in a sample of more than 200 spectra of Galactic and
extragalactic H ii regions and star-forming galaxies (SFGs) of the local
Universe. Methods. We use the DEep Spectra of Ionised REgions Database
(DESIRED) Extended project (DESIRED-E) that comprises more than 2000 spectra of
H ii regions and SFGs with direct determinations of electron temperature
($T_e$). From this database we select those spectra where it is possible to
determine the Cl$^{2+}$ abundance and whose line ratios meet certain
observational criteria. We calculate the physical conditions and Cl, O, S and
Ar abundances in an homogeneous manner for all the spectra. We compare with
results of photoionisation models to carry out an analysis of which is the most
appropriate $T_e$ indicator for the nebular volume where Cl$^{2+}$ lies,
proposing a scheme that improves the determination of the Cl$^{2+}$ abundance.
We compare the Cl/O ratios obtained using two different ionisation correction
factor (ICF) schemes. We also compare the nebular Cl/O distribution with
stellar determinations. Results. Our analysis indicates that the ICF scheme
proposed by Izotov et al. (2006) better reproduces the observed distributions
of the Cl/O ratio. We find that the log(Cl/O) vs. 12+log(O/H) and log(Cl/Ar)
vs. 12+log(Ar/H) distributions are not correlated in the whole metallicity
range covered by our objects indicating a lockstep evolution of those elements.
In contrast, the log(Cl/S) vs. 12+log(S/H) distribution shows a weak
correlation with a slight negative slope.",['astro-ph.GA'],2502.08984," We present [NII] 205 $\mu$m fine structure line observations of three
submillimeter galaxies (SMGs) and three quasar host galaxies at
4$\lesssim$z$\lesssim$6 using the Institut de radioastronomie millim\'etrique
(IRAM) interferometer. The [NII] emission is detected in three sources, and we
report detections of the underlying dust continuum emission in all sources. The
observed [NII]-to-infrared luminosity ratio spans at least 0.5 dex for our
sources. Comparing our estimates with sources detected in the [NII] 205 $\mu$m
at similar redshifts shows that the overall [NII]-to-IR luminosity ratio spans
over a dex in magnitude from L$_{[NII]}$/L$_{IR}$ ~ 10$^{-4}$ - 10$^{-5}$ and
follows the trend of the so-called [NII] fine structure line deficit observed
in (ultra)-luminous infrared galaxies in the local Universe. The [CII]-to-[NII]
luminosity ratio is >10 for most of our sources, indicating that the bulk of
the [CII] 158 $\mu$m line emission (f([CII]$^{PDR}$)>75%) arises from the
neutral medium. From our analysis, we do not find significant differences in
the [NII] 205 $\mu$m emission and the respective ratios between SMGs and QSOs,
suggesting a negligible contribution to the boosting of [NII] 205 $\mu$m
emission due to the active galactic nucleus (AGN) photoionization. Future
investigations involving other fine structure lines and optical diagnostics
will provide further insight into a suite of ionized medium properties and
reveal the diversity between AGN and non-AGN environments.",['astro-ph.GA'],False,,,,Chlorine abundances in star-forming regions of the local Universe,The [NII] 205 $\mu$m line emission from high-z SMGs and QSOs
neg-d2-777,2025-01-18,,2501.10921," A digraph is semicomplete multipartite if its underlying graph is a complete
multipartite graph. As a special case of semicomplete multipartite digraphs,
J{\o}rgensen et al. \cite{JG14} initiated the study of doubly regular team
tournaments. As a natural extension, we introduce doubly regular team
semicomplete multipartite digraphs and show that such digraphs fall into three
types. Furthermore, we give a characterization of all semicomplete multipartite
commutative weakly distance-regular digraphs.",['math.CO'],2502.13518," We generalize the Rubik's cube, together with its group of configurations, to
any abstract regular polytope. After discussing general aspects, we study the
Rubik's simplex of arbitrary dimension and provide a complete description of
the associated group. We sketch an analogous argument for the Rubik's hypercube
as well.",['math.CO'],False,,,,Semicomplete multipartite weakly distance-regular digraphs,Rubik's Abstract Polytopes
neg-d2-778,2025-01-29,,2501.17997," Developing algorithms to search through data efficiently is a challenging
part of searching for signs of technology beyond our solar system. We have
built a digital signal processing system and computer cluster on the backend of
the Karl G. Jansky Very Large Array (VLA) in New Mexico in order to search for
signals throughout the Galaxy consistent with our understanding of artificial
radio emissions. In our first paper, we described the system design and
software pipelines. In this paper, we describe a postprocessing pipeline to
identify persistent sources of interference, filter out false positives, and
search for signals not immediately identifiable as anthropogenic radio
frequency interference during the VLA Sky Survey. As of 01 September 2024, the
Commensal Open-source Multi-mode Interferometric Cluster had observed more than
950,000 unique pointings. This paper presents the strategy we employ when
commensally observing during the VLA Sky Survey and a postprocessing strategy
for the data collected during the survey. To test this postprocessing pipeline,
we searched toward 511 stars from the $Gaia$ catalog with coherent beams. This
represents about 30 minutes of observation during VLASS, where we typically
observe about 2000 sources per hour in the coherent beamforming mode. We did
not detect any unidentifiable signals, setting isotropic power limits ranging
from 10$^{11}$ to 10$^{16}$W.",['astro-ph.IM'],2501.17997," Developing algorithms to search through data efficiently is a challenging
part of searching for signs of technology beyond our solar system. We have
built a digital signal processing system and computer cluster on the backend of
the Karl G. Jansky Very Large Array (VLA) in New Mexico in order to search for
signals throughout the Galaxy consistent with our understanding of artificial
radio emissions. In our first paper, we described the system design and
software pipelines. In this paper, we describe a postprocessing pipeline to
identify persistent sources of interference, filter out false positives, and
search for signals not immediately identifiable as anthropogenic radio
frequency interference during the VLA Sky Survey. As of 01 September 2024, the
Commensal Open-source Multi-mode Interferometric Cluster had observed more than
950,000 unique pointings. This paper presents the strategy we employ when
commensally observing during the VLA Sky Survey and a postprocessing strategy
for the data collected during the survey. To test this postprocessing pipeline,
we searched toward 511 stars from the $Gaia$ catalog with coherent beams. This
represents about 30 minutes of observation during VLASS, where we typically
observe about 2000 sources per hour in the coherent beamforming mode. We did
not detect any unidentifiable signals, setting isotropic power limits ranging
from 10$^{11}$ to 10$^{16}$W.",['astro-ph.IM'],False,,,,"COSMIC's Large-Scale Search for Technosignatures during the VLA sky
  Survey: Survey Description and First Results","COSMIC's Large-Scale Search for Technosignatures during the VLA sky
  Survey: Survey Description and First Results"
neg-d2-779,2025-02-10,,2502.06985," Online communities can offer many benefits for youth including peer learning,
cultural expression, and skill development. However, most HCI research on
youth-focused online communities has centered communities developed by adults
for youth rather than by the youth themselves. In this work, we interviewed 11
teenagers (ages 13-17) who moderate online Discord communities created by
youth, for youth. Participants were identified by Discord platform staff as
leaders of well-moderated servers through an intensive exam and
application-based process. We also interviewed 2 young adults who volunteered
as mentors of some of our teen participants. We present our findings about the
benefits, motivations, and risks of teen-led online communities, as well as the
role of external stakeholders of these youth spaces. We contextualize our work
within the broader teen online safety landscape to provide recommendations to
better support, encourage, and protect teen moderators and their online
communities. This empirical work contributes one of the first studies to date
with teen Discord moderators and aims to empower safe youth-led online
communities.",['cs.HC'],2501.06901," The field of serious games for health has grown significantly, demonstrating
effectiveness in various clinical contexts such as stroke, spinal cord injury,
and degenerative neurological diseases. Despite their potential benefits,
therapists face barriers to adopting serious games in rehabilitation, including
limited training and game literacy, concerns about cost and equipment
availability, and a lack of evidence-based research on game effectiveness.
Serious games for rehabilitation often involve repetitive exercises, which can
be tedious and reduce motivation for continued rehabilitation, treating clients
as passive recipients of clinical outcomes rather than players. This study
identifies gaps and provides essential insights for advancing serious games in
rehabilitation, aiming to enhance their engagement for clients and
effectiveness as a therapeutic tool. Addressing these challenges requires a
paradigm shift towards developing and co-creating serious games for
rehabilitation with therapists, researchers, and stakeholders. Furthermore,
future research is crucial to advance the development of serious games,
ensuring they adhere to evidence-based principles and engage both clients and
therapists. This endeavor will identify gaps in the field, inspire new
directions, and support the creation of practical guidelines for serious games
research.",['cs.HC'],False,,,,"""It's Great Because It's Ran By Us"": Empowering Teen Volunteer Discord
  Moderators to Design Healthy and Engaging Youth-Led Online Communities","Games! What are they good for? The Struggle of Serious Game Adoption for
  Rehabilitation"
neg-d2-780,2025-02-11,,2502.07999," Online, visual artists have more places than ever to routinely share their
creative work and connect with other artists. These interactions support the
routine enactment of creative identity in artists and provide inspirational
opportunities for artists. As creative work shifts online, interactions between
artists and routines around how these artists get inspired to do creative work
are mediated by and through the logics of the online platforms where they take
place. In an interview study of 22 artists, this paper explores the interplay
between the development of artists' creative identities and the, at times,
contradictory practices they have around getting inspired. We find platforms
which support the disciplined practice of creative work while supporting
spontaneous moments of inspiration, play an increasing role in passive
approaches to searching for inspiration, and foster numerous small community
spaces for artists to negotiate their creative identities. We discuss how
platforms can better support and embed mechanisms for inspiration into their
infrastructures into their design and platform policy.",['cs.HC'],2502.06985," Online communities can offer many benefits for youth including peer learning,
cultural expression, and skill development. However, most HCI research on
youth-focused online communities has centered communities developed by adults
for youth rather than by the youth themselves. In this work, we interviewed 11
teenagers (ages 13-17) who moderate online Discord communities created by
youth, for youth. Participants were identified by Discord platform staff as
leaders of well-moderated servers through an intensive exam and
application-based process. We also interviewed 2 young adults who volunteered
as mentors of some of our teen participants. We present our findings about the
benefits, motivations, and risks of teen-led online communities, as well as the
role of external stakeholders of these youth spaces. We contextualize our work
within the broader teen online safety landscape to provide recommendations to
better support, encourage, and protect teen moderators and their online
communities. This empirical work contributes one of the first studies to date
with teen Discord moderators and aims to empower safe youth-led online
communities.",['cs.HC'],False,,,,"Infrastructures for Inspiration: The Routine Construction of Creative
  Identity and Inspiration","""It's Great Because It's Ran By Us"": Empowering Teen Volunteer Discord
  Moderators to Design Healthy and Engaging Youth-Led Online Communities"
neg-d2-781,2025-01-29,,2501.17526," We investigate the charging dynamics of a frequency-modulated quantum battery
(QB) placed within a dissipative cavity environment. Our study focuses on the
interaction of such a battery under both weak and strong coupling regimes,
employing a model in which the quantum battery and charger are represented as
frequency-modulated qubits indirectly coupled through a zero-temperature
environment. It is demonstrated that both the modulation frequency and
amplitude are crucial for optimizing the charging process and the ergotropy of
the quantum battery. Specifically, high-amplitude, low-frequency modulation
significantly enhances charging performance and work extraction in the strong
coupling regime. As an intriguing result, it is deduced that modulation at very
low frequencies leads to the emergence of energy storage and work extraction in
the weak coupling regime. Such a result can never be achieved without
modulation in the weak coupling regime. These results highlight the importance
of adjusting modulation parameters to optimize the performance of quantum
batteries for real-world applications in quantum technologies.",['quant-ph'],2502.13598," The polarization tensor of graphene derived in the framework of the Dirac
model using the methods of thermal quantum field theory in (2+1) dimensions is
recast in a mathematically equivalent but more compact and convenient in
computations form along the real frequency axis. The obtained unified
expressions for the components of the polarization tensor are equally
applicable in the regions of the on- and off-the-mass-shell electromagnetic
waves. The advantages of the presented formalism are demonstrated on the
example of nonequilibrium Casimir force in the configuration of two parallel
graphene-coated dielectric plates one of which is either hotter or colder than
the environment. This force is investigated as a function of temperature, the
energy gap, and chemical potential of graphene coatings with account of the
effects of spatial dispersion. Besides the thermodynamically nonequilibrium
Casimir and Casimir-Polder forces, the obtained form of the polarization tensor
can be useful for investigation of many diverse physical phenomena in graphene
systems, such as surface plasmons, reflectances, electrical conductivity,
radiation heat transfer, etc.",['quant-ph'],False,,,,Amplified quantum battery via dynamical modulation,"Polarization tensor in spacetime of three dimensions and quantum field
  theoretical description of the nonequilibrium Casimir force in graphene
  systems"
neg-d2-782,2025-03-12,,2503.09281," Accurate graph annotation typically requires substantial labeled data, which
is often challenging and resource-intensive to obtain. In this paper, we
present Crowdsourced Homophily Ties Based Graph Annotation via Large Language
Model (CSA-LLM), a novel approach that combines the strengths of crowdsourced
annotations with the capabilities of large language models (LLMs) to enhance
the graph annotation process. CSA-LLM harnesses the structural context of graph
data by integrating information from 1-hop and 2-hop neighbors. By emphasizing
homophily ties - key connections that signify similarity within the graph -
CSA-LLM significantly improves the accuracy of annotations. Experimental
results demonstrate that this method enhances the performance of Graph Neural
Networks (GNNs) by delivering more precise and reliable annotations.",['cs.SI'],2503.17026," Climate change is one of the most critical challenges of the twenty-first
century. Public understanding of climate issues and of the goals regarding the
climate transition is essential to translate awareness into concrete actions.
Social media platforms play a crucial role in disseminating information about
climate change and climate policy. In this context, we propose a model that
analyses the Supply and Demand of information to better understand information
circulation and information voids within the Italian climate-transition
discourse. We conceptualise information supply as the production of content on
Facebook and Instagram while leveraging Google searches to capture information
demand. Our findings highlight the persistence of information voids, which can
hinder informed decision-making and collective action. Furthermore, we observe
that the dynamics of information supply and demand on climate-related topics
tend to intensify in response to significant external events, shaping public
attention and social media discourse.",['cs.SI'],False,,,,"Crowdsourced Homophily Ties Based Graph Annotation Via Large Language
  Model","Modelling the Climate Change Debate in Italy through Information Supply
  and Demand"
neg-d2-783,2025-03-04,,2503.02356," Long context fine-tuning of large language models(LLMs) involves training on
datasets that are predominantly composed of short sequences and a small
proportion of longer sequences. However, existing approaches overlook this
long-tail distribution and employ training strategies designed specifically for
long sequences. Moreover, these approaches also fail to address the challenges
posed by variable sequence lengths during distributed training, such as load
imbalance in data parallelism and severe pipeline bubbles in pipeline
parallelism. These issues lead to suboptimal training performance and poor GPU
resource utilization. To tackle these problems, we propose a chunk-centric
training method named ChunkFlow. ChunkFlow reorganizes input sequences into
uniformly sized chunks by consolidating short sequences and splitting longer
ones. This approach achieves optimal computational efficiency and balance among
training inputs. Additionally, ChunkFlow incorporates a state-aware chunk
scheduling mechanism to ensure that the peak memory usage during training is
primarily determined by the chunk size rather than the maximum sequence length
in the dataset. Integrating this scheduling mechanism with existing pipeline
scheduling algorithms further enhances the performance of distributed training.
Experimental results demonstrate that, compared with Megatron-LM, ChunkFlow can
be up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we
believe that ChunkFlow serves as an effective solution for a broader range of
scenarios, such as long context continual pre-training, where datasets contain
variable-length sequences.",['cs.DC'],2501.09557," Realizing a shared responsibility between providers and consumers is critical
to manage the sustainability of HPC. However, while cost may motivate
efficiency improvements by infrastructure operators, broader progress is
impeded by a lack of user incentives. We conduct a survey of HPC users that
reveals fewer than 30 percent are aware of their energy consumption, and that
energy efficiency is among users' lowest priority concerns. One explanation is
that existing pricing models may encourage users to prioritize performance over
energy efficiency. We propose two transparent multi-resource pricing schemes,
Energy- and Carbon-Based Accounting, that seek to change this paradigm by
incentivizing more efficient user behavior. These two schemes charge for
computations based on their energy consumption or carbon footprint,
respectively, rewarding users who leverage efficient hardware and software. We
evaluate these two pricing schemes via simulation, in a prototype, and a user
study.",['cs.DC'],False,,,,Efficient Long Context Fine-tuning with Chunk Flow,Core Hours and Carbon Credits: Incentivizing Sustainability in HPC
neg-d2-784,2025-02-12,,2502.08712," We present an analysis of deep $\textit{JWST}$/NIRSpec spectra of
star-forming galaxies at $z\simeq1.4-10$, observed as part of the AURORA
survey. We infer median low-ionization electron densities of
$268_{-49}^{+45}~\rm cm^{-3}$, $350_{-76}^{+140}~\rm cm^{-3}$, and
$480_{-310}^{+390}~\rm cm^{-3}$ at redshifts z$=2.3$, $z=3.2$, and $z=5.3$,
respectively, revealing an evolutionary trend following $(1+z)^{1.5\pm0.6}$. We
identify weak positive correlations between electron density and star formation
rate (SFR) as well as SFR surface density, but no significant trends with
stellar mass or specific SFR. Correlations with rest-optical emission line
ratios show densities increasing with $\rm [NeIII]\lambda3869/[OII]\lambda3727$
and, potentially, $\rm [OIII]\lambda5007/[OII]\lambda3727$, although variations
in dust attenuation complicate the latter. Additionally, electron density is
more strongly correlated with distance from the local BPT sequence than can be
explained by simple photoionization models. We further derive electron
densities from the CIII] doublet probing higher-ionization gas, and find a
median value of $1.4_{-0.5}^{+0.7}\times10^4~\rm cm^{-3}$, $\sim30$ times
higher than densities inferred from [SII]. This comparison suggests a
consistent HII region structure across cosmic time with dense, high-ionization
interiors surrounded by less dense, low-ionization gas. We compare measurements
of AURORA galaxies to predictions from the SPHINX galaxy formations,
highlighting the interplay between residual molecular cloud pressure in young
galaxies and feedback from stellar winds and supernovae as galaxies mature.",['astro-ph.GA'],2503.03219," Filament G37 exhibits a distinctive ""caterpillar"" shape, characterized by two
semicircular structures within its 40\,pc-long body, providing an ideal target
to investigate the formation and evolution of filaments. By analyzing multiple
observational data, such as CO spectral line, the H$\alpha$\,RRL, and
multi-wavelength continuum, we find that the expanding H\,{\scriptsize II}
regions surrounding filament G37 exert pressure on the structure of the
filament body, which kinetic process present as the gas flows in multiple
directions along its skeleton. The curved magnetic field structure of filament
G37 derived by employing the Velocity Gradient Technique with CO is found to be
parallel to the filament body and keeps against the pressure from expanded
H\,{\scriptsize II} regions. The multi-directional flows in the filament G37
could cause the accumulation and subsequent collapse of gas, resulting in the
formation of massive clumps. The curved structure and star formation observed
in filament G37 are likely a result of the filament body being squeezed by the
expanding H\,{\scriptsize II} region. This physical process occurs over a
timescale of approximately 5\,Myr. The filament G37 provides a potential
candidate for end-dominated collapse.",['astro-ph.GA'],False,,,,"The AURORA Survey: The Evolution of Multi-phase Electron Densities at
  High Redshift","The Impact of Expanding HII Regions on Filament G37:Curved Magnetic
  Field and Multiple Direction Material Flows"
neg-d2-785,2025-03-18,,2503.14482," Image generation has witnessed significant advancements in the past few
years. However, evaluating the performance of image generation models remains a
formidable challenge. In this paper, we propose ICE-Bench, a unified and
comprehensive benchmark designed to rigorously assess image generation models.
Its comprehensiveness could be summarized in the following key features: (1)
Coarse-to-Fine Tasks: We systematically deconstruct image generation into four
task categories: No-ref/Ref Image Creating/Editing, based on the presence or
absence of source images and reference images. And further decompose them into
31 fine-grained tasks covering a broad spectrum of image generation
requirements, culminating in a comprehensive benchmark. (2) Multi-dimensional
Metrics: The evaluation framework assesses image generation capabilities across
6 dimensions: aesthetic quality, imaging quality, prompt following, source
consistency, reference consistency, and controllability. 11 metrics are
introduced to support the multi-dimensional evaluation. Notably, we introduce
VLLM-QA, an innovative metric designed to assess the success of image editing
by leveraging large models. (3) Hybrid Data: The data comes from real scenes
and virtual generation, which effectively improves data diversity and
alleviates the bias problem in model evaluation. Through ICE-Bench, we conduct
a thorough analysis of existing generation models, revealing both the
challenging nature of our benchmark and the gap between current model
capabilities and real-world generation requirements. To foster further
advancements in the field, we will open-source ICE-Bench, including its
dataset, evaluation code, and models, thereby providing a valuable resource for
the research community.",['cs.CV'],2501.01371," Recent Vision-Language Models (VLMs) have demonstrated remarkable
capabilities in visual understanding and reasoning, and in particular on
multiple-choice Visual Question Answering (VQA). Still, these models can make
distinctly unnatural errors, for example, providing (wrong) answers to
unanswerable VQA questions, such as questions asking about objects that do not
appear in the image. To address this issue, we propose CLIP-UP: CLIP-based
Unanswerable Problem detection, a novel lightweight method for equipping VLMs
with the ability to withhold answers to unanswerable questions. By leveraging
CLIP to extract question-image alignment information, CLIP-UP requires only
efficient training of a few additional layers, while keeping the original VLMs'
weights unchanged. Tested across LLaVA models, CLIP-UP achieves
state-of-the-art results on the MM-UPD benchmark for assessing unanswerability
in multiple-choice VQA, while preserving the original performance on other
tasks.",['cs.CV'],False,,,,"ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and
  Editing","CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question
  Answering"
neg-d2-786,2025-02-03,,2502.01206," Predicting the performance of deep learning (DL) models, such as execution
time and resource utilization, is crucial for Neural Architecture Search (NAS),
DL cluster schedulers, and other technologies that advance deep learning. The
representation of a model is the foundation for its performance prediction.
However, existing methods cannot comprehensively represent diverse model
configurations, resulting in unsatisfactory accuracy. To address this, we
represent a model as a graph that includes the topology, along with the node,
edge, and global features, all of which are crucial for effectively capturing
the performance of the model. Based on this representation, we propose
PerfSeer, a novel predictor that uses a Graph Neural Network (GNN)-based
performance prediction model, SeerNet. SeerNet fully leverages the topology and
various features, while incorporating optimizations such as Synergistic
Max-Mean aggregation (SynMM) and Global-Node Perspective Boost (GNPB) to
capture the critical performance information more effectively, enabling it to
predict the performance of models accurately. Furthermore, SeerNet can be
extended to SeerNet-Multi by using Project Conflicting Gradients (PCGrad),
enabling efficient simultaneous prediction of multiple performance metrics
without significantly affecting accuracy. We constructed a dataset containing
performance metrics for 53k+ model configurations, including execution time,
memory usage, and Streaming Multiprocessor (SM) utilization during both
training and inference. The evaluation results show that PerfSeer outperforms
nn-Meter, Brp-NAS, and DIPPM.",['cs.PF'],2502.01206," Predicting the performance of deep learning (DL) models, such as execution
time and resource utilization, is crucial for Neural Architecture Search (NAS),
DL cluster schedulers, and other technologies that advance deep learning. The
representation of a model is the foundation for its performance prediction.
However, existing methods cannot comprehensively represent diverse model
configurations, resulting in unsatisfactory accuracy. To address this, we
represent a model as a graph that includes the topology, along with the node,
edge, and global features, all of which are crucial for effectively capturing
the performance of the model. Based on this representation, we propose
PerfSeer, a novel predictor that uses a Graph Neural Network (GNN)-based
performance prediction model, SeerNet. SeerNet fully leverages the topology and
various features, while incorporating optimizations such as Synergistic
Max-Mean aggregation (SynMM) and Global-Node Perspective Boost (GNPB) to
capture the critical performance information more effectively, enabling it to
predict the performance of models accurately. Furthermore, SeerNet can be
extended to SeerNet-Multi by using Project Conflicting Gradients (PCGrad),
enabling efficient simultaneous prediction of multiple performance metrics
without significantly affecting accuracy. We constructed a dataset containing
performance metrics for 53k+ model configurations, including execution time,
memory usage, and Streaming Multiprocessor (SM) utilization during both
training and inference. The evaluation results show that PerfSeer outperforms
nn-Meter, Brp-NAS, and DIPPM.",['cs.PF'],False,,,,"PerfSeer: An Efficient and Accurate Deep Learning Models Performance
  Predictor","PerfSeer: An Efficient and Accurate Deep Learning Models Performance
  Predictor"
neg-d2-787,2025-03-05,,2503.0385," Although physics has become increasingly computational, with computing even
being considered the third pillar of physics [1], it is still not well
integrated into physics education [2]. Research suggests that integrating
Computational Thinking (CT) into physics enhances conceptual understanding and
strengthens students ability to model and analyze phenomena [3]. Building on
this, we designed a didactic sequence for K9 students to foster specific CT
practices while reinforcing fundamental kinematics concepts. Assessments
highlight student's ability to apply CT skills to analyze accelerated motion.
This activity can be seamlessly integrated into introductory kinematics
courses.",['physics.ed-ph'],2503.0385," Although physics has become increasingly computational, with computing even
being considered the third pillar of physics [1], it is still not well
integrated into physics education [2]. Research suggests that integrating
Computational Thinking (CT) into physics enhances conceptual understanding and
strengthens students ability to model and analyze phenomena [3]. Building on
this, we designed a didactic sequence for K9 students to foster specific CT
practices while reinforcing fundamental kinematics concepts. Assessments
highlight student's ability to apply CT skills to analyze accelerated motion.
This activity can be seamlessly integrated into introductory kinematics
courses.",['physics.ed-ph'],False,,,,"Code in Motion: Integrating Computational Thinking with Kinematics
  Exploration","Code in Motion: Integrating Computational Thinking with Kinematics
  Exploration"
neg-d2-788,2025-01-27,,2501.15917," This article presents a novel perspective to model and simulate
reconfigurable intelligent surface (RIS)-assisted communication systems.
Traditional methods in antenna design often rely on array method to simulate,
whereas communication system modeling tends to idealize antenna behavior.
Neither approach sufficiently captures the detailed characteristics of
RIS-assisted communication. To address this limitation, we propose a
comprehensive simulation framework that jointly models RIS antenna design and
the communication process. This framework simulates the entire communication
pipeline, encompassing signal generation, modulation, propagation, RIS-based
radiation, signal reception, alignment, demodulation, decision, and processing.
Using a QPSK-modulated signal for validation, we analyze system performance and
investigate the relationship between bit error rate (BER), aperture fill time,
array size, and baseband symbol frequency. The results indicate that larger
array sizes and higher baseband symbol frequencies exacerbate aperture fill
time effects, leading to increased BER. Furthermore, we examine BER variation
with respect to signal-to-noise ratio (SNR) and propose an optimal
matching-based alignment algorithm, which significantly reduces BER compared to
conventional pilot-based alignment methods. This work demonstrates the entire
process of RIS communication, and reveals the source of bit errors, which
provides valuable insights into the design and performance optimization of
RIS-assisted communication systems.",['physics.app-ph'],2502.19713," Small-angle neutron scattering (SANS) is a powerful technique for probing the
nanoscale structure of materials. However, the fundamental limitations of
neutron flux pose significant challenges for rapid, high-fidelity data
acquisition required in many experiments. To circumvent this difficulty, we
introduce a Bayesian statistical framework based on Gaussian process regression
(GPR) to infer high-quality SANS intensity profiles from measurements with
suboptimal signal-to-noise ratios (SNR). Unlike machine learning approaches
that depend on extensive training datasets, the proposed one-shot method
leverages the intrinsic mathematical properties of the scattering function,
smoothness and continuity, offering a generalizable solution beyond the
constraints of data-intensive techniques. By examining existing SANS
experimental data, we demonstrate that this approach can reduce measurement
time by between one and two orders of magnitude while maintaining accuracy and
adaptability across different SANS instruments. By improving both efficiency
and reliability, this method extends the capabilities of SANS, enabling broader
applications in time-sensitive and low-flux experimental conditions.",['physics.app-ph'],False,,,,"RIS Assisted Wireless Communication: Advanced Modeling, Simulation, and
  Analytical Insights","Unlocking Hidden Information in Sparse Small-Angle Neutron Scattering
  Measurement"
neg-d2-789,2025-02-07,,2502.04916," New regulations are continuously introduced to ensure that software
development complies with the ethical concerns and prioritizes public safety. A
prerequisite for demonstrating compliance involves tracing software
requirements to legal provisions. Requirements traceability is a fundamental
task where requirements engineers are supposed to analyze technical
requirements against target artifacts, often under limited time budget. Doing
this analysis manually for complex systems with hundreds of requirements is
infeasible. The legal dimension introduces additional challenges that only
exacerbate manual effort.
  In this paper, we investigate two automated solutions based on large language
models (LLMs) to predict trace links between requirements and legal provisions.
The first solution, Kashif, is a classifier that leverages sentence
transformers. The second solution prompts a recent generative LLM based on
Rice, a prompt engineering framework.
  On a benchmark dataset, we empirically evaluate Kashif and compare it against
a baseline classifier from the literature. Kashif can identify trace links with
an average recall of ~67%, outperforming the baseline with a substantial gain
of 54 percentage points (pp) in recall. However, on unseen, more complex
requirements documents traced to the European general data protection
regulation (GDPR), Kashif performs poorly, yielding an average recall of 15%.
On the same documents, however, our Rice-based solution yields an average
recall of 84%, with a remarkable gain of about 69 pp over Kashif. Our results
suggest that requirements traceability in the legal context cannot be simply
addressed by building classifiers, as such solutions do not generalize and fail
to perform well on complex regulations and requirements. Resorting to
generative LLMs, with careful prompt engineering, is thus a more promising
alternative.",['cs.SE'],2503.10876," The proliferation of Large Language Models (LLMs) in recent years has
realized many applications in various domains. Being trained with a huge of
amount of data coming from various sources, LLMs can be deployed to solve
different tasks, including those in Software Engineering (SE). Though they have
been widely adopted, the potential of using LLMs cooperatively has not been
thoroughly investigated. In this paper, we proposed Metagente as a novel
approach to amplify the synergy of various LLMs. Metagente is a Multi-Agent
framework based on a series of LLMs to self-optimize the system through
evaluation, feedback, and cooperation among specialized agents. Such a
framework creates an environment where multiple agents iteratively refine and
optimize prompts from various perspectives. The results of these explorations
are then reviewed and aggregated by a teacher agent. To study its performance,
we evaluated Metagente with an SE task, i.e., summarization of README.MD files,
and compared it with three well-established baselines, i.e., GitSum, LLaMA-2,
and GPT-4o. The results show that our proposed approach works efficiently and
effectively, consuming a small amount of data for fine-tuning but still getting
a high accuracy, thus substantially outperforming the baselines. The
performance gain compared to GitSum, the most relevant benchmark, ranges from
27.63% to 60.43%. More importantly, compared to using only one LLM, Metagente
boots up the accuracy to multiple folds.",['cs.SE'],False,,,,"Classification or Prompting: A Case Study on Legal Requirements
  Traceability","Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD
  Summarization"
neg-d2-790,2025-01-14,,2501.07861," Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)
hold promise in knowledge-intensive tasks but face limitations in complex
multi-step reasoning. While recent methods have integrated RAG with
chain-of-thought reasoning or test-time search using Process Reward Models
(PRMs), these approaches encounter challenges such as a lack of explanations,
bias in PRM training data, early-step bias in PRM scores, and insufficient
post-training optimization of reasoning potential. To address these issues, we
propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding
(ReARTeR), a framework that enhances RAG systems' reasoning capabilities
through post-training and test-time scaling. At test time, ReARTeR introduces
Trustworthy Process Rewarding via a Process Reward Model for accurate scalar
scoring and a Process Explanation Model (PEM) for generating natural language
explanations, enabling step refinement. During post-training, it utilizes Monte
Carlo Tree Search guided by Trustworthy Process Rewarding to collect
high-quality step-level preference data, optimized through Iterative Preference
Optimization. ReARTeR addresses three core challenges: (1) misalignment between
PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM
training data, mitigated by balanced annotation methods and stronger
annotations for challenging examples; and (3) early-step bias in PRM, resolved
through a temporal-difference-based look-ahead search strategy. Experimental
results on multi-step reasoning benchmarks demonstrate significant
improvements, underscoring ReARTeR's potential to advance the reasoning
capabilities of RAG systems.",['cs.CL'],2502.14451," Natural Language Generation (NLG) popularity has increased owing to the
progress in Large Language Models (LLMs), with zero-shot inference
capabilities. However, most neural systems utilize decoder-only causal
(unidirectional) transformer models, which are effective for English but may
reduce the richness of languages with less strict word order, subject omission,
or different relative clause attachment preferences. This is the first work
that analytically addresses optimal text generation order for non-causal
language models. We present a novel Viterbi algorithm-based methodology for
maximum likelihood word order estimation. We analyze the non-causal
most-likelihood order probability for NLG in Spanish and, then, the probability
of generating the same phrases with Spanish causal NLG. This comparative
analysis reveals that causal NLG prefers English-like SVO structures. We also
analyze the relationship between optimal generation order and causal
left-to-right generation order using Spearman's rank correlation. Our results
demonstrate that the ideal order predicted by the maximum likelihood estimator
is not closely related to the causal order and may be influenced by the
syntactic structure of the target sentence.",['cs.CL'],False,,,,"ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process
  Rewarding","Optimal word order for non-causal text generation with Large Language
  Models: the Spanish case"
neg-d2-791,2025-02-09,,2502.05956," Divided power algebras form an important variety of non-binary universal
algebras. We identify the universal enveloping algebra and K\""ahler
differentials associated to a divided power algebra over a general commutative
ring, simplifying and generalizing work of Roby and Dokas.",['math.AC'],2501.10005," Let R be a commutative ring with identity and M be an R-module. The purpose
of this paper is to introduce and investigate the dual notion of morphic
modules over a commutative ring.",['math.AC'],False,,,,"Divided powers and K\""ahler differentials",The dual notion of morphic modules over commutative rings
neg-d2-792,2025-01-05,,2501.02726," In this paper, we discuss optimal $1$-toroidal graphs (abbreviated as O1TG),
which are drawn on the torus so that every edge crosses another edge at most
once, and has $n$ vertices and exactly $4n$ edges. We first consider
connectivity of O1TGs, and give the characterization of O1TGs having
connectivity exactly $k$ for each $k\in \{4, 5, 6, 8\}$. In our argument, we
also show that there exists no O1TG having connectivity exactly $7$.
Furthermore, using the result above, we discuss extendability of matchings, and
give the characterization of $1$-, $2$- and $3$-extendable O1TGs in turn.",['math.CO'],2503.0882," Recently, Pan and Yu showed that Lascoux polynomials can be defined in terms
of certain collections of diagrams consisting of unit cells arranged in the
first quadrant. Starting from certain initial diagrams, one forms a finite set
of diagrams by applying two types of moves: Kohnert and ghost moves. Both moves
cause at most one cell to move to a lower row with ghost moves leaving a new
""ghost cell"" in its place. Each diagram formed in this way defines a monomial
in the associated Lascoux polynomial. Restricting attention to diagrams formed
by applying sequences of only Kohnert moves in the definition of Lascoux
polynomials, one obtains the family of key polynomials. Recent articles have
considered a poset structure on the collections of diagrams formed when one
uses only Kohnert moves. In general, these posets are not ""well-behaved,"" not
usually having desirable poset properties. Here, as an intermediate step to
studying the analogous posets associated with Lascoux polynomials, we consider
the posets formed by restricting attention to those diagrams formed by using
only ghost moves. Unlike in the case of Kohnert posets, we show that such
""ghost Kohnert posets"" are always ranked join semi-lattices. In addition, we
establish a necessary condition for when ghost Kohnert posets are bounded and,
consequently, lattices.",['math.CO'],False,,,,"Connectivity and matching extendability of optimal $1$-embedded graphs
  on the torus",Ghost Kohnert posets
neg-d2-793,2025-02-16,,2502.11032," Model-assisted estimation combines sample survey data with auxiliary
information to increase precision when estimating finite population quantities.
Accurately estimating the variance of model-assisted estimators is challenging:
the classical approach ignores uncertainty from estimating the working model
for the functional relationship between survey and auxiliary variables. This
approach may be asymptotically valid, but can underestimate variance in
practical settings with limited sample sizes. In this work, we develop a
connection between model-assisted estimation and the theory of U- and
V-statistics. We demonstrate that when predictions from the working model for
the variable of interest can be represented as a U- or V-statistic, the
resulting model-assisted estimator also admits a U- or V-statistic
representation. We exploit this connection to derive an improved estimator of
the exact variance of such model-assisted estimators. The class of working
models for which this strategy can be used is broad, ranging from linear models
to modern ensemble methods. We apply our approach to the model-assisted
estimator constructed with a linear regression working model, commonly referred
to as the generalized regression estimator, show that it can be re-written as a
U-statistic, and propose an estimator of its exact variance. We illustrate our
proposal and compare it against the classical asymptotic variance estimator
using household survey data from the American Community Survey.",['stat.ME'],2502.02392," Synthetic datasets are widely used in many applications, such as missing data
imputation, examining non-stationary scenarios, in simulations, training
data-driven models, and analyzing system robustness. Typically, synthetic data
are based on historical data obtained from the observed system. The data needs
to represent a specific behavior of the system, yet be new and diverse enough
so that the system is challenged with a broad range of inputs. This paper
presents a method, based on discrete Fourier transform, for generating
synthetic time series with similar statistical moments for any given signal.
The suggested method makes it possible to control the level of similarity
between the given signal and the generated synthetic signals. Proof shows
analytically that this method preserves the first two statistical moments of
the input signal, and its autocorrelation function. The method is compared to
known methods, ARMA, GAN, and CoSMoS. A large variety of environmental datasets
with different temporal resolutions, and from different domains are used,
testing the generality and flexibility of the method. A Python library
implementing this method is made available as open-source software.",['stat.ME'],False,,,,"Exact variance estimation for model-assisted survey estimators using U-
  and V-statistics","Synthetic Random Environmental Time Series Generation with Similarity
  Control, Preserving Original Signal's Statistical Characteristics"
neg-d2-794,2025-02-15,,2502.12195," We consider the problem of test-time domain generalization, where a model is
trained on several source domains and adjusted on target domains never seen
during training. Different from the common methods that fine-tune the model or
adjust the classifier parameters online, we propose to generate multiple layer
parameters on the fly during inference by a lightweight meta-learned
transformer, which we call \textit{GeneralizeFormer}. The layer-wise parameters
are generated per target batch without fine-tuning or online adjustment. By
doing so, our method is more effective in dynamic scenarios with multiple
target distributions and also avoids forgetting valuable source distribution
characteristics. Moreover, by considering layer-wise gradients, the proposed
method adapts itself to various distribution shifts. To reduce the
computational and time cost, we fix the convolutional parameters while only
generating parameters of the Batch Normalization layers and the linear
classifier. Experiments on six widely used domain generalization datasets
demonstrate the benefits and abilities of the proposed method to efficiently
handle various distribution shifts, generalize in dynamic scenarios, and avoid
forgetting.",['cs.LG'],2501.16186," This paper studies the uplink and downlink power allocation for interactive
augmented reality (AR) services, where live video captured by an AR device is
uploaded to the network edge and then the augmented video is subsequently
downloaded. By modeling the AR transmission process as a tandem queuing system,
we derive an upper bound for the probabilistic quality of service (QoS)
requirement concerning end-to-end latency and reliability. The resource
allocation with the QoS constraints results in a functional optimization
problem. To address it, we design a deep neural network to learn the power
allocation policy, leveraging the structure of optimal power allocation to
enhance learning performance. Simulation results demonstrate that the proposed
method effectively reduces transmit powers while meeting the QoS requirement.",['cs.LG'],False,,,,"GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time
  Distribution Shifts",Learn to Optimize Resource Allocation under QoS Constraint of AR
neg-d2-795,2025-01-18,,2502.10397," The Metaverse represents a transformative shift beyond traditional mobile
Internet, creating an immersive, persistent digital ecosystem where users can
interact, socialize, and work within 3D virtual environments. Powered by large
models such as ChatGPT and Sora, the Metaverse benefits from precise
large-scale real-world modeling, automated multimodal content generation,
realistic avatars, and seamless natural language understanding, which enhance
user engagement and enable more personalized, intuitive interactions. However,
challenges remain, including limited scalability, constrained responsiveness,
and low adaptability in dynamic environments. This paper investigates the
integration of large models within the Metaverse, examining their roles in
enhancing user interaction, perception, content creation, and service quality.
To address existing challenges, we propose a generative AI-based framework for
optimizing Metaverse rendering. This framework includes a cloud-edge-end
collaborative model to allocate rendering tasks with minimal latency, a
mobility-aware pre-rendering mechanism that dynamically adjusts to user
movement, and a diffusion model-based adaptive rendering strategy to fine-tune
visual details. Experimental results demonstrate the effectiveness of our
approach in enhancing rendering efficiency and reducing rendering overheads,
advancing large model deployment for a more responsive and immersive Metaverse.",['cs.CY'],2502.03472," The rapid advancement of Large Language Models (LLMs) has created a critical
gap in consumer protection due to the lack of standardized certification
processes for LLM-powered Artificial Intelligence (AI) systems. This paper
argues that current regulatory approaches, which focus on compute-level
thresholds and generalized model evaluations, are insufficient to ensure the
safety and effectiveness of specific LLM-based user experiences. We propose a
shift towards a certification process centered on actual user-facing
experiences and the curation of high-quality datasets for evaluation. This
approach offers several benefits: it drives consumer confidence in AI system
performance, enables businesses to demonstrate the credibility of their
products, and allows regulators to focus on direct consumer protection. The
paper outlines a potential certification workflow, emphasizing the importance
of domain-specific datasets and expert evaluation. By repositioning data as the
strategic center of regulatory efforts, this framework aims to address the
challenges posed by the probabilistic nature of AI systems and the rapid pace
of technological advancement. This shift in regulatory focus has the potential
to foster innovation while ensuring responsible AI development, ultimately
benefiting consumers, businesses, and government entities alike.",['cs.CY'],False,,,,"Large Model Empowered Metaverse: State-of-the-Art, Challenges and
  Opportunities","Powering LLM Regulation through Data: Bridging the Gap from Compute
  Thresholds to Customer Experiences"
neg-d2-796,2025-01-13,,2501.0733," ML and HPC applications increasingly combine dense and sparse memory access
computations to maximize storage efficiency. However, existing CPUs and GPUs
struggle to flexibly handle these heterogeneous workloads with consistently
high compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,
dual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical
interconnect and in-core streaming units (SUs) designed to accelerate dense and
sparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets
in 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense
linear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On
stencil codes, Occamy reaches an FPU utilization of 83% and a
technology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading
state-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On
sparse-dense linear algebra (LA), it achieves 42% FPU utilization and a
normalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x
and 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up
to 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.
Finally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and
graph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available
under a permissive open-source license.",['cs.AR'],2501.0902," Compute Express Link (CXL) is widely-supported interconnect standard that
promises to enable memory disaggregation in data centers. CXL allows for memory
pooling, which can be used to create a shared memory space across multiple
servers. However, CXL does not specify how to actually build a memory pool.
Existing proposals for CXL memory pools are expensive, as they require CXL
switches or large multi-headed devices. In this paper, we propose a new design
for CXL memory pools that is cost-effective. We call these designs Octopus
topologies. Our design uses small CXL devices that can be made cheaply and
offer fast access latencies. Specifically, we propose asymmetric CXL topologies
where hosts connect to different sets of CXL devices. This enables pooling and
sharing memory across multiple hosts even as each individual CXL device is only
connected to a small number of hosts. Importantly, this uses hardware that is
readily available today. We also show the trade-off in terms of CXL pod size
and cost overhead per host. Octopus improves the Pareto frontier defined by
prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost
per host.",['cs.AR'],False,,,,"Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System
  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET",Octopus: Scalable Low-Cost CXL Memory Pooling
neg-d2-797,2025-02-19,,2502.13518," We generalize the Rubik's cube, together with its group of configurations, to
any abstract regular polytope. After discussing general aspects, we study the
Rubik's simplex of arbitrary dimension and provide a complete description of
the associated group. We sketch an analogous argument for the Rubik's hypercube
as well.",['math.CO'],2501.05145," We are examining a specific type of graph called a balanced bipartite graph.
The balanced bipartite graph, such as $\mathcal{B}$, has two parts, $V_1$ and
$V_2$, each containing $n$ vertices, for a total of $2n$ vertices. The degree
of a vertex $v$ in $V_1\cup\,V_2$ is denoted by $d_\mathcal{B}(v)$. The minimum
degree of any vertex in the graph $\mathcal{B}$ is represented by
$\delta(\mathcal{B})$. If $S$ is a subset of $V_1\cup\,V_2$, then the subgraph
of $\mathcal{B}$ induced by $S$ is the graph that has $S$ as its vertex set and
contains all the edges of $\mathcal{B}$ that have both endpoints in $S$. This
subgraph is denoted by $\mathcal{B}[S]$. The forest number of a graph
$\mathcal{B}$ is the size of the largest subset of vertices of $\mathcal{B}$
that form an induced forest. We use $f(\mathcal{B})$ to represent the forest
number of graph $\mathcal{B}$. A decycling set or a feedback vertex set of a
graph is a set of vertices whose removal results in a forest. The smallest
possible size of a decycling set of $\mathcal{B}$ is represented by
$\nabla(\mathcal{B})$. Finding the decycling number of $\mathcal{B}$ is
equivalent to determining the largest order of an induced forest, i.e.,
$f(\mathcal{B})+\nabla(\mathcal{B})=2n$. In this essay, we study the structure
and cardinality of the largest subsets of vertices of graph $\mathcal{B}$ that
form induced forests.",['math.CO'],False,,,,Rubik's Abstract Polytopes,On Maximum Induced Forests of the Balanced Bipartite Graphs
neg-d2-798,2025-03-02,,2503.00831," Recycled Gumbel Noise Improves Consistency in
  Natural Language Generation; Consistency in the output of language models is critical for their
reliability and practical utility. Due to their training objective, language
models learn to model the full space of possible continuations, leading to
outputs that can vary significantly in style and content, even for similar or
repeated inputs. To address this, we propose a novel decoding algorithm that
enhances response consistency across different prompts with no degradation in
response quality. By incorporating a latent variable into the next-token
sampling process based on the Gumbel reparametrisation trick, our method
outperforms standard sampling by up to 10% across semantic and stylistic
consistency benchmarks. Additionally, our approach integrates seamlessly with
existing sampling methods with negligible computational overhead, providing a
practical solution for improving the reliability of language model outputs.",['cs.CL'],2502.15401," In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.",['cs.CL'],False,,,,"Waste Not, Want Not","Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs
  Complex Reasoning"
neg-d2-799,2025-03-10,,2503.07403," Solving short and long time dynamics of closed quantum many-body systems is
one of the main challenges of both atomic and condensed matter physics. For
locally interacting closed systems, the dynamics of local observables can
always be expanded into (pseudolocal) eigenmodes of the Liouvillian, so called
dynamical symmetries. They come in two classes - transient operators, which
decay in time and perpetual operators, which either oscillate forever or stay
the same (conservation laws). These operators provide a full characterization
of the dynamics of the system. Deriving these operators, apart from a very
limited class of models, has not been possible. Here, we present a method to
numerically and analytically derive some of these dynamical symmetries in
infinite closed systems by introducing a naturally emergent open boundary
condition on the Krylov chain. This boundary condition defines a partitioning
of the Krylov space into system and environment degrees of freedom, where
non-local operators make up an effective bath for the local operators. We
demonstrate the practicality of the method on some numerical examples and
derive analytical results in two idealized cases. Our approach lets us directly
relate the operator growth hypothesis to thermalization and exponential decay
of observables in chaotic systems.",['quant-ph'],2503.1183," We study the time-optimal robust control of a two-level quantum system
subjected to field inhomogeneities. We apply the Pontryagin Maximum Principle
and we introduce a reduced space onto which the optimal dynamics is projected
down. This reduction leads to a complete analytical derivation of the optimal
solution in terms of elliptic functions and elliptic integrals. Necessary
optimality conditions are then obtained for the original system. These
conditions are verified numerically and lead to the optimal control protocol.
Various examples, ranging from state-to-state transfer to the generation of a
Not gate, illustrate this study. The connection with other geometric
optimization approaches that have been used to solve this problem is also
discussed.",['quant-ph'],False,,,,"Opening Krylov space to access all-time dynamics via dynamical
  symmetries","Application of the Pontryagin Maximum Principle to the robust
  time-optimal control of two-level quantum systems"
neg-d2-800,2025-03-08,,2503.06377," In 2023, Greaves et~al.\ constructed several sets of 57 equiangular lines in
dimension 18. Using the concept of switching root introduced by Cao et~al.\ in
2021, these sets of equiangular lines are embedded in a lattice of rank 19
spanned by norm 3 vectors together with a switching root. We characterize this
lattice as an overlattice of the root lattice $A_9\oplus A_9\oplus A_1$, and
show that there are at least $246896$ sets of 57 equiangular lines in dimension
$18$ arising in this way, up to isometry. Additionally, we prove that all of
these sets of equiangular lines are strongly maximal. Here, a set of
equiangular lines is said to be strongly maximal if there is no set of
equiangular lines properly containing it even if the dimension of the
underlying space is increased. Among these sets, there are ones with only six
distinct Seidel eigenvalues.",['math.CO'],2503.14022," As the scale of data centers continues to grow, there is an increasing demand
for interconnection networks to resist malicious attacks. Hence, it is
necessary to evaluate the reliability of networks under various fault patterns.
The family of generalized $K_4$-hypercubes serve as interconnection networks of
data centers, characterized by topological structures with exceptional
properties. The $h$-extra edge-connectivity $\lambda_h$, the $l$-super
edge-connectivity $\lambda^l$, the $l$-average degree edge-connectivity
$\overline{\lambda^l}$, the $l$-embedded edge-connectivity $\eta_l$ and the
cyclic edge-connectivity $\lambda_c$ are vital parameters to accurately assess
the reliability of interconnection networks. Let integer $n\geq3$. This paper
obtains the optimal solution of the edge isoperimetric problem and its explicit
representation, which offers an upper bound of the $h$-extra edge-connectivity
of an $n$-dimensional $K_4$-hypercube $H_n^4$. As an application, we presents
$\lambda_h(H_n^4)$ for $1\leq h\leq 2^{\lceil n/2 \rceil }$. Moreover, for
$2^{\lceil n/2\rceil+t}-g_t \le h\le2^{\lceil n/2\rceil+t}$,
$g_t=\lceil(2^{2t+2+\gamma})/3\rceil$,
  $0\leq t \leq\lfloor n/2\rfloor-1 $, $\gamma=0$ for even $n$ and $\gamma=1$
for odd $n$, $\lambda_h(H_n^4)$ is a constant $(\lfloor n/2\rfloor-t)2^{\lceil
n/2\rceil+t}$. The above lower and upper bounds of the integer $h$ are both
sharp. Furthermore, $\lambda^l(H_n^4)$, $\overline{\lambda^l}(H_n^4)$,
$\lambda_{2^l}(H_n^4)$, and $\eta_l(H_n^4)$ share a common value $(n-l)2^l$ for
$2\leq l\leq n-1$, and we determines the values of $\lambda_c(H_n^4)$.",['math.CO'],False,,,,"Sets of equiangular lines in dimension $18$ constructed from $A_9 \oplus
  A_9 \oplus A_1$","Reliability Evaluation of Generalized $K_4$-Hypercubes Based on Five
  Link Fault Patterns"
neg-d2-801,2025-03-12,,2503.09169," Entanglement distribution is a fundamental property in quantum many-body
physics, but the effect of long-range interactions on the distribution has not
been fully understood. Here, we study long-range two-party entanglement (TPE)
and explore its distribution properties in XXZ spin chains with the exponential
and power-law long-range interactions(ELRIs and PLRIs). In the thermodynamic
limit case with the ELRIs, the TPE quantified by two-qubit concurrence decays
exponentially along with two-site distance and the long-range concurrences can
indicate the paramagnetic-ferromagnetic phase transition. We present a
fine-grained entanglement distribution relations among the entanglement
truncation length, total concurrences and two-tangles in the infinite spin
chains. Moreover, in the finite XXZ chain with the more common PLRIs, the TPE
decays algebraically along with the two-spin distance, and the total
concurrence can exhibit a piecewise function with respect to total two-tangles.
These new presented TPE distribution relations can be regarded as the
generalization of Koashi-Bu\v{z}ek-and-Imoto bound for the long-range quantum
models, and have potential applications in quantum information processing.",['quant-ph'],2503.14874," Dissipative light-matter coupling plays a vital role in non-Hermitian
physics, but it remains largely unexplored in waveguide QED systems. In this
work, we find that by employing pseudo-Hermitian symmetry rather than anti-PT
symmetry, the concept of dissipative coupling could be generalized and applied
to the field of waveguide QED. This leads to a series of intriguing results,
such as spontaneous breaking of pseudo-Hermitian symmetry across the
exceptional points (EPs), level attraction between the bound states, and
critical transition across the EPs for the population of quantum emitters in
the bound state. Thanks to the tunability of photonic bands in crystal
waveguides, we also demonstrate that dissipative light-matter coupling leads to
the emergence of nonstandard third-order exceptional points with chiral spatial
profiles in a topological waveguide QED system. This work provides a promising
paradigm for studying non-Hermitian quantum phenomena in waveguide QED systems.",['quant-ph'],False,,,,"Two-party entanglement distribution in XXZ spin chains with the
  exponential and power-law long-range interactions",Waveguide QED with dissipative light-matter couplings
neg-d2-802,2025-03-05,,2503.03527," The equivalence between the Schr\""odinger and Heisenberg representations is a
cornerstone of quantum mechanics. However, this relationship remains unclear in
the non-Hermitian regime, particularly when the Hamiltonian is time-dependent.
In this study, we address this gap by establishing the connection between the
two representations, incorporating the metric of the Hilbert space bundle. We
not only demonstrate the consistency between the Schr\""odinger and Heisenberg
representations but also present a Heisenberg-like representation grounded in
the generalized vielbein formalism, which provides a clear and intuitive
geometric interpretation. Unlike the standard Heisenberg representation, where
the metric of the Hilbert space is encoded solely in the dual states, the
Heisenberg-like representation distributes the metric information between both
the states and the dual states. Despite this distinction, it retains the same
Heisenberg equation of motion for operators. Within this formalism, the
Hamiltonian is replaced by a Hermitian counterpart, while the ""non-Hermiticity""
is transferred to the operators. Moreover, this approach extends to regimes
with a dynamical metric (beyond the pseudo-Hermitian framework) and to systems
governed by time-dependent Hamiltonians.",['quant-ph'],2502.2071," One of the predominant causes of program distortion in the real quantum
computing system may be attributed to the probability deviation caused by
thermal relaxation. We introduce Barber (Balancing reAdout Results using
Bit-invErted ciRcuits), a method designed to counteract the asymmetric thermal
relaxation deviation and improve the reliability of near-term quantum programs.
Barber collaborates with a bit-inverted quantum circuit, where the excited
quantum state of qubits is assigned to the $\lvert 0 \rangle$ and the unexcited
state to the $\lvert 1 \rangle$. In doing so, bit-inverted quantum circuits can
experience thermal relaxation in the opposite direction compared to standard
quantum circuits. Barber can effectively suppress the thermal relaxation
deviation in program's readout results by selectively merging distributions
from the standard and bit-inverted circuits.",['quant-ph'],False,,,,"Heisenberg and Heisenberg-Like Representations via Hilbert Space Bundle
  Geometry in the Non-Hermitian Regime","Balancing Thermal Relaxation Deviations of Near-Future Quantum Computing
  Results via Bit-Inverted Programs"
neg-d2-803,2025-02-19,,2502.13855," Geometric diagrams are critical in conveying mathematical and scientific
concepts, yet traditional diagram generation methods are often manual and
resource-intensive. While text-to-image generation has made strides in
photorealistic imagery, creating accurate geometric diagrams remains a
challenge due to the need for precise spatial relationships and the scarcity of
geometry-specific datasets. This paper presents MagicGeo, a training-free
framework for generating geometric diagrams from textual descriptions. MagicGeo
formulates the diagram generation process as a coordinate optimization problem,
ensuring geometric correctness through a formal language solver, and then
employs coordinate-aware generation. The framework leverages the strong
language translation capability of large language models, while formal
mathematical solving ensures geometric correctness. We further introduce
MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and
demonstrate that MagicGeo outperforms current methods in both qualitative and
quantitative evaluations. This work provides a scalable, accurate solution for
automated diagram generation, with significant implications for educational and
academic applications.",['cs.CV'],2502.12723," This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.",['cs.CV'],False,,,,MagicGeo: Training-Free Text-Guided Geometric Diagram Generation,"myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  Dataset"
neg-d2-804,2025-01-03,,2501.07586," Let X be a smooth cubic hypersurface. We prove that a general cubic surface
is isomorphic to a hyperplane section of X .",['math.AG'],2503.16255," We investigate limit linear series on chains of elliptic curves, giving a
simple proof of a conjecture of Farkas stating the existence of curves with a
theta-characteristic with a given number of sections for the expected range of
genera. Using the additional structure afforded by considering limit linear
series on chains of elliptic curves, we find examples of reducible
Brill-Noether loci, admitting at least two components, with and without a
theta-characteristic respectively. This allows us to display reducible Hilbert
schemes for $r\ge 3$ and the largest possible value of $d$, namely $d=g-1$. We
also give examples of Brill-Noether loci with three components. On the positive
side, we provide optimal bounds on the degree under which Brill-Noether loci
are irreducible when $r=2$.",['math.AG'],False,,,,Hyperplane sections of cubic threefolds,Some reducible and irreducible Brill-Noether loci
neg-d2-805,2025-02-17,,2502.11591," Classification and quantitative characterization of neuronal morphologies
from histological neuronal reconstruction is challenging since it is still
unclear how to delineate a neuronal cell class and which are the best features
to define them by. The morphological neuron characterization represents a
primary source to address anatomical comparisons, morphometric analysis of
cells, or brain modeling. The objectives of this paper are (i) to develop and
integrate a pipeline that goes from morphological feature extraction to
classification and (ii) to assess and compare the accuracy of machine learning
algorithms to classify neuron morphologies. The algorithms were trained on 430
digitally reconstructed neurons subjectively classified into layers and/or
m-types using young and/or adult development state population of the
somatosensory cortex in rats. For supervised algorithms, linear discriminant
analysis provided better classification results in comparison with others. For
unsupervised algorithms, the affinity propagation and the Ward algorithms
provided slightly better results.",['q-bio.NC'],2502.13661," Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation
(DBS) in DYT1 dystonia and disease progression under DBS was studied.
Twenty-six patients of this open-label study were divided into two groups: (A)
with single bilateral GPi lead, (B) with a second bilateral GPi lead implanted
owning to subsequent worsening of symptomatology. Dystonia was assessed with
the Burke Scale. Appearance of new symptoms and distribution according to body
region were recorded. In the whole cohort, significant decreases in motor and
disability subscores (P < 0.0001) were observed at 1 year and maintained up to
10 years. Group B showed worsening of the symptoms. At 1 year, there were no
significant differences between Groups A (without subsequent worsening) and B;
at 5 years, a significant difference was found for motor and disability scores.
Within Group B, four patients exhibited additional improvement after the second
DBS surgery. In the 26 patients, significant difference (P = 0.001) was found
between the number of body regions affected by dystonia preoperatively and over
the whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10
years (two patients). New symptoms appear with long-term follow-up and may
improve with additional leads in a subgroup of patients.",['q-bio.NC'],False,,,,Morphological Neuron Classification Using Machine Learning,"Long-term follow-up of DYT1 dystonia patients treated by deep brain
  stimulation: an open-label study"
neg-d2-806,2025-02-12,,2502.08642," Recent advancements in large vision-language models have enabled highly
expressive and diverse vector sketch generation. However, state-of-the-art
methods rely on a time-consuming optimization process involving repeated
feedback from a pretrained model to determine stroke placement. Consequently,
despite producing impressive sketches, these methods are limited in practical
applications. In this work, we introduce SwiftSketch, a diffusion model for
image-conditioned vector sketch generation that can produce high-quality
sketches in less than a second. SwiftSketch operates by progressively denoising
stroke control points sampled from a Gaussian distribution. Its
transformer-decoder architecture is designed to effectively handle the discrete
nature of vector representation and capture the inherent global dependencies
between strokes. To train SwiftSketch, we construct a synthetic dataset of
image-sketch pairs, addressing the limitations of existing sketch datasets,
which are often created by non-artists and lack professional quality. For
generating these synthetic sketches, we introduce ControlSketch, a method that
enhances SDS-based techniques by incorporating precise spatial control through
a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across
diverse concepts, efficiently producing sketches that combine high fidelity
with a natural and visually appealing style.",['cs.CV'],2502.11532," Text-to-image diffusion models have shown remarkable capabilities of
generating high-quality images closely aligned with textual inputs. However,
the effectiveness of text guidance heavily relies on the CLIP text encoder,
which is trained to pay more attention to general content but struggles to
capture semantics in specific domains like styles. As a result, generation
models tend to fail on prompts like ""a photo of a cat in Pokemon style"" in
terms of simply producing images depicting ""a photo of a cat"". To fill this
gap, we propose Control-CLIP, a novel decoupled CLIP fine-tuning framework that
enables the CLIP model to learn the meaning of category and style in a
complement manner. With specially designed fine-tuning tasks on minimal data
and a modified cross-attention mechanism, Control-CLIP can precisely guide the
diffusion model to a specific domain. Moreover, the parameters of the diffusion
model remain unchanged at all, preserving the original generation performance
and diversity. Experiments across multiple domains confirm the effectiveness of
our approach, particularly highlighting its robust plug-and-play capability in
generating content with various specific styles.",['cs.CV'],False,,,,SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation,"Control-CLIP: Decoupling Category and Style Guidance in CLIP for
  Specific-Domain Generation"
neg-d2-807,2025-03-10,,2503.07436," Yielding of amorphous glasses and gels is a mechanically driven
transformation of a material from the solid to liquid state on the experimental
timescale. It is a ubiquitous fundamental problem of nonequilibrium physics of
high importance in material science, biology, and engineering applications such
as processing, ink printing, and manufacturing. However, the underlying
microscopic mechanisms and degree of universality of the yielding problem
remain theoretically poorly understood. We address this problem for dense
Brownian suspensions of nanoparticles or colloids that interact via repulsions
that induce steric caging and tunable short range attractions that drive
physical bond formation. In the absence of deformation, these competing forces
can result in fluids, repulsive glasses, attractive glasses, and dense gels of
widely varying elastic rigidity and viscosity. Building on a quiescent
microscopic theoretical approach that explicitly treats attractive bonding and
thermally-induced activated hopping, we formulate a self-consistent theory for
the coupled evolution of the transient and steady state mechanical response,
and structure as a function of stress, strain, and deformation rate over a wide
range of high packing fractions and attraction strengths and ranges. Depending
on the latter variables, under step rate shear the theory predicts three
qualitatively different transient responses: plastic-like (of two distinct
types), static yielding via a single elastic-viscous stress overshoot, and
double or 2-step yielding due to an intricate competition between
deformation-induced bond breaking and de-caging. A predictive understanding of
multiple puzzling experimental observations is achieved, and the approach can
be extended to other nonlinear rheological protocols and soft matter systems.",['cond-mat.soft'],2503.08894," Origami metamaterials made of repeating unit cells of parallelogram panels
joined at folds dramatically change their shape through a collective motion of
their cells. Here we develop an effective elastic model and numerical method to
study the large deformation response of these metamaterials under a broad class
of loads. The model builds on an effective plate theory derived in our prior
work [64]. The theory captures the overall shape change of all slightly
stressed parallelogram origami deformations through nonlinear geometric
compatibility constraints that couple the origami's (cell averaged) effective
deformation to an auxiliary angle field quantifying its cell-by-cell actuation.
It also assigns to each such origami deformation a plate energy associated to
these effective fields. Seeking a constitutive model that is faithful to the
theory but also practical to simulate, we relax the geometric constraints via
corresponding elastic energy penalties; we also simplify the plate energy
density to embrace its essential character as a regularization to the geometric
penalties. The resulting model for parallelogram origami is a generalized
elastic continuum that is nonlinear in the effective deformation gradient and
angle field and regularized by high-order gradients thereof. We provide a
finite element formulation of this model using the $C^0$ interior penalty
method to handle second gradients of deformation, and implement it using the
open source computing platform Firedrake. We end by using the model and
numerical method to study two canonical parallelogram origami patterns, in
Miura and Eggbox origami, under a variety of loading conditions.",['cond-mat.soft'],False,,,,"Microscopic Theory of Nonlinear Rheology and Double Yielding in Dense
  Attractive Glass Forming Colloidal Suspensions","Modeling and computation of the effective elastic behavior of
  parallelogram origami metamaterials"
neg-d2-808,2025-02-20,,2502.14451," Natural Language Generation (NLG) popularity has increased owing to the
progress in Large Language Models (LLMs), with zero-shot inference
capabilities. However, most neural systems utilize decoder-only causal
(unidirectional) transformer models, which are effective for English but may
reduce the richness of languages with less strict word order, subject omission,
or different relative clause attachment preferences. This is the first work
that analytically addresses optimal text generation order for non-causal
language models. We present a novel Viterbi algorithm-based methodology for
maximum likelihood word order estimation. We analyze the non-causal
most-likelihood order probability for NLG in Spanish and, then, the probability
of generating the same phrases with Spanish causal NLG. This comparative
analysis reveals that causal NLG prefers English-like SVO structures. We also
analyze the relationship between optimal generation order and causal
left-to-right generation order using Spearman's rank correlation. Our results
demonstrate that the ideal order predicted by the maximum likelihood estimator
is not closely related to the causal order and may be influenced by the
syntactic structure of the target sentence.",['cs.CL'],2502.14739," Large language models (LLMs) have demonstrated remarkable proficiency in
mainstream academic disciplines such as mathematics, physics, and computer
science. However, human knowledge encompasses over 200 specialized disciplines,
far exceeding the scope of existing benchmarks. The capabilities of LLMs in
many of these specialized fields-particularly in light industry, agriculture,
and service-oriented disciplines-remain inadequately evaluated. To address this
gap, we present SuperGPQA, a comprehensive benchmark that evaluates
graduate-level knowledge and reasoning capabilities across 285 disciplines. Our
benchmark employs a novel Human-LLM collaborative filtering mechanism to
eliminate trivial or ambiguous questions through iterative refinement based on
both LLM responses and expert feedback. Our experimental results reveal
significant room for improvement in the performance of current state-of-the-art
LLMs across diverse knowledge domains (e.g., the reasoning-focused model
DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting
the considerable gap between current model capabilities and artificial general
intelligence. Additionally, we present comprehensive insights from our
management of a large-scale annotation process, involving over 80 expert
annotators and an interactive Human-LLM collaborative system, offering valuable
methodological guidance for future research initiatives of comparable scope.",['cs.CL'],False,,,,"Optimal word order for non-causal text generation with Large Language
  Models: the Spanish case",SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines
neg-d2-809,2025-03-17,,2503.12952," As quantum computing advances, modern cryptographic standards face an
existential threat, necessitating a transition to post-quantum cryptography
(PQC). The National Institute of Standards and Technology (NIST) has selected
CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure
key exchange and digital signatures, respectively. This study conducts a
comprehensive performance analysis of these algorithms by benchmarking
execution times across cryptographic operations such as key generation,
encapsulation, decapsulation, signing, and verification. Additionally, the
impact of AVX2 optimizations is evaluated to assess hardware acceleration
benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient
execution times, outperforming classical cryptographic schemes such as RSA and
ECDSA at equivalent security levels. Beyond technical performance, the
real-world deployment of PQC introduces challenges in telecommunications
networks, where large-scale infrastructure upgrades, interoperability with
legacy systems, and regulatory constraints must be addressed. This paper
examines the feasibility of PQC adoption in telecom environments, highlighting
key transition challenges, security risks, and implementation strategies.
Through industry case studies, we illustrate how telecom operators are
integrating PQC into 5G authentication, subscriber identity protection, and
secure communications. Our analysis provides insights into the computational
trade-offs, deployment considerations, and standardization efforts shaping the
future of quantum-safe cryptographic infrastructure.",['cs.CR'],2501.06084," The Fisher-Yates shuffle is a well-known algorithm for shuffling a finite
sequence, such that every permutation is equally likely. Despite its
simplicity, it is prone to implementation errors that can introduce bias into
the generated permutations. We verify its correctness in Dafny as follows.
First, we define a functional model that operates on sequences and streams of
random bits. Second, we establish that the functional model has the desired
distribution. Third, we define an executable imperative implementation that
operates on arrays and prove it equivalent to the functional model. The
approach may serve as a blueprint for the verification of more complex
algorithms.",['cs.CR'],False,,,,"Performance Analysis and Industry Deployment of Post-Quantum
  Cryptography Algorithms",Verifying the Fisher-Yates Shuffle Algorithm in Dafny
neg-d2-810,2025-01-02,,2501.04034," This paper is devoted to the variational inequality problems. We consider two
classes of problems, the first is classical constrained variational inequality
and the second is the same problem with functional (inequality type)
constraints. To solve these problems, we propose mirror descent-type methods
with a weighting scheme for the generated points in each iteration of the
algorithms. This scheme assigns smaller weights to the initial points and
larger weights to the most recent points, thus it improves the convergence rate
of the proposed methods. For the variational inequality problem with functional
constraints, the proposed method switches between adaptive and non-adaptive
steps in the dependence on the values of the functional constraints at
iterations. We analyze the proposed methods for the time-varying step sizes and
prove the optimal convergence rate for variational inequality problems with
bounded and monotone operators. The results of numerical experiments of the
proposed methods for classical constrained variational inequality problems show
a significant improvement over the modified projection method.",['math.OC'],2503.09179," This paper concerns the problem of reachability of a given state for a
multiagent control system in $\mathbb{R}^d$. In such a system, at every time
each agent can choose his/her velocity which depends both on his/her position
and on the position of the whole crowd of agents (modeled by a probability
measure on $ \mathbb{R}^d$). The main contribution of the paper is to study the
above reachability problem with a given rate of attainability through a
Lyapunov method adapted to the Wasserstein space of probability measures. As a
byproduct we obtain a new comparison result for viscosity solutions of Hamilton
Jacobi equations in the Wasserstein space.",['math.OC'],False,,,,"Mirror Descent Methods with Weighting Scheme for Outputs for Constrained
  Variational Inequality Problems",Reachability for multiagent control systems via Lyapunov functions
neg-d2-811,2025-02-25,,2502.18056," The reliance on large-scale datasets and extensive computational resources
has become a major barrier to advancing representation learning in vision,
especially in data-scarce domains. In this paper, we address the critical
question: Can we escape the big data paradigm in self-supervised representation
learning from images? We introduce SCOTT (Sparse Convolutional Tokenizer for
Transformers), a shallow tokenization architecture that is compatible with
Masked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biases
into Vision Transformers (ViTs), enhancing their efficacy in small-scale data
regimes. Alongside, we propose to train on a Joint-Embedding Predictive
Architecture within a MIM framework (MIM-JEPA), operating in latent
representation space to capture more semantic features. Our approach enables
ViTs to be trained from scratch on datasets orders of magnitude smaller than
traditionally required --without relying on massive external datasets for
pretraining. We validate our method on three small-size, standard-resoultion,
fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, and
ImageNet-100. Despite the challenges of limited data and high intra-class
similarity, frozen SCOTT models pretrained with MIM-JEPA significantly
outperform fully supervised methods and achieve competitive results with SOTA
approaches that rely on large-scale pretraining, complex image augmentations
and bigger model sizes. By demonstrating that robust off-the-shelf
representations can be learned with limited data, compute, and model sizes, our
work paves the way for computer applications in resource constrained
environments such as medical imaging or robotics. Our findings challenge the
prevailing notion that vast amounts of data are indispensable for effective
representation learning in vision, offering a new pathway toward more
accessible and inclusive advancements in the field.",['cs.CV'],2501.06312," Foundation models are becoming increasingly popular due to their strong
generalization capabilities resulting from being trained on huge datasets.
These generalization capabilities are attractive in areas such as NIR Iris
Presentation Attack Detection (PAD), in which databases are limited in the
number of subjects and diversity of attack instruments, and there is no
correspondence between the bona fide and attack images because, most of the
time, they do not belong to the same subjects. This work explores an iris PAD
approach based on two foundation models, DinoV2 and VisualOpenClip. The results
show that fine-tuning prediction with a small neural network as head overpasses
the state-of-the-art performance based on deep learning approaches. However,
systems trained from scratch have still reached better results if bona fide and
attack images are available.",['cs.CV'],False,,,,"Escaping The Big Data Paradigm in Self-Supervised Representation
  Learning",Towards Iris Presentation Attack Detection with Foundation Models
neg-d2-812,2025-02-26,,2502.19045," The investigation of remnants associated with the QCD chiral critical point
is a primary objective in high-energy ion collision experiments. Numerous
studies indicate that a scaling relation between higher-order factorial moments
of hadron multiplicity distributions and the second factorial moment may serve
as a diagnostic tool for identifying the QCD critical point. However, we
demonstrate that this scaling behavior is not exclusive to critical phenomena
but rather arises as a general consequence of the phase-space partitioning
procedure employed in the analysis. This finding is examined in the context of
recent intermittency analyses conducted by the STAR experiment at RHIC.",['hep-ph'],2502.12487," We present the explicit form of the Regge trajectory relations for the doubly
heavy baryons $\Xi_{QQ'}$ and $\Omega_{QQ'}$ $(Q,Q'=b,c)$ in the diquark
picture. Using the derived Regge trajectory relations, we estimate the masses
of the $\lambda$-excited states and the $\rho$-excited states, which are
consistent with other theoretical predictions. Both the $\lambda$-trajectories
and $\rho$-trajectories are discussed. We show that the $\rho$-trajectories
behave differently from the $\lambda$-trajectories. Specifically, the
$\rho$-trajectories behave as $M{\sim}x_{\rho}^{2/3}$ $(x_{\rho}=n_r,l)$,
whereas the $\lambda$-trajectories follow $M{\sim}x_{\lambda}^{1/2}$
$(x_{\lambda}=N_r,L)$. By using the obtained relations, the baryon Regge
trajectory provides a straightforward and easy method for estimating the
spectra of both the $\lambda$-excited states and $\rho$-excited states.",['hep-ph'],False,,,,Scaling for count-in-cell and factorial moment analysis,"$\lambda$ and $\rho$ trajectories for the doubly heavy baryons in the
  diquark picture"
neg-d2-813,2025-03-10,,2503.07554," Recent inductive logic programming (ILP) approaches learn optimal hypotheses.
An optimal hypothesis minimises a given cost function on the training data.
There are many cost functions, such as minimising training error, textual
complexity, or the description length of hypotheses. However, selecting an
appropriate cost function remains a key question. To address this gap, we
extend a constraint-based ILP system to learn optimal hypotheses for seven
standard cost functions. We then empirically compare the generalisation error
of optimal hypotheses induced under these standard cost functions. Our results
on over 20 domains and 1000 tasks, including game playing, program synthesis,
and image reasoning, show that, while no cost function consistently outperforms
the others, minimising training error or description length has the best
overall performance. Notably, our results indicate that minimising the size of
hypotheses does not always reduce generalisation error.",['cs.LG'],2503.01048," Personalizing large language models (LLMs) is essential for delivering
tailored interactions that improve user experience. Many existing
personalization methods require fine-tuning LLMs for each user, rendering them
prohibitively expensive for widespread adoption. Although retrieval-based
approaches offer a more compute-efficient alternative, they still depend on
large, high-quality datasets that are not consistently available for all users.
To address this challenge, we propose CHAMELEON, a scalable and efficient
personalization approach that uses (1) self-generated personal preference data
and (2) representation editing to enable quick and cost-effective
personalization. Our experiments on various tasks, including those from the
LaMP personalization benchmark, show that CHAMELEON efficiently adapts models
to personal preferences, improving instruction-tuned models and outperforms two
personalization baselines by an average of 40% across two model architectures.",['cs.LG'],False,,,,An Empirical Comparison of Cost Functions in Inductive Logic Programming,Personalize Your LLM: Fake it then Align it
neg-d2-814,2025-02-18,,2502.12874," Causality is widely used in fairness analysis to prevent discrimination on
sensitive attributes, such as genders in career recruitment and races in crime
prediction. However, the current data-based Potential Outcomes Framework (POF)
often leads to untrustworthy fairness analysis results when handling
high-dimensional data. To address this, we introduce a distribution-based POF
that transform fairness analysis into Distributional Closeness Testing (DCT) by
intervening on sensitive attributes. We define counterfactual closeness
fairness as the null hypothesis of DCT, where a sensitive attribute is
considered fair if its factual and counterfactual potential outcome
distributions are sufficiently close. We introduce the Norm-Adaptive Maximum
Mean Discrepancy Treatment Effect (N-TE) as a statistic for measuring
distributional closeness and apply DCT using the empirical estimator of NTE,
referred to Counterfactual Fairness-CLOseness Testing ($\textrm{CF-CLOT}$). To
ensure the trustworthiness of testing results, we establish the testing
consistency of N-TE through rigorous theoretical analysis. $\textrm{CF-CLOT}$
demonstrates sensitivity in fairness analysis through the flexibility of the
closeness parameter $\epsilon$. Unfair sensitive attributes have been
successfully tested by $\textrm{CF-CLOT}$ in extensive experiments across
various real-world scenarios, which validate the consistency of the testing.",['cs.LG'],2502.08949," Self-supervised graph representation learning has driven significant
advancements in domains such as social network analysis, molecular design, and
electronics design automation (EDA). However, prior works in EDA have mainly
focused on the representation of gate-level digital circuits, failing to
capture analog and mixed-signal circuits. To address this gap, we introduce
DICE: Device-level Integrated Circuits Encoder, the first self-supervised
pretrained graph neural network (GNN) model for any circuit expressed at the
device level. DICE is a message-passing neural network (MPNN) trained through
graph contrastive learning, and its pretraining process is simulation-free,
incorporating two novel data augmentation techniques. Experimental results
demonstrate that DICE achieves substantial performance gains across three
downstream tasks, underscoring its effectiveness for both analog and digital
circuits.",['cs.LG'],False,,,,Testing for Causal Fairness,"Self-Supervised Graph Contrastive Pretraining for Device-level
  Integrated Circuits"
neg-d2-815,2025-02-15,,2502.1073," Emergent phenomena take place in symmetry-breaking systems, notably the
recently discovered two-dimensional electron gas and its tunable
superconductivities near the KTaO3 interfaces. Here, we synthesized perovskite
Ca0.5TaO3 films along both [001] and [111] orientations. Different from the
KTaO3 system, Ca0.5TaO3 films show semiconducting behaviors when capped with
LaAlO3 films in both [001] and [111] orientations. By growing films at higher
temperatures, more oxygen vacancies can be introduced, and the carrier density
can be tuned from ~ 1014 cm-2 to ~ 1016 cm-2. Another difference is that the
superconducting transition temperature Tc in KTaO3 (111) increases linearly
along with its carrier density, while the Ca0.5TaO3 (111) remains
semiconducting when carrier density ranges from ~ 1014 cm-2 to ~ 1016 cm-2.
Based on the density function theory calculation, Ca0.5TaO3 and KTaO3 show
similar electronic band structures. According to the energy-dispersive X-ray
spectroscopy, we found heavy Sr diffusion from the substrate to the Ca0.5TaO3
layer, which may destroy the interfacial conductivity. Our work demonstrates
that besides the oxygen vacancies, electronic transport is sensitive to the
atomic intermixing near the interface in tantulates.",['cond-mat.mtrl-sci'],2502.00337," Exotic nondiffusive heat transfer regimes such as the second sound, where
heat propagates as a damped wave at speeds comparable to those of mechanical
disturbances, often occur at cryogenic temperatures (T) and nanosecond
timescales in semiconductors. First-principles prediction of such rapid, low-T
phonon dynamics requires finely-resolved temporal tracking of large, dense, and
coupled linear phonon dynamical systems arising from the governing linearized
Peierls-Boltzmann equation (LPBE). Here, we uncover a rigorous low-rank
representation of these linear dynamical systems, derived from the spectral
properties of the phonon collision matrix, that accelerates the
first-principles prediction of phonon dynamics by a factor of over a million
without compromising on the computational accuracy. By employing this low-rank
representation of the LPBE, we predict strong amplification of the wave-like
second sound regime upon isotopic enrichment in diamond - a finding that would
have otherwise been computationally intractable using the conventional
brute-force approaches. Our framework enables a rapid and accurate discovery of
the conditions under which wave-like heat flow can be realized in common
semiconductors.",['cond-mat.mtrl-sci'],False,,,,Semiconducting behaviors at epitaxial Ca0.5TaO3 interfaces,"Efficient calculation of phonon dynamics through a low-rank solution of
  the Boltzmann equation"
neg-d2-816,2025-01-16,,2501.09748," In recent years, numerical simulations have become indispensable for
addressing complex astrophysical problems. The MagnetoHydroDynamics (MHD)
framework represents a key tool for investigating the dynamical evolution of
astrophysical plasmas, which are described as a set of partial differential
equations that enforce the conservation of mass, momentum, and energy, along
with Maxwell's equation for the evolution of the electromagnetic fields. Due to
the high nonlinearity of the MHD equations (regardless of their specifications,
e.g., classical/relativistic or ideal/resistive), a general analytical solution
is precluded, making the numerical approach crucial. Numerical simulations
usually end up producing large sets of data files and their scientific analysis
leans on dedicated software designed for data visualization. However, in order
to encompass all of the code output features, specialized tools focusing on the
numerical code may represent a more versatile and built-in tool. Here, we
present PyPLUTO, a Python package tailored for efficient loading, manipulation,
and visualization of outputs produced with the PLUTO code (Mignone et al.,
2007; Mignone et al., 2012). PyPLUTO uses memory mapping to optimize data
loading and provides general routines for data manipulation and visualization.
PyPLUTO also supports the particle modules of the PLUTO code, enabling users to
load and visualize particles, such as cosmic rays (Mignone et al., 2018),
Lagrangian (Vaidya et al., 2018), or dust (Mignone et al., 2019) particles,
from hybrid simulations. A dedicated Graphical User Interface (GUI) simplifies
the generation of single-subplot figures, making PyPLUTO a powerful yet
user-friendly toolkit for astrophysical data analysis.",['astro-ph.IM'],2501.17997," Developing algorithms to search through data efficiently is a challenging
part of searching for signs of technology beyond our solar system. We have
built a digital signal processing system and computer cluster on the backend of
the Karl G. Jansky Very Large Array (VLA) in New Mexico in order to search for
signals throughout the Galaxy consistent with our understanding of artificial
radio emissions. In our first paper, we described the system design and
software pipelines. In this paper, we describe a postprocessing pipeline to
identify persistent sources of interference, filter out false positives, and
search for signals not immediately identifiable as anthropogenic radio
frequency interference during the VLA Sky Survey. As of 01 September 2024, the
Commensal Open-source Multi-mode Interferometric Cluster had observed more than
950,000 unique pointings. This paper presents the strategy we employ when
commensally observing during the VLA Sky Survey and a postprocessing strategy
for the data collected during the survey. To test this postprocessing pipeline,
we searched toward 511 stars from the $Gaia$ catalog with coherent beams. This
represents about 30 minutes of observation during VLASS, where we typically
observe about 2000 sources per hour in the coherent beamforming mode. We did
not detect any unidentifiable signals, setting isotropic power limits ranging
from 10$^{11}$ to 10$^{16}$W.",['astro-ph.IM'],False,,,,PyPLUTO: a data analysis Python package for the PLUTO code,"COSMIC's Large-Scale Search for Technosignatures during the VLA sky
  Survey: Survey Description and First Results"
neg-d2-817,2025-02-08,,2502.05621," In this paper, we present several machine learning approaches for predicting
the behavior of both classical and quantum systems. For the classical domain,
we model a pendulum subject to multiple forces using both a standard artificial
neural network (ANN) and a physics-informed neural network (PINN). For the
quantum domain, we predict the ground state energy of a quantum anharmonic
oscillator from discretized potential data using an ANN with convolutional
layers (CNN), a long short-term memory (LSTM) network, and a PINN that
incorporates the Schr\""odinger equation. Detailed training outputs and
comparisons are provided.",['quant-ph'],2502.07605," In contrast to the commonly used qubit resonator transverse coupling via the
$\sigma_{xy}$-degree of freedom, longitudinal coupling through $\sigma_z$
presents a tantalizing alternative: it does not hybridize the modes,
eliminating Purcell decay, and it enables quantum-non-demolishing qubit readout
independent of the qubit-resonator frequency detuning. Here, we demonstrate
longitudinal coupling between a {Cr$_7$Ni} molecular spin qubit ensemble and
the kinetic inductance of a granular aluminum superconducting microwave
resonator. The inherent frequency-independence of this coupling allows for the
utilization of a 7.8 GHz readout resonator to measure the full {Cr$_7$Ni}
magnetization curve spanning 0-600 mT, corresponding to a spin frequency range
of $f_\text{spin}=$0-15 GHz. For 2 GHz detuning from the readout resonator, we
measure a $1/e$ spin relaxation time $\tau=$0.38 s, limited by phonon decay to
the substrate. Based on these results, we propose a path towards longitudinal
coupling of single spins to a superconducting fluxonium qubit.",['quant-ph'],False,,,,"Predictive Modeling of Classical and Quantum Mechanics Using Machine
  Learning: A Case Study with TensorFlow",Kinetic inductance coupling for circuit QED with spins
neg-d2-818,2025-02-04,,2502.02074," Stance detection has emerged as a popular task in natural language processing
research, enabled largely by the abundance of target-specific social media
data. While there has been considerable research on the development of stance
detection models, datasets, and application, we highlight important gaps
pertaining to (i) a lack of theoretical conceptualization of stance, and (ii)
the treatment of stance at an individual- or user-level, as opposed to
message-level. In this paper, we first review the interdisciplinary origins of
stance as an individual-level construct to highlight relevant attributes (e.g.,
psychological features) that might be useful to incorporate in stance detection
models. Further, we argue that recent pre-trained and large language models
(LLMs) might offer a way to flexibly infer such user-level attributes and/or
incorporate them in modelling stance. To better illustrate this, we briefly
review and synthesize the emerging corpus of studies on using LLMs for
inferring stance, and specifically on incorporating user attributes in such
tasks. We conclude by proposing a four-point agenda for pursuing stance
detection research that is theoretically informed, inclusive, and practically
impactful.",['cs.CL'],2502.19953," Regular updates are essential for maintaining up-to-date knowledge in large
language models (LLMs). Consequently, various model editing methods have been
developed to update specific knowledge within LLMs. However, training-based
approaches often struggle to effectively incorporate new knowledge while
preserving unrelated general knowledge. To address this challenge, we propose a
novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes
the geometric relationships of parameter updates from fine-tuning to
differentiate between neurons associated with new knowledge updates and those
related to general knowledge perturbations. By employing a direction-aware
knowledge identification method, we avoid updating neurons with directions
approximately orthogonal to existing knowledge, thus preserving the model's
generalization ability. For the remaining neurons, we integrate both old and
new knowledge for aligned directions and apply a ""forget-then-learn"" editing
strategy for opposite directions. Additionally, we introduce an
importance-guided task vector fusion technique that filters out redundant
information and provides adaptive neuron-level weighting, further enhancing
model editing performance. Extensive experiments on two publicly available
datasets demonstrate the superiority of GeoEdit over existing state-of-the-art
methods.",['cs.CL'],False,,,,"Rethinking stance detection: A theoretically-informed research agenda
  for user-level inference using language models",GeoEdit: Geometric Knowledge Editing for Large Language Models
neg-d2-819,2025-01-20,,2501.11523," In this work, our interest lies in proving the existence of solutions to the
following Fractional Lane-Emden Hamiltonian system: $$ \begin{cases}
(-\Delta)^s u = H_v(x,u,v) & \text{in }\Omega,\\ (-\Delta)^s v = H_u(x,u,v) &
\text{in }\Omega,\\ u=v=0 & \text{in } \R^n\setminus\Omega. \end{cases} $$ The
method, that can be traced back to the work of De Figueiredo and Felmer
\cite{DF-F}, is flexible enough to deal with more general nonlocal operators
and make use of a combination of fractional order Sobolev spaces together with
functional calculus for self-adjoint operators.",['math.AP'],2501.11693," We investigate the following Kirchhoff-type biharmonic equation
\begin{equation}\label{pr} \left\{ \begin{array}{ll} \Delta^2 u+
\left(a+b\int_{\mathbb{R}^N}|\nabla u|^2d x\right)(-\Delta
u+V(x)u)=f(x,u),\quad x\in \mathbb{R}^N,\\ u\in H^{2}(\mathbb{R}^N),
\end{array} \right. \end{equation} where $a>0$, $b\geq 0$ and $V(x)$ and $f(x,
u)$ are periodic or asymptotically periodic in $x$. We study the existence of
Nehari-type ground state solutions of \eqref{pr} with $f(x,u)u-4F(x,u)$
sign-changing, where $F(x,u):=\int_0^uf(x,s)d s$. We significantly extend some
results from the previous literature.",['math.AP'],False,,,,Fractional Lane-Emden Hamiltonian systems,"Nehari-type ground state solutions for asymptotically periodic
  bi-harmonic Kirchhoff-type problems in $\mathbb{R}^N$"
neg-d2-820,2025-03-10,,2503.07797," News reading helps individuals stay informed about events and developments in
society. Local residents and new immigrants often approach the same news
differently, prompting the question of how technology, such as LLM-powered
chatbots, can best enhance a reader-oriented news experience. The current paper
presents an empirical study involving 144 participants from three groups in
Virginia, United States: local residents born and raised there (N=48), Chinese
immigrants (N=48), and Vietnamese immigrants (N=48). All participants read
local housing news with the assistance of the Copilot chatbot. We collected
data on each participant's Q&A interactions with the chatbot, along with their
takeaways from news reading. While engaging with the news content, participants
in both immigrant groups asked the chatbot fewer analytical questions than the
local group. They also demonstrated a greater tendency to rely on the chatbot
when formulating practical takeaways. These findings offer insights into
technology design that aims to serve diverse news readers.",['cs.HC'],2501.056," Mentorship in open source software (OSS) is a vital, multifaceted process
that includes onboarding newcomers, fostering skill development, and enhancing
community building. This study examines task-focused mentoring strategies that
help mentees complete their tasks and the ideal personal qualities and outcomes
of good mentorship in OSS communities. We conducted two surveys to gather
contributor perceptions: the first survey, with 70 mentors, mapped 17 mentoring
challenges to 21 strategies that help support mentees. The second survey, with
85 contributors, assessed the importance of personal qualities and ideal
mentorship outcomes. Our findings not only provide actionable strategies to
help mentees overcome challenges and become successful contributors but also
guide current and future mentors and OSS communities in understanding the
personal qualities that are the cornerstone of good mentorship and the outcomes
that mentor-mentee pairs should aspire to achieve.",['cs.HC'],False,,,,"The News Says, the Bot Says: How Immigrants and Locals Differ in
  Chatbot-Facilitated News Reading","The Multifaceted Nature of Mentoring in OSS: Strategies, Qualities, and
  Ideal Outcomes"
neg-d2-821,2025-01-31,,2501.19083," Diffusion models have become a popular choice for human motion synthesis due
to their powerful generative capabilities. However, their high computational
complexity and large sampling steps pose challenges for real-time applications.
Fortunately, the Consistency Model (CM) provides a solution to greatly reduce
the number of sampling steps from hundreds to a few, typically fewer than four,
significantly accelerating the synthesis of diffusion models. However, applying
CM to text-conditioned human motion synthesis in latent space yields
unsatisfactory generation results. In this paper, we introduce
\textbf{MotionPCM}, a phased consistency model-based approach designed to
improve the quality and efficiency for real-time motion synthesis in latent
space. Experimental results on the HumanML3D dataset show that our model
achieves real-time inference at over 30 frames per second in a single sampling
step while outperforming the previous state-of-the-art with a 38.9\%
improvement in FID. The code will be available for reproduction.",['cs.CV'],2503.0227," Salient object detection (SOD) in RGB-D images is an essential task in
computer vision, enabling applications in scene understanding, robotics, and
augmented reality. However, existing methods struggle to capture global
dependency across modalities, lack comprehensive saliency priors from both RGB
and depth data, and are ineffective in handling low-quality depth maps. To
address these challenges, we propose SSNet, a saliency-prior and state space
model (SSM)-based network for the RGB-D SOD task. Unlike existing convolution-
or transformer-based approaches, SSNet introduces an SSM-based multi-modal
multi-scale decoder module to efficiently capture both intra- and inter-modal
global dependency with linear complexity. Specifically, we propose a
cross-modal selective scan SSM (CM-S6) mechanism, which effectively captures
global dependency between different modalities. Furthermore, we introduce a
saliency enhancement module (SEM) that integrates three saliency priors with
deep features to refine feature representation and improve the localization of
salient objects. To further address the issue of low-quality depth maps, we
propose an adaptive contrast enhancement technique that dynamically refines
depth maps, making them more suitable for the RGB-D SOD task. Extensive
quantitative and qualitative experiments on seven benchmark datasets
demonstrate that SSNet outperforms state-of-the-art methods.",['cs.CV'],False,,,,MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model,"SSNet: Saliency Prior and State Space Model-based Network for Salient
  Object Detection in RGB-D Images"
neg-d2-822,2025-01-15,,2501.08718," We conduct a comprehensive study into the impact of pixelization on cosmic
shear, uncovering several sources of bias in standard pseudo-$C_\ell$
estimators based on discrete catalogues. We derive models that can bring
residual biases to the percent level on small scales. We elucidate the impact
of aliasing and the varying shape of HEALPix pixels on power spectra and show
how the HEALPix pixel window function approximation is made in the discrete
spin-2 setting. We propose several improvements to the standard estimator and
its modelling, based on the principle that source positions and weights are to
be considered fixed. We show how empty pixels can be accounted for either by
modifying the mixing matrices or applying correction factors that we derive. We
introduce an approximate interlacing scheme for the HEALPix grid and show that
it can mitigate the effects of aliasing. We introduce bespoke pixel window
functions adapted to the survey footprint and show that, for band-limited
spectra, biases from using an isotropic window function can be effectively
reduced to zero. This work partly intends to serve as a useful reference for
pixel-related effects in angular power spectra, which are of relevance for
ongoing and forthcoming lensing and clustering surveys.",['astro-ph.CO'],2501.10622," Cosmological simulations are an important method for investigating the
evolution of the Universe. In order to gain further insight into the processes
of structure formation, it is necessary to identify isolated bound objects
within the simulations, namely, the dark matter halos. The continuous wavelet
transform (CWT) is an effective tool used as a halo finder due to its ability
to extract clustering information from the input data. In this study, we
introduce CWTHF (Continuous Wavelet Transform Halo Finder), the first
wavelet-based, MPI-parallelized halo finder, marking a novel approach in the
field of cosmology. We calculate the CWT from the cloud-in-cell (CIC) grid and
segment the grid based on the local CWT maxima. We then investigate the effects
of the parameters that influence our program and identify the default settings.
A comparison with the conventional friends-of-friends (FOF) method demonstrates
the viability of CWT for halo finding. Although the actual performance is not
faster than FOF, the linear time complexity of $\mathcal{O}(N)$ of our
identification scheme indicates its significant potential for future
optimization and application.",['astro-ph.CO'],False,,,,Pixelization effects in cosmic shear angular power spectra,CWTHF: Identifying Dark Matter Halos with Continuous Wavelet Transform
neg-d2-823,2025-03-19,,2503.15216," In this work, we investigate the non-Markovian dynamical evolution of a
${\Lambda}$-type atom interacting with a semi-infinite one-dimensional photonic
waveguide via two atomic transitions. The waveguide terminates at a perfect
mirror, which reflects the light and introduces boundary effects. We derive
exact analytical expressions and show that, under suitable conditions, the
instantaneous and retarded decay rates reach equilibrium, leading to the
formation of an atom-photon bound state that suppresses dissipation.
Consequently, the atom retains a long-lived population in the asymptotic time
limit. Furthermore, we analyze the output field intensity and demonstrate that
blocking one of the coupling channels forces the atomic system to emit photons
of a single frequency. Finally, we extend the model to a two-atom system and
examine the disentanglement dynamics of the two spatially separated atoms.
These findings elucidate the dynamic process of spontaneous emission involving
multi-frequency photons from multi-level atoms and provide insights into the
complex interference between different decay pathways.",['quant-ph'],2501.06846," As is well known, unital Pauli maps can be eternally non-CP-divisible. In
contrast, here we show that in the case of non-unital maps, eternal
non-Markovianity in the non-unital part is ruled out. In the unital case, the
eternal non-Markovianity can be obtained by a convex combination of two
dephasing semigroups, but not all three of them. We study these results and the
ramifications arising from them.",['quant-ph'],False,,,,"Non-Markovian dynamics with ${\Lambda}$-type atomic systems in a single
  end photonic waveguide",On the eternal non-Markovianity of qubit maps
neg-d2-824,2025-01-30,,2501.1858," This study focuses on the application of deep geometric models to solve the
3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and
defining distance as the model's optimization objective. The distance
approximation task is reformulated as a node classification problem,
effectively addressed using Graph Neural Networks (GNNs). After training the
model on a random subgraph, the predicted classes are used to construct a
heuristic for $A^*$ search. We conclude with experiments comparing our
heuristic to that of the DeepCubeA model.",['cs.LG'],2502.20314," Implicit Neural Representations (INRs) have been recently garnering
increasing interest in various research fields, mainly due to their ability to
represent large, complex data in a compact and continuous manner. Past work
further showed that numerous popular downstream tasks can be performed directly
in the INR parameter-space. Doing so can substantially reduce the computational
resources required to process the represented data in their native domain. A
major difficulty in using modern machine-learning approaches, is their high
susceptibility to adversarial attacks, which have been shown to greatly limit
the reliability and applicability of such methods in a wide range of settings.
In this work, we show that parameter-space models trained for classification
are inherently robust to adversarial attacks -- without the need of any robust
training. To support our claims, we develop a novel suite of adversarial
attacks targeting parameter-space classifiers, and furthermore analyze
practical considerations of attacking parameter-space classifiers.",['cs.LG'],False,,,,Node Classification and Search on the Rubik's Cube Graph with GNNs,Adversarial Robustness in Parameter-Space Classifiers
neg-d2-825,2025-03-20,,2503.15851," Animatable head avatar generation typically requires extensive data for
training. To reduce the data requirements, a natural solution is to leverage
existing data-free static avatar generation methods, such as pre-trained
diffusion models with score distillation sampling (SDS), which align avatars
with pseudo ground-truth outputs from the diffusion model. However, directly
distilling 4D avatars from video diffusion often leads to over-smooth results
due to spatial and temporal inconsistencies in the generated video. To address
this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial
and temporal consistency dataset for 4D avatar reconstruction using the video
diffusion model. Specifically, Zero-1-to-A iteratively constructs video
datasets and optimizes animatable avatars in a progressive manner, ensuring
that avatar quality increases smoothly and consistently throughout the learning
process. This progressive learning involves two stages: (1) Spatial Consistency
Learning fixes expressions and learns from front-to-side views, and (2)
Temporal Consistency Learning fixes views and learns from relaxed to
exaggerated expressions, generating 4D avatars in a simple-to-complex manner.
Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation
quality, and rendering speed compared to existing diffusion-based methods,
providing a solution for lifelike avatar creation. Code is publicly available
at: https://github.com/ZhenglinZhou/Zero-1-to-A.",['cs.CV'],2501.17821," Scene flow enables an understanding of the motion characteristics of the
environment in the 3D world. It gains particular significance in the
long-range, where object-based perception methods might fail due to sparse
observations far away. Although significant advancements have been made in
scene flow pipelines to handle large-scale point clouds, a gap remains in
scalability with respect to long-range. We attribute this limitation to the
common design choice of using dense feature grids, which scale quadratically
with range. In this paper, we propose Sparse Scene Flow (SSF), a general
pipeline for long-range scene flow, adopting a sparse convolution based
backbone for feature extraction. This approach introduces a new challenge: a
mismatch in size and ordering of sparse feature maps between time-sequential
point scans. To address this, we propose a sparse feature fusion scheme, that
augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater
importance to faraway points. Our method, SSF, achieves state-of-the-art
results on the Argoverse2 dataset, demonstrating strong performance in
long-range scene flow estimation. Our code will be released at
https://github.com/KTH-RPL/SSF.git.",['cs.CV'],False,,,,"Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video
  Diffusion",SSF: Sparse Long-Range Scene Flow for Autonomous Driving
neg-d2-826,2025-01-24,,2501.14718," Cancer grade is a critical clinical criterion that can be used to determine
the degree of cancer malignancy. Revealing the condition of the glands, a
precise gland segmentation can assist in a more effective cancer grade
classification. In machine learning, binary classification information about
glands (i.e., benign and malignant) can be utilized as a prompt for gland
segmentation and cancer grade classification. By incorporating prior knowledge
of the benign or malignant classification of the gland, the model can
anticipate the likely appearance of the target, leading to better segmentation
performance. We utilize Segment Anything Model to solve the segmentation task,
by taking advantage of its prompt function and applying appropriate
modifications to the model structure and training strategies. We improve the
results from fine-tuned Segment Anything Model and produce SOTA results using
this approach.",['eess.IV'],2501.1721," Sentinel-5P (S5P) satellite provides atmospheric measurements for air quality
and climate monitoring. While the S5P satellite offers rich spectral
resolution, it inherits physical limitations that restricts its spatial
resolution. Super-resolution (SR) techniques can overcome these limitations and
enhance the spatial resolution of S5P data. In this work, we introduce a novel
SR model specifically designed for S5P data that have eight spectral bands with
around 500 channels for each band. Our proposed S5-DSCR model relies on Depth
Separable Convolution (DSC) architecture to effectively perform spatial SR by
exploiting cross-channel correlations. Quantitative evaluation demonstrates
that our model outperforms existing methods for the majority of the spectral
bands. This work highlights the potential of leveraging DSC architecture to
address the challenges of hyperspectral SR. Our model allows for capturing fine
details necessary for precise analysis and paves the way for advancements in
air quality monitoring as well as remote sensing applications.",['eess.IV'],False,,,,Gland Segmentation Using SAM With Cancer Grade as a Prompt,Depth Separable architecture for Sentinel-5P Super-Resolution
neg-d2-827,2025-03-12,,2503.09145," Despite the rapid advancements in 5G technology, accurately assessing the
energy consumption of its Radio Access Networks (RANs) remains a challenge due
to the diverse range of applicable technologies and implementation solutions.
Designing a versatile power model for estimating the 5G RANspecific power
consumption requires extensive data collection and experimental studies to
capture the diverse range of technologies and implementation solutions. The
objective is to outline a versatile energy model capable of estimating
RAN-specific energy consumption, encompassing both mobile terminals and the
physical layer (PHY) of base stations. In this paper, we focus on the
computational complexity of the baseband part of the model. The developed (part
of the) model is compared with the estimation of the number of cycles (and
energy per cycle) used by a specific implementation (here a Matlab code ported
on an Intel target), enabling the assessment of the model with the estimation
of energy consumed on a real target. The study's results show a good agreement
between the model and the implementation, even if some parts need to be refined
to take specific algorithms into account. The key contribution is the
development of an initial flexible energy model with finer granularity,
enabling comparisons of energy use across various applications and contexts,
and offering a comprehensive tool for optimizing 5G network energy consumption.",['cs.NI'],2501.151," The rapid development of programmable network devices and the widespread use
of machine learning (ML) in networking have facilitated efficient research into
intelligent data plane (IDP). Offloading ML to programmable data plane (PDP)
enables quick analysis and responses to network traffic dynamics, and efficient
management of network links. However, PDP hardware pipeline has significant
resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in
each stage, and lacks support for multiplication, division and floating-point
operations. These constraints significantly hinder the development of IDP. This
paper presents \quark, a framework that fully offloads convolutional neural
network (CNN) inference onto PDP. \quark employs model pruning to simplify the
CNN model, and uses quantization to support floating-point operations.
Additionally, \quark divides the CNN into smaller units to improve resource
utilization on the PDP. We have implemented a testbed prototype of \quark on
both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2).
Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy
in anomaly detection task while using only 22.7\% of the SRAM resources on the
Intel Tofino ASIC switch, completing inference tasks at line rate with an
average latency of 42.66$\mu s$.",['cs.NI'],False,,,,"Charting 5G Energy Efficiency: Flexible Energy Modeling for Sustainable
  Networks","Quark: Implementing Convolutional Neural Networks Entirely on
  Programmable Data Plane"
neg-d2-828,2025-01-28,,2501.1721," Sentinel-5P (S5P) satellite provides atmospheric measurements for air quality
and climate monitoring. While the S5P satellite offers rich spectral
resolution, it inherits physical limitations that restricts its spatial
resolution. Super-resolution (SR) techniques can overcome these limitations and
enhance the spatial resolution of S5P data. In this work, we introduce a novel
SR model specifically designed for S5P data that have eight spectral bands with
around 500 channels for each band. Our proposed S5-DSCR model relies on Depth
Separable Convolution (DSC) architecture to effectively perform spatial SR by
exploiting cross-channel correlations. Quantitative evaluation demonstrates
that our model outperforms existing methods for the majority of the spectral
bands. This work highlights the potential of leveraging DSC architecture to
address the challenges of hyperspectral SR. Our model allows for capturing fine
details necessary for precise analysis and paves the way for advancements in
air quality monitoring as well as remote sensing applications.",['eess.IV'],2501.15246," Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular
environments. Accurate reconstruction of high-resolution volumes is complicated
by the very low signal-to-noise ratio and a restricted range of sample tilts,
creating a missing wedge of Fourier information. Recent self-supervised deep
learning approaches, which post-process initial reconstructions done by
filtered backprojection (FBP), have significantly improved reconstruction
quality, but they are computationally expensive, demand large memory, and
require retraining for each new dataset. End-to-end supervised learning is an
appealing alternative but is impeded by the lack of ground truth and the large
memory demands of high-resolution volumetric data. Training on synthetic data
often leads to overfitting and poor generalization to real data, and, to date,
no general end-to-end deep learning reconstructors exist for cryo-ET. In this
work, we introduce CryoLithe, a local, memory-efficient reconstruction network
that directly estimates the volume from an aligned tilt-series, overcoming the
suboptimal FBP. We demonstrate that leveraging transform-domain locality makes
our network robust to distribution shifts, enabling effective supervised
training and giving excellent results on real data -- without retraining or
fine-tuning.",['eess.IV'],False,,,,Depth Separable architecture for Sentinel-5P Super-Resolution,End-to-end localized deep learning for Cryo-ET
neg-d2-829,2025-01-15,,2501.0861," As the Internet rapidly expands, the increasing complexity and diversity of
network activities pose significant challenges to effective network governance
and security regulation. Network traffic, which serves as a crucial data
carrier of network activities, has become indispensable in this process.
Network traffic detection aims to monitor, analyze, and evaluate the data flows
transmitted across the network to ensure network security and optimize
performance. However, existing network traffic detection methods generally
suffer from several limitations: 1) a narrow focus on characterizing traffic
features from a single perspective; 2) insufficient exploration of
discriminative features for different traffic; 3) poor generalization to
different traffic scenarios. To address these issues, we propose a multi-view
correlation-aware framework named FlowID for network traffic detection. FlowID
captures multi-view traffic features via temporal and interaction awareness,
while a hypergraph encoder further explores higher-order relationships between
flows. To overcome the challenges of data imbalance and label scarcity, we
design a dual-contrastive proxy task, enhancing the framework's ability to
differentiate between various traffic flows through traffic-to-traffic and
group-to-group contrast. Extensive experiments on five real-world datasets
demonstrate that FlowID significantly outperforms existing methods in accuracy,
robustness, and generalization across diverse network scenarios, particularly
in detecting malicious traffic.",['cs.CR'],2503.09953," In this digital age, ensuring the security of digital data, especially the
image data is critically important. Image encryption plays an important role in
securing the online transmission/storage of images from unauthorized access. In
this regard, this paper presents a novel diffusion-confusion-based image
encryption algorithm named as X-CROSS. The diffusion phase involves a
dual-layer block permutation. It involves a bit-level permutation termed
Inter-Bit Transference (IBT) using a Bit-Extraction key, and pixel permutation
with a unique X-crosspermutation algorithm to effectively scramble the pixels
within an image. The proposed algorithm utilizes a resilient 2D chaotic map
with non-linear dynamical behavior, assisting in generating complex Extraction
Keys. After the permutation phase, the confusion phase proceeds with a dynamic
substitution technique on the permuted images, establishing the final
encryption layer. This combination of novel permutation and confusion results
in the removal of the image's inherent patterns and increases its resistance to
cyber-attacks. The close to ideal statistical security results for information
entropy, correlation, homogeneity, contrast, and energy validate the proposed
scheme's effectiveness in hiding the information within the image.",['cs.CR'],False,,,,"Multi-view Correlation-aware Network Traffic Detection on Flow
  Hypergraph","X-Cross: Image Encryption Featuring Novel Dual-Layer Block Permutation
  and Dynamic Substitution Techniques"
neg-d2-830,2025-01-09,,2501.05288," We present the confirmation of a compact galaxy group candidate, CGG-z4, at
$z=4.3$ in the COSMOS field. This structure was identified by two
spectroscopically confirmed $z=4.3$ $K_s$-dropout galaxies with ALMA $870\rm\,
\mu m$ and 3 mm continuum detections, surrounded by an overdensity of
NIR-detected galaxies with consistent photometric redshifts of $4.0<z<4.6$. The
two ALMA sources, CGG-z4.a and CGG-z4.b, are detected with both CO(4-3) and
CO(5-4) lines. [CI](1-0) is detected on CGG-z4.a, and
H$_{2}$O($1_{1,0}-1_{0,1}$) absorption is detected on CGG-z4.b. We model an
integrated spectral energy distribution by combining the FIR-to-radio
photometry of this group and estimate a total star formation rate of
$\rm\sim2000\, M_{\odot}$ yr$^{-1}$, making it one of the most star-forming
groups known at $z>4$. Their high CO(5-4)/CO(4-3) ratios indicate that the
inter-stellar mediums (ISMs) are close to thermalization, suggesting either
high gas temperatures, densities, and/or pressure, while the low
[CI](1-0)/CO(4-3) line ratios indicate high star formation efficiencies. With
[CI]-derived gas masses we found the two galaxies have extremely short gas
depletion times of $99$ Myr and $<63$ Myr respectively, suggesting the onset of
quenching. With an estimated halo mass of $\rm log (M_{\rm
halo}[M_{\odot}])\sim12.8$, we suggest that this structure is likely in the
process of forming a massive galaxy cluster.",['astro-ph.GA'],2501.01613," The cold and hot interstellar medium (ISM) in star forming galaxies resembles
the reservoir for star formation and associated heating by stellar winds and
explosions during stellar evolution, respectively. We utilize data from deep
$Chandra$ observations and archival millimeter surveys to study the
interconnection between these two phases and the relation to star formation
activities in M51 on kiloparsec scales. A sharp radial decrease is present in
the hot gas surface brightness profile within the inner 2 kpc of M51. The ratio
between the total infrared luminosity ($L_{\rm IR}$) and the hot gas luminosity
($L_{\rm 0.5 - 2\,keV}^{\rm gas}$) shows a positive correlation with the
galactic radius in the central region. For the entire galaxy, a twofold
correlation is revealed in the $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm
IR}$ diagram, where $L_{\rm 0.5 - 2\,keV}^{\rm gas}$ sharply increases with
$L_{\rm IR}$ in the center but varies more slowly in the disk. The best fit
gives a steep relation of ${\rm log}(L_{\rm 0.5-2\,keV}^{\rm gas} /{\rm
erg\,s^{-1}})=1.82\,{\rm log}(L_{\rm IR} /{L_{\rm \odot}})+22.26$ for the
center of M51. The similar twofold correlations are also found in the $L_{\rm
0.5 - 2\,keV}^{\rm gas}$${-}$molecular line luminosity ($L^\prime_{\rm gas}$)
relations for the four molecular emission lines CO(1-0), CO(2-1), HCN(1-0), and
HCO$^+$(1-0). We demonstrate that the core-collapse supernovae (SNe) are the
primary source of energy for heating gas in the galactic center of M51, leading
to the observed steep $L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L_{\rm IR}$ and
$L_{\rm 0.5 - 2\,keV}^{\rm gas}$${-}$$L^\prime_{\rm gas}$ relations, as their
X-ray radiation efficiencies ($\eta$ $\equiv$ $L_{\rm 0.5 - 2\,keV}^{\rm
gas}$/$\dot{E}_\mathrm{SN}$) increase with the star formation rate surface
densities, where $\dot{E}_\mathrm{SN}$ is the SN mechanical energy input rate.",['astro-ph.GA'],False,,,,"Revealing the hidden cosmic feast: A z=4.3 galaxy group hosting two
  optically dark, efficiently star-forming galaxies","Fire and Ice in the Whirlpool: Spatially Resolved Scaling Relations
  between X-ray Emitting Hot Gas and Cold Molecular Gas in M51"
neg-d2-831,2025-02-07,,2502.04676," In this paper, we establish refined regularity estimates for nonnegative
solutions to the fractional Poisson equation $$ (-\Delta)^s u(x) =f(x),\,\,
x\in B_1(0). $$ Specifically, we have derived H\""{o}lder, Schauder, and
Ln-Lipschitz regularity estimates for any nonnegative solution $u,$ provided
that only the local $L^\infty$ norm of $u$ is bounded. These estimates stand in
sharp contrast to the existing results where the global $L^\infty$ norm of $u$
is required. Our findings indicate that the local values of the solution $u$
and $f$ are sufficient to control the local values of higher order derivatives
of $u$. Notably, this makes it possible to establish a priori estimates in
unbounded domains by using blowing up and re-scaling argument.
  As applications, we derive singularity and decay estimates for solutions to
some super-linear nonlocal problems in unbounded domains, and in particular, we
obtain a priori estimates for a family of fractional Lane-Emden type equations
in $\mathbb{R}^n.$ This is achieved by adopting a different method using
auxiliary functions, which is applicable to both local and nonlocal problems.",['math.AP'],2503.03209," Isolated skyrmion solutions to the 2D Landau-Lifshitz equation with the
Dzyaloshinskii-Moriya interaction, Zeeman interaction, and easy-plane
anisotropy are considered. In a wide range of parameters illustrating the
various interaction strengths, we construct exact solutions and examine their
monotonicity, exponential decay, and stability using a careful mathematical
analysis. We also estimate the distance between the constructed solutions and
the harmonic maps by exploiting the structure of the linearized equation and by
proving a resolvent estimate for the linearized operator that is uniform in
extra implicit potentials.",['math.AP'],False,,,,Refined regularity for nonlocal elliptic equations and applications,"Global perturbation of isolated equivariant chiral skyrmions from the
  harmonic maps"
neg-d2-832,2025-02-15,,2502.10742," Despite excelling in high-level reasoning, current language models lack
robustness in real-world scenarios and perform poorly on fundamental
problem-solving tasks that are intuitive to humans. This paper argues that both
challenges stem from a core discrepancy between human and machine cognitive
development. While both systems rely on increasing representational power, the
absence of core knowledge-foundational cognitive structures in humans-prevents
language models from developing robust, generalizable abilities, where complex
skills are grounded in simpler ones within their respective domains. It
explores empirical evidence of core knowledge in humans, analyzes why language
models fail to acquire it, and argues that this limitation is not an inherent
architectural constraint. Finally, it outlines a workable proposal for
systematically integrating core knowledge into future multi-modal language
models through the large-scale generation of synthetic training data using a
cognitive prototyping strategy.",['cs.AI'],2501.08168," While autonomous driving technology has made remarkable strides, data-driven
approaches still struggle with complex scenarios due to their limited reasoning
capabilities. Meanwhile, knowledge-driven autonomous driving systems have
evolved considerably with the popularization of visual language models. In this
paper, we propose LeapVAD, a novel method based on cognitive perception and
dual-process thinking. Our approach implements a human-attentional mechanism to
identify and focus on critical traffic elements that influence driving
decisions. By characterizing these objects through comprehensive attributes -
including appearance, motion patterns, and associated risks - LeapVAD achieves
more effective environmental representation and streamlines the decision-making
process. Furthermore, LeapVAD incorporates an innovative dual-process
decision-making module miming the human-driving learning process. The system
consists of an Analytic Process (System-II) that accumulates driving experience
through logical reasoning and a Heuristic Process (System-I) that refines this
knowledge via fine-tuning and few-shot learning. LeapVAD also includes
reflective mechanisms and a growing memory bank, enabling it to learn from past
mistakes and continuously improve its performance in a closed-loop environment.
To enhance efficiency, we develop a scene encoder network that generates
compact scene representations for rapid retrieval of relevant driving
experiences. Extensive evaluations conducted on two leading autonomous driving
simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior
performance compared to camera-only approaches despite limited training data.
Comprehensive ablation studies further emphasize its effectiveness in
continuous learning and domain adaptation. Project page:
https://pjlab-adg.github.io/LeapVAD/.",['cs.AI'],False,,,,The Philosophical Foundations of Growing AI Like A Child,"LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and
  Dual-Process Thinking"
neg-d2-833,2025-01-02,,2501.01197," Layers have become indispensable tools for professional artists, allowing
them to build a hierarchical structure that enables independent control over
individual visual elements. In this paper, we propose LayeringDiff, a novel
pipeline for the synthesis of layered images, which begins by generating a
composite image using an off-the-shelf image generative model, followed by
disassembling the image into its constituent foreground and background layers.
By extracting layers from a composite image, rather than generating them from
scratch, LayeringDiff bypasses the need for large-scale training to develop
generative capabilities for individual layers. Furthermore, by utilizing a
pretrained off-the-shelf generative model, our method can produce diverse
contents and object scales in synthesized layers. For effective layer
decomposition, we adapt a large-scale pretrained generative prior to estimate
foreground and background layers. We also propose high-frequency alignment
modules to refine the fine-details of the estimated layers. Our comprehensive
experiments demonstrate that our approach effectively synthesizes layered
images and supports various practical applications.",['cs.CV'],2502.06338," Depth completion, predicting dense depth maps from sparse depth measurements,
is an ill-posed problem requiring prior knowledge. Recent methods adopt
learning-based approaches to implicitly capture priors, but the priors
primarily fit in-domain data and do not generalize well to out-of-domain
scenarios. To address this, we propose a zero-shot depth completion method
composed of an affine-invariant depth diffusion model and test-time alignment.
We use pre-trained depth diffusion models as depth prior knowledge, which
implicitly understand how to fill in depth for scenes. Our approach aligns the
affine-invariant depth prior with metric-scale sparse measurements, enforcing
them as hard constraints via an optimization loop at test-time. Our zero-shot
depth completion method demonstrates generalization across various domain
datasets, achieving up to a 21\% average performance improvement over the
previous state-of-the-art methods while enhancing spatial understanding by
sharpening scene details. We demonstrate that aligning a monocular
affine-invariant depth prior with sparse metric measurements is a proven
strategy to achieve domain-generalizable depth completion without relying on
extensive training data. Project page:
https://hyoseok1223.github.io/zero-shot-depth-completion/.",['cs.CV'],False,,,,"LayeringDiff: Layered Image Synthesis via Generation, then Disassembly
  with Generative Knowledge","Zero-shot Depth Completion via Test-time Alignment with Affine-invariant
  Depth Prior"
neg-d2-834,2025-02-05,,2502.0376," Multi-object tracking (MOT) in UAV-based video is challenging due to
variations in viewpoint, low resolution, and the presence of small objects.
While other research on MOT dedicated to aerial videos primarily focuses on the
academic aspect by developing sophisticated algorithms, there is a lack of
attention to the practical aspect of these systems. In this paper, we propose a
novel real-time MOT framework that integrates Apache Kafka and Apache Spark for
efficient and fault-tolerant video stream processing, along with
state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for
accurate object detection and tracking. Our work highlights the importance of
not only the advanced algorithms but also the integration of these methods with
scalable and distributed systems. By leveraging these technologies, our system
achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set
while maintaining a real-time processing speed of 28 FPS on a single GPU. Our
work demonstrates the potential of big data technologies and deep learning for
addressing the challenges of MOT in UAV applications.",['cs.CV'],2502.06445," This paper introduces an open-source benchmark for evaluating Vision-Language
Models (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video
environments. We present a curated dataset containing 1,477 manually annotated
frames spanning diverse domains, including code editors, news broadcasts,
YouTube videos, and advertisements. Three state of the art VLMs - Claude-3,
Gemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as
EasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),
Character Error Rate (CER), and Accuracy. Our results highlight the strengths
and limitations of VLMs in video-based OCR tasks, demonstrating their potential
to outperform conventional OCR models in many scenarios. However, challenges
such as hallucinations, content security policies, and sensitivity to occluded
or stylized text remain. The dataset and benchmarking framework are publicly
available to foster further research.",['cs.CV'],False,,,,"RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on
  Deep Learning and Big Data Technology","Benchmarking Vision-Language Models on Optical Character Recognition in
  Dynamic Video Environments"
neg-d2-835,2025-03-03,,2503.01203," Hypergraph neural networks (HGNNs) effectively model complex high-order
relationships in domains like protein interactions and social networks by
connecting multiple vertices through hyperedges, enhancing modeling
capabilities, and reducing information loss. Developing foundation models for
hypergraphs is challenging due to their distinct data, which includes both
vertex features and intricate structural information. We present Hyper-FM, a
Hypergraph Foundation Model for multi-domain knowledge extraction, featuring
Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex
feature representation and Hierarchical Multi-Hypergraph Guided Structural
Knowledge Extraction for structural information. Additionally, we curate 10
text-attributed hypergraph datasets to advance research between HGNNs and LLMs.
Experiments on these datasets show that Hyper-FM outperforms baseline methods
by approximately 13.3\%, validating our approach. Furthermore, we propose the
first scaling law for hypergraph foundation models, demonstrating that
increasing domain diversity significantly enhances performance, unlike merely
augmenting vertex and hyperedge counts. This underscores the critical role of
domain diversity in scaling hypergraph models.",['cs.LG'],2503.05079," This work studies the alignment of large language models with preference data
from an imitation learning perspective. We establish a close theoretical
connection between reinforcement learning from human feedback RLHF and
imitation learning (IL), revealing that RLHF implicitly performs imitation
learning on the preference data distribution. Building on this connection, we
propose DIL, a principled framework that directly optimizes the imitation
learning objective. DIL provides a unified imitation learning perspective on
alignment, encompassing existing alignment algorithms as special cases while
naturally introducing new variants. By bridging IL and RLHF, DIL offers new
insights into alignment with RLHF. Extensive experiments demonstrate that DIL
outperforms existing methods on various challenging benchmarks.",['cs.LG'],False,,,,Hypergraph Foundation Model,On a Connection Between Imitation Learning and RLHF
neg-d2-836,2025-01-31,,2501.18932," The zero divisor graph of a commutative ring $R$ with unity is a graph whose
vertices are the nonzero zero-divisors of the ring, with two distinct vertices
being adjacent if their product is zero. This graph is denoted by $\Gamma(R)$.
In this article we determine the cut-edges and central vertices in the graph
$\Gamma(\mathbb{Z}_{n})$.",['math.RA'],2501.0869," It is known that an inverse monoid $M$ is E-unitary if and only if the
following diagram is an extension: $E(M) \to M \to M/\sigma$, where $E(M)$ is
the semilattice of idempotents and $M/\sigma$ is the minimal group quotient.
F-inverse monoids are another fundamental class of inverse semigroup and all
F-inverse monoids are E-unitary. Thus given that F-inverse monoids have an
associated extension it is natural to ask if these extensions satisfy any
special properties. Indeed we show that $M$ is F-inverse if and only if the
aforementioned extension is weakly Schreier. This latter result allows us to
make use of relaxed factor systems to provide a new characterization of
F-inverse monoids. We end by restricting to the Clifford case and find a new
characterization of these with much in common with Artin gluings of frames.",['math.RA'],False,,,,"Cut edges and Central vertices of zero divisor graph of the ring of
  integers modulo n",F-Inverse Monoids as Weakly Schreier Extensions
neg-d2-837,2025-01-10,,2501.06028," We develop a new algorithm for factoring a bivariate polynomial $F\in
\mathbb{K}[x,y]$ which takes fully advantage of the geometry of the Newton
polygon of $F$. Under a non degeneracy hypothesis, the complexity is
$\tilde{\mathcal{O}}(Vr_0^{\omega-1} )$ where $V$ is the volume of the polygon
and $r_0$ is its minimal lower lattice length. This improves the complexity
$\tilde{\mathcal{O}}(d^{\omega+1})$ of the classical algorithms which consider
the total degree $d$ of $F$ as the main complexity indicator. The integer
$r_0\le d$ reflects some combinatorial constraints imposed by the Newton
polygon, giving a reasonable and easy-to-compute upper bound for the number of
its indecomposable Minkovski summands of positive volume. The proof is based on
a new fast factorization algorithm in $\mathbb{K}[[x]][y]$ with respect to a
slope valuation, a result which has its own interest.",['math.AC'],2502.05956," Divided power algebras form an important variety of non-binary universal
algebras. We identify the universal enveloping algebra and K\""ahler
differentials associated to a divided power algebra over a general commutative
ring, simplifying and generalizing work of Roby and Dokas.",['math.AC'],False,,,,Improvements of convex-dense factorization of bivariate polynomials,"Divided powers and K\""ahler differentials"
neg-d2-838,2025-01-09,,2501.05275," Achieving uniform nanowire size, density, and alignment across a wafer is
challenging, as small variations in growth parameters can impact performance in
energy harvesting devices like solar cells and photodetectors. This study
demonstrates the in-depth characterization of uniformly grown GaAs/AlGaAs
core-shell nanowires on a two-inch Si(111) substrate using Ga-induced
self-catalyzed molecular beam epitaxy. By integrating Scanning Electron
Microscopy and Time Correlated Single-Photon Counting, we establish a detailed
model of structural and optoelectronic properties across wafer and micron
scales. While emission intensity varies by up to 35%, carrier lifetime shows
only 9% variation, indicating stable material quality despite structural
inhomogeneities. These findings indicate that, for the two-inch GaAs/AlGaAs
nanowire wafer, achieving uniform nanowire coverage had a greater impact on
consistent optoelectronic properties than variations in material quality,
highlighting its significance for scalable III-V semiconductor integration on
silicon in advanced optoelectronic devices such as solar cells and
photodetectors.",['physics.optics'],2501.0735," Photochemistry in the earth's atmosphere is driven by the sun, continuously
altering the concentration and spatial distribution of pollutants. Precisely
monitoring their atmospheric abundance relies predominantly on optical sensing,
which requires the knowledge of exact absorption cross sections. One key
pollutant which impacts many photochemical reaction-pathways is formaldehyde.
Agreement on formaldehyde absolute absorption cross section remains elusive in
the photochemically-relevant ultraviolet spectral region, hampering sensitive
concentration tracking. Here, we introduce free-running ultraviolet dual comb
spectroscopy, combining high spectral resolution (1 GHz), broad spectral
coverage (12 THz), and fast acquisition speed (500 ms), as a novel method for
absolute absorption cross section determination with unprecedented fidelity.
Within this bandwidth, our method uncovers almost one order of magnitude more
rovibrational transitions than detected before which leads to refined
rotational constants for high-level quantum simulations of molecular
eigenstates. This ultra-resolution method can be generalized to provide a
universal tool for fast electronic fingerprinting of atmospherically-relevant
species, both for sensing applications and to benchmark improvements of
ab-initio quantum theory.",['physics.optics'],False,,,,"Wafer-scale correlated morphology and optoelectronic properties in
  GaAs/AlGaAs core-shell nanowires",Ultra-resolution photochemical sensing
neg-d2-839,2025-03-13,,2503.10829," Linear relations, defined as submodules of the direct sum of two modules, can
be viewed as objects that carry dynamical information and reflect the inherent
uncertainty of sampled dynamics. These objects also provide an algebraic
structure that enables the definition of subtle invariants for dynamical
systems. In this paper, we prove that linear relations defined on modules of
finite length are shift equivalent to bijective mappings.",['math.DS'],2503.03457," For a minimal Anosov $\mathbb R^{\kappa}$-action on a closed manifold, we
study the measure of maximal entropy constructed by Carrasco and
Rodriguez-Hertz in \cite{CarHer} and show that it fits into the theory of
Ruelle-Taylor resonances introduced by Guedes Bonthonneau, Guillarmou, Hilgert,
and Weich in \cite{GBGHW}. More precisely, we show that the topological entropy
corresponds to the first Ruelle-Taylor resonance for the action on a certain
bundle of forms and that the measure of maximal entropy can be retrieved as the
distributional product of the corresponding resonant and co-resonant states. As
a consequence, we prove a Bowen-type formula for the measure of maximal entropy
and a counting result on the number of periodic torii.",['math.DS'],False,,,,Linear Relations of Finite Length Modules are Shift Equivalent to Maps,Measure of maximal entropy for minimal Anosov actions
neg-d2-840,2025-02-27,,2502.19875," Numerical simulations are a valuable research and layout tool for fluid flow
problems, yet repeated evaluations of parametrized problems, necessary to solve
optimization problems, can be very costly. One option to speed up this process
is to replace the costly CFD model with a cheaper one. These surrogate models
can be either data-driven or they can also rely on reduced basis (RB) methods
to speed up the calculations. In contrast to data-driven surrogate models, the
latter are not based on regression techniques but are still aimed at explicitly
solving the conservation equations. Their speed-up comes from a strong
reduction of the solution space, which results in much smaller algebraic
systems that need to be solved. Within this work, an RB model, suited for
slightly compressible flow, is presented and tested on different flow
configurations. The model is stabilized using a Petrov-Galerkin method with
trial and test function spaces of different dimensionality to generate stable
results for a wide range of Reynolds numbers. The presented model applies to
geometrically and physically parametrized flow problems. Finally, a data-driven
approach was used to extend it to turbulent flows.",['physics.flu-dyn'],2503.05964," Proper-orthogonal decomposition (POD) based reduced-order models (ROM) of
structurally dominant fluid flow can support a wide range of engineering
applications. Yet, although they perform well for unsteady laminar flows, their
straightforward extension to turbulent flows fails to capture the effects of
small scale eddies and often leads to divergent solutions. Several approaches
to mimic nonlinear closure terms modeling techniques within ROM frameworks have
been employed to include the effect of higher modes that are often neglected.
Recent success of neural network based models show promising results in
modeling the effects of turbulence. In this study, we augment POD-ROM with a
recurrent neural network (RNN) to develop ROM for turbulent flows. We simulate
a three dimensional flow past a circular cylinder at Reynolds number of 1000.
We first compute the POD modes and project the Navier-Stokes equations onto the
limited number of modes in a Galerkin approach to develop a conventional ROM
and LES-inspired ROM for comparison. We then develop a hybrid model by
integrating the output of Galerkin projection ROM and long short-term memory
(LSTM) RNN and term it as a physics-guided machine learning (PGML) model. The
novelty of this study is to introduce a hybrid model that integrates LES
inspired ROM and RNN to achieve more accurate and reliable predictions of
turbulent flows. The results demonstrate that PGML for higher temporal
coefficients outperforms the conventional and LES-inspired ROM.",['physics.flu-dyn'],False,,,,Reduced Basis Model for Compressible Flow,"Hybrid Reduced-Order Models for Turbulent Flows Using Recurrent Neural
  Architectures"
neg-d2-841,2025-01-20,,2501.11359," We consider two types of dynamical systems namely non-autonomous discrete
dynamical systems(NDDS) and generic dynamical systems(GDS). In both of them, we
study various notions of transitivity. We give many equivalent conditions for
each of these notions and present the implications among these in NDDS and GDS.
For a given NDDS, we associate a GDS and discuss whether if the given NDDS has
a particular variation of transitivity then the associated GDS also has such a
variation and vice versa.",['math.DS'],2503.10829," Linear relations, defined as submodules of the direct sum of two modules, can
be viewed as objects that carry dynamical information and reflect the inherent
uncertainty of sampled dynamics. These objects also provide an algebraic
structure that enables the definition of subtle invariants for dynamical
systems. In this paper, we prove that linear relations defined on modules of
finite length are shift equivalent to bijective mappings.",['math.DS'],False,,,,"Various notions of topological transitivity in non-autonomous and
  generic dynamical systems",Linear Relations of Finite Length Modules are Shift Equivalent to Maps
neg-d2-842,2025-02-27,,2502.20314," Implicit Neural Representations (INRs) have been recently garnering
increasing interest in various research fields, mainly due to their ability to
represent large, complex data in a compact and continuous manner. Past work
further showed that numerous popular downstream tasks can be performed directly
in the INR parameter-space. Doing so can substantially reduce the computational
resources required to process the represented data in their native domain. A
major difficulty in using modern machine-learning approaches, is their high
susceptibility to adversarial attacks, which have been shown to greatly limit
the reliability and applicability of such methods in a wide range of settings.
In this work, we show that parameter-space models trained for classification
are inherently robust to adversarial attacks -- without the need of any robust
training. To support our claims, we develop a novel suite of adversarial
attacks targeting parameter-space classifiers, and furthermore analyze
practical considerations of attacking parameter-space classifiers.",['cs.LG'],2501.10774," Model monitoring involves analyzing AI algorithms once they have been
deployed and detecting changes in their behaviour. This thesis explores machine
learning model monitoring ML before the predictions impact real-world decisions
or users. This step is characterized by one particular condition: the absence
of labelled data at test time, which makes it challenging, even often
impossible, to calculate performance metrics.
  The thesis is structured around two main themes: (i) AI alignment, measuring
if AI models behave in a manner consistent with human values and (ii)
performance monitoring, measuring if the models achieve specific accuracy goals
or desires.
  The thesis uses a common methodology that unifies all its sections. It
explores feature attribution distributions for both monitoring dimensions.
Using these feature attribution explanations, we can exploit their theoretical
properties to derive and establish certain guarantees and insights into model
monitoring.",['cs.LG'],False,,,,Adversarial Robustness in Parameter-Space Classifiers,"Model Monitoring in the Absence of Labeled Data via Feature Attributions
  Distributions"
neg-d2-843,2025-03-04,,2503.03054," Federated learning (FL) is an emerging machine learning paradigm with immense
potential to support advanced services and applications in future industries.
However, when deployed over wireless communication systems, FL suffers from
significant communication overhead, which can be alleviated by integrating
over-the-air computation (AirComp). Despite its advantages, AirComp introduces
learning inaccuracies due to the inherent randomness of wireless channels,
which can degrade overall learning performance. To address this issue, this
paper explores the integration of fluid antenna systems (FAS) into
AirComp-based FL to enhance system robustness and efficiency. Fluid antennas
offer dynamic spatial diversity by adaptively selecting antenna ports, thereby
mitigating channel variations and improving signal aggregation. Specifically,
we propose an antenna selection rule for fluid-antenna-equipped devices that
optimally enhances learning robustness or training performance. Building on
this, we develop a learning algorithm and provide a theoretical convergence
analysis. The simulation results validate the effectiveness of fluid antennas
in improving FL performance, demonstrating their potential as a key enabler for
wireless AI applications.",['eess.SP'],2502.18636," In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.",['eess.SP'],False,,,,"Federated Learning Meets Fluid Antenna: Towards Robust and Scalable Edge
  Intelligence","Transfer Learning Assisted Fast Design Migration Over Technology Nodes:
  A Study on Transformer Matching Network"
neg-d2-844,2025-01-02,,2501.01121," While current high-resolution depth estimation methods achieve strong
results, they often suffer from computational inefficiencies due to reliance on
heavyweight models and multiple inference steps, increasing inference time. To
address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner
models with lightweight encoders. This reduces model size and inference time
but introduces noisy features. To overcome this, we propose a Coarse-to-Fine
(C2F) module with a Guided Denoising Unit for refining and denoising the
refiner features and a Noisy Pretraining strategy to pretrain the refiner
branch to fully exploit the potential of the lightweight refiner branch.
Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching
(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms
state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy
and speed, using fewer parameters and faster inference. It also shows improved
depth boundary delineation on real-world datasets like CityScape, ScanNet++,
and KITTI, demonstrating its versatility across domains.",['cs.CV'],2503.16832," We introduce a novel approach for simultaneous self-supervised video
alignment and action segmentation based on a unified optimal transport
framework. In particular, we first tackle self-supervised video alignment by
developing a fused Gromov-Wasserstein optimal transport formulation with a
structural prior, which trains efficiently on GPUs and needs only a few
iterations for solving the optimal transport problem. Our single-task method
achieves the state-of-the-art performance on multiple video alignment
benchmarks and outperforms VAVA, which relies on a traditional Kantorovich
optimal transport formulation with an optimality prior. Furthermore, we extend
our approach by proposing a unified optimal transport framework for joint
self-supervised video alignment and action segmentation, which requires
training and storing a single model and saves both time and memory consumption
as compared to two different single-task models. Extensive evaluations on
several video alignment and action segmentation datasets demonstrate that our
multi-task method achieves comparable video alignment yet superior action
segmentation results over previous methods in video alignment and action
segmentation respectively. Finally, to the best of our knowledge, this is the
first work to unify video alignment and action segmentation into a single
model.",['cs.CV'],False,,,,"PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric
  Depth Estimation",Joint Self-Supervised Video Alignment and Action Segmentation
neg-d2-845,2025-03-05,,2503.03219," Filament G37 exhibits a distinctive ""caterpillar"" shape, characterized by two
semicircular structures within its 40\,pc-long body, providing an ideal target
to investigate the formation and evolution of filaments. By analyzing multiple
observational data, such as CO spectral line, the H$\alpha$\,RRL, and
multi-wavelength continuum, we find that the expanding H\,{\scriptsize II}
regions surrounding filament G37 exert pressure on the structure of the
filament body, which kinetic process present as the gas flows in multiple
directions along its skeleton. The curved magnetic field structure of filament
G37 derived by employing the Velocity Gradient Technique with CO is found to be
parallel to the filament body and keeps against the pressure from expanded
H\,{\scriptsize II} regions. The multi-directional flows in the filament G37
could cause the accumulation and subsequent collapse of gas, resulting in the
formation of massive clumps. The curved structure and star formation observed
in filament G37 are likely a result of the filament body being squeezed by the
expanding H\,{\scriptsize II} region. This physical process occurs over a
timescale of approximately 5\,Myr. The filament G37 provides a potential
candidate for end-dominated collapse.",['astro-ph.GA'],2501.04089," We present deep optical observations of the stellar halo of NGC 300, an
LMC-mass galaxy, acquired with the DEEP sub-component of the DECam Local Volume
Exploration survey (DELVE) using the 4 m Blanco Telescope. Our resolved star
analysis reveals a large, low surface brightness stellar stream
($M_{V}\sim-8.5$; [Fe/H] $= -1.4\pm0.15$) extending more than 40 kpc north from
the galaxy's center. We also find other halo structures, including potentially
an additional stream wrap to the south, which may be associated with the main
stream. The morphology and derived low metallicities of the streams and shells
discovered surrounding NGC 300 are highly suggestive of a past accretion event.
Assuming a single progenitor, the accreted system is approximately Fornax-like
in luminosity, with an inferred mass ratio to NGC 300 of approximately $1:15$.
We also present the discovery of a metal-poor globular cluster
($R_{\rm{proj}}=23.3$~kpc; $M_{V}=-8.99\pm0.16$; [Fe/H] $\approx-1.6\pm0.6$) in
the halo of NGC 300, the furthest identified globular cluster associated with
NGC 300. The stellar structures around NGC 300 represent the richest features
observed in a Magellanic Cloud analog to date, strongly supporting the idea
that accretion and subsequent disruption is an important mechanism in the
assembly of dwarf galaxy stellar halos.",['astro-ph.GA'],False,,,,"The Impact of Expanding HII Regions on Filament G37:Curved Magnetic
  Field and Multiple Direction Material Flows","Streams, Shells, and Substructures in the Accretion-Built Stellar Halo
  of NGC 300"
neg-d2-846,2025-02-16,,2502.11253," Quantum error correction is a cornerstone of reliable quantum computing, with
surface codes emerging as a prominent method for protecting quantum
information. Surface codes are efficient for Clifford gates but require magic
state distillation protocols to process non-Clifford gates, such as T gates,
essential for universal quantum computation. In large-scale quantum
architectures capable of correcting arbitrary circuits, specialized surface
codes for data qubits and distinct codes for magic state distillation are
needed. These architectures can be organized into data blocks and distillation
blocks. The system works by having distillation blocks produce magic states and
data blocks consume them, causing stalls due to either a shortage or excess of
magic states. This bottleneck presents an opportunity to optimize quantum space
by balancing data and distillation blocks. While prior research offers insights
into selecting distillation protocols and estimating qubit requirements, it
lacks a tailored optimization approach. We present a framework for optimizing
large-scale quantum architectures, focusing on data block layouts and magic
state distillation protocols. We evaluate three data block layouts and four
distillation protocols under three optimization strategies: minimizing tiles,
minimizing steps, and achieving a balanced trade-off. Through a comparative
analysis of brute force, dynamic programming, greedy, and random algorithms, we
find that brute force delivers optimal results, while greedy deviates by 7% for
minimizing steps and dynamic programming matches brute force in tile
minimization. We observe that total steps increase with columns, while total
tiles scale with qubits. Finally, we propose a heuristic to help users select
algorithms suited to their objectives, enabling scalable and efficient quantum
architectures.",['quant-ph'],2501.06179," Classically hard to simulate quantum states, or ""magic states"", are
prerequisites to quantum advantage, highlighting an apparent separation between
classically and quantumly tractable problems. Classically simulable states such
as Clifford circuits on stabilizer states, free bosonic states, free fermions,
and matchgate circuits are all in some sense Gaussian. While free bosons and
fermions arise from quadratic Hamiltonians, recent works have demonstrated that
bosonic and qudit systems converge to Gaussians and stabilizers under
convolution. In this work, we similarly identify convolution for fermions and
find efficient measures of non-Gaussian magic in pure fermionic states. We
demonstrate that three natural notions for the Gaussification of a state, (1)
the Gaussian state with the same covariance matrix, (2) the fixed point of
convolution, and (3) the closest Gaussian in relative entropy, coincide by
proving a central limit theorem for fermionic systems. We then utilize the
violation of Wick's theorem and the matchgate identity to quantify non-Gaussian
magic in addition to a SWAP test.",['quant-ph'],False,,,,"The Q-Spellbook: Crafting Surface Code Layouts and Magic State Protocols
  for Large-Scale Quantum Computing","Measuring Non-Gaussian Magic in Fermions: Convolution, Entropy, and the
  Violation of Wick's Theorem and the Matchgate Identity"
neg-d2-847,2025-01-22,,2501.13173," Gaussian Process Regression (GPR) is a powerful tool for nonparametric
regression, but its fully Bayesian application in high-dimensional settings is
hindered by two primary challenges: the computational burden (exacerbated by
fully Bayesian inference) and the difficulty of variable selection. This paper
introduces a novel methodology that combines hierarchical global-local
shrinkage priors with normalizing flows to address these challenges. The
hierarchical triple gamma prior offers a principled framework for inducing
sparsity in high-dimensional GPR, effectively excluding irrelevant covariates
while preserving interpretability and flexibility in model size. Normalizing
flows are employed within a variational inference framework to approximate the
posterior distribution of hyperparameters, capturing complex dependencies while
ensuring computational scalability. Simulation studies demonstrate the efficacy
of the proposed approach, outperforming traditional maximum likelihood
estimation and mean-field variational methods, particularly in high-sparsity
and high-dimensional settings. The results highlight the robustness and
flexibility of hierarchical shrinkage priors and the computational efficiency
of normalizing flows for Bayesian GPR. This work provides a scalable and
interpretable solution for high-dimensional regression, with implications for
sparse modeling and posterior approximation in broader Bayesian contexts.",['stat.ME'],2501.13173," Gaussian Process Regression (GPR) is a powerful tool for nonparametric
regression, but its fully Bayesian application in high-dimensional settings is
hindered by two primary challenges: the computational burden (exacerbated by
fully Bayesian inference) and the difficulty of variable selection. This paper
introduces a novel methodology that combines hierarchical global-local
shrinkage priors with normalizing flows to address these challenges. The
hierarchical triple gamma prior offers a principled framework for inducing
sparsity in high-dimensional GPR, effectively excluding irrelevant covariates
while preserving interpretability and flexibility in model size. Normalizing
flows are employed within a variational inference framework to approximate the
posterior distribution of hyperparameters, capturing complex dependencies while
ensuring computational scalability. Simulation studies demonstrate the efficacy
of the proposed approach, outperforming traditional maximum likelihood
estimation and mean-field variational methods, particularly in high-sparsity
and high-dimensional settings. The results highlight the robustness and
flexibility of hierarchical shrinkage priors and the computational efficiency
of normalizing flows for Bayesian GPR. This work provides a scalable and
interpretable solution for high-dimensional regression, with implications for
sparse modeling and posterior approximation in broader Bayesian contexts.",['stat.ME'],False,,,,"Normalizing Flows for Gaussian Process Regression under Hierarchical
  Shrinkage Priors","Normalizing Flows for Gaussian Process Regression under Hierarchical
  Shrinkage Priors"
neg-d2-848,2025-01-13,,2501.0752," We present the results of combined hydrodynamic and particle tracking
post-processing modeling to study the transport of small dust in a
protoplanetary disk containing an embedded embryo in 3D. We use a suite of
FARGO3D hydrodynamic simulations of disks containing a planetary embryo varying
in mass up to 300 $M_\oplus$ on a fixed orbit in both high and low viscosity
disks. We then simulate solid particles through the disk as a post-processing
step using a Monte Carlo integration, allowing us to track the trajectories of
individual particles as they travel throughout the disk. We find that gas
advection onto the planet can carry small, well-coupled solids across the gap
opened in the disk by the embedded planet for planetary masses above the pebble
isolation mass. This mixing between the inner and outer disk can occur in both
directions, with solids in the inner disk mixing to the outer disk as well.
Additionally, in low viscosity disks, multiple pile-ups in the outer disk may
preserve isotopic heterogeneities, possibly providing an outermost tertiary
isotopic reservoir. Throughout Jupiter's growth, the extent of mixing between
isotopic reservoirs varied depending on dust size, gas turbulence, and the
Jovian embryo mass.",['astro-ph.EP'],2502.01606," High-resolution spectroscopy has provided a wealth of information about the
climate and composition of ultra-hot Jupiters. However, the 3D structure of
their atmospheres makes observations more challenging to interpret,
necessitating 3D forward-modeling studies. In this work, we model
phase-dependent thermal emission spectra of the archetype ultra-hot Jupiter
WASP-76b to understand how the line strengths and Doppler shifts of Fe, CO,
H$_2$O, and OH evolve throughout the orbit. We post-process outputs of the
SPARC/MITgcm global circulation model with the 3D Monte-Carlo radiative
transfer code gCMCRT to simulate emission spectra at 36 orbital phases. We then
cross-correlate the spectra with different templates to obtain CCF and
$K_{\text{p}}$$-$$V_{\text{sys}}$ maps. For each species, our models produce
consistently negative $K_{\text{p}}$ offsets in pre- and post-eclipse, which
are driven by planet rotation. The size of these offsets is similar to the
equatorial rotation velocity of the planet. Furthermore, we demonstrate how the
weak vertical temperature gradient on the nightside of ultra-hot Jupiters mutes
the absorption features of CO and H$_2$O, which significantly hampers their
detectability in pre- and post-transit. We also show that the $K_{\text{p}}$
and $V_{\text{sys}}$ offsets in pre- and post-transit are not always a measure
for the line-of-sight velocities in the atmosphere. This is because the
cross-correlation signal is a blend of dayside emission and nightside
absorption features. Finally, we highlight that the observational uncertainty
in the known orbital velocity of ultra-hot Jupiters can be multiple km/s, which
makes it hard for certain targets to meaningfully report absolute
$K_{\text{p}}$ offsets.",['astro-ph.EP'],False,,,,"Three-dimensional transport of solids in a protoplanetary disk
  containing a growing giant planet","From pre-transit to post-eclipse: investigating the impact of 3D
  temperature, chemistry, and dynamics on high-resolution emission spectra of
  the ultra-hot Jupiter WASP-76b"
neg-d2-849,2025-03-14,,2503.11598," We investigate the thermodynamic properties of the Hubbard model on the Bethe
lattice with a coordination number of 3 using the thermal canonical tree tensor
network method. Our findings reveal two distinct thermodynamic phases: a
low-temperature antiferromagnetic phase, where spin SU(2) symmetry is broken,
and a high-temperature paramagnetic phase. A key feature of the system is the
separation of energy scales for charge and spin excitations, which is reflected
in the temperature dependence of thermodynamic quantities and the disparity
between spin and charge gaps extracted from their respective susceptibilities.
At the critical point, both spin and charge susceptibilities exhibit
singularities, suggesting that charge excitations are not fully decoupled from
their spin counterparts. Additionally, the double occupancy number exhibits a
non-monotonic temperature dependence, indicative of an entropy-driven
Pomeranchuk effect. These results demonstrate that the loopless Bethe lattice
effectively captures the essential physics of the Hubbard model while providing
a computationally efficient framework for studying strongly correlated
electronic systems.",['cond-mat.str-el'],2502.19259," Quantum spin liquid represents an intriguing state where electron spins are
highly entangled yet spin fluctuation persists even at 0 K. Recently, the
hexaaluminates \textit{R}MgAl$_{11}$O$_{19}$ (\textit{R} = rare earth) have
been proposed to be a platform for realizing the quantum spin liquid state with
dominant Ising anisotropic correlations. Here, we report detailed
low-temperature magnetic susceptibility, muon spin relaxation, and
thermodynamic studies on the CeMgAl$_{11}$O$_{19}$ single crystal. Ising
anisotropy is revealed by magnetic susceptibility measurements. Muon spin
relaxation and ac susceptibility measurements rule out any long-range magnetic
ordering or spin freezing down to 50 mK despite the onset of spin correlations
below $\sim$0.8 K. Instead, the spins keep fluctuating at a rate of 1.0(2) MHz
at 50 mK. Specific heat results indicate a gapless excitation with a power-law
dependence on temperature, $C_m(T) \propto T^{\alpha}$. The quasi-quadratic
temperature dependence with $\alpha$ = 2.28(4) in zero field and linear
temperature dependence in 0.25 T support the possible realization of the U(1)
Dirac quantum spin liquid state.",['cond-mat.str-el'],False,,,,Thermodynamics of the Hubbard Model on the Bethe Lattice,"U(1) Dirac quantum spin liquid candidate in triangular-lattice
  antiferromagnet CeMgAl$_{11}$O$_{19}$"
neg-d2-850,2025-02-07,,2502.04718," Text Style Transfer (TST) is the task of transforming a text to reflect a
particular style while preserving its original content. Evaluating TST outputs
is a multidimensional challenge, requiring the assessment of style transfer
accuracy, content preservation, and naturalness. Using human evaluation is
ideal but costly, same as in other natural language processing (NLP) tasks,
however, automatic metrics for TST have not received as much attention as
metrics for, e.g., machine translation or summarization. In this paper, we
examine both set of existing and novel metrics from broader NLP tasks for TST
evaluation, focusing on two popular subtasks-sentiment transfer and
detoxification-in a multilingual context comprising English, Hindi, and
Bengali. By conducting meta-evaluation through correlation with human
judgments, we demonstrate the effectiveness of these metrics when used
individually and in ensembles. Additionally, we investigate the potential of
Large Language Models (LLMs) as tools for TST evaluation. Our findings
highlight that certain advanced NLP metrics and experimental-hybrid-techniques,
provide better insights than existing TST metrics for delivering more accurate,
consistent, and reproducible TST evaluations.",['cs.CL'],2502.12516," Frame-semantic parsing is a critical task in natural language understanding,
yet the ability of large language models (LLMs) to extract frame-semantic
arguments remains underexplored. This paper presents a comprehensive evaluation
of LLMs on frame-semantic argument identification, analyzing the impact of
input representation formats, model architectures, and generalization to unseen
and out-of-domain samples. Our experiments, spanning models from 0.5B to 78B
parameters, reveal that JSON-based representations significantly enhance
performance, and while larger models generally perform better, smaller models
can achieve competitive results through fine-tuning. We also introduce a novel
approach to frame identification leveraging predicted frame elements, achieving
state-of-the-art performance on ambiguous targets. Despite strong
generalization capabilities, our analysis finds that LLMs still struggle with
out-of-domain data.",['cs.CL'],False,,,,"Evaluating Text Style Transfer Evaluation: Are There Any Reliable
  Metrics?",Can LLMs Extract Frame-Semantic Arguments?
neg-d2-851,2025-02-26,,2502.18867," Accurate skier tracking is essential for performance analysis, injury
prevention, and optimizing training strategies in alpine sports. Traditional
tracking methods often struggle with occlusions, dynamic movements, and varying
environmental conditions, limiting their effectiveness. In this work, we used
STARK (Spatio-Temporal Transformer Network for Visual Tracking), a
transformer-based model, to track skiers. We adapted STARK to address
domain-specific challenges such as camera movements, camera changes,
occlusions, etc. by optimizing the model's architecture and hyperparameters to
better suit the dataset.",['cs.CV'],2502.19896," Existing point cloud completion methods, which typically depend on predefined
synthetic training datasets, encounter significant challenges when applied to
out-of-distribution, real-world scans. To overcome this limitation, we
introduce a zero-shot completion framework, termed GenPC, designed to
reconstruct high-quality real-world scans by leveraging explicit 3D generative
priors. Our key insight is that recent feed-forward 3D generative models,
trained on extensive internet-scale data, have demonstrated the ability to
perform 3D generation from single-view images in a zero-shot setting. To
harness this for completion, we first develop a Depth Prompting module that
links partial point clouds with image-to-3D generative models by leveraging
depth images as a stepping stone. To retain the original partial structure in
the final results, we design the Geometric Preserving Fusion module that aligns
the generated shape with input by adaptively adjusting its pose and scale.
Extensive experiments on widely used benchmarks validate the superiority and
generalizability of our approach, bringing us a step closer to robust
real-world scan completion.",['cs.CV'],False,,,,"Enhanced Transformer-Based Tracking for Skiing Events: Overcoming
  Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual
  Tracking Challenge 2025",GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors
neg-d2-852,2025-02-11,,2502.07873," Quantum phase estimation is fundamental to advancing quantum science and
technology. While much of the research has concentrated on estimating a single
phase, the simultaneous estimation of multiple phases can yield significantly
enhanced sensitivities when using specially tailored input quantum states. This
work reviews recent theoretical and experimental advancements in the parallel
estimation of multiple arbitrary phases. We highlight strategies for
constructing optimal measurement protocols and discuss the experimental
platforms best suited for implementing these techniques.",['quant-ph'],2501.15137," Matrix operations are of great significance in quantum computing, which
manipulate quantum states in information processing. This paper presents
quantum algorithms for several important matrix operations. By leveraging
multi-qubit Toffoli gates and basic single-qubit operations, these algorithms
efficiently carry out matrix row addition, row swapping, trace calculation and
transpose. By using the ancillary measurement techniques to eliminate redundant
information, these algorithms achieve streamlined and efficient computations,
and demonstrate excellent performance with the running time increasing
logarithmically as the matrix dimension grows, ensuring scalability. The
success probability depends on the matrix dimensions for the trace calculation,
and on the matrix elements for row addition. Interestingly, the success
probability is a constant for matrix row swapping and transpose, highlighting
the reliability and efficiency.",['quant-ph'],False,,,,Quantum multiphase estimation,"Quantum Algorithms for Matrix Operations Based on Unitary
  Transformations and Ancillary State Measurements"
neg-d2-853,2025-03-19,,2503.15346," This paper investigates properties of Blackwell $\epsilon$-optimal strategies
in zero-sum stochastic games when the adversary is restricted to stationary
strategies, motivated by applications to robust Markov decision processes. For
a class of absorbing games, we show that Markovian Blackwell $\epsilon$-optimal
strategies may fail to exist, yet we prove the existence of Blackwell
$\epsilon$-optimal strategies that can be implemented by a two-state automaton
whose internal transitions are independent of actions. For more general
absorbing games, however, there need not exist Blackwell $\epsilon$-optimal
strategies that are independent of the adversary's decisions. Our findings
point to a contrast between absorbing games and generalized Big Match games,
and provide new insights into the properties of optimal policies for robust
Markov decision processes.",['cs.GT'],2501.19294," Many ethical issues in machine learning are connected to the training data.
Online data markets are an important source of training data, facilitating both
production and distribution. Recently, a trend has emerged of for-profit
""ethical"" participants in online data markets. This trend raises a fascinating
question: Can online data markets sustainably and efficiently address ethical
issues in the broader machine-learning economy?
  In this work, we study this question in a stylized model of an online data
market. We investigate the effects of intervening in the data market to achieve
balanced training-data production. The model reveals the crucial role of market
conditions. In small and emerging markets, an intervention can drive the data
producers out of the market, so that the cost of fairness is maximal. Yet, in
large and established markets, the cost of fairness can vanish (as a fraction
of overall welfare) as the market grows.
  Our results suggest that ""ethical"" online data markets can be economically
feasible under favorable market conditions, and motivate more models to
consider the role of data production and distribution in mediating the impacts
of ethical interventions.",['cs.GT'],False,,,,Playing against a stationary opponent,The Cost of Balanced Training-Data Production in an Online Data Market
neg-d2-854,2025-01-20,,2501.11693," We investigate the following Kirchhoff-type biharmonic equation
\begin{equation}\label{pr} \left\{ \begin{array}{ll} \Delta^2 u+
\left(a+b\int_{\mathbb{R}^N}|\nabla u|^2d x\right)(-\Delta
u+V(x)u)=f(x,u),\quad x\in \mathbb{R}^N,\\ u\in H^{2}(\mathbb{R}^N),
\end{array} \right. \end{equation} where $a>0$, $b\geq 0$ and $V(x)$ and $f(x,
u)$ are periodic or asymptotically periodic in $x$. We study the existence of
Nehari-type ground state solutions of \eqref{pr} with $f(x,u)u-4F(x,u)$
sign-changing, where $F(x,u):=\int_0^uf(x,s)d s$. We significantly extend some
results from the previous literature.",['math.AP'],2501.03963," We prove global well-posedness and scattering for the massive
Dirac-Klein-Gordon system with small and low regularity initial data in
dimension two. To achieve this, we impose a non-resonance condition on the
masses.",['math.AP'],False,,,,"Nehari-type ground state solutions for asymptotically periodic
  bi-harmonic Kirchhoff-type problems in $\mathbb{R}^N$","Global well-posedness and scattering for the massive Dirac-Klein-Gordon
  system in two dimensions"
neg-d2-855,2025-01-17,,2501.10005," Let R be a commutative ring with identity and M be an R-module. The purpose
of this paper is to introduce and investigate the dual notion of morphic
modules over a commutative ring.",['math.AC'],2501.06028," We develop a new algorithm for factoring a bivariate polynomial $F\in
\mathbb{K}[x,y]$ which takes fully advantage of the geometry of the Newton
polygon of $F$. Under a non degeneracy hypothesis, the complexity is
$\tilde{\mathcal{O}}(Vr_0^{\omega-1} )$ where $V$ is the volume of the polygon
and $r_0$ is its minimal lower lattice length. This improves the complexity
$\tilde{\mathcal{O}}(d^{\omega+1})$ of the classical algorithms which consider
the total degree $d$ of $F$ as the main complexity indicator. The integer
$r_0\le d$ reflects some combinatorial constraints imposed by the Newton
polygon, giving a reasonable and easy-to-compute upper bound for the number of
its indecomposable Minkovski summands of positive volume. The proof is based on
a new fast factorization algorithm in $\mathbb{K}[[x]][y]$ with respect to a
slope valuation, a result which has its own interest.",['math.AC'],False,,,,The dual notion of morphic modules over commutative rings,Improvements of convex-dense factorization of bivariate polynomials
neg-d2-856,2025-02-19,,2502.13598," The polarization tensor of graphene derived in the framework of the Dirac
model using the methods of thermal quantum field theory in (2+1) dimensions is
recast in a mathematically equivalent but more compact and convenient in
computations form along the real frequency axis. The obtained unified
expressions for the components of the polarization tensor are equally
applicable in the regions of the on- and off-the-mass-shell electromagnetic
waves. The advantages of the presented formalism are demonstrated on the
example of nonequilibrium Casimir force in the configuration of two parallel
graphene-coated dielectric plates one of which is either hotter or colder than
the environment. This force is investigated as a function of temperature, the
energy gap, and chemical potential of graphene coatings with account of the
effects of spatial dispersion. Besides the thermodynamically nonequilibrium
Casimir and Casimir-Polder forces, the obtained form of the polarization tensor
can be useful for investigation of many diverse physical phenomena in graphene
systems, such as surface plasmons, reflectances, electrical conductivity,
radiation heat transfer, etc.",['quant-ph'],2501.06179," Classically hard to simulate quantum states, or ""magic states"", are
prerequisites to quantum advantage, highlighting an apparent separation between
classically and quantumly tractable problems. Classically simulable states such
as Clifford circuits on stabilizer states, free bosonic states, free fermions,
and matchgate circuits are all in some sense Gaussian. While free bosons and
fermions arise from quadratic Hamiltonians, recent works have demonstrated that
bosonic and qudit systems converge to Gaussians and stabilizers under
convolution. In this work, we similarly identify convolution for fermions and
find efficient measures of non-Gaussian magic in pure fermionic states. We
demonstrate that three natural notions for the Gaussification of a state, (1)
the Gaussian state with the same covariance matrix, (2) the fixed point of
convolution, and (3) the closest Gaussian in relative entropy, coincide by
proving a central limit theorem for fermionic systems. We then utilize the
violation of Wick's theorem and the matchgate identity to quantify non-Gaussian
magic in addition to a SWAP test.",['quant-ph'],False,,,,"Polarization tensor in spacetime of three dimensions and quantum field
  theoretical description of the nonequilibrium Casimir force in graphene
  systems","Measuring Non-Gaussian Magic in Fermions: Convolution, Entropy, and the
  Violation of Wick's Theorem and the Matchgate Identity"
neg-d2-857,2025-03-09,,2503.06482," Computational pathology and whole-slide image (WSI) analysis are pivotal in
cancer diagnosis and prognosis. However, the ultra-high resolution of WSIs
presents significant modeling challenges. Recent advancements in pathology
foundation models have improved performance, yet most approaches rely on [CLS]
token representation of tile ViT as slide-level inputs (16x16 pixels is
refereed as patch and 224x224 pixels as tile). This discards critical spatial
details from patch tokens, limiting downstream WSI analysis tasks. We find that
leveraging all spatial patch tokens benefits WSI analysis but incurs nearly
200x higher storage and training costs (e.g., 196 tokens in ViT$_{224}$). To
address this, we introduce vector quantized (VQ) distillation on patch feature,
which efficiently compresses spatial patch tokens using discrete indices and a
decoder. Our method reduces token dimensionality from 1024 to 16, achieving a
64x compression rate while preserving reconstruction fidelity. Furthermore, we
employ a multi-scale VQ (MSVQ) strategy, which not only enhances VQ
reconstruction performance but also serves as a Self-supervised Learning (SSL)
supervision for a seamless slide-level pretraining objective. Built upon the
quantized patch features and supervision targets of tile via MSVQ, we develop a
progressive convolutional module and slide-level SSL to extract representations
with rich spatial-information for downstream WSI tasks. Extensive evaluations
on multiple datasets demonstrate the effectiveness of our approach, achieving
state-of-the-art performance in WSI analysis. Code will be available soon.",['cs.CV'],2503.08016," Predicting pedestrian trajectories is essential for autonomous driving
systems, as it significantly enhances safety and supports informed
decision-making. Accurate predictions enable the prevention of collisions,
anticipation of crossing intent, and improved overall system efficiency. In
this study, we present SGNetPose+, an enhancement of the SGNet architecture
designed to integrate skeleton information or body segment angles with bounding
boxes to predict pedestrian trajectories from video data to avoid hazards in
autonomous driving. Skeleton information was extracted using a pose estimation
model, and joint angles were computed based on the extracted joint data. We
also apply temporal data augmentation by horizontally flipping video frames to
increase the dataset size and improve performance. Our approach achieves
state-of-the-art results on the JAAD and PIE datasets using pose data with the
bounding boxes, outperforming the SGNet model. Code is available on Github:
SGNetPose+.",['cs.CV'],False,,,,"PathVQ: Reforming Computational Pathology Foundation Model for Whole
  Slide Image Analysis via Vector Quantization","SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for
  Trajectory Prediction in Autonomous Driving"
neg-d2-858,2025-03-12,,2503.09282," In operating system development, concurrency poses significant challenges. It
is difficult for humans to manually review concurrent behaviors or to write
test cases covering all possible executions, often resulting in critical bugs.
Preemption in schedulers serves as a typical example. This paper proposes a
development method for concurrent software, such as schedulers. Our method
incorporates model checking as an aid for tracing code, simplifying the
analysis of concurrent behavior; we refer to this as model checking-assisted
code review. While this approach aids in tracing behaviors, the accuracy of the
results is limited because of the semantics gap between the modeling language
and the programming language. Therefore, we also introduce runtime verification
to address this limitation in model checking-assisted code review. We applied
our approach to a real-world operating system, Awkernel, as a case study. This
new operating system, currently under development for autonomous driving, is
designed for preemptive task execution using asynchronous functions in Rust.
After implementing our method, we identified several bugs that are difficult to
detect through manual reviews or simple tests.",['cs.SE'],2503.15626," Selecting the combination of security controls that will most effectively
protect a system's assets is a difficult task. If the wrong controls are
selected, the system may be left vulnerable to cyber-attacks that can impact
the confidentiality, integrity, and availability of critical data and services.
In practical settings, as standardized control catalogues can be quite large,
it is not possible to select and implement every control possible. Instead,
considerations, such as budget, effectiveness, and dependencies among various
controls, must be considered to choose a combination of security controls that
best achieve a set of system security objectives. In this paper, we present a
game-theoretic approach for selecting effective combinations of security
controls based on expected attacker profiles and a set budget. The control
selection problem is set up as a two-person zero-sum one-shot game. Valid
control combinations for selection are generated using an algebraic formalism
to account for dependencies among selected controls. Using a software tool, we
apply the approach on a fictional Canadian military system with Canada's
standardized control catalogue, ITSG-33. Through this case study, we
demonstrate the approach's scalability to assist in selecting an effective set
of security controls for large systems. The results illustrate how a security
analyst can use the proposed approach and supporting tool to guide and support
decision-making in the control selection activity when developing secure
systems of all sizes.",['cs.SE'],False,,,,A Case Study on Model Checking and Runtime Verification for Awkernel,"A Scalable Game-Theoretic Approach for Selecting Security Controls from
  Standardized Catalogues"
neg-d2-859,2025-02-10,,2502.06338," Depth completion, predicting dense depth maps from sparse depth measurements,
is an ill-posed problem requiring prior knowledge. Recent methods adopt
learning-based approaches to implicitly capture priors, but the priors
primarily fit in-domain data and do not generalize well to out-of-domain
scenarios. To address this, we propose a zero-shot depth completion method
composed of an affine-invariant depth diffusion model and test-time alignment.
We use pre-trained depth diffusion models as depth prior knowledge, which
implicitly understand how to fill in depth for scenes. Our approach aligns the
affine-invariant depth prior with metric-scale sparse measurements, enforcing
them as hard constraints via an optimization loop at test-time. Our zero-shot
depth completion method demonstrates generalization across various domain
datasets, achieving up to a 21\% average performance improvement over the
previous state-of-the-art methods while enhancing spatial understanding by
sharpening scene details. We demonstrate that aligning a monocular
affine-invariant depth prior with sparse metric measurements is a proven
strategy to achieve domain-generalizable depth completion without relying on
extensive training data. Project page:
https://hyoseok1223.github.io/zero-shot-depth-completion/.",['cs.CV'],2503.00641," Post-hoc importance attribution methods are a popular tool for ""explaining""
Deep Neural Networks (DNNs) and are inherently based on the assumption that the
explanations can be applied independently of how the models were trained.
Contrarily, in this work we bring forward empirical evidence that challenges
this very notion. Surprisingly, we discover a strong dependency on and
demonstrate that the training details of a pre-trained model's classification
layer (less than 10 percent of model parameters) play a crucial role, much more
than the pre-training scheme itself. This is of high practical relevance: (1)
as techniques for pre-training models are becoming increasingly diverse,
understanding the interplay between these techniques and attribution methods is
critical; (2) it sheds light on an important yet overlooked assumption of
post-hoc attribution methods which can drastically impact model explanations
and how they are interpreted eventually. With this finding we also present
simple yet effective adjustments to the classification layers, that can
significantly enhance the quality of model explanations. We validate our
findings across several visual pre-training frameworks (fully-supervised,
self-supervised, contrastive vision-language training) and analyse how they
impact explanations for a wide range of attribution methods on a diverse set of
evaluation metrics.",['cs.CV'],False,,,,"Zero-shot Depth Completion via Test-time Alignment with Affine-invariant
  Depth Prior","How to Probe: Simple Yet Effective Techniques for Improving Post-hoc
  Explanations"
neg-d2-860,2025-02-28,,2502.20909," Pseudoline arrangements are fundamental objects in discrete and computational
geometry, and different works have tackled the problem of improving the known
bounds on the number of simple arrangements of $n$ pseudolines over the past
decades. The lower bound in particular has seen two successive improvements in
recent years (Dumitrescu and Mandal in 2020 and Cort\'es K\""uhnast et al. in
2024). Here we focus on the upper bound, and show that for large enough $n$,
there are at most $2^{0.6496n^2}$ different simple arrangements of $n$
pseudolines. This follows a series of incremental improvements starting with
work by Knuth in 1992 showing a bound of roughly $2^{0.7925n^2},$ then a bound
of $2^{0.6975n^2}$ by Felsner in 1997, and finally the previous best known
bound of $2^{0.6572n^2}$ by Felsner and Valtr in 2011. The improved bound
presented here follows from a simple argument to combine the approach of this
latter work with the use of the Zone Theorem.",['cs.CG'],2501.01901," Simplicial complexes arising from real-world settings may not be directly
observable. Hence, for an unknown simplicial complex in Euclidean space, we
want to efficiently reconstruct it by querying local structure. In particular,
we are interested in queries for the indegree of a simplex $\sigma$ in some
direction: the number of cofacets of $\sigma$ contained in some halfspace
""below"" $\sigma$. Fasy et al. proposed a method that, given the vertex set of a
simplicial complex, uses indegree queries to reconstruct the set of edges. In
particular, they use a sweep algorithm through the vertex set, identifying
edges adjacent to and above each vertex in the sweeping order. The algorithm
relies on a natural but crucial property of the sweeping order: at a given
vertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have
another endpoint that appeared earlier in the order.
  The edge reconstruction algorithm does not immediately extend to
higher-dimensional simplex reconstruction. In particular, it is not possible to
sweep through a set of $i$-simplices in a fixed direction and maintain that all
$(i+1)$-cofacets of a given simplex $\sigma$ that come below $\sigma$ are
known. We circumvent this by defining a sweeping order on a set of
$i$-simplices, that additionally pairs each $i$-simplex $\sigma$ with a
direction perpendicular to $\sigma$. Analogous to Fasy et al., our order has
the crucial property that, at any $i$-simplex $\sigma$ paired with direction
$s$, each $(i+1)$-dimensional coface of $\sigma$ that lies in the halfspace
below $\sigma$ with respect to the direction $s$ has an $i$-dimensional face
that appeared earlier in the order. We show how to compute such an order and
use it to extend the edge reconstruction algorithm of Fasy et al. to simplicial
complex reconstruction. Our algorithm can reconstruct arbitrary embedded
simplicial complexes.",['cs.CG'],False,,,,"Improved Bound on the Number of Pseudoline Arrangements via the Zone
  Theorem",Sweeping Orders for Simplicial Complex Reconstruction
neg-d2-861,2025-02-18,,2502.12663," Large language models (LLMs) are designed to perform a wide range of tasks.
To improve their ability to solve complex problems requiring multi-step
reasoning, recent research leverages process reward modeling to provide
fine-grained feedback at each step of the reasoning process for reinforcement
learning (RL), but it predominantly focuses on English. In this paper, we
tackle the critical challenge of extending process reward models (PRMs) to
multilingual settings. To achieve this, we train multilingual PRMs on a dataset
spanning seven languages, which is translated from English. Through
comprehensive evaluations on two widely used reasoning benchmarks across 11
languages, we demonstrate that multilingual PRMs not only improve average
accuracy but also reduce early-stage reasoning errors. Furthermore, our results
highlight the sensitivity of multilingual PRMs to both the number of training
languages and the volume of English data, while also uncovering the benefits
arising from more candidate responses and trainable parameters. This work opens
promising avenues for robust multilingual applications in complex, multi-step
reasoning tasks. In addition, we release the code to foster research along this
line.",['cs.CL'],2501.04249," Despite the remarkable advancements and widespread applications of deep
neural networks, their ability to perform reasoning tasks remains limited,
particularly in domains requiring structured, abstract thought. In this paper,
we investigate the linguistic reasoning capabilities of state-of-the-art large
language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from
International Linguistics Olympiad (IOL) problems. This dataset encompasses
diverse problems testing syntax, morphology, phonology, and semantics, all
carefully designed to be self-contained and independent of external knowledge.
These tasks challenge models to engage in metacognitive linguistic reasoning,
requiring the deduction of linguistic rules and patterns from minimal examples.
Through extensive benchmarking of leading LLMs, we find that even the most
advanced models struggle to handle the intricacies of linguistic complexity,
particularly in areas demanding compositional generalization and rule
abstraction. Our analysis highlights both the strengths and persistent
limitations of current models in linguistic problem-solving, offering valuable
insights into their reasoning capabilities. By introducing IOLBENCH, we aim to
foster further research into developing models capable of human-like reasoning,
with broader implications for the fields of computational linguistics and
artificial intelligence.",['cs.CL'],False,,,,Demystifying Multilingual Chain-of-Thought in Process Reward Modeling,IOLBENCH: Benchmarking LLMs on Linguistic Reasoning
neg-d2-862,2025-02-18,,2502.12898," This study examines the potential causal relationship between head injury and
the risk of developing Alzheimer's disease (AD) using Bayesian networks and
regression models. Using a dataset of 2,149 patients, we analyze key medical
history variables, including head injury history, memory complaints,
cardiovascular disease, and diabetes. Logistic regression results suggest an
odds ratio of 0.88 for head injury, indicating a potential but statistically
insignificant protective effect against AD. In contrast, memory complaints
exhibit a strong association with AD, with an odds ratio of 4.59. Linear
regression analysis further confirms the lack of statistical significance for
head injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive
importance of memory complaints. These findings highlight the complex interplay
of medical history factors in AD risk assessment and underscore the need for
further research utilizing larger datasets and advanced causal modeling
techniques.",['cs.LG'],2502.09374," Deep neural networks (DNNs) are increasingly used in safety-critical
applications. Reliable fault analysis and mitigation are essential to ensure
their functionality in harsh environments that contain high radiation levels.
This study analyses the impact of multiple single-bit single-event upsets in
DNNs by performing fault injection at the level of a DNN model. Additionally, a
fault aware training (FAT) methodology is proposed that improves the DNNs'
robustness to faults without any modification to the hardware. Experimental
results show that the FAT methodology improves the tolerance to faults up to a
factor 3.",['cs.LG'],False,,,,"The Relationship Between Head Injury and Alzheimer's Disease: A Causal
  Analysis with Bayesian Networks","Mitigating multiple single-event upsets during deep neural network
  inference using fault-aware training"
neg-d2-863,2025-02-10,,2502.06292," Joint optimization of poses and features has been extensively studied and
demonstrated to yield more accurate results in feature-based SLAM problems.
However, research on jointly optimizing poses and non-feature-based maps
remains limited. Occupancy maps are widely used non-feature-based environment
representations because they effectively classify spaces into obstacles, free
areas, and unknown regions, providing robots with spatial information for
various tasks. In this paper, we propose Occupancy-SLAM, a novel
optimization-based SLAM method that enables the joint optimization of robot
trajectory and the occupancy map through a parameterized map representation.
The key novelty lies in optimizing both robot poses and occupancy values at
different cell vertices simultaneously, a significant departure from existing
methods where the robot poses need to be optimized first before the map can be
estimated. Evaluations using simulations and practical 2D laser datasets
demonstrate that the proposed approach can robustly obtain more accurate robot
trajectories and occupancy maps than state-of-the-art techniques with
comparable computational time. Preliminary results in the 3D case further
confirm the potential of the proposed method in practical 3D applications,
achieving more accurate results than existing methods.",['cs.RO'],2503.07547," In human-robot interactions, human and robot agents maintain internal mental
models of their environment, their shared task, and each other. The accuracy of
these representations depends on each agent's ability to perform theory of
mind, i.e. to understand the knowledge, preferences, and intentions of their
teammate. When mental models diverge to the extent that it affects task
execution, reconciliation becomes necessary to prevent the degradation of
interaction. We propose a framework for bi-directional mental model
reconciliation, leveraging large language models to facilitate alignment
through semi-structured natural language dialogue. Our framework relaxes the
assumption of prior model reconciliation work that either the human or robot
agent begins with a correct model for the other agent to align to. Through our
framework, both humans and robots are able to identify and communicate missing
task-relevant context during interaction, iteratively progressing toward a
shared mental model.",['cs.RO'],False,,,,"Occupancy-SLAM: An Efficient and Robust Algorithm for Simultaneously
  Optimizing Robot Poses and Occupancy Map","Bi-Directional Mental Model Reconciliation for Human-Robot Interaction
  with Large Language Models"
neg-d2-864,2025-02-20,,2502.14739," Large language models (LLMs) have demonstrated remarkable proficiency in
mainstream academic disciplines such as mathematics, physics, and computer
science. However, human knowledge encompasses over 200 specialized disciplines,
far exceeding the scope of existing benchmarks. The capabilities of LLMs in
many of these specialized fields-particularly in light industry, agriculture,
and service-oriented disciplines-remain inadequately evaluated. To address this
gap, we present SuperGPQA, a comprehensive benchmark that evaluates
graduate-level knowledge and reasoning capabilities across 285 disciplines. Our
benchmark employs a novel Human-LLM collaborative filtering mechanism to
eliminate trivial or ambiguous questions through iterative refinement based on
both LLM responses and expert feedback. Our experimental results reveal
significant room for improvement in the performance of current state-of-the-art
LLMs across diverse knowledge domains (e.g., the reasoning-focused model
DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting
the considerable gap between current model capabilities and artificial general
intelligence. Additionally, we present comprehensive insights from our
management of a large-scale annotation process, involving over 80 expert
annotators and an interactive Human-LLM collaborative system, offering valuable
methodological guidance for future research initiatives of comparable scope.",['cs.CL'],2501.07861," Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs)
hold promise in knowledge-intensive tasks but face limitations in complex
multi-step reasoning. While recent methods have integrated RAG with
chain-of-thought reasoning or test-time search using Process Reward Models
(PRMs), these approaches encounter challenges such as a lack of explanations,
bias in PRM training data, early-step bias in PRM scores, and insufficient
post-training optimization of reasoning potential. To address these issues, we
propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding
(ReARTeR), a framework that enhances RAG systems' reasoning capabilities
through post-training and test-time scaling. At test time, ReARTeR introduces
Trustworthy Process Rewarding via a Process Reward Model for accurate scalar
scoring and a Process Explanation Model (PEM) for generating natural language
explanations, enabling step refinement. During post-training, it utilizes Monte
Carlo Tree Search guided by Trustworthy Process Rewarding to collect
high-quality step-level preference data, optimized through Iterative Preference
Optimization. ReARTeR addresses three core challenges: (1) misalignment between
PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM
training data, mitigated by balanced annotation methods and stronger
annotations for challenging examples; and (3) early-step bias in PRM, resolved
through a temporal-difference-based look-ahead search strategy. Experimental
results on multi-step reasoning benchmarks demonstrate significant
improvements, underscoring ReARTeR's potential to advance the reasoning
capabilities of RAG systems.",['cs.CL'],False,,,,SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines,"ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process
  Rewarding"
neg-d2-865,2025-03-05,,2503.05826," The present work is devoted to Computability Logic (CoL), the young and
volcanic research-project developed by Giorgi Japaridze. Our main goal is to
provide the reader with a clear panoramic view of this vast new land, starting
from its core knots and making our way towards the outer threads, in a somewhat
three-dimensional, spacial gait. Furthermore, through the present work, we
provide a tentative proof for the decidability of one of CoL's numerous
axiomatisations, namely CL15. Thus, our expedition initially takes off for an
aerial, perusal overview of this fertile steppe. The first chapter introduces
CoL in a philosophical fashion, exposing and arguing its main key points. We
then move over to unfold its semantics and syntax profiles, allowing the reader
to become increasingly more familiar with this new environment. Landing on to
the second chapter, we thoroughly introduce Cirquent Calculus, the new
deductive system Japaridze has developed in order to axiomatise Computability
Logic. Indeed, this new proof-system can also be a useful tool for many other
logics. We then review each of the 17 axiomatisations found so far. The third
chapter zooms-in on CL15, in order to come up with a possible solution to its
open problem. We outline its soundness and completeness proofs; then provide
some few deductive examples; and, finally, build a tentative proof of its
decidability. Lastly, the fourth chapter focuses on the potential and actual
applications of Computability Logic, both in arithmetic (clarithmetic) and in
Artificial Intelligence systems (meaning knowledgebase and planning-and-action
ones). We close our journey with some final remarks on the richness of this
framework and, hence, the research-worthiness it entails.",['cs.LO'],2501.15913," Stream-based runtime monitoring frameworks are safety assurance tools that
check the runtime behavior of a system against a formal specification. This
tutorial provides a hands-on introduction to RTLola, a real-time monitoring
toolkit for cyber-physical systems and networks. RTLola processes, evaluates,
and aggregates streams of input data, such as sensor readings, and provides a
real-time analysis in the form of comprehensive statistics and logical
assessments of the system's health. RTLola has been applied successfully in
monitoring autonomous systems such as unmanned aircraft. The tutorial guides
the reader through the development of a stream-based specification for an
autonomous drone observing other flying objects in its flight path. Each
tutorial section provides an intuitive introduction, highlighting useful
language features and specification patterns, and gives a more in-depth
explanation of technical details for the advanced reader. Finally, we discuss
how runtime monitors generated from RTLola specifications can be integrated
into a variety of systems and discuss different monitoring applications.",['cs.LO'],False,,,,"The Fertile Steppe: Computability Logic and the decidability of one of
  its fragments",A Tutorial on Stream-based Monitoring
neg-d2-866,2025-01-03,,2501.01736," The Pierre Auger Observatory concluded its first phase of data taking after
seventeen years of operation. The dataset collected by its surface and
fluorescence detectors (FD and SD) provides us with the most precise estimates
of the energy spectrum and mass composition of ultra-high energy cosmic rays
yet available. We present measurements of the depth of shower maximum, the main
quantity used to derive species of primary particles, determined either from
the direct observation of longitudinal profiles of showers by the FD, or
indirectly through the analysis of signals in the SD stations. The energy
spectrum of primaries is also determined from both FD and SD measurements,
where the former exhibits lower systematic uncertainty in the energy
determination while the latter exploits unprecedentedly large exposure. The
data for primaries with energy below 1 EeV are also available thanks to the
high-elevation telescopes of FD and the denser array of SD, making measurements
possible down to 6 PeV and 60 PeV, respectively.",['astro-ph.HE'],2503.08487," Black holes can launch powerful jets through the Blandford-Znajek process.
This relies on enough plasma in the jet funnel to conduct the necessary
current. However, in some low luminosity active galactic nuclei, the plasma
supply near the jet base may be an issue. It has been proposed that spark gaps
-- local regions with unscreened electric field -- can form in the
magnetosphere, accelerating particles to initiate pair cascades, thus filling
the jet funnel with plasma. In this paper, we carry out 2D general relativistic
particle-in-cell (GRPIC) simulations of the gap, including self-consistent
treatment of inverse Compton scattering and pair production. We observe gap
dynamics that is fully consistent with our earlier 1D GRPIC simulations. We
find strong dependence of the gap power on the soft photon spectrum and energy
density, as well as the strength of the horizon magnetic field. We derive
physically motivated scaling relations, and applying to M87, we find that the
gap may be energetically viable for the observed TeV flares. For Sgr A$^*$, the
energy dissipated in the gap may also be sufficient to power the X-ray flares.",['astro-ph.HE'],False,,,,"Energy spectrum and mass composition of cosmic rays from Phase I data
  measured using the Pierre Auger Observatory","Physics of Pair Producing Gaps in Black Hole Magnetospheres: Two
  Dimensional General Relativistic Particle-in-cell Simulations"
neg-d2-867,2025-03-20,,2503.16607," The baryonic Tully Fisher relation (bTFR) provides an empirical connection
between baryonic mass and dynamical mass (measured by the maximum rotation
velocity) for galaxies. Due to the impact of baryonic feedback in the shallower
potential wells of dwarf galaxies, the bTFR is predicted to turn down at low
masses from the extrapolated power-law relation at high masses. The low-mass
end of the bTFR is poorly constrained due to small samples and difficulty in
connecting the galaxy's gas kinematics to its dark matter halo. Simulations can
help us understand this connection and interpret observations. We measure the
bTFR with 66 dwarf galaxies from the Marvel-ous and Marvelous Massive Dwarfs
hydrodynamic simulations. Our sample has M$_\star = 10^6-10^9$ M$_\odot$, and
is mostly gas dominated. We compare five velocity methods: V$_\text{out,circ}$
(spatially resolved mass-enclosed), V$_\text{out,mid}$ (spatially resolved
midplane gravitational potential), and unresolved HI linewidths at different
percentages of the peak flux (W$_\text{10}$, W$_\text{20}$, and W$_\text{50}$).
We find an intrinsic turndown in the bTFR for maximum halo speeds $\lesssim 50$
km s$^{-1}$ (or total baryonic mass, M$_\text{bary}\lesssim 10^{8.5}$
M$_\odot$). We find that observing HI in lower-mass galaxies to the
conventional surface density limit of 1M$_\odot$pc$^{-2}$ is not enough to
detect a turndown in the bTFR; none of the HI velocity methods (spatially
resolved or unresolved) recover the turndown, and we find bTFR slopes
consistent with observations of higher-mass galaxies. However, we predict that
the turndown can be recovered by resolved rotation curves if the HI limit is
$\lesssim 0.08$ M$_\odot$ pc$^{-2}$, which is within the sensitivity of current
HI surveys like FEASTS and MHONGOOSE.",['astro-ph.GA'],2502.01728," Gravitational microlensing is a unique probe of the stellar content in strong
lens galaxies. Flux ratio anomalies from gravitationally lensed supernovae
(glSNe), just like lensed quasars, can be used to constrain the stellar mass
fractions at the image positions. Type Ia supernovae are of particular interest
as knowledge of the intrinsic source brightness helps constrain the amount of
(de)magnification from the macromodel predictions that might be due to
microlensing. In addition, the presence or absence of caustic crossings in the
light curves of glSNe can be used to constrain the mass of the microlenses. We
find that a sample of 50 well-modeled glSNe Ia systems with single epoch
observations at peak intrinsic supernova luminosity should be able to constrain
a stellar mass-to-light ratio to within $\sim 15\%$. A set of systems with
light curve level information providing the location (or absence) of caustic
crossing events can also constrain the mass of the microlenses to within $\sim
50\%$. Much work is needed to make such a measurement in practice, but our
results demonstrate the feasibility of microlensing to place constraints on
astrophysical parameters related to the initial mass function of lensing
galaxies without any prior assumptions on the stellar mass.",['astro-ph.GA'],False,,,,"Predictions for Detecting a Turndown in the Baryonic Tully Fisher
  Relation","Stars as cosmic scales: measuring stellar mass with microlensed
  supernovae"
neg-d2-868,2025-02-27,,2502.19955," Camera pose estimation is crucial for many computer vision applications, yet
existing benchmarks offer limited insight into method limitations across
different geometric challenges. We introduce RUBIK, a novel benchmark that
systematically evaluates image matching methods across well-defined geometric
difficulty levels. Using three complementary criteria - overlap, scale ratio,
and viewpoint angle - we organize 16.5K image pairs from nuScenes into 33
difficulty levels. Our comprehensive evaluation of 14 methods reveals that
while recent detector-free approaches achieve the best performance (>47%
success rate), they come with significant computational overhead compared to
detector-based methods (150-600ms vs. 40-70ms). Even the best performing method
succeeds on only 54.8% of the pairs, highlighting substantial room for
improvement, particularly in challenging scenarios combining low overlap, large
scale differences, and extreme viewpoint changes. Benchmark will be made
publicly available.",['cs.CV'],2502.06338," Depth completion, predicting dense depth maps from sparse depth measurements,
is an ill-posed problem requiring prior knowledge. Recent methods adopt
learning-based approaches to implicitly capture priors, but the priors
primarily fit in-domain data and do not generalize well to out-of-domain
scenarios. To address this, we propose a zero-shot depth completion method
composed of an affine-invariant depth diffusion model and test-time alignment.
We use pre-trained depth diffusion models as depth prior knowledge, which
implicitly understand how to fill in depth for scenes. Our approach aligns the
affine-invariant depth prior with metric-scale sparse measurements, enforcing
them as hard constraints via an optimization loop at test-time. Our zero-shot
depth completion method demonstrates generalization across various domain
datasets, achieving up to a 21\% average performance improvement over the
previous state-of-the-art methods while enhancing spatial understanding by
sharpening scene details. We demonstrate that aligning a monocular
affine-invariant depth prior with sparse metric measurements is a proven
strategy to achieve domain-generalizable depth completion without relying on
extensive training data. Project page:
https://hyoseok1223.github.io/zero-shot-depth-completion/.",['cs.CV'],False,,,,"RUBIK: A Structured Benchmark for Image Matching across Geometric
  Challenges","Zero-shot Depth Completion via Test-time Alignment with Affine-invariant
  Depth Prior"
neg-d2-869,2025-03-24,,2503.18343," A horoboundary is one of the attempts to compactify metric spaces, and is
constructed using continuous functions on metric spaces. It is a concept that
includes global information of metric spaces, and its correspondence with an
ideal boundary constructed using geodesics has been studied in nonpositive
curvature spaces such as CAT(0) spaces and geodesic Gromov hyperbolic spaces.
We will introduce a certain correspondence between the horoboundary and the
ideal boundary of coarsely convex spaces, which can be regarded as a
generalization of spaces of nonpositive curvature.",['math.MG'],2501.11444," In this work, we review the concept of center of a geometric object as an
equivariant map, unifying and generalizing different approaches followed by
authors such as C. Kimberling or A. Edmonds. We provide examples to illustrate
that this general approach encompasses many interesting spaces of geometric
objects arising from different settings. Additionally, we discuss two results
that characterize centers for some particular spaces of geometric objects, and
we pose five open questions related to the generalization of these
characterizations to other spaces. Finally, we conclude this article by briefly
discussing other central objects and their relation to this concept of center.",['math.MG'],False,,,,Horoboundaries of coarsely convex spaces,On the concept of center for geometric objects and related problems
neg-d2-870,2025-03-15,,2503.12265," In this letter, we demonstrate that previously proposed improved state
parameterizations for soft and continuum robots are specific cases of Clarke
coordinates. By explicitly deriving these improved parameterizations from a
generalized Clarke transformation matrix, we unify various approaches into one
comprehensive mathematical framework. This unified representation provides
clarity regarding their relationships and generalizes them beyond existing
constraints, including arbitrary joint numbers, joint distributions, and
underlying modeling assumptions. This unification consolidates prior insights
and establishes Clarke coordinates as a foundational tool, enabling systematic
knowledge transfer across different subfields within soft and continuum
robotics.",['cs.RO'],2503.16559," This paper proposes a design scheme of reward function that constantly
evaluates both driving states and actions for applying reinforcement learning
to automated driving. In the field of reinforcement learning, reward functions
often evaluate whether the goal is achieved by assigning values such as +1 for
success and -1 for failure. This type of reward function can potentially obtain
a policy that achieves the goal, but the process by which the goal is reached
is not evaluated. However, process to reach a destination is important for
automated driving, such as keeping velocity, avoiding risk, retaining distance
from other cars, keeping comfortable for passengers. Therefore, the reward
function designed by the proposed scheme is suited for automated driving by
evaluating driving process. The effects of the proposed scheme are demonstrated
on simulated circuit driving and highway cruising. Asynchronous Advantage
Actor-Critic is used, and models are trained under some situations for
generalization. The result shows that appropriate driving positions are
obtained, such as traveling on the inside of corners, and rapid deceleration to
turn along sharp curves. In highway cruising, the ego vehicle becomes able to
change lane in an environment where there are other vehicles with suitable
deceleration to avoid catching up to a front vehicle, and acceleration so that
a rear vehicle does not catch up to the ego vehicle.",['cs.RO'],False,,,,"Clarke Coordinates Are Generalized Improved State Parametrization for
  Continuum Robots","Design of Reward Function on Reinforcement Learning for Automated
  Driving"
neg-d2-871,2025-01-16,,2501.09652," A single light-driven Janus particle confined in a very thin oil droplet at
an air--water interface displays intriguing dynamics. While laser activation
induces rapid horizontal motion (1mm/s--1cm/s) by thermal Marangoni flow, the
particle exhibits unexpected periodic circular motions or intermittent
irregular motions. We show that the periodic trajectories are the result of a
coupling between the self-propulsion of the particle and the spatiotemporal
droplet thickness changes. We propose a simple model where the properties of
the active particle trajectories are governed by capillary forces and torques
due to the confinement of the particle in the thin droplet.",['cond-mat.soft'],2502.03959," The hard-sphere potential has become a cornerstone in the study of both
molecular and complex fluids. Despite its mathematical simplicity, its
implementation in fixed time-step molecular simulations remains a formidable
challenge due to the discontinuity at contact. To circumvent the issues
associated with the ill-defined force at contact, a continuous
potential--referred to here as the pseudo-hard-sphere (pHS) potential--has
recently been proposed [J. Chem, Phys. 149, 164907 (2018)]. This potential is
constructed to match the second virial coefficient of the hard-sphere potential
and is expected to mimic its thermodynamic properties. However, this hypothesis
has only been partially validated within the fluid region of the phase diagram
for hard-sphere dispersions in two and three dimensions. In this contribution,
we examine the ability of the continuous pHS potential to reproduce the
equation of state of a hard-sphere fluid, not only in the fluid phase but also
across the fluid-solid coexistence region. Our focus is primarily on
hard-sphere systems in three and four dimensions. We compare the results
obtained from Brownian dynamics simulations of the pHS potential with those
derived from refined event-driven simulations of the corresponding hard-sphere
potential. Furthermore, we provide a comparative analysis with theoretical
equations of state based on both mean-field and integral equation
approximations.",['cond-mat.soft'],False,,,,Active particle in a very thin interfacial droplet,"Phase diagram of the hard-sphere potential model in three and four
  dimensions using a pseudo-hard-sphere potential"
neg-d2-872,2025-02-05,,2502.03726," Text-to-image diffusion models are capable of generating high-quality images,
but these images often fail to align closely with the given text prompts.
Classifier-free guidance (CFG) is a popular and effective technique for
improving text-image alignment in the generative process. However, using CFG
introduces significant computational overhead and deviates from the established
theoretical foundations of diffusion models. In this paper, we present
DIstilling CFG by enhancing text Embeddings (DICE), a novel approach that
removes the reliance on CFG in the generative process while maintaining the
benefits it provides. DICE distills a CFG-based text-to-image diffusion model
into a CFG-free version by refining text embeddings to replicate CFG-based
directions. In this way, we avoid the computational and theoretical drawbacks
of CFG, enabling high-quality, well-aligned image generation at a fast sampling
speed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL
and PixArt-$\alpha$ demonstrate the effectiveness of our method. Furthermore,
DICE supports negative prompts for image editing to improve image quality
further. Code will be available soon.",['cs.CV'],2501.01121," While current high-resolution depth estimation methods achieve strong
results, they often suffer from computational inefficiencies due to reliance on
heavyweight models and multiple inference steps, increasing inference time. To
address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner
models with lightweight encoders. This reduces model size and inference time
but introduces noisy features. To overcome this, we propose a Coarse-to-Fine
(C2F) module with a Guided Denoising Unit for refining and denoising the
refiner features and a Noisy Pretraining strategy to pretrain the refiner
branch to fully exploit the potential of the lightweight refiner branch.
Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching
(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms
state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy
and speed, using fewer parameters and faster inference. It also shows improved
depth boundary delineation on real-world datasets like CityScape, ScanNet++,
and KITTI, demonstrating its versatility across domains.",['cs.CV'],False,,,,DICE: Distilling Classifier-Free Guidance into Text Embeddings,"PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric
  Depth Estimation"
neg-d2-873,2025-02-14,,2502.10493," Eclipsing binaries with pulsating components are a distinct subclass of
binaries, merging orbital and pulsational analyses. In recent years, that
subclass led to the definition of a newly formed branch of tidal
asteroseismology. While single-star pulsators are well understood, the effects
of binarity and possible mass transfer on pulsational characteristics,
particularly in mass-gaining stars, remain to be systematically explored. Here,
I present preliminary results on the asteroseismic properties of a
mass-accreting model for a 10 $M_{\odot}$ $\beta$ Cephei-type star",['astro-ph.SR'],2502.11838," The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is
among the early evidences that they contain multiple populations of stars.
Indeed, the location of each star along the HB depends both on its initial
helium content (Y) and on the global average mass loss along the red giant
branch ($\mu$). In most GCs, it is generally straightforward to analyse the
first stellar population (standard Y), and the most extreme one (largest Y),
while it is more tricky to look at the ""intermediate"" populations (mildly
enhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever
possible the helium abundance for each stellar populations is constrained by
using independent measurements present in the literature. We compare population
synthesis models with photometric catalogues from the Hubble Space Telescope
Treasury survey to derive the parameters of these HB stars. We find that the
location of helium enriched stars on the HB is reproduced only by adopting a
higher value of $\mu$ with respect to the first generation stars in all the
analysed stellar populations. We also find that $\mu$ correlates with the
helium enhancement of the populations. This holds for both clusters. This
finding is naturally predicted by the model of ''pre-main sequence disc early
loss'', previously suggested in the literature, and is consistent with the
findings of multiple-populations formation models that foresee the formation of
second generation stars in a cooling flow.",['astro-ph.SR'],False,,,,"Pulsational characteristics of mass accreting stars in close binary
  systems","Mass loss along the red giant branch of the intermediate stellar
  populations in NGC6752 and NGC2808"
neg-d2-874,2025-01-16,,2501.09829," The purpose of this is the study of certain coherent sheaves of meromorphic
forms on reduced complex space and particularly their behavior with respect to
pull back and higher direct image.",['math.AG'],2501.07586," Let X be a smooth cubic hypersurface. We prove that a general cubic surface
is isomorphic to a hyperplane section of X .",['math.AG'],False,,,,"Sur certains faisceaux de formes m\'eromorphes sur un espace complexe
  r\'eduit",Hyperplane sections of cubic threefolds
neg-d2-875,2025-02-25,,2502.1815," Recent approaches to jointly reconstruct 3D humans and objects from a single
RGB image represent 3D shapes with template-based or coarse models, which fail
to capture details of loose clothing on human bodies. In this paper, we
introduce a novel implicit approach for jointly reconstructing realistic 3D
clothed humans and objects from a monocular view. For the first time, we model
both the human and the object with an implicit representation, allowing to
capture more realistic details such as clothing. This task is extremely
challenging due to human-object occlusions and the lack of 3D information in 2D
images, often leading to poor detail reconstruction and depth ambiguity. To
address these problems, we propose a novel attention-based neural implicit
model that leverages image pixel alignment from both the input human-object
image for a global understanding of the human-object scene and from local
separate views of the human and object images to improve realism with, for
example, clothing details. Additionally, the network is conditioned on semantic
features derived from an estimated human-object pose prior, which provides 3D
spatial information about the shared space of humans and objects. To handle
human occlusion caused by objects, we use a generative diffusion model that
inpaints the occluded regions, recovering otherwise lost details. For training
and evaluation, we introduce a synthetic dataset featuring rendered scenes of
inter-occluded 3D human scans and diverse objects. Extensive evaluation on both
synthetic and real-world datasets demonstrates the superior quality of the
proposed human-object reconstructions over competitive methods.",['cs.CV'],2503.12539," 3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.",['cs.CV'],False,,,,"Realistic Clothed Human and Object Joint Reconstruction from a Single
  Image","BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature
  Analysis"
neg-d2-876,2025-01-29,,2501.17564," We present the first results of the HI intensity mapping power spectrum
analysis with the MeerKAT International GigaHertz Tiered Extragalactic
Exploration (MIGHTEE) survey. We use data covering $\sim$ 4 square degrees in
the COSMOS field using a frequency range 962.5 MHz to 1008.42 MHz, equivalent
to HI emission in $0.4<z<0.48$. The data consists of 15 pointings with a total
of 94.2 hours on-source. We verify the suitability of the MIGHTEE data for HI
intensity mapping by testing for residual systematics across frequency,
baselines and pointings. We also vary the window used for HI signal
measurements and find no significant improvement using stringent Fourier mode
cuts. Averaging in the power spectrum domain, i.e. using incoherent averaging,
we calculate the first upper limits from MIGHTEE on the HI power spectrum at
scales $0.5 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$. We obtain the best
1$\sigma$ upper limit of 28.6 mK$^{2}$Mpc${^3}$ on $k\sim$2 Mpc$^{-1}$. Our
results are consistent with the power spectrum detected with observations in
the DEEP2 field with MeerKAT. The data we use here constitutes a small fraction
of the MIGHTEE survey and demonstrates that combined analysis of the full
MIGHTEE survey can potentially detect the HI power spectrum at $z\lesssim0.5$
in the range $0.1 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$ or quasi-linear
scales.",['astro-ph.CO'],2501.17564," We present the first results of the HI intensity mapping power spectrum
analysis with the MeerKAT International GigaHertz Tiered Extragalactic
Exploration (MIGHTEE) survey. We use data covering $\sim$ 4 square degrees in
the COSMOS field using a frequency range 962.5 MHz to 1008.42 MHz, equivalent
to HI emission in $0.4<z<0.48$. The data consists of 15 pointings with a total
of 94.2 hours on-source. We verify the suitability of the MIGHTEE data for HI
intensity mapping by testing for residual systematics across frequency,
baselines and pointings. We also vary the window used for HI signal
measurements and find no significant improvement using stringent Fourier mode
cuts. Averaging in the power spectrum domain, i.e. using incoherent averaging,
we calculate the first upper limits from MIGHTEE on the HI power spectrum at
scales $0.5 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$. We obtain the best
1$\sigma$ upper limit of 28.6 mK$^{2}$Mpc${^3}$ on $k\sim$2 Mpc$^{-1}$. Our
results are consistent with the power spectrum detected with observations in
the DEEP2 field with MeerKAT. The data we use here constitutes a small fraction
of the MIGHTEE survey and demonstrates that combined analysis of the full
MIGHTEE survey can potentially detect the HI power spectrum at $z\lesssim0.5$
in the range $0.1 Mpc^{-1} \lesssim k \lesssim 10 Mpc^{-1}$ or quasi-linear
scales.",['astro-ph.CO'],False,,,,"HI Intensity Mapping with the MIGHTEE Survey: First Results of the HI
  Power Spectrum","HI Intensity Mapping with the MIGHTEE Survey: First Results of the HI
  Power Spectrum"
neg-d2-877,2025-01-22,,2501.12783," Serverless computing adopts a pay-as-you-go billing model where applications
are executed in stateless and shortlived containers triggered by events,
resulting in a reduction of monetary costs and resource utilization. However,
existing platforms do not provide an upper bound for the billing model which
makes the overall cost unpredictable, precluding many organizations from
managing their budgets. Due to the diverse ranges of serverless functions and
the heterogeneous capacity of edge devices, it is challenging to receive
near-optimal solutions for deployment cost in a polynomial time. In this paper,
we investigated the function scheduling problem with a budget constraint for
serverless computing in wireless networks. Users and IoT devices are sending
requests to edge nodes, improving the latency perceived by users. We propose
two online scheduling algorithms based on reinforcement learning, incorporating
several important characteristics of serverless functions. Via extensive
simulations, we justify the superiority of the proposed algorithm by comparing
with an ILP solver (Midaco). Our results indicate that the proposed algorithms
efficiently approximate the results of Midaco within a factor of 1.03 while our
decision-making time is 5 orders of magnitude less than that of Midaco.",['cs.NI'],2501.08229," This research proposes a system as a solution for the challenges faced by Sri
Lanka' s historic railway system, such as scheduling delays, overcrowding,
manual ticketing, and management inefficiencies. It proposes a multi-subsystem
approach, incorporating GPS tracking, RFID-based e-ticketing, seat reservation,
and vision-based people counting. The GPS based real time train tracking system
performs accurately within 24 meters, with the MQTT protocol showing twice the
speed of the HTTP-based system. All subsystems use the MQTT protocol to enhance
efficiency, reliability, and passenger experience. The study's data and
methodology demonstrate the effectiveness of these innovations in improving
scheduling, passenger flow, and overall system performance, offering promising
solutions for modernizing Sri Lanka's railway infrastructure.",['cs.NI'],False,,,,"Cost Optimization for Serverless Edge Computing with Budget Constraints
  using Deep Reinforcement Learning","Enhancing Train Transportation in Sri Lanka: A Smart IOT based
  Multi-Subsystem Approach using MQTT"
neg-d2-878,2025-03-24,,2503.18717," In this work, we study the existence and nonexistence of nonnegative
solutions to a class of nonlocal elliptic systems set in a bounded open subset
of $\mathbb{R}^N$. The diffusion operators are of type $u_i\mapsto
d_i(-\Delta)^{s_i}u_i$ where $0<s_1\neq s_2<1$, and the gradients of the
unknowns act as source terms. Existence results are obtained by proving some
fine estimates when data belong to weighted Lebesgue spaces. Those estimates
are new and interesting in themselves.",['math.AP'],2501.0863," The paper is concerned with the effect of the spatio-temporal heterogeneity
on the principal eigenvalue of some linear time-periodic parabolic system.
Various asymptotic behaviors of the principal eigenvalue and its monotonicity,
as a function of the diffusion rate and frequency, are first derived. In
particular, some singular behaviors of the principal eigenvalues are observed
when both diffusion rate and frequency approach zero, with some scalar
time-periodic Hamilton-Jacobi equation as the limiting equation. Furthermore,
we completely classify the topological structures of the level sets for the
principal eigenvalues in the plane of frequency and diffusion rate. Our results
not only generalize most of the findings in [S. Liu and Y. Lou, J. Funct.
Anal., 282 (2022), 109338] for scalar periodic-parabolic operators, but also
reveal more rich global information, for time-periodic parabolic systems, on
the dependence of the principal eigenvalues upon the spatio-temporal
heterogeneity.",['math.AP'],False,,,,"Fractional elliptic reaction-diffusion systems with coupled gradient
  terms and different diffusion","On principal eigenvalues of linear time-periodic parabolic systems:
  symmetric mutation case"
neg-d2-879,2025-01-16,,2501.0991," Apologies serve essential functions for moral agents such as expressing
remorse, taking responsibility, and repairing trust. LLM-based chatbots
routinely produce output that has the linguistic form of an apology. However,
they do this simply because they are echoing the kinds of things that humans
say. Moreover, there are reasons to think that chatbots are not the kind of
linguistic or moral agents capable of apology. To put the point bluntly:
Chatbot apologies are bullshit. This paper offers several arguments for this
conclusion, drawing on the nature of morally-serious apologies, the linguistic
agency required to perform them, and the moral agency required for them to
matter. We conclude by considering some consequences for how chatbots should be
designed and how we ought to think about them.",['cs.HC'],2501.07748," The vertical ground reaction force (vGRF) and its characteristic weight
acceptance and push-off peaks measured during walking are important for gait
and biomechanical analysis. Current wearable vGRF estimation methods suffer
from drifting errors or low generalization performances, limiting their
practical application. This paper proposes a novel method for reliably
estimating vGRF and its characteristic peaks using data collected from the
smart insole, including inertial measurement unit data and the newly introduced
center of the pressed sensor data. These data were fused with machine learning
algorithms including artificial neural networks, random forest regression, and
bi-directional long-short-term memory. The proposed method outperformed the
state-of-the-art methods with the root mean squared error, normalized root mean
squared error, and correlation coefficient of 0.024 body weight (BW), 1.79% BW,
and 0.997 in intra-participant testing, and 0.044 BW, 3.22% BW, and 0.991 in
inter-participant testing, respectively. The difference between the reference
and estimated weight acceptance and push-off peak values are 0.022 BW and 0.017
BW with a delay of 1.4% and 1.8% of the gait cycle for the intra-participant
testing and 0.044 BW and 0.025 BW with a delay of 1.5% and 2.3% of the gait
cycle for the inter-participant testing. The results indicate that the proposed
vGRF estimation method has the potential to achieve accurate vGRF measurement
during walking in free living environments.",['cs.HC'],False,,,,Chatbot apologies: Beyond bullshit,"Reliable Vertical Ground Reaction Force Estimation with Smart Insole
  During Walking"
neg-d2-880,2025-03-10,,2503.07906," Image captioning has long been a pivotal task in visual understanding, with
recent advancements in vision-language models (VLMs) significantly enhancing
the ability to generate detailed image captions. However, the evaluation of
detailed image captioning remains underexplored due to outdated evaluation
metrics and coarse annotations. In this paper, we introduce DeCapBench along
with a novel metric, DCScore, specifically designed for detailed captioning
tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by
deconstructing responses into the smallest self-sufficient units, termed
primitive information units, and assessing them individually. Our evaluation
shows that DCScore aligns more closely with human judgment than other
rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high
correlation with VLM arena results on descriptive tasks, surpassing existing
benchmarks for vision-language models. Additionally, we present an automatic
fine-grained feedback collection method, FeedQuill, for preference optimization
based on our advanced metric, showing robust generalization capabilities across
auto-generated preference data. Extensive experiments on multiple VLMs
demonstrate that our method not only significantly reduces hallucinations but
also enhances performance across various benchmarks, achieving superior detail
captioning performance while surpassing GPT-4o.",['cs.CV'],2501.16904," The study of adversarial defense still struggles to combat with advanced
adversarial attacks. In contrast to most prior studies that rely on the
diffusion model for test-time defense to remarkably increase the inference
time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked
AutoEncoder (MAE) into an adversarial purifier framework for test-time
purification. While MAEP achieves promising adversarial robustness, it
particularly features model defense transferability and attack generalization
without relying on using additional data that is different from the training
dataset. To our knowledge, MAEP is the first study of adversarial purifier
based on MAE. Extensive experimental results demonstrate that our method can
not only maintain clear accuracy with only a slight drop but also exhibit a
close gap between the clean and robust accuracy. Notably, MAEP trained on
CIFAR10 achieves state-of-the-art performance even when tested directly on
ImageNet, outperforming existing diffusion-based models trained specifically on
ImageNet.",['cs.CV'],False,,,,"Painting with Words: Elevating Detailed Image Captioning with Benchmark
  and Alignment Learning",Adversarial Masked Autoencoder Purifier with Defense Transferability
neg-d2-881,2025-02-28,,2502.21187," AI models for lung cancer screening are limited by data scarcity, impacting
generalizability and clinical applicability. Generative models address this
issue but are constrained by training data variability. We introduce SYN-LUNGS,
a framework for generating high-quality 3D CT images with detailed annotations.
SYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for
nodule simulation (varying size, location, and appearance), and DukeSim for CT
image formation with vendor and parameter variability. The dataset includes
3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174
digital twins. Models trained on clinical + simulated data outperform clinical
only models, achieving 10% improvement in detection, 2-9% in segmentation and
classification, and enhanced synthesis.By incorporating anatomy-informed
simulations, SYN-LUNGS provides a scalable approach for AI model development,
particularly in rare disease representation and improving model reliability.",['cs.LG'],2501.0461," Collaborative learning in peer-to-peer networks offers the benefits of
distributed learning while mitigating the risks associated with single points
of failure inherent in centralized servers. However, adversarial workers pose
potential threats by attempting to inject malicious information into the
network. Thus, ensuring the resilience of peer-to-peer learning emerges as a
pivotal research objective. The challenge is exacerbated in the presence of
non-convex loss functions and non-iid data distributions. This paper introduces
a resilient aggregation technique tailored for such scenarios, aimed at
fostering similarity among peers' learning processes. The aggregation weights
are determined through an optimization procedure, and use the loss function
computed using the neighbor's models and individual private data, thereby
addressing concerns regarding data privacy in distributed machine learning.
Theoretical analysis demonstrates convergence of parameters with non-convex
loss functions and non-iid data distributions. Empirical evaluations across
three distinct machine learning tasks support the claims. The empirical
findings, which encompass a range of diverse attack models, also demonstrate
improved accuracy when compared to existing methodologies.",['cs.LG'],False,,,,"SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital
  Twins for AI Training",Resilient Peer-to-peer Learning based on Adaptive Aggregation
neg-d2-882,2025-03-21,,2503.16912," The purpose of this paper is to introduce the construction of a stochastic
process called ``diffusion house-moving'' and to explore its properties. We
study the weak convergence of diffusion bridges conditioned to stay between two
curves, and we refer to this limit as diffusion house-moving. Applying this
weak convergence result, we give the sample path properties of diffusion
house-moving.",['math.PR'],2502.02991," We consider a generalized Derrida-Retaux model on a Galton-Watson tree with a
geometric offspring distribution. For a class of recursive systems, including
the Derrida-Retaux model with either a geometric or exponential initial
distribution, we characterize the critical curve using an involution-type
equation and prove that the free energy satisfies the Derrida-Retaux
conjecture.",['math.PR'],False,,,,"Construction and sample path properties of diffusion house-moving
  between two curves",The Derrida-Retaux model on a geometric Galton-Watson tree
neg-d2-883,2025-02-11,,2502.07314," The use of monoids in the study of word languages recognized by finite-state
automata has been quite fruitful. In this work, we look at the same idea of
""recognizability by finite monoids"" for other monoids. In particular, we
attempt to characterize recognizable subsets of various additive and
multiplicative monoids over integers, rationals, reals, and complex numbers.
While these recognizable sets satisfy properties such as closure under Boolean
operations and inverse morphisms, they do not enjoy many of the nice properties
that recognizable word languages do.",['cs.FL'],2502.07314," The use of monoids in the study of word languages recognized by finite-state
automata has been quite fruitful. In this work, we look at the same idea of
""recognizability by finite monoids"" for other monoids. In particular, we
attempt to characterize recognizable subsets of various additive and
multiplicative monoids over integers, rationals, reals, and complex numbers.
While these recognizable sets satisfy properties such as closure under Boolean
operations and inverse morphisms, they do not enjoy many of the nice properties
that recognizable word languages do.",['cs.FL'],False,,,,Recognizing Numbers,Recognizing Numbers
neg-d2-884,2025-01-02,,2501.01137," Since the geometry structure of ultra-high-pressure (UHP) water-jet nozzle is
a critical factor to enhance its hydrodynamic performance, it is critical to
obtain a suitable geometry for a UHP water jet nozzle. In this study, a
CFD-based optimization loop for UHP nozzle structure has been developed by
integrating an approximate model to optimize nozzle structure for increasing
the radial peak wall shear stress. In order to improve the optimization
accuracy of the sparrow search algorithm (SSA), an enhanced version called the
Logistic-Tent chaotic sparrow search algorithm (LTC-SSA) is proposed. The
LTC-SSA algorithm utilizes the Logistic-Tent Chaotic (LTC) map, which is
designed by combining the Logistic and Tent maps. This new approach aims to
overcome the shortcoming of ""premature convergence"" for the SSA algorithm by
increasing the diversity of the sparrow population. In addition, to improve the
prediction accuracy of peak wall shear stress, a data prediction method based
on LTC-SSA-support vector machine (SVM) is proposed. Herein, LTC-SSA algorithm
is used to train the penalty coefficient C and parameter gamma g of SVM model.
In order to build LTC-SSA-SVM model, optimal Latin hypercube design (Opt LHD)
is used to design the sampling nozzle structures, and the peak wall shear
stress (objective function) of these nozzle structures are calculated by CFD
method. For the purpose of this article, this optimization framework has been
employed to optimize original nozzle structure. The results show that the
optimization framework developed in this study can be used to optimize nozzle
structure with significantly improved its hydrodynamic performance.",['physics.flu-dyn'],2503.09359," We explore the mechanisms and regimes of mixing in yield-stress fluids by
simulating the stirring of an infinite, two-dimensional domain filled with a
Bingham fluid. A cylindrical stirrer moves along a circular path at constant
speed to stir the fluid, with an initially quiescent domain marked by a passive
dye in the lower half, facilitating the analysis of dye interface evolution and
mixing dynamics. We first examine the mixing process in Newtonian fluids,
identifying three key mechanisms: interface stretching and folding around the
stirrer's path, diffusion across streamlines, and dye advection and interface
stretching due to vortex shedding. Introducing yield stress into the system
leads to notable localization effects in mixing, manifesting through three
mechanisms: advection of vortices within a finite distance of the stirrer,
vortex entrapment near the stirrer, and complete suppression of vortex shedding
at high yield stresses. Based on these mechanisms, we classify three distinct
mixing regimes in yield-stress fluids: (i) Regime SE, where shed vortices
escape the central region, (ii) Regime ST, where shed vortices remain trapped
near the stirrer, and (iii) Regime NS, where no vortex shedding occurs. These
regimes are quantitatively distinguished through spectral analysis of energy
oscillations, revealing transitions and the critical Bingham and Reynolds
numbers. The transitions are captured through effective Reynolds numbers,
supporting a hypothesis that mixing regime transitions in yield-stress fluids
share fundamental characteristics with bluff-body flow dynamics. The findings
provide a mechanistic framework for understanding and predicting mixing
behaviors in yield-stress fluids, suggesting that the localization mechanisms
and mixing regimes observed here are archetypal for stirred-tank applications.",['physics.flu-dyn'],False,,,,"Computational fluid dynamics-based structure optimization of
  ultra-high-pressure water-jet nozzle using approximation method","Yield-Stress Fluid Mixing: Localization Mechanisms and Regime
  Transitions"
neg-d2-885,2025-01-22,,2501.12684," We have been developing silicon-on-insulator (SOI) pixel detectors with a
pinned depleted diode (PDD) structure, named ""XRPIX"", for X-ray astronomy. In
our previous study, we successfully optimized the design of the PDD structure,
achieving both the suppression of large leakage current and satisfactory X-ray
spectroscopic performance. Here, we report a detailed study on the X-ray
spectroscopic performance of the XRPIX with the optimized PDD structure. The
data were obtained at $-60^\circ\mathrm{C}$ with the ""event-driven readout
mode"", in which only a triggering pixel and its surroundings are read out. The
energy resolutions in full width at half maximum at 6.4 keV are $178\pm1$ eV
and $291\pm1$ eV for single-pixel and all-pixel event spectra, respectively.
The all-pixel events include charge-sharing pixel events as well as the
single-pixel events. These values are the best achieved in the history of our
development. We argue that the gain non-linearity in the low energy side due to
excessive charge injection to the charge-sensitive amplifier is a major factor
to limit the current spectroscopic performance. Optimization of the amount of
the charge injection is expected to lead to further improvement in the
spectroscopic performance of XRPIX, especially for the all-pixel event
spectrum.",['astro-ph.IM'],2501.09748," In recent years, numerical simulations have become indispensable for
addressing complex astrophysical problems. The MagnetoHydroDynamics (MHD)
framework represents a key tool for investigating the dynamical evolution of
astrophysical plasmas, which are described as a set of partial differential
equations that enforce the conservation of mass, momentum, and energy, along
with Maxwell's equation for the evolution of the electromagnetic fields. Due to
the high nonlinearity of the MHD equations (regardless of their specifications,
e.g., classical/relativistic or ideal/resistive), a general analytical solution
is precluded, making the numerical approach crucial. Numerical simulations
usually end up producing large sets of data files and their scientific analysis
leans on dedicated software designed for data visualization. However, in order
to encompass all of the code output features, specialized tools focusing on the
numerical code may represent a more versatile and built-in tool. Here, we
present PyPLUTO, a Python package tailored for efficient loading, manipulation,
and visualization of outputs produced with the PLUTO code (Mignone et al.,
2007; Mignone et al., 2012). PyPLUTO uses memory mapping to optimize data
loading and provides general routines for data manipulation and visualization.
PyPLUTO also supports the particle modules of the PLUTO code, enabling users to
load and visualize particles, such as cosmic rays (Mignone et al., 2018),
Lagrangian (Vaidya et al., 2018), or dust (Mignone et al., 2019) particles,
from hybrid simulations. A dedicated Graphical User Interface (GUI) simplifies
the generation of single-subplot figures, making PyPLUTO a powerful yet
user-friendly toolkit for astrophysical data analysis.",['astro-ph.IM'],False,,,,"A detailed study on spectroscopic performance of SOI pixel detector with
  a pinned depleted diode structure for X-ray astronomy",PyPLUTO: a data analysis Python package for the PLUTO code
neg-d2-886,2025-02-11,,2502.07302," Multi-class cell segmentation in high-resolution gigapixel whole slide images
(WSIs) is crucial for various clinical applications. However, training such
models typically requires labor-intensive, pixel-wise annotations by domain
experts. Recent efforts have democratized this process by involving lay
annotators without medical expertise. However, conventional non-corrective
approaches struggle to handle annotation noise adaptively because they lack
mechanisms to mitigate false positives (FP) and false negatives (FN) at both
the image-feature and pixel levels. In this paper, we propose a consensus-aware
self-corrective AI agent that leverages the Consensus Matrix to guide its
learning process. The Consensus Matrix defines regions where both the AI and
annotators agree on cell and non-cell annotations, which are prioritized with
stronger supervision. Conversely, areas of disagreement are adaptively weighted
based on their feature similarity to high-confidence consensus regions, with
more similar regions receiving greater attention. Additionally, contrastive
learning is employed to separate features of noisy regions from those of
reliable consensus regions by maximizing their dissimilarity. This paradigm
enables the model to iteratively refine noisy labels, enhancing its robustness.
Validated on one real-world lay-annotated cell dataset and two reasoning-guided
simulated noisy datasets, our method demonstrates improved segmentation
performance, effectively correcting FP and FN errors and showcasing its
potential for training robust models on noisy datasets. The official
implementation and cell annotations are publicly available at
https://github.com/ddrrnn123/CASC-AI.",['cs.CV'],2501.15201," Training semantic segmenter with synthetic data has been attracting great
attention due to its easy accessibility and huge quantities. Most previous
methods focused on producing large-scale synthetic image-annotation samples and
then training the segmenter with all of them. However, such a solution remains
a main challenge in that the poor-quality samples are unavoidable, and using
them to train the model will damage the training process. In this paper, we
propose a training-free Synthetic Data Selection (SDS) strategy with CLIP to
select high-quality samples for building a reliable synthetic dataset.
Specifically, given massive synthetic image-annotation pairs, we first design a
Perturbation-based CLIP Similarity (PCS) to measure the reliability of
synthetic image, thus removing samples with low-quality images. Then we propose
a class-balance Annotation Similarity Filter (ASF) by comparing the synthetic
annotation with the response of CLIP to remove the samples related to
low-quality annotations. The experimental results show that using our method
significantly reduces the data size by half, while the trained segmenter
achieves higher performance. The code is released at
https://github.com/tanghao2000/SDS.",['cs.CV'],False,,,,"CASC-AI: Consensus-aware Self-corrective Learning for Noise Cell
  Segmentation","A Training-free Synthetic Data Selection Method for Semantic
  Segmentation"
neg-d2-887,2025-03-04,,2503.03162," In this study, we investigate the features of a charged gravastar within the
framework of $f(\mathbb{Q})$ gravity ($\mathbb{Q}$ represents non-metricity)
using the Finch-Skea metric. This metric is applied to both the interior and
shell regions of the charged gravastar and the field equations are derived
accordingly. For the exterior regions, we consider various black holes, i.e.,
Reissner-Nordstr$\ddot{o}$m, Bardeen and Hayward regular black holes. The
interior and exterior layers are matched using the Israel junction conditions,
which help to determine the surface energy density and surface pressure for
these black holes. We examine some physical properties such as proper length,
entropy, energy and the equation of state parameter. The stability of the
developed gravastar model is discussed through the effective potential, the
causality condition and adiabatic index. We conclude that the compact gravastar
structure could be a viable alternative to black holes within this framework.",['gr-qc'],2503.03162," In this study, we investigate the features of a charged gravastar within the
framework of $f(\mathbb{Q})$ gravity ($\mathbb{Q}$ represents non-metricity)
using the Finch-Skea metric. This metric is applied to both the interior and
shell regions of the charged gravastar and the field equations are derived
accordingly. For the exterior regions, we consider various black holes, i.e.,
Reissner-Nordstr$\ddot{o}$m, Bardeen and Hayward regular black holes. The
interior and exterior layers are matched using the Israel junction conditions,
which help to determine the surface energy density and surface pressure for
these black holes. We examine some physical properties such as proper length,
entropy, energy and the equation of state parameter. The stability of the
developed gravastar model is discussed through the effective potential, the
causality condition and adiabatic index. We conclude that the compact gravastar
structure could be a viable alternative to black holes within this framework.",['gr-qc'],False,,,,"Charged Gravastar Solutions in the Finch-Skea Framework with
  f(\mathbb{Q}) Gravity","Charged Gravastar Solutions in the Finch-Skea Framework with
  f(\mathbb{Q}) Gravity"
neg-d2-888,2025-02-25,,2502.18767," Ptychography is a computational imaging technique that achieves high spatial
resolution over large fields of view. It involves scanning a coherent beam
across overlapping regions and recording diffraction patterns. Conventional
reconstruction algorithms require substantial overlap, increasing data volume
and experimental time. We propose a reconstruction method employing a
physics-guided score-based diffusion model. Our approach trains a diffusion
model on representative object images to learn an object distribution prior.
During reconstruction, we modify the reverse diffusion process to enforce data
consistency, guiding reverse diffusion toward a physically plausible solution.
This method requires a single pretraining phase, allowing it to generalize
across varying scan overlap ratios and positions. Our results demonstrate that
the proposed method achieves high-fidelity reconstructions with only a 20%
overlap, while the widely employed rPIE method requires a 62% overlap to
achieve similar accuracy. This represents a significant reduction in data
requirements, offering an alternative to conventional techniques.",['eess.IV'],2503.0345," The medial axis transform is a well-known tool for shape recognition. Instead
of the object contour, it equivalently describes a binary object in terms of a
skeleton containing all centres of maximal inscribed discs. While this shape
descriptor is useful for many applications, it is also sensitive to noise:
Small boundary perturbations can result in large unwanted expansions of the
skeleton. Pruning offers a remedy by removing unwanted skeleton parts. In our
contribution, we generalise this principle to skeleton sparsification: We show
that subsequently removing parts of the skeleton simplifies the associated
shape in a hierarchical manner that obeys scale-space properties.
  To this end, we provide both a continuous and discrete theory that
incorporates architectural and simplification statements as well as
invariances. We illustrate how our skeletonisation scale-spaces can be employed
for practical applications with two proof-of-concept implementations for
pruning and compression.",['eess.IV'],False,,,,"Ptychographic Image Reconstruction from Limited Data via Score-Based
  Diffusion Models with Physics-Guidance",Skeletonisation Scale-Spaces
neg-d2-889,2025-01-06,,2501.03423," Blockchain technology has revolutionized industries by enabling secure and
decentralized transactions. However, the isolated nature of blockchain
ecosystems hinders the seamless transfer of digital assets across different
chains. Cross-chain bridges have emerged as vital web3 infrastructure to
address this challenge by facilitating interoperability between distinct
blockchains. Cross-chain bridges remain vulnerable to various attacks despite
sophisticated designs and security measures. The industry has experienced a
surge in bridge attacks, resulting in significant financial losses. The largest
hack impacted Axie Infinity Ronin Bridge, with a loss of almost \$600 million
USD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and
examines the exploited vulnerabilities. By understanding the attack nature and
underlying weaknesses, the paper aims to enhance bridge security and propose
potential countermeasures. The findings contribute to developing industry-wide
standards for bridge security and operational resilience. Addressing the
vulnerabilities and weaknesses exploited in recent cross-chain bridge hacks
fosters trust and confidence in cross-chain interoperability.",['cs.CR'],2501.12034," Like most computer systems, a manycore can also be the target of security
attacks. It is essential to ensure the security of the NoC since all
information travels through its channels, and any interference in the traffic
of messages can reflect on the entire chip, causing communication problems.
Among the possible attacks on NoC, Denial of Service (DoS) attacks are the most
cited in the literature. The state of the art shows a lack of work that can
detect such attacks through learning techniques. On the other hand, these
techniques are widely explored in computer network security via an Intrusion
Detection System (IDS). In this context, the main goal of this document is to
present the progress of a work that explores an IDS technique using machine
learning and temporal series for detecting DoS attacks in NoC-based manycore
systems. To fulfill this goal, it is necessary to extract traffic data from a
manycore NoC and execute the learning techniques in the extracted data.
However, while low-level platforms offer precision and slow execution,
high-level platforms offer higher speed and data incompatible with reality.
Therefore, a platform is being developed using the OVP tool, which has a higher
level of abstraction. To solve the low precision problem, the developed
platform will have its data validated with a low-level platform.",['cs.CR'],False,,,,SoK: A Review of Cross-Chain Bridge Hacks in 2023,"Application of Machine Learning Techniques for Secure Traffic in
  NoC-based Manycores"
neg-d2-890,2025-02-04,,2502.02392," Synthetic datasets are widely used in many applications, such as missing data
imputation, examining non-stationary scenarios, in simulations, training
data-driven models, and analyzing system robustness. Typically, synthetic data
are based on historical data obtained from the observed system. The data needs
to represent a specific behavior of the system, yet be new and diverse enough
so that the system is challenged with a broad range of inputs. This paper
presents a method, based on discrete Fourier transform, for generating
synthetic time series with similar statistical moments for any given signal.
The suggested method makes it possible to control the level of similarity
between the given signal and the generated synthetic signals. Proof shows
analytically that this method preserves the first two statistical moments of
the input signal, and its autocorrelation function. The method is compared to
known methods, ARMA, GAN, and CoSMoS. A large variety of environmental datasets
with different temporal resolutions, and from different domains are used,
testing the generality and flexibility of the method. A Python library
implementing this method is made available as open-source software.",['stat.ME'],2501.13173," Gaussian Process Regression (GPR) is a powerful tool for nonparametric
regression, but its fully Bayesian application in high-dimensional settings is
hindered by two primary challenges: the computational burden (exacerbated by
fully Bayesian inference) and the difficulty of variable selection. This paper
introduces a novel methodology that combines hierarchical global-local
shrinkage priors with normalizing flows to address these challenges. The
hierarchical triple gamma prior offers a principled framework for inducing
sparsity in high-dimensional GPR, effectively excluding irrelevant covariates
while preserving interpretability and flexibility in model size. Normalizing
flows are employed within a variational inference framework to approximate the
posterior distribution of hyperparameters, capturing complex dependencies while
ensuring computational scalability. Simulation studies demonstrate the efficacy
of the proposed approach, outperforming traditional maximum likelihood
estimation and mean-field variational methods, particularly in high-sparsity
and high-dimensional settings. The results highlight the robustness and
flexibility of hierarchical shrinkage priors and the computational efficiency
of normalizing flows for Bayesian GPR. This work provides a scalable and
interpretable solution for high-dimensional regression, with implications for
sparse modeling and posterior approximation in broader Bayesian contexts.",['stat.ME'],False,,,,"Synthetic Random Environmental Time Series Generation with Similarity
  Control, Preserving Original Signal's Statistical Characteristics","Normalizing Flows for Gaussian Process Regression under Hierarchical
  Shrinkage Priors"
neg-d2-891,2025-02-08,,2502.05595," Pick-and-place (PnP) operations, featuring object grasping and trajectory
planning, are fundamental in industrial robotics applications. Despite many
advancements in the field, PnP is limited by workspace constraints, reducing
flexibility. Pick-and-throw (PnT) is a promising alternative where the robot
throws objects to target locations, leveraging extrinsic resources like gravity
to improve efficiency and expand the workspace. However, PnT execution is
complex, requiring precise coordination of high-speed movements and object
dynamics. Solutions to the PnT problem are categorized into analytical and
learning-based approaches. Analytical methods focus on system modeling and
trajectory generation but are time-consuming and offer limited generalization.
Learning-based solutions, in particular Model-Free Reinforcement Learning
(MFRL), offer automation and adaptability but require extensive interaction
time. This paper introduces a Model-Based Reinforcement Learning (MBRL)
framework, MC-PILOT, which combines data-driven modeling with policy
optimization for efficient and accurate PnT tasks. MC-PILOT accounts for model
uncertainties and release errors, demonstrating superior performance in
simulations and real-world tests with a Franka Emika Panda manipulator. The
proposed approach generalizes rapidly to new targets, offering advantages over
analytical and Model-Free methods.",['cs.RO'],2502.0738," Simulation has been pivotal in recent robotics milestones and is poised to
play a prominent role in the field's future. However, recent robotic advances
often rely on expensive and high-maintenance platforms, limiting access to
broader robotics audiences. This work introduces Wheeled Lab, a framework for
the low-cost, open-source wheeled platforms that are already widely established
in education and research. Through integration with Isaac Lab, Wheeled Lab
introduces modern techniques in Sim2Real, such as domain randomization, sensor
simulation, and end-to-end learning, to new user communities. To kickstart
education and demonstrate the framework's capabilities, we develop three
state-of-the-art policies for small-scale RC cars: controlled drifting,
elevation traversal, and visual navigation, each trained in simulation and
deployed in the real world. By bridging the gap between advanced Sim2Real
methods and affordable, available robotics, Wheeled Lab aims to democratize
access to cutting-edge tools, fostering innovation and education in a broader
robotics context. The full stack, from hardware to software, is low cost and
open-source.",['cs.RO'],False,,,,"Data efficient Robotic Object Throwing with Model-Based Reinforcement
  Learning","Demonstrating Wheeled Lab: Modern Sim2Real for Low-cost, Open-source
  Wheeled Robotics"
neg-d2-892,2025-02-04,,2502.0256," The automorphism group of a transitive graph defines a weight function on the
vertices through the Haar modulus. Benjamini, Lyons, Peres, and Schramm
introduced the notion of weighted-amenability for a transitive graph, which is
equivalent to the amenability of its automorphism group. We prove that this
property is equivalent to level-amenability, that is, the property that the
collection of vertices of weights in a given finite set always induces an
amenable graph. We then use this to prove a version of Hutchcroft's conjecture
about $p_h<p_u$, relaxed \`a la Pak-Smirnova-Nagnibeda, where $p_h$ is the
critical probability for the regime where clusters of infinite total weight
arise, and $p_u$ is the uniqueness threshold. Further characterizations are
given in terms of the spectral radius and invariant spanning forests. One
consequence is the continuity of the phase transition at $p_h$ for
weighted-nonamenable graphs.",['math.PR'],2501.09543," We introduce and study a multiparameter Poisson process (MPP). In a
particular case, it is observed that the MPP has a unique representation. Its
subordination with the multivariate subordinator and inverse subordinator are
studied in detail. Also, we consider a multivariate multiparameter Poisson
process and establish its connection with the MPP. An integral of the MPP is
defined, and its asymptotic distribution is obtained. Later, we study some
properties of the multiparameter martingales. Moreover, the multiparameter
martingale characterizations for the MPP and its subordinated variants are
derived.",['math.PR'],False,,,,Weighted-amenability and percolation,Multiparameter Poisson Processes and Martingales
neg-d2-893,2025-03-11,,2503.08837," We study a system of reflecting Brownian motions on the positive half-line in
which each particle has a drift toward the origin determined by the local times
at the origin of all the particles. We show that if this local time drift is
too strong, such systems can exhibit a breakdown in their solutions in that
there is a time beyond which the system cannot be extended. In the finite
particle case we give a complete characterization of the finite time breakdown,
relying on a novel dynamic graph structure. We consider the mean-field limit of
the system in the symmetric case and show that there is a McKean--Vlasov
representation. If the drift is too strong, the solution to the corresponding
Fokker--Planck equation has a blow up in its solution. We also establish the
existence of stationary and self-similar solutions to the McKean--Vlasov
equation in the case where there is no breakdown of the system. This work is
motivated by models for liquidity in financial markets, the supercooled Stefan
problem, and a toy model for cell polarization.",['math.PR'],2502.02991," We consider a generalized Derrida-Retaux model on a Galton-Watson tree with a
geometric offspring distribution. For a class of recursive systems, including
the Derrida-Retaux model with either a geometric or exponential initial
distribution, we characterize the critical curve using an involution-type
equation and prove that the free energy satisfies the Derrida-Retaux
conjecture.",['math.PR'],False,,,,"Particle Systems and McKean--Vlasov Dynamics with Singular Interaction
  through Local Times",The Derrida-Retaux model on a geometric Galton-Watson tree
neg-d2-894,2025-01-09,,2501.05164," Superconductors, which are crucial for modern advanced technologies due to
their zero-resistance properties, are limited by low Tc and the difficulty of
accurate prediction. This article made the initial endeavor to apply machine
learning to predict the critical temperature (Tc) of liquid metal (LM) alloy
superconductors. Leveraging the SuperCon dataset, which includes extensive
superconductor property data, we developed a machine learning model to predict
Tc. After addressing data issues through preprocessing, we compared multiple
models and found that the Extra Trees model outperformed others with an R2 of
0.9519 and an RMSE of 6.2624 K. This model is subsequently used to predict Tc
for LM alloys, revealing In0.5Sn0.5 as having the highest Tc at 7.01 K.
Furthermore, we extended the prediction to 2,145 alloys binary and 45,670
ternary alloys across 66 metal elements and promising results were achieved.
This work demonstrates the advantages of tree-based models in predicting Tc and
would help accelerate the discovery of high-performance LM alloy
superconductors in the coming time.",['cond-mat.supr-con'],2502.15373," Intensive research has revealed intriguing optical responses in topological
materials. This paper focuses on the optical responses in $s$-wave
superconductors with a Rashba spin-orbit coupling and a magnetic field, one of
the platforms of topological superconductivity. On the one hand, to satisfy
some conservation laws in superconducting responses, it is essential to take
into account collective excitation modes. On the other hand, the optical
response is a promising phenomenon for detecting hidden collective modes in
superconductors. In this paper, we investigate the effect of collective
excitation modes on the linear and second-order optical responses based on the
self-consistent response approximation, which is formulated using the
Kadanoff-Baym method. Our main results reveal that the Higgs mode enhances the
optical responses when the Fermi level is close to the Dirac point. The
enhancement is due to the multiband effects characterized by interband pairing.
We also demonstrate the sign reversal of the photocurrent conductivity around
the topological transition with increasing the Zeeman field. This finding
supports the prediction in our previous work without considering collective
excitation modes [H. Tanaka, et al., Phys. Rev. B 110, 014520 (2024)]. The sign
reversal phenomenon is attributed to the magnetic injection current modified by
the Higgs mode, and is proposed for a bulk probe of topological
superconductors. We also discuss the interplay of quantum geometry and
collective modes.",['cond-mat.supr-con'],False,,,,"Tree Models Machine Learning to Identify Liquid Metal based Alloy
  Superconductor","Vertex correction for the linear and nonlinear optical responses in
  superconductors: multiband effect and topological superconductivity"
neg-d2-895,2025-01-16,,2501.09951," In light of the diminishing presence of physical third places -- informal
gathering spaces essential for social connection -- this study explores how the
social media platform Discord fosters third-place experiences. Drawing on
Oldenburg's conceptual framework, we analyze how Discord's design elements
support the creation of virtual third places that foster both dyadic and
community-based relationships. Through 25 semi-structured interviews with
active Discord users, we identified 21 design elements aligned with Oldenburg's
third-place characteristics. These elements cluster around four core
principles: providing themed spaces for repeated interactions, supporting user
autonomy and customization, facilitating mutually engaging activities, and
enabling casual, low-pressure interactions. This work contributes to
understanding how intentional platform design can cultivate virtual spaces that
support meaningful social connections. The findings have implications for
designing future social technologies that can help address growing concerns
about social isolation in an increasingly digital world.",['cs.HC'],2502.05324," The prevailing methodologies for visualizing AI risks have focused on
technical issues such as data biases and model inaccuracies, often overlooking
broader societal risks like job loss and surveillance. Moreover, these
visualizations are typically designed for tech-savvy individuals, neglecting
those with limited technical skills. To address these challenges, we propose
the Atlas of AI Risks-a narrative-style tool designed to map the broad risks
associated with various AI technologies in a way that is understandable to
non-technical individuals as well. To both develop and evaluate this tool, we
conducted two crowdsourcing studies. The first, involving 40 participants,
identified the design requirements for visualizing AI risks for decision-making
and guided the development of the Atlas. The second study, with 140
participants reflecting the US population in terms of age, sex, and ethnicity,
assessed the usability and aesthetics of the Atlas to ensure it met those
requirements. Using facial recognition technology as a case study, we found
that the Atlas is more user-friendly than a baseline visualization, with a more
classic and expressive aesthetic, and is more effective in presenting a
balanced assessment of the risks and benefits of facial recognition. Finally,
we discuss how our design choices make the Atlas adaptable for broader use,
allowing it to generalize across the diverse range of technology applications
represented in a database that reports various AI incidents.",['cs.HC'],False,,,,"Discord's Design Encourages ""Third Place"" Social Media Experiences",Atlas of AI Risks: Enhancing Public Understanding of AI Risks
neg-d2-896,2025-03-18,,2503.14037," We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that
explore useful features from visual foundation models for image restoration.
Specifically, PPTformer contains two parts: an Image Restoration Network
(IRNet) for restoring images from degraded observations and a Parser-Prompted
Feature Generation Network (PPFGNet) for providing IRNet with reliable parser
information to boost restoration. To enhance the integration of the parser
within IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter
Parser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful
parser features to facilitate restoration. The IntraPPA re-considers cross
attention between parser and restoration features, enabling implicit perception
of the parser from a long-range and intra-layer perspective. Conversely, the
InterPPA initially fuses restoration features with those of the parser,
followed by formulating these fused features within an attention mechanism to
explicitly perceive parser information. Further, we propose a parser-prompted
feed-forward network to guide restoration within pixel-wise gating modulation.
Experimental results show that PPTformer achieves state-of-the-art performance
on image deraining, defocus deblurring, desnowing, and low-light enhancement.",['cs.CV'],2503.14037," We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that
explore useful features from visual foundation models for image restoration.
Specifically, PPTformer contains two parts: an Image Restoration Network
(IRNet) for restoring images from degraded observations and a Parser-Prompted
Feature Generation Network (PPFGNet) for providing IRNet with reliable parser
information to boost restoration. To enhance the integration of the parser
within IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter
Parser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful
parser features to facilitate restoration. The IntraPPA re-considers cross
attention between parser and restoration features, enabling implicit perception
of the parser from a long-range and intra-layer perspective. Conversely, the
InterPPA initially fuses restoration features with those of the parser,
followed by formulating these fused features within an attention mechanism to
explicitly perceive parser information. Further, we propose a parser-prompted
feed-forward network to guide restoration within pixel-wise gating modulation.
Experimental results show that PPTformer achieves state-of-the-art performance
on image deraining, defocus deblurring, desnowing, and low-light enhancement.",['cs.CV'],False,,,,"Intra and Inter Parser-Prompted Transformers for Effective Image
  Restoration","Intra and Inter Parser-Prompted Transformers for Effective Image
  Restoration"
neg-d2-897,2025-03-19,,2503.14874," Dissipative light-matter coupling plays a vital role in non-Hermitian
physics, but it remains largely unexplored in waveguide QED systems. In this
work, we find that by employing pseudo-Hermitian symmetry rather than anti-PT
symmetry, the concept of dissipative coupling could be generalized and applied
to the field of waveguide QED. This leads to a series of intriguing results,
such as spontaneous breaking of pseudo-Hermitian symmetry across the
exceptional points (EPs), level attraction between the bound states, and
critical transition across the EPs for the population of quantum emitters in
the bound state. Thanks to the tunability of photonic bands in crystal
waveguides, we also demonstrate that dissipative light-matter coupling leads to
the emergence of nonstandard third-order exceptional points with chiral spatial
profiles in a topological waveguide QED system. This work provides a promising
paradigm for studying non-Hermitian quantum phenomena in waveguide QED systems.",['quant-ph'],2502.12429," One-way quantum computation is a promising approach to achieving universal,
scalable, and fault-tolerant quantum computation. However, a main challenge
lies in the creation of universal, scalable three-dimensional cluster states.
Here, an experimental scheme is proposed for building large-scale canonical
three-dimensional cubic cluster states, which are compatible with the majority
of qubit error-correcting codes, using the spatiospectral modes of an optical
parametric oscillator. Combining with Gottesman-Kitaev-Preskill states, one-way
fault-tolerant optical quantum computation can be achieved with a lower
fault-tolerant squeezing threshold. Our scheme drastically simplify
experimental configurations, paving the way for compact realizations of one-way
fault-tolerant optical quantum computation.",['quant-ph'],False,,,,Waveguide QED with dissipative light-matter couplings,A Compact One-Way Fault-Tolerant Optical Quantum Computation
neg-d2-898,2025-01-16,,2501.09812," The Benguela Upwelling System (BUS), off the south-western African coast, is
one of the four major eastern boundary upwelling ecosystems in the oceans.
However, despite its very interesting characteristics, this area has been
almost overlooked in the field of environmental radioactivity. In this work, it
has been carried out for the first time the combined study of $^{236}$U and
$^{237}$Np in the coast of Namibia within the northern BUS. Surface seawater
exhibited similar $^{236}$U and $^{237}$Np concentrations, ranging from
$3.9\times10^6$ to $5.6\times10^6$ atoms/kg and from $4.6\times10^6$ to
$8.5\times10^6$ atoms/kg, respectively. The observed inventories in a water
column from the continental margin, of $(2.10 \pm 0.11)\times10^12$ atoms
m$^{-2}$ for $^{236}$U and $(3.48 \pm 0.13)\times10^{12}$ atoms/m$^{-2}$ for
$^{237}$Np, were in agreement with the global fallout (GF) source term in the
Southern Hemisphere, which was recognized as the main source of actinides to
the region. A pattern was observed in the surface samples, with $^{237}$Np
concentrations that decreased by 25-30% when moving from inshore to offshore
stations, but such an effect could not be clearly discerned in the case of
$^{236}$U within the data uncertainties. An explanation based on the larger
particle reactivity of GF $^{237}$Np compared to GF $^{236}$U was proposed.
Such an effect would have been important at the studied site due to the enhance
presence of particles in the continental shelf triggered by the upwelling
phenomenon. A value of $1.77 \pm 0.20$ was obtained for the
$^{237}$Np/$^{236}$U atom ratio for the GF source term in the marine
environment.",['physics.app-ph'],2501.09812," The Benguela Upwelling System (BUS), off the south-western African coast, is
one of the four major eastern boundary upwelling ecosystems in the oceans.
However, despite its very interesting characteristics, this area has been
almost overlooked in the field of environmental radioactivity. In this work, it
has been carried out for the first time the combined study of $^{236}$U and
$^{237}$Np in the coast of Namibia within the northern BUS. Surface seawater
exhibited similar $^{236}$U and $^{237}$Np concentrations, ranging from
$3.9\times10^6$ to $5.6\times10^6$ atoms/kg and from $4.6\times10^6$ to
$8.5\times10^6$ atoms/kg, respectively. The observed inventories in a water
column from the continental margin, of $(2.10 \pm 0.11)\times10^12$ atoms
m$^{-2}$ for $^{236}$U and $(3.48 \pm 0.13)\times10^{12}$ atoms/m$^{-2}$ for
$^{237}$Np, were in agreement with the global fallout (GF) source term in the
Southern Hemisphere, which was recognized as the main source of actinides to
the region. A pattern was observed in the surface samples, with $^{237}$Np
concentrations that decreased by 25-30% when moving from inshore to offshore
stations, but such an effect could not be clearly discerned in the case of
$^{236}$U within the data uncertainties. An explanation based on the larger
particle reactivity of GF $^{237}$Np compared to GF $^{236}$U was proposed.
Such an effect would have been important at the studied site due to the enhance
presence of particles in the continental shelf triggered by the upwelling
phenomenon. A value of $1.77 \pm 0.20$ was obtained for the
$^{237}$Np/$^{236}$U atom ratio for the GF source term in the marine
environment.",['physics.app-ph'],False,,,,"Presence of $^{236}$U and $^{237}$Np in a marine ecosystem: the northern
  Benguela Upwelling System, a case study","Presence of $^{236}$U and $^{237}$Np in a marine ecosystem: the northern
  Benguela Upwelling System, a case study"
neg-d2-899,2025-02-06,,2502.04425," We present a quantum solver for partial differential equations based on a
flexible matrix product operator representation. Utilizing mid-circuit
measurements and a state-dependent norm correction, this scheme overcomes the
restriction of unitary operators. Hence, it allows for the direct
implementation of a broad class of differential equations governing the
dynamics of classical and quantum systems. The capabilities of the framework
are demonstrated for an example system governed by Euler equations with
absorbing boundaries.",['quant-ph'],2502.2071," One of the predominant causes of program distortion in the real quantum
computing system may be attributed to the probability deviation caused by
thermal relaxation. We introduce Barber (Balancing reAdout Results using
Bit-invErted ciRcuits), a method designed to counteract the asymmetric thermal
relaxation deviation and improve the reliability of near-term quantum programs.
Barber collaborates with a bit-inverted quantum circuit, where the excited
quantum state of qubits is assigned to the $\lvert 0 \rangle$ and the unexcited
state to the $\lvert 1 \rangle$. In doing so, bit-inverted quantum circuits can
experience thermal relaxation in the opposite direction compared to standard
quantum circuits. Barber can effectively suppress the thermal relaxation
deviation in program's readout results by selectively merging distributions
from the standard and bit-inverted circuits.",['quant-ph'],False,,,,Tensor-Programmable Quantum Circuits for Solving Differential Equations,"Balancing Thermal Relaxation Deviations of Near-Future Quantum Computing
  Results via Bit-Inverted Programs"
neg-d2-900,2025-02-07,,2502.04936," We study the problem of controlling the initial condition of a vibrating
beam. The optimal control problem seeks to determine solutions of initial
velocity that assure the approach of the state of the beam to a given target
function in the $L^2-$norm. We prove both the existence and uniqueness of the
optimal solution. Employing identities based on the adjoint and difference
problems, we determine the Fr\'echet derivative of the cost functional. We
further derive the necessary optimality conditions of this control problem.
Finally, we provide a sketch of a gradient-based algorithm, that rests on the
explicit formula of the gradient of the cost functional, to obtain numerical
solutions.",['math.OC'],2503.18164," We compute the closest convex piecewise linear-quadratic (PLQ) function with
minimal number of pieces to a given univariate piecewise linear-quadratic
function. The Euclidean norm is used to measure the distance between functions.
First, we assume that the number and positions of the breakpoints of the output
function are fixed, and solve a convex optimization problem. Next, we assume
the number of breakpoints is fixed, but not their position, and solve a
nonconvex optimization problem to determine optimal breakpoints placement.
Finally, we propose an algorithm composed of a greedy search preprocessing and
a dichotomic search that solves a logarithmic number of optimization problems
to obtain an approximation of any PLQ function with minimal number of pieces
thereby obtaining in two steps the closest convex function with minimal number
of pieces.
  We illustrate our algorithms with multiple examples, compare our approach
with a previous globally optimal univariate spline approximation algorithm, and
apply our method to simplify vertical alignment curves in road design
optimization. CPLEX, Gurobi, and BARON are used with the YALMIP library in
MATLAB to effectively select the most efficient solver.",['math.OC'],False,,,,"On the optimal control of initial velocity in a hyperbolic beam equation
  by the variational method","Closest univariate convex linear-quadratic function approximation with
  minimal number of Pieces"
neg-d2-901,2025-03-07,,2503.05452," In this study, we explore the ferroelectric domain structure and mechanical
properties of PbTiO$_3$-based membranes, which develops a well-ordered and
crystallographic-oriented ripple pattern upon release from their growth
substrate. The ferrolectric domain structure of the PbTiO$_3$ layer was
examined at various length scales using optical second harmonic generation,
piezoresponse force microscopy, and scanning transmission electron microscopy.
These methods reveal the presence of purely in-plane domains organized into
superdomains at the crest of the ripples, while an in-plane/out-of-plane domain
structure was observed in the flat regions separating the ripples, in agreement
with phase-field simulations. The mechanical properties of the membrane were
assessed using contact resonance force microscopy, which identified distinct
mechanical behaviors at the ripples compared to the flat regions. This study
shows that the physical properties of the ferroelectric layer in membranes can
be locally controlled within an ordered array of ripples, with well-defined
geometric characteristics.",['cond-mat.mtrl-sci'],2501.15334," Palladium diselenide (PdSe$_2$) -- a layered van der Waals material -- is
attracting significant attention for optoelectronics due to the wide tunability
of its band gap from the infrared through the visible range as a function of
the number of layers. However, there continues to be disagreement over the
precise nature and value of the optical band gap of bulk PdSe$_2$, owing to the
rather small value of this gap that complicates experimental measurements and
their interpretation. Here, we design and employ a Wannier-localized
optimally-tuned screened range-separated hybrid (WOT-SRSH) functional to
investigate the electronic bandstructures and optical absorption spectra of
bulk and monolayer PdSe$_2$. In particular, we account carefully for the finite
exciton center-of-mass momentum within a time-dependent WOT-SRSH framework to
calculate the \emph{indirect} optical gap and absorption onset accurately. Our
results agree well with the best available photoconductivity measurements, as
well as with state-of-the-art many-body perturbation theory calculations,
confirming that bulk PdSe$_2$ has an optical gap in the mid-infrared
(upper-bound of 0.44 eV). More generally, this work further bolsters the
utility of the WOT-SRSH approach for predictive modeling of layered
semiconductors.",['cond-mat.mtrl-sci'],False,,,,Curvature-Controlled Polarization in Adaptive Ferroelectric Membranes,"Resolving Contradictory Estimates of Band Gaps of Bulk PdSe$_2$: A
  Wannier-Localized Optimally-Tuned Screened Range-Separated Hybrid Density
  Functional Theory Study"
neg-d2-902,2025-03-18,,2503.14079," Boolean formulae compactly encode huge, constrained search spaces. Thus,
variability-intensive systems are often encoded with Boolean formulae. The
search space of a variability-intensive system is usually too large to explore
without statistical inference (e.g. testing). Testing every valid configuration
is computationally expensive (if not impossible) for most systems. This leads
most testing approaches to sample a few configurations before analyzing them. A
desirable property of such samples is uniformity: Each solution should have the
same selection probability. Uniformity is the property that facilitates
statistical inference. This property motivated the design of uniform random
samplers, relying on SAT solvers and counters and achieving different
trade-offs between uniformity and scalability. Though we can observe their
performance in practice, judging the quality of the generated samples is
different. Assessing the uniformity of a sampler is similar in nature to
assessing the uniformity of a pseudo-random number (PRNG) generator. However,
sampling is much slower and the nature of sampling also implies that the
hyperspace containing the samples is constrained. This means that testing PRNGs
is subject to fewer constraints than testing samplers. We propose a framework
that contains five statistical tests which are suited to test uniform random
samplers. Moreover, we demonstrate their use by testing seven samplers.
Finally, we demonstrate the influence of the Boolean formula given as input to
the samplers under test on the test results.",['cs.LO'],2503.09831," It is well-known that intersection type assignment systems can be used to
characterize strong normalization (SN). Typical proofs that typable
lambda-terms are SN in these systems rely on semantical techniques. In this
work, we study $\Lambda_\cap^e$, a variant of Coppo and Dezani's (Curry-style)
intersection type system, and we propose a syntactical proof of strong
normalization for it. We first design $\Lambda_\cap^i$, a Church-style version,
in which terms closely correspond to typing derivations. Then we prove that
typability in $\Lambda_\cap^i$ implies SN through a measure that, given a term,
produces a natural number that decreases along with reduction. Finally, the
result is extended to $\Lambda_\cap^e$, since the two systems simulate each
other.",['cs.LO'],False,,,,"Testing Uniform Random Samplers: Methods, Datasets and Protocols","Strong normalization through idempotent intersection types: a new
  syntactical approach"
neg-d2-903,2025-02-10,,2502.06663," Modern large language models (LLMs) driven by scaling laws, achieve
intelligence emergency in large model sizes. Recently, the increasing concerns
about cloud costs, latency, and privacy make it an urgent requirement to
develop compact edge language models. Distinguished from direct pretraining
that bounded by the scaling law, this work proposes the pruning-aware
pretraining, focusing on retaining performance of much larger optimized models.
It features following characteristics: 1) Data-scalable: we introduce minimal
parameter groups in LLM and continuously optimize structural pruning, extending
post-training pruning methods like LLM-Pruner and SparseGPT into the
pretraining phase. 2) Architecture-agnostic: the LLM architecture is
auto-designed using saliency-driven pruning, which is the first time to exceed
SoTA human-designed LLMs in modern pretraining. We reveal that it achieves
top-quality edge language models, termed EfficientLLM, by scaling up LLM
compression and extending its boundary. EfficientLLM significantly outperforms
SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM,
Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first
attempt, EfficientLLM bridges the performance gap between traditional LLM
compression and direct pretraining methods, and we will fully open source at
https://github.com/Xingrun-Xing2/EfficientLLM.",['cs.LG'],2502.09374," Deep neural networks (DNNs) are increasingly used in safety-critical
applications. Reliable fault analysis and mitigation are essential to ensure
their functionality in harsh environments that contain high radiation levels.
This study analyses the impact of multiple single-bit single-event upsets in
DNNs by performing fault injection at the level of a DNN model. Additionally, a
fault aware training (FAT) methodology is proposed that improves the DNNs'
robustness to faults without any modification to the hardware. Experimental
results show that the FAT methodology improves the tolerance to faults up to a
factor 3.",['cs.LG'],False,,,,"EfficientLLM: Scalable Pruning-Aware Pretraining for
  Architecture-Agnostic Edge Language Models","Mitigating multiple single-event upsets during deep neural network
  inference using fault-aware training"
neg-d2-904,2025-01-20,,2501.11888," The T center in silicon has recently emerged as a promising candidate for
scalable quantum technologies, due to its telecommunications band optical
transition and microwave addressable ground state spin. The immense promise of
the T center is driven by its silicon host material; silicon is by far the most
mature, manufacturable semiconductor material for integrated photonic and
electronic devices. Here, we present the first study of T-centers in an
electrical device. We study an ensemble of T centers coupled to a buried
lateral P-I-N diode in silicon, observing the T-center's optical response to
static and dynamic electric fields. We utilize the defect's optical response as
a probe of device nonlinearity, observing a phase transition of the carrier
density into a stable oscillatory regime characteristic of negative
differential resistance. These findings provide fundamental insight into the
physics of the T-center for improved quantum device performance and open a
promising new direction for defect-based local quantum sensing in semiconductor
devices.",['quant-ph'],2503.16314," Understanding the impact of disturbances in quantum channels is of paramount
importance for the implementation of many quantum technologies, as noise can be
detrimental to quantum correlations. Among the various types of disturbances,
we explore the effects of white and colored noise and experimentally test the
resilience of a quantum ghost spectrometer against these two types of noise,
showing that it is always robust against white noise, whereas colored noise
introduces a huge impact on the process.",['quant-ph'],False,,,,"Probing negative differential resistance in silicon with a P-I-N
  diode-integrated T center ensemble","An experimental investigation of quantum frequency correlations
  resilience against white and colored noise"
neg-d2-905,2025-01-03,,2501.01815," A mechanical model of a laminated composite ring on a nonreciprocal elastic
foundation is a valuable engineering tool during the early design stages of
various applications, such as non-pneumatic wheels, flexible bearings,
expandable tubulars in oil wells, and vascular stents interacting with blood
vessel linings, especially under non-axisymmetric loadings. Despite its
importance, limited research has focused on the interaction between laminated
composite rings and nonreciprocal elastic foundations. Moreover, no
quantitative studies have yet explored the influence of foundation stiffness on
the ring deformation. This work aims to develop an analytical framework for a
laminated composite ring supported by a nonreciprocal elastic foundation under
non-axisymmetric loading conditions. The model generates a design map that
correlates the foundation stiffness with the ring deformation, accounting for
ring dimensions, laminate layup architecture, and lamina anisotropy. The
closed-form solution provides an efficient design tool for analyzing
non-axisymmetric and nonuniform loadings at a low computational cost. The
resulting design map provides a valuable resource for exploring the interaction
between the nonreciprocal foundation and the laminated ring. The proposed
analytical framework and design map hold broad potential applications in
automotive, mechanical, civil, and biomedical engineering fields.",['physics.app-ph'],2501.15917," This article presents a novel perspective to model and simulate
reconfigurable intelligent surface (RIS)-assisted communication systems.
Traditional methods in antenna design often rely on array method to simulate,
whereas communication system modeling tends to idealize antenna behavior.
Neither approach sufficiently captures the detailed characteristics of
RIS-assisted communication. To address this limitation, we propose a
comprehensive simulation framework that jointly models RIS antenna design and
the communication process. This framework simulates the entire communication
pipeline, encompassing signal generation, modulation, propagation, RIS-based
radiation, signal reception, alignment, demodulation, decision, and processing.
Using a QPSK-modulated signal for validation, we analyze system performance and
investigate the relationship between bit error rate (BER), aperture fill time,
array size, and baseband symbol frequency. The results indicate that larger
array sizes and higher baseband symbol frequencies exacerbate aperture fill
time effects, leading to increased BER. Furthermore, we examine BER variation
with respect to signal-to-noise ratio (SNR) and propose an optimal
matching-based alignment algorithm, which significantly reduces BER compared to
conventional pilot-based alignment methods. This work demonstrates the entire
process of RIS communication, and reveals the source of bit errors, which
provides valuable insights into the design and performance optimization of
RIS-assisted communication systems.",['physics.app-ph'],False,,,,"Analytical modeling of laminated composite rings on nonreciprocal
  elastic foundations under non-axisymmetric loading","RIS Assisted Wireless Communication: Advanced Modeling, Simulation, and
  Analytical Insights"
neg-d2-906,2025-01-17,,2501.10049," To take the esports scene to the next level, we introduce PandaSkill, a
framework for assessing player performance and skill rating. Traditional rating
systems like Elo and TrueSkill often overlook individual contributions and face
challenges in professional esports due to limited game data and fragmented
competitive scenes. PandaSkill leverages machine learning to estimate in-game
player performance from individual player statistics. Each in-game role is
modeled independently, ensuring a fair comparison between them. Then, using
these performance scores, PandaSkill updates the player skill ratings using the
Bayesian framework OpenSkill in a free-for-all setting. In this setting, skill
ratings are updated solely based on performance scores rather than game
outcomes, hightlighting individual contributions. To address the challenge of
isolated rating pools that hinder cross-regional comparisons, PandaSkill
introduces a dual-rating system that combines players' regional ratings with a
meta-rating representing each region's overall skill level. Applying PandaSkill
to five years of professional League of Legends matches worldwide, we show that
our method produces skill ratings that better predict game outcomes and align
more closely with expert opinions compared to existing methods.",['cs.LG'],2502.09374," Deep neural networks (DNNs) are increasingly used in safety-critical
applications. Reliable fault analysis and mitigation are essential to ensure
their functionality in harsh environments that contain high radiation levels.
This study analyses the impact of multiple single-bit single-event upsets in
DNNs by performing fault injection at the level of a DNN model. Additionally, a
fault aware training (FAT) methodology is proposed that improves the DNNs'
robustness to faults without any modification to the hardware. Experimental
results show that the FAT methodology improves the tolerance to faults up to a
factor 3.",['cs.LG'],False,,,,"PandaSkill - Player Performance and Skill Rating in Esports: Application
  to League of Legends","Mitigating multiple single-event upsets during deep neural network
  inference using fault-aware training"
neg-d2-907,2025-03-20,,2503.16114," Interfaces for interacting with large language models (LLMs) are often
designed to mimic human conversations, typically presenting a single response
to user queries. This design choice can obscure the probabilistic and
predictive nature of these models, potentially fostering undue trust and
over-anthropomorphization of the underlying model. In this paper, we
investigate (i) the effect of displaying multiple responses simultaneously as a
countermeasure to these issues, and (ii) how a cognitive support
mechanism-highlighting structural and semantic similarities across
responses-helps users deal with the increased cognitive load of that
intervention. We conducted a within-subjects study in which participants
inspected responses generated by an LLM under three conditions: one response,
ten responses with cognitive support, and ten responses without cognitive
support. Participants then answered questions about workload, trust and
reliance, and anthropomorphization. We conclude by reporting the results of
these studies and discussing future work and design opportunities for future
LLM interfaces.",['cs.HC'],2503.15523," Children tend to be constantly exposed to technologies, such as smartphones,
tablets, and gaming consoles, drawn by the interactive and visually stimulating
nature of digital platforms. Thus, integrating the teaching process with
technological gadgets may enhance engagement and foster interactive learning
experiences, besides equipping students with the digital skills for today's
increasingly technology-driven world. The main goal of this work is to provide
an open-source and manageable tool that teachers can use as an everyday
activity and as an exergame. For this, we present a prototype of an interactive
platform that students use to answer a quiz by moving to segments available on
an interactive floor. All the platform design and implementation directions are
publicly available.",['cs.HC'],False,,,,"The Impact of Revealing Large Language Model Stochasticity on Trust,
  Reliability, and Anthropomorphization","InteractiveEdu: An Open-source Interactive Floor for Exergame as a
  Learning Platform"
neg-d2-908,2025-01-07,,2501.04248," Nonreciprocal optical devices are key components in photonic integrated
circuits for light reflection blocking and routing. Most reported silicon
integrated nonreciprocal optical devices to date were unit devices. To allow
complex signal routing between multi-ports in photonic networks, multi-port
magneto-optical (MO) nonreciprocal photonic devices are desired. In this study,
we report experimental demonstration of a silicon integrated 5*5 multiport
nonreciprocal photonic device based on magneto-optical waveguides. By
introducing different nonreciprocal phase shift effect to planar photonic
waveguides, the device focuses light to different ports for both forward and
backward propagation. The device shows designable nonreciprocal transmission
between 5*5 ports, achieving 16 dB isolation ratio and -18 dB crosstalk.",['physics.optics'],2501.04907," Generation and propagation of optical skyrmions provide a versatile plalform
for topologically nontrivial optical informatics and light-matter interactions,
but their acceleration along curved trajectories is to be studied. In this
study, we experimentally demonstrate the first accelerating skyrmion lattices
conveyed by Airy structured light, characterized by topologically stable
skyrmion textures with self-acceleration along parabolic trajectories. We show
that the skyrmion unit cell can maintain a Skyrme number $|N_\text{sk}|>0.9$
within a propagation range of $\pm1.22\ z_R$ upon parabolic acceleration.
Notably, the meron structure remains $|N_\text{sk}|$ stable within $0.5\pm0.02$
over a significantly extended range of $\pm3.06\ z_R$. Our work provides a new
potential carrier for topologically robust information distribution, particle
sorting and manipulation.",['physics.optics'],False,,,,"Nonreciprocal Optical Routing in Multi-port Magneto-Optical Devices on
  Silicon",Optical skyrmion lattices accelerating in free space
neg-d2-909,2025-03-10,,2503.07702," Collaboration is a fundamental and essential characteristic of many complex
systems, ranging from ant colonies to human societies. Each component within a
complex system interacts with others, even at a distance, to accomplish a given
task. A network of collaboration can be defined to study the collective
behavior of such systems within the framework of complex networks. The nodes in
these networks may represent simple organisms or more sophisticated intelligent
agents, such as humans. In this study, we utilize intelligent agents (nodes)
trained through reinforcement learning techniques to establish connections with
their neighbors, ultimately leading to the emergence of a large-scale
communication cluster. Notably, there is no centralized administrator; instead,
agents must adjust their connections based on information obtained from local
observations. The connection strategy is formulated using a physical
Hamiltonian, thereby categorizing this intelligent system under the paradigm of
""Physics-Guided Machine Learning"".",['cs.MA'],2502.18893," In multi-robot system (MRS) applications, efficient task assignment is
essential not only for coordinating agents and ensuring mission success but
also for maintaining overall system security. In this work, we first propose an
optimization-based distributed task assignment algorithm that dynamically
assigns mandatory security-critical tasks and optional tasks among teams.
Leveraging an inexact Alternating Direction Method of Multipliers (ADMM)-based
approach, we decompose the task assignment problem into separable and
non-separable subproblems. The non-separable subproblems are transformed into
an inexact ADMM update by projected gradient descent, which can be performed
through several communication steps within the team.
  In the second part of this paper, we formulate a comprehensive framework that
enables MRS under plan-deviation attacks to handle online tasks without
compromising security. The process begins with a security analysis that
determines whether an online task can be executed securely by a robot and, if
so, the required time and location for the robot to rejoin the team. Next, the
proposed task assignment algorithm is used to allocate security-related tasks
and verified online tasks. Finally, task fulfillment is managed using a Control
Lyapunov Function (CLF)-based controller, while security enforcement is ensured
through a Control Barrier Function (CBF)-based security filter. Through
simulations, we demonstrate that the proposed framework allows MRS to
effectively respond to unplanned online tasks while maintaining security
guarantees.",['cs.MA'],False,,,,"A Reliable Self-Organized Distributed Complex Network for Communication
  of Smart Agents","Distributed Online Task Assignment via Inexact ADMM for unplanned online
  tasks and its Applications to Security"
neg-d2-910,2025-03-12,,2503.09189," Observability of a software system aims at allowing its engineers and
operators to keep the system robust and highly available. With this paper, we
present the Kieker Observability Framework Version 2, the successor of the
Kieker Monitoring Framework.
  In this tool artifact paper, we do not just present the Kieker framework, but
also a demonstration of its application to the TeaStore benchmark, integrated
with the visual analytics tool ExplorViz. This demo is provided both as an
online service and as an artifact to deploy it yourself.",['cs.SE'],2501.1938," Background: Software engineering requires both technical skills and creative
problem-solving. Blind and low-vision software professionals (BLVSPs) encounter
numerous workplace challenges, including inaccessible tools and collaboration
hurdles with sighted colleagues. Objective: This study explores the innovative
strategies employed by BLVSPs to overcome these accessibility barriers,
focusing on their custom solutions and the importance of supportive
communities. Methodology: We conducted semi-structured interviews with 30
BLVSPs and used reflexive thematic analysis to identify key themes. Results:
Findings reveal that BLVSPs are motivated to develop creative and adaptive
solutions, highlighting the vital role of collaborative communities in
fostering shared problem-solving. Conclusion: For BLVSPs, creative
problem-solving is essential for navigating inaccessible work environments, in
contrast to sighted peers, who pursue optimization. This study enhances
understanding of how BLVSPs navigate accessibility challenges through
innovation.",['cs.SE'],False,,,,The Kieker Observability Framework Version 2,"Creative Problem-Solving: A Study with Blind and Low Vision Software
  Professionals"
neg-d2-911,2025-01-28,,2501.16864," In the last years the pervasive use of sensors, as they exist in smart
devices, e.g., phones, watches, medical devices, has increased dramatically the
availability of personal data. However, existing research on data collection
primarily focuses on the objective view of reality, as provided, for instance,
by sensors, often neglecting the integration of subjective human input, as
provided, for instance, by user answers to questionnaires. This limits
substantially the exploitability of the collected data. In this paper we
present a methodology and a platform specifically designed for the collection
of a combination of large-scale sensor data and qualitative human feedback. The
methodology has been designed to be deployed on top, and enriches the
functionalities of, an existing data collection APP, called iLog, which has
been used in large scale, worldwide data collection experiments. The main goal
is to put the key actors involved in an experiment, i.e., the researcher in
charge, the participant, and iLog in better control of the experiment itself,
thus enabling a much improved quality and richness of the data collected. The
novel functionalities of the resulting platform are: (i) a time-wise
representation of the situational context within which the data collection is
performed, (ii) an explicit representation of the temporal context within which
the data collection is performed, (iii) a calendar-based dashboard for the
real-time monitoring of the data collection context(s), and, finally, (iv) a
mechanism for the run-time revision of the data collection plan. The
practicality and utility of the proposed functionalities are demonstrated by
showing how they apply to a case study involving 350 University students.",['cs.HC'],2503.16114," Interfaces for interacting with large language models (LLMs) are often
designed to mimic human conversations, typically presenting a single response
to user queries. This design choice can obscure the probabilistic and
predictive nature of these models, potentially fostering undue trust and
over-anthropomorphization of the underlying model. In this paper, we
investigate (i) the effect of displaying multiple responses simultaneously as a
countermeasure to these issues, and (ii) how a cognitive support
mechanism-highlighting structural and semantic similarities across
responses-helps users deal with the increased cognitive load of that
intervention. We conducted a within-subjects study in which participants
inspected responses generated by an LLM under three conditions: one response,
ten responses with cognitive support, and ten responses without cognitive
support. Participants then answered questions about workload, trust and
reliance, and anthropomorphization. We conclude by reporting the results of
these studies and discussing future work and design opportunities for future
LLM interfaces.",['cs.HC'],False,,,,"A methodology and a platform for high-quality rich personal data
  collection","The Impact of Revealing Large Language Model Stochasticity on Trust,
  Reliability, and Anthropomorphization"
neg-d2-912,2025-02-11,,2502.07605," In contrast to the commonly used qubit resonator transverse coupling via the
$\sigma_{xy}$-degree of freedom, longitudinal coupling through $\sigma_z$
presents a tantalizing alternative: it does not hybridize the modes,
eliminating Purcell decay, and it enables quantum-non-demolishing qubit readout
independent of the qubit-resonator frequency detuning. Here, we demonstrate
longitudinal coupling between a {Cr$_7$Ni} molecular spin qubit ensemble and
the kinetic inductance of a granular aluminum superconducting microwave
resonator. The inherent frequency-independence of this coupling allows for the
utilization of a 7.8 GHz readout resonator to measure the full {Cr$_7$Ni}
magnetization curve spanning 0-600 mT, corresponding to a spin frequency range
of $f_\text{spin}=$0-15 GHz. For 2 GHz detuning from the readout resonator, we
measure a $1/e$ spin relaxation time $\tau=$0.38 s, limited by phonon decay to
the substrate. Based on these results, we propose a path towards longitudinal
coupling of single spins to a superconducting fluxonium qubit.",['quant-ph'],2501.1923," Cathodoluminescence spectroscopy has recently emerged as a novel platform for
nanoscale control of nonclassical features of light. Here, we propose a
theoretical model for cathodoluminescence from a multi-level quantum emitter.
Employing a master equation approach and treating the electron-beam excitation
as an incoherent broadband field source, we show that quantum interference can
arise between the different relaxation pathways. The induced-interference can
significantly modify the time-dependent spectra resulting in the enhancement or
suppression of cathodoluminescence. We find that the excitation rate, initial
state of the emitter, and excited level spacing play a crucial role in
determining the influence of interference. Our findings shed light on
electron-beam-induced quantum interference in cathodoluminescence and provides
a theoretical basis for exploring quantum optical phenomena in electron-driven
multi-level systems.",['quant-ph'],False,,,,Kinetic inductance coupling for circuit QED with spins,"Electron-beam-induced quantum interference effects in a multi-level
  quantum emitter"
neg-d2-913,2025-01-05,,2501.02725," The rapid advancements in artificial intelligence (AI), particularly in
generative AI and large language models (LLMs), have profoundly impacted the
creative industries by enabling innovative content creation, enhancing
workflows, and democratizing access to creative tools. This paper explores the
significant technological shifts since our previous review in 2022,
highlighting how these developments have expanded creative opportunities and
efficiency. These technological advancements have enhanced the capabilities of
text-to-image, text-to-video, and multimodal generation technologies. In
particular, key breakthroughs in LLMs have established new benchmarks in
conversational AI, while advancements in image generators have revolutionized
content creation. We also discuss AI integration into post-production
workflows, which has significantly accelerated and refined traditional
processes. Despite these innovations, challenges remain, particularly for the
media industry, due to the demands on communication traffic from creative
content. We therefore include data compression and quality assessment in this
paper. Furthermore, we highlight the trend toward unified AI frameworks capable
of addressing multiple creative tasks and underscore the importance of human
oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's
future potential in the creative sector, stressing the need to navigate
emerging challenges to maximize its benefits while addressing associated risks.",['cs.AI'],2501.07071," As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning
their values with humans has become imperative for their responsible
development and customized applications. However, there still lack evaluations
of LLMs values that fulfill three desirable goals. (1) Value Clarification: We
expect to clarify the underlying values of LLMs precisely and comprehensively,
while current evaluations focus narrowly on safety risks such as bias and
toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are
prone to data contamination and quickly become obsolete as LLMs evolve.
Additionally, these discriminative evaluations uncover LLMs' knowledge about
values, rather than valid assessments of LLMs' behavioral conformity to values.
(3) Value Pluralism: The pluralistic nature of human values across individuals
and cultures is largely ignored in measuring LLMs value alignment. To address
these challenges, we presents the Value Compass Leaderboard, with three
correspondingly designed modules. It (i) grounds the evaluation on
motivationally distinct \textit{basic values to clarify LLMs' underlying values
from a holistic view; (ii) applies a \textit{generative evolving evaluation
framework with adaptive test items for evolving LLMs and direct value
recognition from behaviors in realistic scenarios; (iii) propose a metric that
quantifies LLMs alignment with a specific value as a weighted sum over multiple
dimensions, with weights determined by pluralistic values.",['cs.AI'],False,,,,Artificial Intelligence in Creative Industries: Advances Prior to 2025,"Value Compass Leaderboard: A Platform for Fundamental and Validated
  Evaluation of LLMs Values"
neg-d2-914,2025-01-16,,2501.09296," The organization of neurons into functionally related assemblies is a
fundamental feature of cortical networks, yet our understanding of how these
assemblies maintain distinct identities while sharing members remains limited.
Here we analyze how spike-timing-dependent plasticity (STDP) shapes the
formation and stability of overlapping neuronal assemblies in recurrently
coupled networks of spiking neuron models. Using numerical simulations and an
associated mean-field theory, we demonstrate that the temporal structure of the
STDP rule, specifically its degree of causality, critically determines whether
assemblies that share neurons maintain segregation or merge together after
training is completed. We find that causal STDP rules, where
potentiation/depression occurs strictly when presynaptic spikes precede/proceed
postsynaptic spikes, allow assemblies to remain distinct even with substantial
overlap in membership. This stability arises because causal STDP effectively
cancels the symmetric correlations introduced by common inputs from shared
neurons. In contrast, acausal STDP rules lead to assembly fusion when overlap
exceeds a critical threshold, due to unchecked growth of common input
correlations. Our results provide theoretical insight into how
spike-timing-dependent learning rules can support distributed representation
where individual neurons participate in multiple assemblies while maintaining
functional specificity.",['q-bio.NC'],2501.09296," The organization of neurons into functionally related assemblies is a
fundamental feature of cortical networks, yet our understanding of how these
assemblies maintain distinct identities while sharing members remains limited.
Here we analyze how spike-timing-dependent plasticity (STDP) shapes the
formation and stability of overlapping neuronal assemblies in recurrently
coupled networks of spiking neuron models. Using numerical simulations and an
associated mean-field theory, we demonstrate that the temporal structure of the
STDP rule, specifically its degree of causality, critically determines whether
assemblies that share neurons maintain segregation or merge together after
training is completed. We find that causal STDP rules, where
potentiation/depression occurs strictly when presynaptic spikes precede/proceed
postsynaptic spikes, allow assemblies to remain distinct even with substantial
overlap in membership. This stability arises because causal STDP effectively
cancels the symmetric correlations introduced by common inputs from shared
neurons. In contrast, acausal STDP rules lead to assembly fusion when overlap
exceeds a critical threshold, due to unchecked growth of common input
correlations. Our results provide theoretical insight into how
spike-timing-dependent learning rules can support distributed representation
where individual neurons participate in multiple assemblies while maintaining
functional specificity.",['q-bio.NC'],False,,,,"Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in
  Recurrent Networks","Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in
  Recurrent Networks"
neg-d2-915,2025-02-10,,2502.06445," This paper introduces an open-source benchmark for evaluating Vision-Language
Models (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video
environments. We present a curated dataset containing 1,477 manually annotated
frames spanning diverse domains, including code editors, news broadcasts,
YouTube videos, and advertisements. Three state of the art VLMs - Claude-3,
Gemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as
EasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),
Character Error Rate (CER), and Accuracy. Our results highlight the strengths
and limitations of VLMs in video-based OCR tasks, demonstrating their potential
to outperform conventional OCR models in many scenarios. However, challenges
such as hallucinations, content security policies, and sensitivity to occluded
or stylized text remain. The dataset and benchmarking framework are publicly
available to foster further research.",['cs.CV'],2503.04478," General-purpose AI models, particularly those designed for text and vision,
demonstrate impressive versatility across a wide range of deep-learning tasks.
However, they often underperform in specialised domains like medical imaging,
where domain-specific solutions or alternative knowledge transfer approaches
are typically required. Recent studies have noted that general-purpose models
can exhibit similar latent spaces when processing semantically related data,
although this alignment does not occur naturally. Building on this insight, it
has been shown that applying a simple transformation - at most affine -
estimated from a subset of semantically corresponding samples, known as
anchors, enables model stitching across diverse training paradigms,
architectures, and modalities. In this paper, we explore how semantic alignment
- estimating transformations between anchors - can bridge general-purpose AI
with specialised medical knowledge. Using multiple public chest X-ray datasets,
we demonstrate that model stitching across model architectures allows general
models to integrate domain-specific knowledge without additional training,
leading to improved performance on medical tasks. Furthermore, we introduce a
novel zero-shot classification approach for unimodal vision encoders that
leverages semantic alignment across modalities. Our results show that our
method not only outperforms general multimodal models but also approaches the
performance levels of fully trained, medical-specific multimodal solutions",['cs.CV'],False,,,,"Benchmarking Vision-Language Models on Optical Character Recognition in
  Dynamic Video Environments",Semantic Alignment of Unimodal Medical Text and Vision Representations
neg-d2-916,2025-01-08,,2501.04384," We explore the existence of closed geodesics and geodesic spirals for the
Szeg\""o metric in a $C^{\infty}$-smoothly bounded strongly pseudoconvex domain
$\Omega\subset\mathbb{C}^n$, which is not simply connected for $n \geq 2$.",['math.CV'],2501.04384," We explore the existence of closed geodesics and geodesic spirals for the
Szeg\""o metric in a $C^{\infty}$-smoothly bounded strongly pseudoconvex domain
$\Omega\subset\mathbb{C}^n$, which is not simply connected for $n \geq 2$.",['math.CV'],False,,,,"On the geodesics of the Szeg\""o metric","On the geodesics of the Szeg\""o metric"
neg-d2-917,2025-01-23,,2501.13882," The role of grain size in determining fracture toughness in metals is
incompletely understood with apparently contradictory experimental
observations. We study this grain-size dependence computationally by building a
model that combines the phase-field formulation of fracture mechanics with
dislocation density-based crystal plasticity. We apply the model to cleavage
fracture of body-centered cubic materials in plane strain conditions, and find
non-monotonic grain-size dependence of plastic-brittle transgranular fracture.
We find two mechanisms at play. The first is the nucleation of failure due to
cross-slip in critically located grains within transgranular band of localized
deformation, and this follows the classical Hall-Petch law that predicts a
higher failure stress for smaller grains. The second is the resistance to the
propagation of a mode I crack, where grain boundaries can potentially pin a
crack, and this follows an inverse Hall-Petch law with higher toughness for
larger grains. The result of the competition between the two mechanisms gives
rise to non-monotonic behavior and reconciles the apparently contradictory
experimental observations.",['cond-mat.mtrl-sci'],2501.03387," Controlled modulation of electronic band structure in two-dimensional (2D)
materials via doping is crucial for devices fabrication. For instance doped
graphene has been envisaged for various applications like sensors,
super-capacitors, transistors, p-n junctions, photo-detectors, etc. Many
different techniques have been developed to achieve desired doping in 2D
materials, like chemical doping, electrostatic doping, substrate doping, etc.
Here, we have combined space charge doping with space and angle resolved
photoemission (nano-ARPES), in order to directly observe the Fermi level
modulation on micron-sized flakes of monolayer and bilayer graphene. The doping
level can be tuned in a controlled manner, which allows us to directly observe
the Fermi level tuning. In our experiment we successfully doped the graphene
with p- and n-type carriers (holes/electrons) which are directly observed
through band shift in ARPES measurements. The observed band shift is $\sim$250
meV for bilayer and $\sim$500 meV for monolayer graphene. The results from our
experiment promote the space charge doping technique and nano-ARPES into other
materials such as 2D semiconductors and superconductors, in order to directly
observe the physical phenomena such as band gap transition and phase transition
as function of carrier doping.",['cond-mat.mtrl-sci'],False,,,,Grain-size dependence of plastic-brittle transgranular fracture,"Space Charge Doping Induced Band Modulation in Mono- and Bi-layer
  Graphene: a nano-ARPES study"
neg-d2-918,2025-03-19,,2503.14899," This study presents a robust optimization algorithm for automated highway
merge. The merging scenario is one of the challenging scenes in automated
driving, because it requires adjusting ego vehicle's speed to match other
vehicles before reaching the end point. Then, we model the speed planning
problem as a deterministic Markov decision process. The proposed scheme is able
to compute each state value of the process and reliably derive the optimal
sequence of actions. In our approach, we adopt jerk as the action of the
process to prevent a sudden change of acceleration. However, since this expands
the state space, we also consider ways to achieve a real-time operation. We
compared our scheme with a simple algorithm with the Intelligent Driver Model.
We not only evaluated the scheme in a simulation environment but also conduct a
real world testing.",['cs.RO'],2502.15613," Current robotic pick-and-place policies typically require consistent gripper
configurations across training and inference. This constraint imposes high
retraining or fine-tuning costs, especially for imitation learning-based
approaches, when adapting to new end-effectors. To mitigate this issue, we
present a diffusion-based policy with a hybrid learning-optimization framework,
enabling zero-shot adaptation to novel grippers without additional data
collection for retraining policy. During training, the policy learns
manipulation primitives from demonstrations collected using a base gripper. At
inference, a diffusion-based optimization strategy dynamically enforces
kinematic and safety constraints, ensuring that generated trajectories align
with the physical properties of unseen grippers. This is achieved through a
constrained denoising procedure that adapts trajectories to gripper-specific
parameters (e.g., tool-center-point offsets, jaw widths) while preserving
collision avoidance and task feasibility. We validate our method on a Franka
Panda robot across six gripper configurations, including 3D-printed fingertips,
flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a
93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusion
policy baselines), supporting tool-center-point variations of 16-23.5 cm and
jaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusion
enables robust cross-gripper manipulation while maintaining the sample
efficiency of imitation learning, eliminating the need for gripper-specific
retraining. Video and code are available at https://github.com/yaoxt3/GADP.",['cs.RO'],False,,,,"Speed Optimization Algorithm based on Deterministic Markov Decision
  Process for Automated Highway Merge","Pick-and-place Manipulation Across Grippers Without Retraining: A
  Learning-optimization Diffusion Policy Approach"
neg-d2-919,2025-02-25,,2502.18135," This paper introduces a novel method for solving the single-source
localization problem, specifically addressing the case of trilateration. We
formulate the problem as a weighted least-squares problem in the squared
distances and demonstrate how suitable weights are chosen to accommodate
different noise distributions. By transforming this formulation into an
eigenvalue problem, we leverage existing eigensolvers to achieve a fast,
numerically stable, and easily implemented solver. Furthermore, our theoretical
analysis establishes that the globally optimal solution corresponds to the
largest real eigenvalue, drawing parallels to the existing literature on the
trust-region subproblem. Unlike previous works, we give special treatment to
degenerate cases, where multiple and possibly infinitely many solutions exist.
We provide a geometric interpretation of the solution sets and design the
proposed method to handle these cases gracefully. Finally, we validate against
a range of state-of-the-art methods using synthetic and real data,
demonstrating how the proposed method is among the fastest and most numerically
stable.",['math.OC'],2501.13886," The stochastic three points (STP) algorithm is a derivative-free optimization
technique designed for unconstrained optimization problems in $\mathbb{R}^d$.
In this paper, we analyze this algorithm for three classes of functions: smooth
functions that may lack convexity, smooth convex functions, and smooth
functions that are strongly convex. Our work provides the first almost sure
convergence results of the STP algorithm, alongside some convergence results in
expectation. For the class of smooth functions, we establish that the best
gradient iterate of the STP algorithm converges almost surely to zero at a rate
arbitrarily close to $o(\frac{1}{\sqrt{T}})$, where $T$ is the number of
iterations. Furthermore, within the same class of functions, we establish both
almost sure convergence and convergence in expectation of the final gradient
iterate towards zero. For the class of smooth convex functions, we establish
that $f(\theta^T)$ converges to $\inf_{\theta \in \mathbb{R}^d} f(\theta)$
almost surely at a rate arbitrarily close to $o(\frac{1}{T})$, and in
expectation at a rate of $O(\frac{d}{T})$ where $d$ is the dimension of the
space. Finally, for the class of smooth functions that are strongly convex, we
establish that when step sizes are obtained by approximating the directional
derivatives of the function, $f(\theta^T)$ converges to $\inf_{\theta \in
\mathbb{R}^d} f(\theta)$ in expectation at a rate of $O((1-\frac{\mu}{dL})^T)$,
and almost surely at a rate arbitrarily close to $o((1-\frac{\mu}{dL})^T)$,
where $\mu$ and $L$ are the strong convexity and smoothness parameters of the
function.",['math.OC'],False,,,,Single-Source Localization as an Eigenvalue Problem,On the Almost Sure Convergence of the Stochastic Three Points Algorithm
neg-d2-920,2025-01-07,,2501.03985," We investigate AGN feedback from an intermediate-mass black hole at the
center of a dwarf spheroidal galaxy, by performing isolated galaxy simulations
using a modified version of the GADGET-3 code. We consider Leo II (PGC 34176)
in the Local Group as our simulation reference model. Beginning with black hole
seeds ranging from $10^3$ to $10^6$ M$_{\odot}$, our simulations focus on
comparing stellar-only feedback with AGN+stellar/SN feedback over 13.7 Gyr of
galactic evolution. Our results indicate that a low-mass AGN in a dwarf galaxy
influences the star formation history under specific physical conditions. While
AGN feedback is generally negative on star formation, instances of positive
feedback were also identified. Despite measurable effects on the evolution of
the dwarf host galaxy, black hole seeds exhibited only marginal growth. We
tested several physical scenarios as modified models in our simulations,
primarily concerning the dynamics of the central black holes, which may wander
within dwarf galaxies rather than being centrally located. However, none of
these adjustments significantly impacted the growth of the black hole seeds.
This suggests that intermediate-mass black holes may struggle to achieve higher
masses in isolated environments, with mergers and interactions likely playing
crucial roles in their growth. Nevertheless, AGN feedback exhibited
non-negligible effects in our simulated dwarf spheroidal galaxies, despite the
assumed dominant role of stellar feedback in the low-mass regime.",['astro-ph.GA'],2503.08535," We report on the astrophysical properties of a sample of star clusters in the
Small Magellanic Cloud (SMC). They have been selected with the aim of looking
for the connection between their ages, heliocentric distances and metallicities
with the existence of tidally perturbed/induced outermost SMC regions. We
derived the star cluster fundamental parameters from relatively deep Survey of
the Magellanic Stellar History (SMASH) DR2 color magnitude diagrams, cleaned
from field star contamination, and compared to thousand synthetic CMDs covering
a wide range of heliocentric distances, ages and metal content. Heliocentric
distances for 15 star clusters are derived for the first time, which represents
an increase of 50 per cent of SMC clusters with estimated heliocentric
distances. The analysis of the age-metallicity relationships (AMRs) of cluster
located in outermost regions distributed around the SMC and in the SMC Main
Body reveals that they have followed the overall galaxy chemical enrichment
history. However, since half of the studied clusters are placed in front of or
behind the SMC Main Body, we concluded that they formed in the SMC and have
traveled outward because of the tidal effects from the interaction with the
Large Magellanic Cloud (LMC). Furthermore, metal rich clusters formed recently
in some of these outermost regions from gas that was also dragged by tidal
effects from the inner SMC. This outcome leads to consider the SMC as a galaxy
scarred by the LMC tidal interaction with distance-perturbed and newly induced
outermost stellar substructures.",['astro-ph.GA'],False,,,,"Exploring the evolution of a dwarf spheroidal galaxy with SPH
  simulations: II. AGN feedback","Astrophysical properties of star clusters projected toward tidally
  perturbed SMC regions"
neg-d2-921,2025-03-09,,2503.06556," In the context of a growing interdisciplinary interest in the angular
momentum of wave fields, the spin-wave case has yet to be fully explored, with
the extensively studied notion of spin transport being only part of the broader
picture. Here we report experimental evidence for magnon orbital angular
momentum, demonstrating that the mode exhibits rotation rather than remaining
stationary. This conclusion is drawn from observations of the lifted degeneracy
of waves with counter-rotating wave fronts. This requires an unambiguous
formulation of spin and orbital angular momenta for spin waves, which we
provide in full generality based on a systematic application of quantum field
theory techniques. The results unequivocally establish magnetic dipole-dipole
interactions as a magnetic-field controllable spin-orbit interaction for
magnons. Our findings open a new research direction, leveraging the
spectroscopic readability of angular momentum for azimuthal spin waves and
other related systems.",['cond-mat.mes-hall'],2503.01833," Lateral heterostructures built of monolayers of transition metal
dichalcogenides (TMDs) are characterized by a thin 1D interface exhibiting a
large energy offset. Recently, the formation of spatially separated
charge-transfer (CT) excitons at the interface has been demonstrated. The
technologically important exciton propagation across the interface and the
impact of CT excitons has remained in the dark so far. In this work, we
microscopically investigate the spatiotemporal exciton dynamics in the
exemplary hBN-encapsulated WSe$_2$-MoSe$_2$ lateral heterostructure and reveal
a highly interesting interplay of energy offset-driven unidirectional exciton
drift across the interface and efficient capture into energetically lower CT
excitons at the interface. This interplay triggers a counterintuitive thermal
control of exciton transport with a less efficient propagation at lower
temperatures - opposite to the behavior in conventional semiconductors. We
predict clear signatures of this intriguing exciton propagation both in far-
and near-field photoluminescence experiments. Our results present an important
step toward a microscopic understanding of the technologically relevant
unidirectional exciton transport in lateral heterostructures.",['cond-mat.mes-hall'],False,,,,The Orbital Angular Momentum of Azimuthal Spin-Waves,"Impact of charge transfer excitons on unidirectional exciton transport
  in lateral TMD heterostructures"
neg-d2-922,2025-02-01,,2502.00653," While multimodal large language models (MLLMs) have achieved remarkable
success in recent advancements, their susceptibility to jailbreak attacks has
come to light. In such attacks, adversaries exploit carefully crafted prompts
to coerce models into generating harmful or undesirable content. Existing
defense mechanisms often rely on external inference steps or safety alignment
training, both of which are less effective and impractical when facing
sophisticated adversarial perturbations in white-box scenarios. To address
these challenges and bolster MLLM robustness, we introduce SafeMLLM by adopting
an adversarial training framework that alternates between an attack step for
generating adversarial noise and a model updating step. At the attack step,
SafeMLLM generates adversarial perturbations through a newly proposed
contrastive embedding attack (CoE-Attack), which optimizes token embeddings
under a contrastive objective. SafeMLLM then updates model parameters to
neutralize the perturbation effects while preserving model utility on benign
inputs. We evaluate SafeMLLM across six MLLMs and six jailbreak methods
spanning multiple modalities. Experimental results show that SafeMLLM
effectively defends against diverse attacks, maintaining robust performance and
utilities.",['cs.CR'],2502.13929," Formal verification plays a crucial role in making smart contracts safer,
being able to find bugs or to guarantee their absence, as well as checking
whether the business logic is correctly implemented. For Solidity, even though
there already exist several mature verification tools, the semantical quirks of
the language can make verification quite hard in practice. Move, on the other
hand, has been designed with security and verification in mind, and it has been
accompanied since its early stages by a formal verification tool, the Move
Prover. In this paper, we investigate through a comparative analysis: 1) how
the different designs of the two contract languages impact verification, and 2)
what is the state-of-the-art of verification tools for the two languages, and
how do they compare on three paradigmatic use cases. Our investigation is
supported by an open dataset of verification tasks performed in Certora and in
the Aptos Move Prover.",['cs.CR'],False,,,,"Towards Robust Multimodal Large Language Models Against Jailbreak
  Attacks","Formal verification in Solidity and Move: insights from a comparative
  analysis"
neg-d2-923,2025-01-12,,2501.06894," Quantum computing is an emerging field with significant potential, yet
software development and maintenance challenges limit its accessibility and
maturity. This work investigates the current state, evolution, and maintenance
practices in the quantum computing community by conducting a large-scale mining
analysis of over 21,000 quantum software repositories on GitHub, containing
more than 1.2 million commits contributed by over 10,000 unique developers.
Specifically, the focus of this paper is to: (i) assess the community's status
and growth by examining the popularity of quantum computing, trends in
programming languages and framework usage, growth of contributors, and insights
from repository documentation; and (ii) analyze maintenance practices through
commit patterns, issue classification, and maintenance levels. Our findings
indicate rapid growth in the quantum computing community, with a 200% increase
in the number of repositories and a 150% rise in contributors since 2017. Our
analysis of commits shows a strong focus on perfective updates, while the
relatively low number of corrective commits highlights potential gaps in bug
resolution. Furthermore, one-third of the quantum computing issues highlight
the need for specialized tools in addition to general software infrastructure.
In summary, this work provides a foundation for targeted improvements in
quantum software to support sustained growth and technical advancement. Based
on our analysis of development activity, community structure, and maintenance
practices, this study offers actionable recommendations to enhance quantum
programming tools, documentation, and resources. We are also open-sourcing our
dataset to support further analysis by the community and to guide future
research and tool development for quantum computing.",['cs.SE'],2501.1938," Background: Software engineering requires both technical skills and creative
problem-solving. Blind and low-vision software professionals (BLVSPs) encounter
numerous workplace challenges, including inaccessible tools and collaboration
hurdles with sighted colleagues. Objective: This study explores the innovative
strategies employed by BLVSPs to overcome these accessibility barriers,
focusing on their custom solutions and the importance of supportive
communities. Methodology: We conducted semi-structured interviews with 30
BLVSPs and used reflexive thematic analysis to identify key themes. Results:
Findings reveal that BLVSPs are motivated to develop creative and adaptive
solutions, highlighting the vital role of collaborative communities in
fostering shared problem-solving. Conclusion: For BLVSPs, creative
problem-solving is essential for navigating inaccessible work environments, in
contrast to sighted peers, who pursue optimization. This study enhances
understanding of how BLVSPs navigate accessibility challenges through
innovation.",['cs.SE'],False,,,,"Analyzing the Evolution and Maintenance of Quantum Computing
  Repositories","Creative Problem-Solving: A Study with Blind and Low Vision Software
  Professionals"
neg-d2-924,2025-01-25,,2501.15246," Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular
environments. Accurate reconstruction of high-resolution volumes is complicated
by the very low signal-to-noise ratio and a restricted range of sample tilts,
creating a missing wedge of Fourier information. Recent self-supervised deep
learning approaches, which post-process initial reconstructions done by
filtered backprojection (FBP), have significantly improved reconstruction
quality, but they are computationally expensive, demand large memory, and
require retraining for each new dataset. End-to-end supervised learning is an
appealing alternative but is impeded by the lack of ground truth and the large
memory demands of high-resolution volumetric data. Training on synthetic data
often leads to overfitting and poor generalization to real data, and, to date,
no general end-to-end deep learning reconstructors exist for cryo-ET. In this
work, we introduce CryoLithe, a local, memory-efficient reconstruction network
that directly estimates the volume from an aligned tilt-series, overcoming the
suboptimal FBP. We demonstrate that leveraging transform-domain locality makes
our network robust to distribution shifts, enabling effective supervised
training and giving excellent results on real data -- without retraining or
fine-tuning.",['eess.IV'],2501.15246," Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular
environments. Accurate reconstruction of high-resolution volumes is complicated
by the very low signal-to-noise ratio and a restricted range of sample tilts,
creating a missing wedge of Fourier information. Recent self-supervised deep
learning approaches, which post-process initial reconstructions done by
filtered backprojection (FBP), have significantly improved reconstruction
quality, but they are computationally expensive, demand large memory, and
require retraining for each new dataset. End-to-end supervised learning is an
appealing alternative but is impeded by the lack of ground truth and the large
memory demands of high-resolution volumetric data. Training on synthetic data
often leads to overfitting and poor generalization to real data, and, to date,
no general end-to-end deep learning reconstructors exist for cryo-ET. In this
work, we introduce CryoLithe, a local, memory-efficient reconstruction network
that directly estimates the volume from an aligned tilt-series, overcoming the
suboptimal FBP. We demonstrate that leveraging transform-domain locality makes
our network robust to distribution shifts, enabling effective supervised
training and giving excellent results on real data -- without retraining or
fine-tuning.",['eess.IV'],False,,,,End-to-end localized deep learning for Cryo-ET,End-to-end localized deep learning for Cryo-ET
neg-d2-925,2025-01-30,,2501.182," The characterization of an interior permanent magnet synchronous machine
(IPMSM) requires numerical analysis of the nonlinear magnetic motor model in
different load conditions. To obtain the case-specific best machine behavior, a
strategy for the determination of stator input current amplitude and angle is
employed for all possible load torques given a limited terminal current
amplitude and DC bus voltage. Various losses are calculated using state of the
art loss models. The electromagnetic performance of the electric machine is
stored in lookup tables. These can then be used for the drive cycle analysis of
the electric drive train in the design and optimization stages.
  To avoid the use of a dedicated mesh generator in the numerical analysis,
volumetric spline-based models are suggested.With this approach, the mesh can
be generated directly from the Computer Aided Design (CAD) geometry. This
enables an automatic adaption of the grid following a geometry perturbation.
With this the approximated solution is kept consistent over the different
iterations of an overlying optimization, improving its convergence behavior.",['cs.CE'],2501.05162," This paper proposes a novel and efficient key conditional quotient filter
(KCQF) for the estimation of state in the nonlinear system which can be either
Gaussian or non-Gaussian, and either Markovian or non-Markovian. The core idea
of the proposed KCQF is that only the key measurement conditions, rather than
all measurement conditions, should be used to estimate the state. Based on key
measurement conditions, the quotient-form analytical integral expressions for
the conditional probability density function, mean, and variance of state are
derived by using the principle of probability conservation, and are calculated
by using the Monte Carlo method, which thereby constructs the KCQF. Two
nonlinear numerical examples were given to demonstrate the superior estimation
accuracy of KCQF, compared to seven existing filters.",['cs.CE'],False,,,,"Characterization of Permanent Magnet Synchronous Machines based on
  semi-analytic model reduction for drive cycle analysis","A Key Conditional Quotient Filter for Nonlinear, non-Gaussian and
  non-Markovian System"
neg-d2-926,2025-02-19,,2502.13658," Purpose: The increasing number of cyber-attacks has elevated the importance
of cybersecurity for organizations. This has also increased the demand for
professionals with the necessary skills to protect these organizations. As a
result, many individuals are looking to enter the field of cybersecurity.
However, there is a lack of clear understanding of the skills required for a
successful career in this field. In this paper, we identify the skills required
for cybersecurity professionals. We also determine how the demand for cyber
skills relates to various cyber roles such as security analyst and security
architect. Furthermore, we identify the programming languages that are
important for cybersecurity professionals. Design/Methodology: For this study,
we have collected and analyzed data from 12,161 job ads and 49,002 Stack
Overflow posts. By examining this, we identified patterns and trends related to
skill requirements, role-specific demands, and programming languages in
cybersecurity. Findings: Our results reveal that (i) communication skills and
project management skills are the most important soft skills, (ii) as compared
to soft skills, the demand for technical skills varies more across various
cyber roles, and (iii) Java is the most commonly used programming language.
Originality: Our findings serve as a guideline for individuals aiming to get
into the field of cybersecurity. Moreover, our findings are useful in terms of
informing educational institutes to teach the correct set of skills to students
doing degrees in cybersecurity.",['cs.CR'],2503.12952," As quantum computing advances, modern cryptographic standards face an
existential threat, necessitating a transition to post-quantum cryptography
(PQC). The National Institute of Standards and Technology (NIST) has selected
CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure
key exchange and digital signatures, respectively. This study conducts a
comprehensive performance analysis of these algorithms by benchmarking
execution times across cryptographic operations such as key generation,
encapsulation, decapsulation, signing, and verification. Additionally, the
impact of AVX2 optimizations is evaluated to assess hardware acceleration
benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient
execution times, outperforming classical cryptographic schemes such as RSA and
ECDSA at equivalent security levels. Beyond technical performance, the
real-world deployment of PQC introduces challenges in telecommunications
networks, where large-scale infrastructure upgrades, interoperability with
legacy systems, and regulatory constraints must be addressed. This paper
examines the feasibility of PQC adoption in telecom environments, highlighting
key transition challenges, security risks, and implementation strategies.
Through industry case studies, we illustrate how telecom operators are
integrating PQC into 5G authentication, subscriber identity protection, and
secure communications. Our analysis provides insights into the computational
trade-offs, deployment considerations, and standardization efforts shaping the
future of quantum-safe cryptographic infrastructure.",['cs.CR'],False,,,,What Skills Do Cyber Security Professionals Need?,"Performance Analysis and Industry Deployment of Post-Quantum
  Cryptography Algorithms"
neg-d2-927,2025-02-19,,2502.13661," Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation
(DBS) in DYT1 dystonia and disease progression under DBS was studied.
Twenty-six patients of this open-label study were divided into two groups: (A)
with single bilateral GPi lead, (B) with a second bilateral GPi lead implanted
owning to subsequent worsening of symptomatology. Dystonia was assessed with
the Burke Scale. Appearance of new symptoms and distribution according to body
region were recorded. In the whole cohort, significant decreases in motor and
disability subscores (P < 0.0001) were observed at 1 year and maintained up to
10 years. Group B showed worsening of the symptoms. At 1 year, there were no
significant differences between Groups A (without subsequent worsening) and B;
at 5 years, a significant difference was found for motor and disability scores.
Within Group B, four patients exhibited additional improvement after the second
DBS surgery. In the 26 patients, significant difference (P = 0.001) was found
between the number of body regions affected by dystonia preoperatively and over
the whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10
years (two patients). New symptoms appear with long-term follow-up and may
improve with additional leads in a subgroup of patients.",['q-bio.NC'],2502.11591," Classification and quantitative characterization of neuronal morphologies
from histological neuronal reconstruction is challenging since it is still
unclear how to delineate a neuronal cell class and which are the best features
to define them by. The morphological neuron characterization represents a
primary source to address anatomical comparisons, morphometric analysis of
cells, or brain modeling. The objectives of this paper are (i) to develop and
integrate a pipeline that goes from morphological feature extraction to
classification and (ii) to assess and compare the accuracy of machine learning
algorithms to classify neuron morphologies. The algorithms were trained on 430
digitally reconstructed neurons subjectively classified into layers and/or
m-types using young and/or adult development state population of the
somatosensory cortex in rats. For supervised algorithms, linear discriminant
analysis provided better classification results in comparison with others. For
unsupervised algorithms, the affinity propagation and the Ward algorithms
provided slightly better results.",['q-bio.NC'],False,,,,"Long-term follow-up of DYT1 dystonia patients treated by deep brain
  stimulation: an open-label study",Morphological Neuron Classification Using Machine Learning
neg-d2-928,2025-03-21,,2503.1722," Infrastructure as Code (IaC) enables scalable and automated IT infrastructure
management but is prone to errors that can lead to security vulnerabilities,
outages, and data loss. While prior research has focused on detecting IaC
issues, Automated Program Repair (APR) remains underexplored, largely due to
the lack of suitable specifications. In this work, we propose InfraFix, the
first technology-agnostic framework for repairing IaC scripts. Unlike prior
approaches, InfraFix allows APR techniques to be guided by diverse information
sources.
  Additionally, we introduce a novel approach for generating repair scenarios,
enabling large-scale evaluation of APR techniques for IaC. We implement and
evaluate InfraFix using an SMT-based repair module and a state inference module
that uses system calls, demonstrating its effectiveness across 254,755 repair
scenarios with a success rate of 95.5%. Our work provides a foundation for
advancing APR in IaC by enabling researchers to experiment with new state
inference and repair techniques using InfraFix and to evaluate their approaches
at scale with our repair scenario generation method.",['cs.SE'],2501.07811," Code generation aims to produce code that fulfills requirements written in
natural languages automatically. Large language Models (LLMs) like ChatGPT have
demonstrated promising effectiveness in this area. Nonetheless, these LLMs
often fail to ensure the syntactic and semantic correctness of the generated
code. Recently, researchers proposed multi-agent frameworks that guide LLMs
with different prompts to analyze programming tasks, generate code, perform
testing in a sequential workflow. However, the performance of the workflow is
not robust as the code generation depends on the performance of each agent. To
address this challenge, we propose CodeCoR, a self-reflective multi-agent
framework that evaluates the effectiveness of each agent and their
collaborations. Specifically, for a given task description, four agents in
CodeCoR generate prompts, code, test cases, and repair advice, respectively.
Each agent generates more than one output and prunes away the low-quality ones.
The generated code is tested in the local environment: the code that fails to
pass the generated test cases is sent to the repair agent and the coding agent
re-generates the code based on repair advice. Finally, the code that passes the
most number of generated test cases is returned to users. Our experiments on
four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,
demonstrate that CodeCoR significantly outperforms existing baselines (e.g.,
CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.",['cs.SE'],False,,,,InfraFix: Technology-Agnostic Repair of Infrastructure as Code,"CodeCoR: An LLM-Based Self-Reflective Multi-Agent Framework for Code
  Generation"
neg-d2-929,2025-03-06,,2503.04983," In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.",['cs.CV'],2503.16194," Autoregressive models have shown remarkable success in image generation by
adapting sequential prediction techniques from language modeling. However,
applying these approaches to images requires discretizing continuous pixel data
through vector quantization methods like VQ-VAE. To alleviate the quantization
errors that existed in VQ-VAE, recent works tend to use larger codebooks.
However, this will accordingly expand vocabulary size, complicating the
autoregressive modeling task. This paper aims to find a way to enjoy the
benefits of large codebooks without making autoregressive modeling more
difficult. Through empirical investigation, we discover that tokens with
similar codeword representations produce similar effects on the final generated
image, revealing significant redundancy in large codebooks. Based on this
insight, we propose to predict tokens from coarse to fine (CTF), realized by
assigning the same coarse label for similar tokens. Our framework consists of
two stages: (1) an autoregressive model that sequentially predicts coarse
labels for each token in the sequence, and (2) an auxiliary model that
simultaneously predicts fine-grained labels for all tokens conditioned on their
coarse labels. Experiments on ImageNet demonstrate our method's superior
performance, achieving an average improvement of 59 points in Inception Score
compared to baselines. Notably, despite adding an inference step, our approach
achieves faster sampling speeds.",['cs.CV'],False,,,,"Leveraging Large Language Models For Scalable Vector Graphics
  Processing: A Review","Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction"
neg-d2-930,2025-03-24,,2503.18668," This paper presents an efficient preference elicitation framework for
uncertain matroid optimization, where precise weight information is
unavailable, but insights into possible weight values are accessible. The core
innovation of our approach lies in its ability to systematically elicit user
preferences, aligning the optimization process more closely with
decision-makers' objectives. By incrementally querying preferences between
pairs of elements, we iteratively refine the parametric uncertainty regions,
leveraging the structural properties of matroids. Our method aims to achieve
the exact optimum by reducing regret with a few elicitation rounds.
Additionally, our approach avoids the computation of Minimax Regret and the use
of Linear programming solvers at every iteration, unlike previous methods.
Experimental results on four standard matroids demonstrate that our method
reaches optimality more quickly and with fewer preference queries than existing
techniques.",['cs.LG'],2503.05079," This work studies the alignment of large language models with preference data
from an imitation learning perspective. We establish a close theoretical
connection between reinforcement learning from human feedback RLHF and
imitation learning (IL), revealing that RLHF implicitly performs imitation
learning on the preference data distribution. Building on this connection, we
propose DIL, a principled framework that directly optimizes the imitation
learning objective. DIL provides a unified imitation learning perspective on
alignment, encompassing existing alignment algorithms as special cases while
naturally introducing new variants. By bridging IL and RLHF, DIL offers new
insights into alignment with RLHF. Extensive experiments demonstrate that DIL
outperforms existing methods on various challenging benchmarks.",['cs.LG'],False,,,,"Geometric Preference Elicitation for Minimax Regret Optimization in
  Uncertainty Matroids",On a Connection Between Imitation Learning and RLHF
neg-d2-931,2025-03-02,,2503.00996," We present a Pure Shape Dynamics (PSD) formulation of General Relativity
(GR), which implements full relationalism by eliminating absolute scale and
external time references from the fundamental description of gravity. Starting
from the Arnowitt-Deser-Misner (ADM) formulation, we derive a decoupled
dynamical system that governs the evolution of the spatial conformal geometry
and relational matter degrees of freedom, while eliminating the total volume
and York time as independent dynamical variables. This results in an autonomous
subsystem describing an unparametrized trajectory in the conformal superspace
of metric and matter configurations, with its evolution encoded in an equation
of state that characterises the intrinsic geometric properties of the curve in
shape space. We show that this equation of state is structurally analogous to
the corresponding PSD description of the Newtonian $N$-body problem,
reinforcing the fundamental similarity between gravity and relational particle
dynamics. Our framework is applied to the homogeneous Bianchi IX cosmological
model, demonstrating that the Janus point evolution through the Big Bang, as
previously found in a symmetry-reduced setting, is a generic feature of the
full inhomogeneous PSD description. This work establishes PSD as a fully scale-
and reparametrization-invariant formulation of classical gravity and lays the
foundation for addressing key open questions that are discussed at the end of
the paper.",['gr-qc'],2501.03125," We revise the role that ultraviolet divergences of quantum fields play in
slow-roll inflation, and discuss the renormalization of cosmological
observables from a space-time perspective, namely the angular power spectrum.
We first derive an explicit expression for the multipole coefficients
$C_{\ell}$ in the Sachs-Wolfe regime in terms of the two-point function of
primordial perturbations. We then analyze the ultraviolet behavior, and point
out that the standard result in the literature is equivalent to a
renormalization of $C_{\ell}$ at zero ``adiabatic'' order. We further argue
that renormalization at second ``adiabatic'' order may be more appropriate from
the viewpoint of standard quantum field theory. This may change significantly
the predictions for $C_{\ell}$, while maintaining scale invariance.",['gr-qc'],False,,,,Pure Shape Dynamics: Relational General Relativity,"On the renormalization of ultraviolet divergences in the inflationary
  angular power spectrum"
neg-d2-932,2025-03-18,,2503.1397," In recent years, there has been extensive research on how to extend
general-purpose programming language semantics with domain-specific modeling
constructs. Two areas of particular interest are (i) universal probabilistic
programming where Bayesian probabilistic models are encoded as programs, and
(ii) differentiable programming where differentiation operators are first class
or differential equations are part of the language semantics. These kinds of
languages and their language constructs are usually studied separately or
composed in restrictive ways. In this paper, we study and formalize the
combination of probabilistic programming constructs, first-class
differentiation, and ordinary differential equations in a higher-order setting.
We propose formal semantics for a core of such differentiable probabilistic
programming language (DPPL), where the type system tracks random computations
and rejects unsafe compositions during type checking. The semantics and its
type system are formalized, mechanized, and proven sound in Agda with respect
to abstract language constructs.",['cs.PL'],2503.1397," In recent years, there has been extensive research on how to extend
general-purpose programming language semantics with domain-specific modeling
constructs. Two areas of particular interest are (i) universal probabilistic
programming where Bayesian probabilistic models are encoded as programs, and
(ii) differentiable programming where differentiation operators are first class
or differential equations are part of the language semantics. These kinds of
languages and their language constructs are usually studied separately or
composed in restrictive ways. In this paper, we study and formalize the
combination of probabilistic programming constructs, first-class
differentiation, and ordinary differential equations in a higher-order setting.
We propose formal semantics for a core of such differentiable probabilistic
programming language (DPPL), where the type system tracks random computations
and rejects unsafe compositions during type checking. The semantics and its
type system are formalized, mechanized, and proven sound in Agda with respect
to abstract language constructs.",['cs.PL'],False,,,,"CoreDPPL: Towards a Sound Composition of Differentiation, ODE Solving,
  and Probabilistic Programming","CoreDPPL: Towards a Sound Composition of Differentiation, ODE Solving,
  and Probabilistic Programming"
neg-d2-933,2025-03-24,,2503.18753," The equivariant behaviour of features is essential in many computer vision
tasks, yet popular self-supervised learning (SSL) methods tend to constrain
equivariance by design. We propose a self-supervised learning approach where
the system learns transformations independently by reconstructing images that
have undergone previously unseen transformations. Specifically, the model is
tasked to reconstruct intermediate transformed images, e.g. translated or
rotated images, without prior knowledge of these transformations. This
auxiliary task encourages the model to develop equivariance-coherent features
without relying on predefined transformation rules. To this end, we apply
transformations to the input image, generating an image pair, and then split
the extracted features into two sets per image. One set is used with a usual
SSL loss encouraging invariance, the other with our loss based on the auxiliary
task to reconstruct the intermediate transformed images. Our loss and the SSL
loss are linearly combined with weighted terms. Evaluating on synthetic tasks
with natural images, our proposed method strongly outperforms all competitors,
regardless of whether they are designed to learn equivariance. Furthermore,
when trained alongside augmentation-based methods as the invariance tasks, such
as iBOT or DINOv2, we successfully learn a balanced combination of invariant
and equivariant features. Our approach performs strong on a rich set of
realistic computer vision downstream tasks, almost always improving over all
baselines.",['cs.CV'],2503.16194," Autoregressive models have shown remarkable success in image generation by
adapting sequential prediction techniques from language modeling. However,
applying these approaches to images requires discretizing continuous pixel data
through vector quantization methods like VQ-VAE. To alleviate the quantization
errors that existed in VQ-VAE, recent works tend to use larger codebooks.
However, this will accordingly expand vocabulary size, complicating the
autoregressive modeling task. This paper aims to find a way to enjoy the
benefits of large codebooks without making autoregressive modeling more
difficult. Through empirical investigation, we discover that tokens with
similar codeword representations produce similar effects on the final generated
image, revealing significant redundancy in large codebooks. Based on this
insight, we propose to predict tokens from coarse to fine (CTF), realized by
assigning the same coarse label for similar tokens. Our framework consists of
two stages: (1) an autoregressive model that sequentially predicts coarse
labels for each token in the sequence, and (2) an auxiliary model that
simultaneously predicts fine-grained labels for all tokens conditioned on their
coarse labels. Experiments on ImageNet demonstrate our method's superior
performance, achieving an average improvement of 59 points in Inception Score
compared to baselines. Notably, despite adding an inference step, our approach
achieves faster sampling speeds.",['cs.CV'],False,,,,"Self-Supervised Learning based on Transformed Image Reconstruction for
  Equivariance-Coherent Feature Representation","Improving Autoregressive Image Generation through Coarse-to-Fine Token
  Prediction"
neg-d2-934,2025-01-02,,2501.01371," Recent Vision-Language Models (VLMs) have demonstrated remarkable
capabilities in visual understanding and reasoning, and in particular on
multiple-choice Visual Question Answering (VQA). Still, these models can make
distinctly unnatural errors, for example, providing (wrong) answers to
unanswerable VQA questions, such as questions asking about objects that do not
appear in the image. To address this issue, we propose CLIP-UP: CLIP-based
Unanswerable Problem detection, a novel lightweight method for equipping VLMs
with the ability to withhold answers to unanswerable questions. By leveraging
CLIP to extract question-image alignment information, CLIP-UP requires only
efficient training of a few additional layers, while keeping the original VLMs'
weights unchanged. Tested across LLaVA models, CLIP-UP achieves
state-of-the-art results on the MM-UPD benchmark for assessing unanswerability
in multiple-choice VQA, while preserving the original performance on other
tasks.",['cs.CV'],2501.13107," We propose Inner Loop Feedback (ILF), a novel approach to accelerate
diffusion models' inference. ILF trains a lightweight module to predict future
features in the denoising process by leveraging the outputs from a chosen
diffusion backbone block at a given time step. This approach exploits two key
intuitions; (1) the outputs of a given block at adjacent time steps are
similar, and (2) performing partial computations for a step imposes a lower
burden on the model than skipping the step entirely. Our method is highly
flexible, since we find that the feedback module itself can simply be a block
from the diffusion backbone, with all settings copied. Its influence on the
diffusion forward can be tempered with a learnable scaling factor from zero
initialization. We train this module using distillation losses; however, unlike
some prior work where a full diffusion backbone serves as the student, our
model freezes the backbone, training only the feedback module. While many
efforts to optimize diffusion models focus on achieving acceptable image
quality in extremely few steps (1-4 steps), our emphasis is on matching best
case results (typically achieved in 20 steps) while significantly reducing
runtime. ILF achieves this balance effectively, demonstrating strong
performance for both class-to-image generation with diffusion transformer (DiT)
and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The
quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP
Image Quality Assessment, ImageReward, and qualitative comparisons. Project
information is available at https://mgwillia.github.io/ilf.",['cs.CV'],False,,,,"CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question
  Answering",Accelerate High-Quality Diffusion Models with Inner Loop Feedback
neg-d2-935,2025-02-20,,2502.14819," A long-standing goal in AI is to build agents that can solve a variety of
tasks across different environments, including previously unseen ones. Two
dominant approaches tackle this challenge: (i) reinforcement learning (RL),
which learns policies through trial and error, and (ii) optimal control, which
plans actions using a learned or known dynamics model. However, their relative
strengths and weaknesses remain underexplored in the setting where agents must
learn from offline trajectories without reward annotations. In this work, we
systematically analyze the performance of different RL and control-based
methods under datasets of varying quality. On the RL side, we consider
goal-conditioned and zero-shot approaches. On the control side, we train a
latent dynamics model using the Joint Embedding Predictive Architecture (JEPA)
and use it for planning. We study how dataset properties-such as data
diversity, trajectory quality, and environment variability-affect the
performance of these approaches. Our results show that model-free RL excels
when abundant, high-quality data is available, while model-based planning
excels in generalization to novel environment layouts, trajectory stitching,
and data-efficiency. Notably, planning with a latent dynamics model emerges as
a promising approach for zero-shot generalization from suboptimal data.",['cs.LG'],2503.07917," Clustering of high-dimensional data sets is a growing need in artificial
intelligence, machine learning and pattern recognition. In this paper, we
propose a new clustering method based on a combinatorial-topological approach
applied to regions of space defined by signs of coordinates (hyperoctants). In
high-dimensional spaces, this approach often reduces the size of the dataset
while preserving sufficient topological features. According to a density
criterion, the method builds clusters of data points based on the partitioning
of a graph, whose vertices represent hyperoctants, and whose edges connect
neighboring hyperoctants under the Levenshtein distance. We call this method
HyperOctant Search Clustering. We prove some mathematical properties of the
method. In order to as assess its performance, we choose the application of
topic detection, which is an important task in text mining. Our results suggest
that our method is more stable under variations of the main hyperparameter, and
remarkably, it is not only a clustering method, but also a tool to explore the
dataset from a topological perspective, as it directly provides information
about the number of hyperoctants where there are data points. We also discuss
the possible connections between our clustering method and other research
fields.",['cs.LG'],False,,,,"Learning from Reward-Free Offline Data: A Case for Planning with Latent
  Dynamics Models","Hyperoctant Search Clustering: A Method for Clustering Data in
  High-Dimensional Hyperspheres"
neg-d2-936,2025-02-08,,2502.05591," Ensuring consistency in distributed systems, especially in adversarial
environments, is a fundamental challenge in theoretical computing. Approximate
Agreement (AA) is a key consensus primitive that allows honest parties to
achieve close but not necessarily identical outputs, even in the presence of
byzantine faults. While the optimal round complexity of synchronous AA on real
numbers is well understood, its extension to tree-structured spaces remains an
open problem.
  We present a protocol achieving AA on trees, with round complexity
$O\left(\frac{\log |V(T)|}{\log \log |V(T)|} \right)$, where $V(T)$ is the set
of vertices in the input tree $T$. Our protocol non-trivially reduces the
problem of AA on trees to AA on real values.
  Additionally, we extend the impossibility results regarding the round
complexity of AA protocols on real numbers to trees: we prove a lower bound of
$\Omega\left(\frac{\log D(T)}{\log \log D(T)} \right)$ rounds, where $D(T)$
denotes the diameter of the tree. This establishes the asymptotic optimality of
our protocol for trees with large diameters $D(T) \in \Theta(|V(T)|)$.",['cs.DC'],2502.14617," Large Language Model (LLM) inference workloads handled by global cloud
providers can include both latency-sensitive and insensitive tasks, creating a
diverse range of Service Level Agreement (SLA) requirements. Managing these
mixed workloads is challenging due to the complexity of the inference stack,
which includes multiple LLMs, hardware configurations, and geographic
distributions. Current optimization strategies often silo these tasks to ensure
that SLAs are met for latency-sensitive tasks, but this leads to significant
under-utilization of expensive GPU resources despite the availability of spot
and on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a
comprehensive LLM serving framework that employs adaptive control knobs at
varying time scales, ensuring SLA compliance while maximizing the utilization
of valuable GPU resources. Short-term optimizations include efficient request
routing to data center regions, while long-term strategies involve scaling GPU
VMs out/in and redeploying models to existing VMs to align with traffic
patterns. These strategies are formulated as an optimization problem for
resource allocation and solved using Integer Linear Programming (ILP). We
perform empirical and simulation studies based on production workload traces
with over 8M requests using four open-source models deployed across three
regions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining
tail latency and satisfying all SLOs, and it reduces the scaling overhead
compared to baselines by up to 80%, confirming the effectiveness of our
proposal. In terms of dollar cost, this can save cloud providers up to $2M over
the course of a month.",['cs.DC'],False,,,,Towards Round-Optimal Approximate Agreement on Trees,"Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing
  Workloads at Scale"
neg-d2-937,2025-02-24,,2502.17305," In this short position paper, we introduce tensor completions and artifacts
and make the case that they are a useful theoretical framework for
understanding certain types of hallucinations and generalizations in language
models.",['cs.CL'],2502.12361," A reliable resume-job matching system helps a company recommend suitable
candidates from a pool of resumes and helps a job seeker find relevant jobs
from a list of job posts. However, since job seekers apply only to a few jobs,
interaction labels in resume-job datasets are sparse. We introduce ConFit v2,
an improvement over ConFit to tackle this sparsity problem. We propose two
techniques to enhance the encoder's contrastive training process: augmenting
job data with hypothetical reference resume generated by a large language
model; and creating high-quality hard negatives from unlabeled resume/job pairs
using a novel hard-negative mining strategy. We evaluate ConFit v2 on two
real-world datasets and demonstrate that it outperforms ConFit and prior
methods (including BM25 and OpenAI text-embedding-003), achieving an average
absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking
and resume-ranking tasks.",['cs.CL'],False,,,,`Generalization is hallucination' through the lens of tensor completions,"ConFit v2: Improving Resume-Job Matching using Hypothetical Resume
  Embedding and Runner-Up Hard-Negative Mining"
neg-d2-938,2025-03-04,,2503.02499," CONTEXT. Attack treesare a recommended threat modeling tool, but there is no
established method to compare them. OBJECTIVE. We aim to establish a method to
compare ""real"" attack trees, based on both the structure of the tree itself and
the meaning of the node labels. METHOD. We define four methods of comparison
(three novel and one established) and compare them to a dataset of attack trees
created from a study run on students (n = 39). These attack trees all follow
from the same scenario, but have slightly different labels. RESULTS. We find
that applying semantic similarity as a means of comparing node labels is a
valid approach. Further, we find that treeedit distance (established) and
radical distance (novel) are themost promising methods of comparison in most
circumstances. CONCLUSION. We show that these two methods are valid as means of
comparing attack trees, and suggest a novel technique for using semantic
similarity to compare node labels. We further suggest that these methods can be
used to compare attack trees in a real-world scenario, and that they can be
used to identify similar attack trees.",['cs.CR'],2501.15145," Current application designers have moved to integrate large language models
(LLMs) into their products. These LLM-integrated applications are vulnerable to
prompt injection vulnerabilities. While attempts have been made to address this
problem by building a detector that can monitor inputs to the LLM and detect
attacks, we find that many detectors are not yet suitable for practical
deployment. To support research in this area, we design the PromptShield
benchmark for evaluating practical prompt injection detectors. We also
construct a new detector, the PromptShield detector, which achieves
significantly better performance at detecting prompt injection attacks than any
prior scheme. Our work suggests that larger models, more training data,
appropriate metrics, and careful curation of training data can contribute to
strong detector performance.",['cs.CR'],False,,,,"Attack Tree Distance: a practical examination of tree difference
  measurement within cyber security",PromptShield: Deployable Detection for Prompt Injection Attacks
neg-d2-939,2025-03-12,,2503.09937," We calculate the spin density matrix for neutral $\rho$ mesons from the
spectral function and thermal shear tensor by Kubo formula in the linear
response theory, which contributes to the $\gamma$ correlator for the CME
search. We derive the spectral function of neutral $\rho$ mesons with
$\rho\pi\pi$ and $\rho\rho\pi\pi$ interactions using the Dyson-Schwinger
equation. The thermal shear tensor contribution is obtained from the Kubo
formula in the linear response theory. We numerically calculate $\rho_{00}-1/3$
and $\mathrm{Re}\rho_{-1,1}$ using the simulation results for the thermal shear
tensor by the hydrodynamical model, which are of the order
$10^{-3}\sim10^{-2}$.",['hep-ph'],2502.01668," The literature establishes that the light fermions contributions to the
decays $H\to Z\gamma$ and $H\to\gamma\gamma$ are negligible since their
coupling with the Higgs is proportional to $m_f$. In the present letter, we
show that although such a conclusion is true for leptons, the light quark
contributions are zero when we consider their non-perturbative effects.",['hep-ph'],False,,,,"Spin density matrix for neutral $\rho$ mesons in a pion gas in linear
  response theory",Light quark contributions to Higgs decays
neg-d2-940,2025-02-20,,2502.14695," The breather solution found by M. Tajiri and Y. Murakami for the Boussinesq
equation is studied analytically. The new parameterization of the solution is
proposed, allowing us to find exactly the existence boundary of the Boussinesq
breather and to show that such a nonlinear excitation emerges from the linear
localized mode of the kink solution corresponding to a shock wave analog in a
crystal. We explicitly find the first integrals, namely the energy and the
field momentum, and faithfully construct the adiabatic invariant for the
Boussinesq breather. As a result, we carry out the quasiclassical quantization
of the nonlinear oscillating solution, obtaining its energy spectrum, i.e., the
energy dependence on the momentum and the number of states, and reveal the
Hamiltonian equations for this particle-like excitation.",['nlin.PS'],2501.15386," We investigate time-independent solutions of a discrete optical cavity model
featuring saturable Kerr nonlinearity, a discrete version of the
Lugiato-Lefever equation. This model supports continuous wave (uniform) and
localized (discrete soliton) solutions. Stationary bright solitons arise
through the interaction of dark and bright uniform states, forming a homoclinic
snaking bifurcation diagram within the Pomeau pinning region. As the system
approaches the anti-continuum limit (weak coupling), this snaking bifurcation
widens and transitions into $\subset$-shaped isolas. We propose a
one-active-site approximation that effectively captures the system's behavior
in this regime. The approximation also provides insight into the stability
properties of soliton states. Numerical continuation and spectral analysis
confirm the accuracy of this semianalytical method, showing excellent agreement
with the full model.",['nlin.PS'],False,,,,"Quasiclassical quantization of the Boussinesq breather emerging from the
  kink localized mode","From Snaking to Isolas: A One-Active-Site Approximation in Discrete
  Optical Cavities"
neg-d2-941,2025-02-03,,2502.01329," Quadratic Programs (QPs) are widely used in the control of walking robots,
especially in Model Predictive Control (MPC) and Whole-Body Control (WBC). In
both cases, the controller design requires the formulation of a QP and the
selection of a suitable QP solver, both requiring considerable time and
expertise. While computational performance benchmarks exist for QP solvers,
studies comparing optimal combinations of computational hardware (HW), QP
formulation, and solver performance are lacking. In this work, we compare dense
and sparse QP formulations, and multiple solving methods on different HW
architectures, focusing on their computational efficiency in dynamic walking of
four legged robots using MPC. We introduce the Solve Frequency per Watt (SFPW)
as a performance measure to enable a cross hardware comparison of the
efficiency of QP solvers. We also benchmark different QP solvers for WBC that
we use for trajectory stabilization in quadrupedal walking. As a result, this
paper provides recommendations for the selection of QP formulations and solvers
for different HW architectures in walking robots and indicates which problems
should be devoted the greater technical effort in this domain in future.",['cs.RO'],2503.1529," In the field of robotics many different approaches ranging from classical
planning over optimal control to reinforcement learning (RL) are developed and
borrowed from other fields to achieve reliable control in diverse tasks. In
order to get a clear understanding of their individual strengths and weaknesses
and their applicability in real world robotic scenarios is it important to
benchmark and compare their performances not only in a simulation but also on
real hardware. The '2nd AI Olympics with RealAIGym' competition was held at the
IROS 2024 conference to contribute to this cause and evaluate different
controllers according to their ability to solve a dynamic control problem on an
underactuated double pendulum system with chaotic dynamics. This paper
describes the four different RL methods submitted by the participating teams,
presents their performance in the swing-up task on a real double pendulum,
measured against various criteria, and discusses their transferability from
simulation to real hardware and their robustness to external disturbances.",['cs.RO'],False,,,,"Benchmarking Different QP Formulations and Solvers for Dynamic
  Quadrupedal Walking","Reinforcement Learning for Robust Athletic Intelligence: Lessons from
  the 2nd 'AI Olympics with RealAIGym' Competition"
neg-d2-942,2025-01-21,,2501.12218," Point tracking in videos is a fundamental task with applications in robotics,
video editing, and more. While many vision tasks benefit from pre-trained
feature backbones to improve generalizability, point tracking has primarily
relied on simpler backbones trained from scratch on synthetic data, which may
limit robustness in real-world scenarios. Additionally, point tracking requires
temporal awareness to ensure coherence across frames, but using
temporally-aware features is still underexplored. Most current methods often
employ a two-stage process: an initial coarse prediction followed by a
refinement stage to inject temporal information and correct errors from the
coarse stage. These approach, however, is computationally expensive and
potentially redundant if the feature backbone itself captures sufficient
temporal information.
  In this work, we introduce Chrono, a feature backbone specifically designed
for point tracking with built-in temporal awareness. Leveraging pre-trained
representations from self-supervised learner DINOv2 and enhanced with a
temporal adapter, Chrono effectively captures long-term temporal context,
enabling precise prediction even without the refinement stage. Experimental
results demonstrate that Chrono achieves state-of-the-art performance in a
refiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, among
common feature backbones used in point tracking as well as DINOv2, with
exceptional efficiency. Project page: https://cvlab-kaist.github.io/Chrono/",['cs.CV'],2502.14282," In the field of MLLM-based GUI agents, compared to smartphones, the PC
scenario not only features a more complex interactive environment, but also
involves more intricate intra- and inter-app workflows. To address these
issues, we propose a hierarchical agent framework named PC-Agent. Specifically,
from the perception perspective, we devise an Active Perception Module (APM) to
overcome the inadequate abilities of current MLLMs in perceiving screenshot
content. From the decision-making perspective, to handle complex user
instructions and interdependent subtasks more effectively, we propose a
hierarchical multi-agent collaboration architecture that decomposes
decision-making processes into Instruction-Subtask-Action levels. Within this
architecture, three agents (i.e., Manager, Progress and Decision) are set up
for instruction decomposition, progress tracking and step-by-step
decision-making respectively. Additionally, a Reflection agent is adopted to
enable timely bottom-up error feedback and adjustment. We also introduce a new
benchmark PC-Eval with 25 real-world complex instructions. Empirical results on
PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task
success rate over previous state-of-the-art methods. The code is available at
https://github.com/X-PLUG/MobileAgent/tree/main/PC-Agent.",['cs.CV'],False,,,,Exploring Temporally-Aware Features for Point Tracking,"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex
  Task Automation on PC"
neg-d2-943,2025-01-19,,2501.1113," This study proposes a new full-field approach for modeling grain boundary
pinning by second phase particles in two-dimensional polycrystals. These
particles are of great importance during thermomechanical treatments, as they
produce deviations from the microstructural evolution that the alloy produces
in the absence of particles. This phenomenon, well-known as Smith-Zener
pinning, is widely used by metallurgists to control the grain size during the
metal forming process of many alloys. Predictive tools are then needed to
accurately model this phenomenon. This article introduces a new methodology for
the simulation of microstructural evolutions subjected to the presence of
second phase particles. The methodology employs a Lagrangian 2D front-tracking
methodology, while the particles are modeled using discretized circular shapes
or pinning nodes. The evolution of the particles can be considered and modeled
using a constant velocity of particle shrinking. This approach has the
advantages of improving the limited description made of the phenomenon in
vertex approaches, to be usable for a wide range of second-phase particle sizes
and to improve calculation times compared to front-capturing type approaches.",['cs.CE'],2501.1113," This study proposes a new full-field approach for modeling grain boundary
pinning by second phase particles in two-dimensional polycrystals. These
particles are of great importance during thermomechanical treatments, as they
produce deviations from the microstructural evolution that the alloy produces
in the absence of particles. This phenomenon, well-known as Smith-Zener
pinning, is widely used by metallurgists to control the grain size during the
metal forming process of many alloys. Predictive tools are then needed to
accurately model this phenomenon. This article introduces a new methodology for
the simulation of microstructural evolutions subjected to the presence of
second phase particles. The methodology employs a Lagrangian 2D front-tracking
methodology, while the particles are modeled using discretized circular shapes
or pinning nodes. The evolution of the particles can be considered and modeled
using a constant velocity of particle shrinking. This approach has the
advantages of improving the limited description made of the phenomenon in
vertex approaches, to be usable for a wide range of second-phase particle sizes
and to improve calculation times compared to front-capturing type approaches.",['cs.CE'],False,,,,"Efficient and accurate simulation of the Smith-Zener pinning mechanism
  during grain growth using a front-tracking numerical framework","Efficient and accurate simulation of the Smith-Zener pinning mechanism
  during grain growth using a front-tracking numerical framework"
neg-d2-944,2025-01-14,,2501.07843," Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$
possesses novel triple-$\mathbf{q}$ magnetic order instead of conventional
single-$\mathbf{q}$ zigzag order. Here we present dedicated experiments in
search for distinct properties expected of the triple-$\mathbf{q}$ order,
namely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking
fields and fictitious magnetic fields generated by the spin vorticity. In
structurally pristine single crystals, we show that $C_3$ symmetry-breaking
in-plane uniaxial strains do not affect the order's magnetic neutron
diffraction signals. We further show that $\mathbf{c}$-axis propagating light
exhibits large Faraday rotations in the ordered state due to the spin
vorticity, the sign of which can be trained via the system's ferrimagnetic
moment. These results are in favor of the triple-$\mathbf{q}$ order in
Na$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",['cond-mat.str-el'],2501.16792," Landau levels in certain models are known to protrude into the zero-field
energy gap. These are known as anomalous Landau levels (ALLs). We study whether
ALLs can lead to in-gap quantum oscillation in the absence of a zero-field
Fermi surface. Focusing on two-dimensional multi-band low-energy models of
electrons with continuous rotation symmetry, we show that an effective-band
description, akin to the semiclassical treatment of Landau level problems in
metals, can be used to predict the Landau level spectrum, including possible
ALLs. This description then predicts quantum oscillation for certain insulating
models, which we demonstrate through numerical calculations.",['cond-mat.str-el'],False,,,,"Robust triple-q magnetic order with trainable spin vorticity in
  Na$_2$Co$_2$TeO$_6$","Anomalous Landau levels and quantum oscillation in rotation-invariant
  insulators"
neg-d2-945,2025-03-16,,2503.1265," Motivated by experiments on rhombohedral tetralayer graphene showing signs of
superconductivity emerging from a valley-polarized normal state, we here
analyze theoretically how scanning tunneling spectroscopy can be used to probe
the superconducting order parameter of the system. To describe different
pairing scenarios on equal footing, we develop a microscopic tunneling approach
that can capture arbitrary, including finite-momentum, superconducting order
parameters and low-symmetry normal-state Hamiltonians. Our analysis shows that
the broken time-reversal symmetry in a single valley leads to unique features
in the weak-tunneling regime that are different for commensurate and
incommensurate Cooper pair momenta. We further uncover an unconventional
spatial dependence of the Andreev conductance, allowing to distinguish between
three topologically distinct classes of single-$\mathbf{q}$ pairing states in
the system, and compute the signatures of a competing translational-symmetry
breaking three-$\mathbf{q}$ ''moir\'e superconductor''.",['cond-mat.supr-con'],2503.17241," Superconducting electronics represents a promising technology, offering not
only efficient integration with quantum computing systems, but also the
potential for significant power reduction in high-performance computing.
Nonetheless, the lack of superconducting memories better than conventional
metal-oxide semiconductor (CMOS) memories represent a major obstacle towards
the development of computing systems entirely based on superconducting
electronics. In this work, we combine the emerging concept of gate-controlled
supercurrent (GCS) with the well-established mechanism of charge-trapping
memory to demonstrate a novel, highly scalable, voltage-controlled and
non-volatile superconducting memory. GCS denotes the observation that the
supercurrent in a superconducting constriction can be suppressed by applying a
certain gate voltage (VG) to it. Our findings show that charge trapping within
the gate dielectric, here sapphire, influences the voltage threshold needed to
suppress the supercurrent. We demonstrate reliable reading and reversible
writing of two distinct charge-trapping memory states, associated with
different supercurrent values. Based on our memory device demonstrator, we
discuss its integration into a NOT AND (NAND) gate layout, outlining the
significant improvements offered by this novel memory concept over other
existing NAND memory technologies.",['cond-mat.supr-con'],False,,,,"Probing superconductivity with tunneling spectroscopy in rhombohedral
  graphene","Superconducting non-volatile memory based on charge trapping and
  gate-controlled superconductivity"
neg-d2-946,2025-02-26,,2502.19256," Observations of stars other than the Sun are sensitive to oscillations of
only low degree. Many are high-order acoustic modes. Acoustic frequencies of
main-sequence stars, for example, satisfy a well-known pattern, which some
astronomers have adopted even for red-giant stars. That is not wise, because
the internal structures of these stars can be quite different from those on the
Main Sequence, which is populated by stars whose structure is regular. Here I
report on pondering this matter, and point out two fundamental deviations from
the commonly adopted relation. There are aspects of the regular relation that
are connected in a simple way to gross properties of the star, such as the
dependence of the eigenfrequencies on the linear combination
$n+\textstyle{\frac {1}{2}}l$ of the order $n$ and degree $l$, which is
characteristic of a regular spherical acoustic cavity. That is not a feature of
red-giant frequencies, because, as experienced by the waves, red-giant stars
appear to have (phantom) singular centres, which substantially modify the
propagation of waves. That requires a generalization of the eigenfrequency
relation, which I present here. When fitted to the observed frequencies of the
Sun, the outcome is consistent with the Sun being round, with no singularity in
the core. That is hardly novel, but at least it provides some assurance that
our understanding of stellar acoustic wave dynamics is on a sound footing.",['astro-ph.SR'],2503.16204," The existence of the binary system SDSS J1257+5428 has been described as
paradoxical. Here we investigate under which conditions SDSS J1257+5428 could
be understood as a descendant of a cataclysmic variable with an evolved donor
star, which is a scenario that has never been explored in detail. We used the
BSE code for pre-common-envelope (CE) evolution and the MESA code for post-CE
evolution to run binary evolution simulations and searched for potential
formation pathways for SDSS J1257+5428 that lead to its observed
characteristics. For the post-CE evolution, we adopted a boosted version of the
CARB model. We find that SDSS J1257+5428 can be explained as a
post-cataclysmic-variable system if (i) the progenitor of the extremely
low-mass WD was initially a solar-type star that evolved into a subgiant before
the onset of mass transfer and underwent hydrogen shell flashes after the mass
transfer stopped, (ii) the massive WD was highly or entirely rejuvenated during
the cataclysmic variable evolution, and (iii) magnetic braking was strong
enough to make the evolution convergent. In this case, the torques due to
magnetic braking need to be stronger than those provided by the CARB model by a
factor of ${\sim100}$. We conclude that SDSS J1257+5428 can be reasonably well
explained as having originated from a cataclysmic variable that hosted an
evolved donor star and should no longer be regarded as paradoxical. If our
formation channel is correct, our findings provide further support that
stronger magnetic braking acts on progenitors of (i) close detached WD
binaries, (ii) close detached millisecond pulsar with extremely low-mass WDs,
(iii) AM CVn binaries, and (iv) ultra-compact X-ray binaries, in comparison to
the magnetic braking strength required to explain binaries hosting
main-sequence stars and single main-sequence stars.",['astro-ph.SR'],False,,,,Some musings on erythrogigantoacoustics,"Resolution of a paradox: SDSS J1257+5428 can be explained as a
  descendant of a cataclysmic variable with an evolved donor"
neg-d2-947,2025-03-10,,2503.07142," We compare the performance of a transition-based parser in regards to
different annotation schemes. We pro-pose to convert some specific syntactic
constructions observed in the universal dependency treebanks into a so-called
more standard representation and to evaluate parsing performances over all the
languages of the project. We show that the ``standard'' constructions do not
lead systematically to better parsing performance and that the scores vary
considerably according to the languages.",['cs.CL'],2502.14451," Natural Language Generation (NLG) popularity has increased owing to the
progress in Large Language Models (LLMs), with zero-shot inference
capabilities. However, most neural systems utilize decoder-only causal
(unidirectional) transformer models, which are effective for English but may
reduce the richness of languages with less strict word order, subject omission,
or different relative clause attachment preferences. This is the first work
that analytically addresses optimal text generation order for non-causal
language models. We present a novel Viterbi algorithm-based methodology for
maximum likelihood word order estimation. We analyze the non-causal
most-likelihood order probability for NLG in Spanish and, then, the probability
of generating the same phrases with Spanish causal NLG. This comparative
analysis reveals that causal NLG prefers English-like SVO structures. We also
analyze the relationship between optimal generation order and causal
left-to-right generation order using Spearman's rank correlation. Our results
demonstrate that the ideal order predicted by the maximum likelihood estimator
is not closely related to the causal order and may be influenced by the
syntactic structure of the target sentence.",['cs.CL'],False,,,,"A Systematic Comparison of Syntactic Representations of Dependency
  Parsing","Optimal word order for non-causal text generation with Large Language
  Models: the Spanish case"
neg-d2-948,2025-01-16,,2501.09557," Realizing a shared responsibility between providers and consumers is critical
to manage the sustainability of HPC. However, while cost may motivate
efficiency improvements by infrastructure operators, broader progress is
impeded by a lack of user incentives. We conduct a survey of HPC users that
reveals fewer than 30 percent are aware of their energy consumption, and that
energy efficiency is among users' lowest priority concerns. One explanation is
that existing pricing models may encourage users to prioritize performance over
energy efficiency. We propose two transparent multi-resource pricing schemes,
Energy- and Carbon-Based Accounting, that seek to change this paradigm by
incentivizing more efficient user behavior. These two schemes charge for
computations based on their energy consumption or carbon footprint,
respectively, rewarding users who leverage efficient hardware and software. We
evaluate these two pricing schemes via simulation, in a prototype, and a user
study.",['cs.DC'],2503.02356," Long context fine-tuning of large language models(LLMs) involves training on
datasets that are predominantly composed of short sequences and a small
proportion of longer sequences. However, existing approaches overlook this
long-tail distribution and employ training strategies designed specifically for
long sequences. Moreover, these approaches also fail to address the challenges
posed by variable sequence lengths during distributed training, such as load
imbalance in data parallelism and severe pipeline bubbles in pipeline
parallelism. These issues lead to suboptimal training performance and poor GPU
resource utilization. To tackle these problems, we propose a chunk-centric
training method named ChunkFlow. ChunkFlow reorganizes input sequences into
uniformly sized chunks by consolidating short sequences and splitting longer
ones. This approach achieves optimal computational efficiency and balance among
training inputs. Additionally, ChunkFlow incorporates a state-aware chunk
scheduling mechanism to ensure that the peak memory usage during training is
primarily determined by the chunk size rather than the maximum sequence length
in the dataset. Integrating this scheduling mechanism with existing pipeline
scheduling algorithms further enhances the performance of distributed training.
Experimental results demonstrate that, compared with Megatron-LM, ChunkFlow can
be up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we
believe that ChunkFlow serves as an effective solution for a broader range of
scenarios, such as long context continual pre-training, where datasets contain
variable-length sequences.",['cs.DC'],False,,,,Core Hours and Carbon Credits: Incentivizing Sustainability in HPC,Efficient Long Context Fine-tuning with Chunk Flow
neg-d2-949,2025-01-23,,2501.13492," Spiking neural networks are emerging as a promising energy-efficient
alternative to traditional artificial neural networks due to their spike-driven
paradigm. However, recent research in the SNN domain has mainly focused on
enhancing accuracy by designing large-scale Transformer structures, which
typically rely on substantial computational resources, limiting their
deployment on resource-constrained devices. To overcome this challenge, we
propose a quantized spike-driven Transformer baseline (QSD-Transformer), which
achieves reduced resource demands by utilizing a low bit-width parameter.
Regrettably, the QSD-Transformer often suffers from severe performance
degradation. In this paper, we first conduct empirical analysis and find that
the bimodal distribution of quantized spike-driven self-attention (Q-SDSA)
leads to spike information distortion (SID) during quantization, causing
significant performance degradation. To mitigate this issue, we take
inspiration from mutual information entropy and propose a bi-level optimization
strategy to rectify the information distribution in Q-SDSA. Specifically, at
the lower level, we introduce an information-enhanced LIF to rectify the
information distribution in Q-SDSA. At the upper level, we propose a
fine-grained distillation scheme for the QSD-Transformer to align the
distribution in Q-SDSA with that in the counterpart ANN. By integrating the
bi-level optimization strategy, the QSD-Transformer can attain enhanced energy
efficiency without sacrificing its high-performance advantage. For instance,
when compared to the prior SNN benchmark on ImageNet, the QSD-Transformer
achieves 80.3% top-1 accuracy, accompanied by significant reductions of
6.0$\times$ and 8.1$\times$ in power consumption and model size, respectively.
Code is available at https://github.com/bollossom/QSD-Transformer.",['cs.CV'],2501.1215," Deferred neural rendering (DNR) is an emerging computer graphics pipeline
designed for high-fidelity rendering and robotic perception. However, DNR
heavily relies on datasets composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the rendering
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
rendering. We first propose a novel view selector for deferred neural rendering
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
rendering. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity rendering results. We conduct detailed experiments and
ablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness
of DNRSelect. The code will be released.",['cs.CV'],False,,,,Quantized Spike-driven Transformer,DNRSelect: Active Best View Selection for Deferred Neural Rendering
neg-d2-950,2025-02-28,,2502.21302," We investigate the impact of quantum well (QW) thickness on efficiency loss
in c-plane InGaN/GaN LEDs using a small-signal electroluminescence (SSEL)
technique. Multiple mechanisms related to efficiency loss are independently
examined, including injection efficiency, carrier density vs. current density
relationship, phase space filling (PSF), quantum confined stark effect (QCSE),
and Coulomb enhancement. An optimal QW thickness of around 2.7 nm in these
InGaN/GaN LEDs was determined for quantum wells having constant In composition.
Despite better control of deep-level defects and lower carrier density at a
given current density, LEDs with thin QWs still suffer from an imbalance of
enhancement effects on the radiative and intrinsic Auger-Meitner recombination
coefficients. The imbalance of enhancement effects results in a decline in
internal quantum efficiency (IQE) and radiative efficiency with decreasing QW
thickness at low current density in LEDs with QW thicknesses below 2.7 nm. We
also investigate how LED modulation bandwidth varies with quantum well
thickness, identifying the key trends and their implications for device
performance.",['physics.app-ph'],2501.01815," A mechanical model of a laminated composite ring on a nonreciprocal elastic
foundation is a valuable engineering tool during the early design stages of
various applications, such as non-pneumatic wheels, flexible bearings,
expandable tubulars in oil wells, and vascular stents interacting with blood
vessel linings, especially under non-axisymmetric loadings. Despite its
importance, limited research has focused on the interaction between laminated
composite rings and nonreciprocal elastic foundations. Moreover, no
quantitative studies have yet explored the influence of foundation stiffness on
the ring deformation. This work aims to develop an analytical framework for a
laminated composite ring supported by a nonreciprocal elastic foundation under
non-axisymmetric loading conditions. The model generates a design map that
correlates the foundation stiffness with the ring deformation, accounting for
ring dimensions, laminate layup architecture, and lamina anisotropy. The
closed-form solution provides an efficient design tool for analyzing
non-axisymmetric and nonuniform loadings at a low computational cost. The
resulting design map provides a valuable resource for exploring the interaction
between the nonreciprocal foundation and the laminated ring. The proposed
analytical framework and design map hold broad potential applications in
automotive, mechanical, civil, and biomedical engineering fields.",['physics.app-ph'],False,,,,"Impact of Quantum Well Thickness on Efficiency Loss in InGaN/GaN LEDs:
  Challenges for Thin-Well Designs","Analytical modeling of laminated composite rings on nonreciprocal
  elastic foundations under non-axisymmetric loading"
neg-d2-951,2025-02-04,,2502.02148," As new Model-X knockoff construction techniques are developed, primarily
concerned with determining the correct conditional distribution from which to
sample, we focus less on deriving the correct multivariate distribution and
instead ask if ``perfect'' knockoffs can be constructed using linear algebra.
Using mean absolute correlation between knockoffs and features as a measure of
quality, we do produce knockoffs that are pseudo-perfect, however, the
optimization algorithm is computationally very expensive. We outline a series
of methods to significantly reduce the computation time of the algorithm.",['stat.ME'],2502.11072," This work presents a novel simulation-based approach for constructing
confidence regions in parametric models, which is particularly suited for
generative models and situations where limited data and conventional asymptotic
approximations fail to provide accurate results. The method leverages the
concept of data depth and depends on creating random hyper-rectangles, i.e.
boxes, in the sample space generated through simulations from the model,
varying the input parameters. A probabilistic acceptance rule allows to
retrieve a Depth-Confidence Distribution for the model parameters from which
point estimators as well as calibrated confidence sets can be read-off. The
method is designed to address cases where both the parameters and test
statistics are multivariate.",['stat.ME'],False,,,,Can linear algebra create perfect knockoffs?,Box Confidence Depth: simulation-based inference with hyper-rectangles
neg-d2-952,2025-02-24,,2502.16893," Minimally stable site (MSS) clusters play a dominant role in shaping
avalanches in the self-organized critical (SOC) systems. The manipulation of
MSS clusters through local smoothings (diffusion) alter the MSS landscape,
suppressing rare avalanches and postponing them until they manifest as spanning
avalanches. By leveraging the Inverse Ising problem, we uncover a duality
between diffusive sandpiles and equilibrium statistical physics. Our analysis
reveals an emergent magnetic instability in the dual Ising model, coinciding
with the formation of spanning avalanches and marking a transition to a
correlated percolation regime. At this point, the MSS loop soups exhibit
fractal self-similarity and power-law distributions, while the effective
pairwise interactions in the dual system vanish, signaling a magnetic
transition characterized by abrupt changes in magnetization and spin
susceptibility. Crucially, we show that diffusion fundamentally reshapes
avalanche dynamics: the spatial anti-correlations of MSSs in standard SOC
systems transform into positive correlations when diffusion is introduced.
These findings bridge self-organized criticality, percolation theory, and
equilibrium phase transitions, shedding new light on emergent criticality and
large-scale correlations in non-equilibrium systems.",['cond-mat.stat-mech'],2502.19561," This work explores the thermodynamic performance of a quantum Stirling heat
engine implemented with an anisotropic spin-1 Heisenberg dimer as the working
medium. Using the Hamiltonian of the system, we analyze the interplay of
anisotropy, magnetic field, and exchange interactions and their influence on
the energy spectrum and the quantum level crossing. Our results reveal that
double-degenerate point (DDP) and a triple-degenerate point (TDP) play pivotal
roles in shaping the operational regimes and efficiency of the quantum Stirling
engine. At those points, the Carnot efficiency reaches higher work output and
enhanced stability, making it a robust candidate for optimal thermodynamic
performance. These findings highlight the potential of anisotropic spin systems
as viable platforms for quantum heat engines and contribute to advancing the
field of quantum thermodynamics.",['cond-mat.stat-mech'],False,,,,Emergent Dynamical Ising Transition in Diffusive Sandpiles,"Quantum Level-Crossing Induced by Anisotropy in Spin-1 Heisenberg
  Dimers: Applications to Quantum Stirling Engines"
neg-d2-953,2025-01-06,,2501.02875," Mutation testing may be used to guide test case generation and as a technique
to assess the quality of test suites. Despite being used frequently, mutation
testing is not so commonly applied in the mobile world. One critical challenge
in mutation testing is dealing with its computational cost. Generating mutants,
running test cases over each mutant, and analyzing the results may require
significant time and resources. This research aims to contribute to reducing
Android mutation testing costs. It implements mutation testing operators
(traditional and Android-specific) according to mutant schemata (implementing
multiple mutants into a single code file). It also describes an Android
mutation testing framework developed to execute test cases and determine
mutation scores. Additional mutation operators can be implemented in JavaScript
and easily integrated into the framework. The overall approach is validated
through case studies showing that mutant schemata have advantages over the
traditional mutation strategy (one file per mutant). The results show mutant
schemata overcome traditional mutation in all evaluated aspects with no
additional cost: it takes 8.50% less time for mutant generation, requires
99.78% less disk space, and runs, on average, 6.45% faster than traditional
mutation. Moreover, considering sustainability metrics, mutant schemata have
8,18% less carbon footprint than traditional strategy.",['cs.SE'],2502.09771," This paper introduces DSrepair, a knowledge-enhanced program repair method
designed to repair the buggy code generated by LLMs in the data science domain.
DSrepair uses knowledge graph based RAG for API knowledge retrieval as well as
bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to
enable knowledge graph based API retrieval, we construct DS-KG (Data Science
Knowledge Graph) for widely used data science libraries. For bug knowledge
enrichment, we employ an abstract syntax tree (AST) to localize errors at the
AST node level. DSrepair's effectiveness is evaluated against five
state-of-the-art LLM-based repair baselines using four advanced LLMs on the
DS-1000 dataset. The results show that DSrepair surpasses all five baselines.
Specifically, when compared to the second-best baseline, DSrepair demonstrates
significant improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code
snippets for each of the four evaluated LLMs, respectively. Additionally, it
achieves greater efficiency, reducing the number of tokens required per code
task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.",['cs.SE'],False,,,,METFORD -- Mutation tEsTing Framework fOR anDroid,Knowledge-Enhanced Program Repair for Data Science Code
neg-d2-954,2025-02-03,,2502.01532," Federated Learning has emerged as a promising approach to train machine
learning models on decentralized data sources while preserving data privacy.
This paper proposes a new federated approach for Naive Bayes (NB)
classification, assuming discrete variables. Our approach federates a
discriminative variant of NB, sharing meaningless parameters instead of
conditional probability tables. Therefore, this process is more reliable
against possible attacks. We conduct extensive experiments on 12 datasets to
validate the efficacy of our approach, comparing federated and non-federated
settings. Additionally, we benchmark our method against the generative variant
of NB, which serves as a baseline for comparison. Our experimental results
demonstrate the effectiveness of our method in achieving accurate
classification.",['cs.LG'],2502.02414," Although deep models have been widely explored in solving partial
differential equations (PDEs), previous works are primarily limited to data
only with up to tens of thousands of mesh points, far from the million-point
scale required by industrial simulations that involve complex geometries. In
the spirit of advancing neural PDE solvers to real industrial applications, we
present Transolver++, a highly parallel and efficient neural solver that can
accurately solve PDEs on million-scale geometries. Building upon previous
advancements in solving PDEs by learning physical states via Transolver,
Transolver++ is further equipped with an extremely optimized parallelism
framework and a local adaptive mechanism to efficiently capture eidetic
physical states from massive mesh points, successfully tackling the thorny
challenges in computation and physics learning when scaling up input mesh size.
Transolver++ increases the single-GPU input capacity to million-scale points
for the first time and is capable of continuously scaling input size in linear
complexity by increasing GPUs. Experimentally, Transolver++ yields 13% relative
promotion across six standard PDE benchmarks and achieves over 20% performance
gain in million-scale high-fidelity industrial simulations, whose sizes are
100$\times$ larger than previous benchmarks, covering car and 3D aircraft
designs.",['cs.LG'],False,,,,Federated Learning with Discriminative Naive Bayes Classifier,"Transolver++: An Accurate Neural Solver for PDEs on Million-Scale
  Geometries"
neg-d2-955,2025-03-06,,2503.04324," In this study, we investigated a phenomenon that one intuitively would assume
does not exist: self-citations on the paper basis. Actually, papers citing
themselves do exist in the Web of Science (WoS) database. In total, we obtained
44,857 papers that have self-citation relations in the WoS raw dataset. In
part, they are database artefacts but in part they are due to papers citing
themselves in the conclusion or appendix. We also found cases where paper
self-citations occur due to publisher-made highlights promoting and citing the
paper. We analyzed the self-citing papers according to selected metadata. We
observed accumulations of the number of self-citing papers across publication
years. We found a skewed distribution across countries, journals, authors,
fields, and document types. Finally, we discuss the implications of paper
self-citations for bibliometric indicators.",['cs.DL'],2502.11923," This editorial explores the significance of research visibility within the
evolving landscape of academic communication, mainly focusing on the role of
search engines as online meta-markets shaping the impact of research. With the
rapid expansion of scientific output and the increasing reliance on
algorithm-driven platforms such as Google and Google Scholar, the online
visibility of scholarly work has become an essential factor in determining its
reach and influence. The need for more rigorous research into academic search
engine optimization (A-SEO), a field still in its infancy despite its growing
relevance, is also discussed, highlighting key challenges in the field,
including the lack of robust research methodologies, the skepticism within the
academic community regarding the commercialization of science, and the need for
standardization in reporting and measurement techniques. This editorial thus
invites a multidisciplinary dialogue on the future of research visibility, with
significant implications for academic publishing, science communication,
research evaluation, and the global scientific ecosystem.",['cs.DL'],False,,,,Paper self-citation: An unexplored phenomenon,Research on Research Visibility
neg-d2-956,2025-02-12,,2502.08263," We study the twofold structure of the vector space of Drinfeld quasi-modular
forms for Hecke congruence subgroups. We provide representations as polynomials
in the false Eisenstein series with coefficients in the space of Drinfeld
modular forms, and as sums of hyperderivatives of Drinfeld modular forms
(whenever possible). Moreover, we offer a well-defined formula (i.e.
independent of the chosen representatives) for Hecke operators, and prove that
they preserve the space of Drinfeld quasi-modular forms of given weight and
type.",['math.NT'],2502.09359," In the theory of integral weight harmonic Maass forms of manageable growth,
two key differential operators, the Bol operator and the shadow operator, play
a fundamental role. Harmonic Maass forms of manageable growth canonically split
into two parts, and each operator controls one of these parts. A third
operator, called the flipping operator, exchanges the role of these two parts.
Maass--Poincar\'e series (of parabolic type) form a convenient basis of
negative weight harmonic Maass forms of manageable growth, and flipping has the
effect of negating an index. Recently, there has been much interest in locally
harmonic Maass forms defined by the first author, Kane, and Kohnen. These are
lifts of Poincar\'e series of hyperbolic type, and are intimately related to
the Shimura and Shintani lifts. In this note, we prove that a similar property
holds for the flipping operator applied to these Poincar\'e series.",['math.NT'],False,,,,Drinfeld Quasi-Modular Forms of Higher Level,Flipping operators and locally harmonic Maass forms
neg-d2-957,2025-01-15,,2501.0863," The paper is concerned with the effect of the spatio-temporal heterogeneity
on the principal eigenvalue of some linear time-periodic parabolic system.
Various asymptotic behaviors of the principal eigenvalue and its monotonicity,
as a function of the diffusion rate and frequency, are first derived. In
particular, some singular behaviors of the principal eigenvalues are observed
when both diffusion rate and frequency approach zero, with some scalar
time-periodic Hamilton-Jacobi equation as the limiting equation. Furthermore,
we completely classify the topological structures of the level sets for the
principal eigenvalues in the plane of frequency and diffusion rate. Our results
not only generalize most of the findings in [S. Liu and Y. Lou, J. Funct.
Anal., 282 (2022), 109338] for scalar periodic-parabolic operators, but also
reveal more rich global information, for time-periodic parabolic systems, on
the dependence of the principal eigenvalues upon the spatio-temporal
heterogeneity.",['math.AP'],2501.16068," Consider a second-order elliptic operator $L$ in the half-plane $\mathbb R
\times (0, \infty)$ with coefficients depending only on the second coordinate.
The Poisson kernel for $L$ is used in the representation of positive
$L$-harmonic functions, that is, solutions of $L u = 0$. In probabilistic
terms, the Poisson kernel is the density function of the distribution of the
diffusion in $\mathbb R \times (0, \infty)$ with generator $L$ at the hitting
time of the boundary. We prove that the Poisson kernel for $L$ is bell-shaped:
its $n$th derivative changes sign $n$ times. In particular, it is unimodal and
it has two inflection points (it is concave, then convex, then concave again).",['math.AP'],False,,,,"On principal eigenvalues of linear time-periodic parabolic systems:
  symmetric mutation case",Poisson kernels on the half-plane are bell-shaped
neg-d2-958,2025-02-21,,2502.15486," In this note, we present an alternative proof of a quantified Tauberian
theorem for vector-valued sequences first proved in \cite{Sei15_Tauberian}. The
theorem relates the decay rate of a bounded sequence with properties of a
certain boundary function. We present a slightly strengthened version of this
result, and illustrate how it can be used to obtain quantified versions of the
Katznelson--Tzafriri theorem as well as results on Ritt operators.",['math.FA'],2501.10101," In this paper, we establish sharp bounds for a family of Kantorovich-type
neural network operators within the general frameworks of Sobolev-Orlicz and
Orlicz spaces. We establish both strong (in terms of the Luxemburg norm) and
weak (in terms of the modular functional) estimates, using different
approaches. The strong estimates are derived for spaces generated by
$\varphi$-functions that are $N$-functions or satisfy the
$\Delta^\prime$-condition. Such estimates also lead to convergence results with
respect to the Luxemburg norm in several instances of Orlicz spaces, including
the exponential case. Meanwhile, the weak estimates are achieved under less
restrictive assumptions on the involved $\varphi$-function. To obtain these
results, we introduce some new tools and techniques in Orlicz spaces. Central
to our approach is the Orlicz Minkowski inequality, which allows us to obtain
unified strong estimates for the operators. We also present a weak (modular)
version of this inequality holding under weaker conditions. Additionally, we
introduce a novel notion of discrete absolute $\varphi$-moments of hybrid type,
and we employ the Hardy-Littlewood maximal operator within Orlicz spaces for
the asymptotic analysis. Furthermore, we introduce the new space
$\mathcal{W}^{1,\varphi}(I)$, which is embedded in the Sobolev-Orlicz space
$W^{1,\varphi}(I)$ and modularly dense in $L^\varphi(I)$. This allows to
achieve asymptotic estimates for a wider class of $\varphi$-functions,
including those that do not meet the $\Delta_2$-condition. For the extension to
the whole Orlicz-setting, we generalize a Sobolev-Orlicz density result given
by H. Musielak using Steklov functions, providing a modular counterpart.
Finally, we explore the relationships between weak and strong Orlicz Lipschitz
classes, providing qualitative results for the rate of convergence of the
operators.",['math.FA'],False,,,,Tauberian theorems for sequences and the Katznelson--Tzafriri theorem,"Strong and weak sharp bounds for Neural Network Operators in
  Sobolev-Orlicz spaces and their quantitative extensions to Orlicz spaces"
neg-d2-959,2025-03-18,,2503.13889," Pairing of nucleons plays a key role in solving various nuclear physics
problems. We investigate the probable effects of pairing correlations on the
calculated Gamow-Teller (GT) strength distributions and the associated
$\beta$-decay half-lives. Computations are performed for a total of 35 fp-shell
nuclei using the proton-neutron quasiparticle random phase approximation
(pn-QRPA) model. The nuclei were selected because of their importance in
various astrophysical environments. Pairing gaps are one of the key parameters
in the pn-QRPA model to compute GT transitions. We employed three different
values of the pairing gaps obtained from three different empirical formulae in
our calculation. The GT strength distributions changed significantly as the
pairing gap values changed. This in turn resulted in contrasting centroid and
total strength values of the calculated GT distributions and led to differences
in calculated half-lives using the three schemes. The half-life values computed
via the three-term pairing formula, based on separation energies of nucleons,
were in best agreement with the measured data. We conclude that the traditional
choice of pairing gap values, $\Delta_p = \Delta_n = 12/\sqrt{A}$, may not lead
to half-life values in good agreement with measured data. The findings of this
study are interesting but warrant further investigation.",['nucl-th'],2503.13889," Pairing of nucleons plays a key role in solving various nuclear physics
problems. We investigate the probable effects of pairing correlations on the
calculated Gamow-Teller (GT) strength distributions and the associated
$\beta$-decay half-lives. Computations are performed for a total of 35 fp-shell
nuclei using the proton-neutron quasiparticle random phase approximation
(pn-QRPA) model. The nuclei were selected because of their importance in
various astrophysical environments. Pairing gaps are one of the key parameters
in the pn-QRPA model to compute GT transitions. We employed three different
values of the pairing gaps obtained from three different empirical formulae in
our calculation. The GT strength distributions changed significantly as the
pairing gap values changed. This in turn resulted in contrasting centroid and
total strength values of the calculated GT distributions and led to differences
in calculated half-lives using the three schemes. The half-life values computed
via the three-term pairing formula, based on separation energies of nucleons,
were in best agreement with the measured data. We conclude that the traditional
choice of pairing gap values, $\Delta_p = \Delta_n = 12/\sqrt{A}$, may not lead
to half-life values in good agreement with measured data. The findings of this
study are interesting but warrant further investigation.",['nucl-th'],False,,,,"Investigation of effects of pairing correlations on calculated
  $\beta$-decay half-lives of fp-shell nuclei","Investigation of effects of pairing correlations on calculated
  $\beta$-decay half-lives of fp-shell nuclei"
neg-d2-960,2025-03-11,,2503.08072," Crystal defects, whether intrinsic or engineered, drive many fundamental
phenomena and novel functionalities of quantum materials. Here, we report
symmetry-breaking phenomena induced by Sn-vacancy defects on the surface of
epitaxial Kagome antiferromagnet FeSn films using low-temperature scanning
tunneling microscopy and spectroscopy. Near the Sn-vacancy defects, anisotropic
quasiparticle interference patterns are observed in the differential
conductance dI/dV maps, indicating two-fold electronic states that break the
6-fold rotational symmetry of the Kagome layer. Furthermore, the Sn-vacancy
defects induce bound states that exhibit anomalous Zeeman shifts under an
out-of-plane magnetic field, where their energy shifts linearly towards higher
energy independent of the direction of the magnetic field. Under an in-plane
magnetic field, the shift of the bound state energy also shows a two-fold
oscillating behavior as a function of the azimuth angle. These findings
demonstrate defect-enabled new functionalities in Kagome antiferromagnets for
potential applications in nanoscale spintronic devices.",['cond-mat.mtrl-sci'],2503.1618," Chalcogenide perovskites have emerged as a promising class of materials for
the next generation of optoelectronic applications, with BaZrS$_\text{3}$
attracting significant attention due to its wide bandgap, earth-abundant
composition, and thermal and chemical stability. However, previous studies have
consistently reported weak and ambiguous photoluminescence (PL), regardless of
synthesis method, raising questions about the intrinsic optoelectronic quality
of this compound. In this work, we demonstrate strong, band-to-band-dominated
PL at room temperature in high-quality BaZrS$_\text{3}$ single crystals.
Despite the narrow, single-component PL emission band, time-resolved PL
measurements reveal a carrier lifetime of $1.0\pm0.2$ ns. To understand the
origin of the strong PL and short carrier lifetime, we perform multiwavelength
excitation and polarization-dependent Raman measurements, supported by
first-principles lattice dynamics calculations. We identify all 23
theoretically predicted Raman-active modes and their symmetries, providing a
comprehensive reference for future studies. Our results indicate that
phonon-assisted carrier decay and strong electron-phonon coupling contribute to
the short carrier lifetimes, as evidenced by Raman spectroscopy and DFT
calculations. Further studies on compositional variations or partial
cation/anion substitutions could mitigate electron-phonon coupling and enhance
carrier lifetimes. By establishing a detailed reference for the intrinsic
vibrational and optoelectronic properties of BaZrS$_\text{3}$, this work paves
the way for further advancements in chalcogenide perovskites for energy and
optoelectronic technologies.",['cond-mat.mtrl-sci'],False,,,,"Anisotropic response of defect bound states to magnetic field in
  epitaxial FeSn films","BaZrS$_\text{3}$ Lights Up: The Interplay of Electrons, Photons, and
  Phonons in Strongly Luminescent Single Crystals"
neg-d2-961,2025-01-20,,2501.11444," In this work, we review the concept of center of a geometric object as an
equivariant map, unifying and generalizing different approaches followed by
authors such as C. Kimberling or A. Edmonds. We provide examples to illustrate
that this general approach encompasses many interesting spaces of geometric
objects arising from different settings. Additionally, we discuss two results
that characterize centers for some particular spaces of geometric objects, and
we pose five open questions related to the generalization of these
characterizations to other spaces. Finally, we conclude this article by briefly
discussing other central objects and their relation to this concept of center.",['math.MG'],2501.11444," In this work, we review the concept of center of a geometric object as an
equivariant map, unifying and generalizing different approaches followed by
authors such as C. Kimberling or A. Edmonds. We provide examples to illustrate
that this general approach encompasses many interesting spaces of geometric
objects arising from different settings. Additionally, we discuss two results
that characterize centers for some particular spaces of geometric objects, and
we pose five open questions related to the generalization of these
characterizations to other spaces. Finally, we conclude this article by briefly
discussing other central objects and their relation to this concept of center.",['math.MG'],False,,,,On the concept of center for geometric objects and related problems,On the concept of center for geometric objects and related problems
neg-d2-962,2025-03-14,,2503.1153," This paper addresses the evolution of an axially symmetric magnetic field in
the core of a neutron star. The matter in the core is modeled as a system of
two fluids, namely neutrons and charged particles, with slightly different
velocity fields, controlled by their mutual collisional friction. This problem
was addressed in our previous work through the so-called ``fictitious
friction'' approach. We study the validity of our previous work and improve it
by comparing the fictitious friction approach to alternatives, making
approximations that allow it to be applied to arbitrary magnetic field
strengths and using realistic equations of state. We assume the neutron star
crust to be perfectly resistive, so its magnetic field reacts instantaneously
to changes in the core, in which we neglect the effects of Cooper pairing. We
explore different approaches to solve the equations to obtain the velocities
and chemical potential perturbations induced by a given, fixed magnetic field
configuration in the core. We also present a new version of our code to perform
time-evolving simulations and discuss the results obtained with it. Our
calculations without fictitious friction further confirm that bulk velocity is
generally much greater than ambipolar velocity, leading to faster evolution.
These findings align with those with fictitious friction, validating this
approach. We also find that, in the long term, the star evolves towards a
barotropic ``Grad-Shafranov equilibrium,'' where the magnetic force is fully
balanced by charged particle fluid forces. Qualitatively, the evolution and the
final equilibrium are independent of the magnetic field strength $B$ and the
equation of state considered. The timescale to reach this equilibrium is
proportional to $B^{-2}$ and becomes shorter for equations of state with a
smaller gradient of the ratio between the densities of protons and neutrons.",['astro-ph.HE'],2502.11511," GRB 240529A is a long-duration gamma-ray burst (GRB) whose light curve of
prompt emission is composed of a triple-episode structure, separated by
quiescent gaps of tens to hundreds of seconds. More interestingly, its X-ray
light curve of afterglow exhibits two-plateau emissions, namely, an internal
plateau emission that is smoothly connected with a $\sim t^{-0.1}$ segment and
followed by a $\sim t^{-2}$ power-law decay. The three episodes in the prompt
emission, together with two plateau emissions in X-ray, are unique in the Swift
era. They are very difficult to explain with the standard internal/external
shock model by invoking a black hole central engine. However, it could be
consistent with the prediction of a supramassive magnetar as the central
engine, the physical process of phase transition from magnetar to strange star,
as well as the cooling and spin-down of the strange star. In this paper, we
propose that the first- and second-episode emissions in the prompt $\gamma-$ray
of GRB 240529A are from the jet emission of a massive star collapsing into a
supramassive magnetar and the re-activity of central engine, respectively.
Then, the third-episode emission of prompt is attributed to the phase
transition from a magnetar to a strange star. Finally, the first- and
second-plateau emissions of the X-ray afterglow are powered by the cooling and
spin-down of the strange star, respectively. The observational data of each
component of GRB 240529A are roughly coincident with the estimations of the
above physical picture.",['astro-ph.HE'],False,,,,"Validating and improving two-fluid simulations of the magnetic field
  evolution in neutron star cores",Signature of strange star as the central engine of GRB 240529A
neg-d2-963,2025-03-03,,2503.01742," The rapid growth of Large Language Models (LLMs) presents significant
privacy, security, and ethical concerns. While much research has proposed
methods for defending LLM systems against misuse by malicious actors,
researchers have recently complemented these efforts with an offensive approach
that involves red teaming, i.e., proactively attacking LLMs with the purpose of
identifying their vulnerabilities. This paper provides a concise and practical
overview of the LLM red teaming literature, structured so as to describe a
multi-component system end-to-end. To motivate red teaming we survey the
initial safety needs of some high-profile LLMs, and then dive into the
different components of a red teaming system as well as software packages for
implementing them. We cover various attack methods, strategies for
attack-success evaluation, metrics for assessing experiment outcomes, as well
as a host of other considerations. Our survey will be useful for any reader who
wants to rapidly obtain a grasp of the major red teaming concepts for their own
use in practical applications.",['cs.CL'],2502.12895," The breakthrough of generative large language models (LLMs) that can solve
different tasks through chat interaction has led to a significant increase in
the use of general benchmarks to assess the quality or performance of these
models beyond individual applications. There is also a need for better methods
to evaluate and also to compare models due to the ever increasing number of new
models published. However, most of the established benchmarks revolve around
the English language. This paper analyses the benefits and limitations of
current evaluation datasets, focusing on multilingual European benchmarks. We
analyse seven multilingual benchmarks and identify four major challenges.
Furthermore, we discuss potential solutions to enhance translation quality and
mitigate cultural biases, including human-in-the-loop verification and
iterative translation ranking. Our analysis highlights the need for culturally
aware and rigorously validated benchmarks to assess the reasoning and
question-answering capabilities of multilingual LLMs accurately.",['cs.CL'],False,,,,"Building Safe GenAI Applications: An End-to-End Overview of Red Teaming
  for Large Language Models","Multilingual European Language Models: Benchmarking Approaches and
  Challenges"
neg-d2-964,2025-01-02,,2501.01519," We construct $S^r$-colored knot Floer homologies and prove that they satisfy
categorified recurrence relations. The associated Euler characteristic implies
$q$-holonomicity of the corresponding sequence of colored Alexander
polynomials, in analogy with the AJ conjecture for colored Jones polynomials.",['math.GT'],2501.01519," We construct $S^r$-colored knot Floer homologies and prove that they satisfy
categorified recurrence relations. The associated Euler characteristic implies
$q$-holonomicity of the corresponding sequence of colored Alexander
polynomials, in analogy with the AJ conjecture for colored Jones polynomials.",['math.GT'],False,,,,Holonomicity from a Heegaard-Floer Perspective,Holonomicity from a Heegaard-Floer Perspective
neg-d2-965,2025-01-22,,2501.1271," Microwave cavity modes with long coherence times are used in many different
quantum computing systems. During normal operation of such systems, these
modes, called memory modes, often need to be set to different coherent photonic
occupations. In this work we present a novel technique we call Rabi Driven
Reset in which the state of a memory mode is transferred into a decaying mode.
This is done through a Rabi driven qubit which is coupled to both modes via
sideband driving tones. The outcome of the method is the initialization of the
memory mode at any required coherent state. Simulations are presented to
demonstrate the effectiveness of this technique, along with a comparison to an
existing coupling method. Our simulations predict an improvement of an order of
magnitude in initialization times compared to existing methods.",['quant-ph'],2501.06846," As is well known, unital Pauli maps can be eternally non-CP-divisible. In
contrast, here we show that in the case of non-unital maps, eternal
non-Markovianity in the non-unital part is ruled out. In the unital case, the
eternal non-Markovianity can be obtained by a convex combination of two
dephasing semigroups, but not all three of them. We study these results and the
ramifications arising from them.",['quant-ph'],False,,,,Cavity Mode Initialization via a Rabi Driven Qubit,On the eternal non-Markovianity of qubit maps
neg-d2-966,2025-03-12,,2503.09496," The integrative analysis of histopathological images and genomic data has
received increasing attention for survival prediction of human cancers.
However, the existing studies always hold the assumption that full modalities
are available. As a matter of fact, the cost for collecting genomic data is
high, which sometimes makes genomic data unavailable in testing samples. A
common way of tackling such incompleteness is to generate the genomic
representations from the pathology images. Nevertheless, such strategy still
faces the following two challenges: (1) The gigapixel whole slide images (WSIs)
are huge and thus hard for representation. (2) It is difficult to generate the
genomic embeddings with diverse function categories in a unified generative
framework. To address the above challenges, we propose a Conditional Latent
Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal
survival prediction, even with missing genomic data. Specifically, a
Variational Information Bottleneck Transformer (VIB-Trans) module is proposed
to learn compressed pathological representations from the gigapixel WSIs. To
generate different functional genomic features, we develop a novel Latent
Differentiation Variational AutoEncoder (LD-VAE) to learn the common and
specific posteriors for the genomic embeddings with diverse functions. Finally,
we use the product-of-experts technique to integrate the genomic common
posterior and image posterior for the joint latent distribution estimation in
LD-CVAE. We test the effectiveness of our method on five different cancer
datasets, and the experimental results demonstrate its superiority in both
complete and missing modality scenarios.",['cs.CV'],2503.01257," Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on
mobile devices. However, due to the manufacturing constraints of compact
devices and the inherent physical principles of imaging, dToF depth maps are
sparse and noisy. In this paper, we propose a novel video depth completion
method, called SVDC, by fusing the sparse dToF data with the corresponding RGB
guidance. Our method employs a multi-frame fusion scheme to mitigate the
spatial ambiguity resulting from the sparse dToF imaging. Misalignment between
consecutive frames during multi-frame fusion could cause blending between
object edges and the background, which results in a loss of detail. To address
this, we introduce an adaptive frequency selective fusion (AFSF) module, which
automatically selects convolution kernel sizes to fuse multi-frame features.
Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to
enhance features and generates an attention map as fusion weights. The AFSF
ensures edge detail recovery while suppressing high-frequency noise in smooth
regions. To further enhance temporal consistency, We propose a cross-window
consistency loss to ensure consistent predictions across different windows,
effectively reducing flickering. Our proposed SVDC achieves optimal accuracy
and consistency on the TartanAir and Dynamic Replica datasets. Code is
available at https://github.com/Lan1eve/SVDC.",['cs.CV'],False,,,,"Robust Multimodal Survival Prediction with the Latent Differentiation
  Conditional Variational AutoEncoder","SVDC: Consistent Direct Time-of-Flight Video Depth Completion with
  Frequency Selective Fusion"
neg-d2-967,2025-01-16,,2501.09411," Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.",['cs.CV'],2502.1815," Recent approaches to jointly reconstruct 3D humans and objects from a single
RGB image represent 3D shapes with template-based or coarse models, which fail
to capture details of loose clothing on human bodies. In this paper, we
introduce a novel implicit approach for jointly reconstructing realistic 3D
clothed humans and objects from a monocular view. For the first time, we model
both the human and the object with an implicit representation, allowing to
capture more realistic details such as clothing. This task is extremely
challenging due to human-object occlusions and the lack of 3D information in 2D
images, often leading to poor detail reconstruction and depth ambiguity. To
address these problems, we propose a novel attention-based neural implicit
model that leverages image pixel alignment from both the input human-object
image for a global understanding of the human-object scene and from local
separate views of the human and object images to improve realism with, for
example, clothing details. Additionally, the network is conditioned on semantic
features derived from an estimated human-object pose prior, which provides 3D
spatial information about the shared space of humans and objects. To handle
human occlusion caused by objects, we use a generative diffusion model that
inpaints the occluded regions, recovering otherwise lost details. For training
and evaluation, we introduce a synthetic dataset featuring rendered scenes of
inter-occluded 3D human scans and diverse objects. Extensive evaluation on both
synthetic and real-world datasets demonstrates the superior quality of the
proposed human-object reconstructions over competitive methods.",['cs.CV'],False,,,,Towards Robust and Realistic Human Pose Estimation via WiFi Signals,"Realistic Clothed Human and Object Joint Reconstruction from a Single
  Image"
neg-d2-968,2025-02-21,,2502.15265," We study the self-assembly of magnetic colloids using the Stockmayer (SM)
model characterized by short-range Lennard-Jones interactions and long-range
dipole-dipole interactions. Using molecular dynamics simulations, we design
cooling protocols that yield perfectly assembled single-domain magnetic
crystals. We identify cooling rates at which the system transforms from an
amorphous glass to a crystal, where magnetic ordering promotes crystalline
order. Remarkably, we observe that the latter develops via a spontaneous
transition rather than through the traditional nucleation and growth mechanism.
For a weakly dipolar fluid ($\mu=1$), this self-assembly results in a
face-centered cubic (FCC) colloidal crystal with dipole moments chained along
the (111) direction. For fluids with higher dipole moment ($\mu = 2.5$), the
crystal structure shifts towards a body-centered orthorhombic (BCO) arrangement
due to the compression of chains from strong dipolar attractions. These results
provide valuable insights into the mechanisms driving crystallization in
magnetic fluids, opening new avenues for understanding the formation of
magnetically responsive colloidal magnetic crystals with promising
applications.",['cond-mat.soft'],2503.08894," Origami metamaterials made of repeating unit cells of parallelogram panels
joined at folds dramatically change their shape through a collective motion of
their cells. Here we develop an effective elastic model and numerical method to
study the large deformation response of these metamaterials under a broad class
of loads. The model builds on an effective plate theory derived in our prior
work [64]. The theory captures the overall shape change of all slightly
stressed parallelogram origami deformations through nonlinear geometric
compatibility constraints that couple the origami's (cell averaged) effective
deformation to an auxiliary angle field quantifying its cell-by-cell actuation.
It also assigns to each such origami deformation a plate energy associated to
these effective fields. Seeking a constitutive model that is faithful to the
theory but also practical to simulate, we relax the geometric constraints via
corresponding elastic energy penalties; we also simplify the plate energy
density to embrace its essential character as a regularization to the geometric
penalties. The resulting model for parallelogram origami is a generalized
elastic continuum that is nonlinear in the effective deformation gradient and
angle field and regularized by high-order gradients thereof. We provide a
finite element formulation of this model using the $C^0$ interior penalty
method to handle second gradients of deformation, and implement it using the
open source computing platform Firedrake. We end by using the model and
numerical method to study two canonical parallelogram origami patterns, in
Miura and Eggbox origami, under a variety of loading conditions.",['cond-mat.soft'],False,,,,Self-assembly of Dipolar Crystals from Magnetic Colloids,"Modeling and computation of the effective elastic behavior of
  parallelogram origami metamaterials"
neg-d2-969,2025-01-27,,2501.16068," Consider a second-order elliptic operator $L$ in the half-plane $\mathbb R
\times (0, \infty)$ with coefficients depending only on the second coordinate.
The Poisson kernel for $L$ is used in the representation of positive
$L$-harmonic functions, that is, solutions of $L u = 0$. In probabilistic
terms, the Poisson kernel is the density function of the distribution of the
diffusion in $\mathbb R \times (0, \infty)$ with generator $L$ at the hitting
time of the boundary. We prove that the Poisson kernel for $L$ is bell-shaped:
its $n$th derivative changes sign $n$ times. In particular, it is unimodal and
it has two inflection points (it is concave, then convex, then concave again).",['math.AP'],2502.19262," This paper presents two remarkable phenomena associated with the heat
equation with a time delay: namely, the propagation of singularities and
periodicity. These are manifested through a distinctive mode of propagation of
singularities in the solutions. Precisely, the singularities of the solutions
propagate periodically in a bidirectional fashion along the time axis.
Furthermore, this propagation occurs in a stepwise manner. More specifically,
when propagating in the positive time direction, the order of the joint
derivatives of the solution increases by 2 for each period; conversely, when
propagating in the reverse time direction, the order of the joint derivatives
decreases by 2 per period. Additionally, we elucidate the way in which the
initial data and historical values impact such a propagation of singularities.
  The phenomena we have discerned not only corroborate the pronounced
differences between heat equations with and without time delay but also vividly
illustrate the substantial divergence between the heat equation with a time
delay and the wave equation, especially when viewed from the point of view of
singularity propagation.",['math.AP'],False,,,,Poisson kernels on the half-plane are bell-shaped,Periodic propagation of singularities for heat equations with time delay
neg-d2-970,2025-01-26,,2501.15449," To address the annotation burden in LiDAR-based 3D object detection, active
learning (AL) methods offer a promising solution. However, traditional active
learning approaches solely rely on a small amount of labeled data to train an
initial model for data selection, overlooking the potential of leveraging the
abundance of unlabeled data. Recently, attempts to integrate semi-supervised
learning (SSL) into AL with the goal of leveraging unlabeled data have faced
challenges in effectively resolving the conflict between the two paradigms,
resulting in less satisfactory performance. To tackle this conflict, we propose
a Synergistic Semi-Supervised Active Learning framework, dubbed as S-SSAL.
Specifically, from the perspective of SSL, we propose a Collaborative
PseudoScene Pre-training (CPSP) method that effectively learns from unlabeled
data without introducing adverse effects. From the perspective of AL, we design
a Collaborative Active Learning (CAL) method, which complements the uncertainty
and diversity methods by model cascading. This allows us to fully exploit the
potential of the CPSP pre-trained model. Extensive experiments conducted on
KITTI and Waymo demonstrate the effectiveness of our S-SSAL framework. Notably,
on the KITTI dataset, utilizing only 2% labeled data, S-SSAL can achieve
performance comparable to models trained on the full dataset. The code has been
released at https://github.com/LandDreamer/S_SSAL.",['cs.CV'],2502.08642," Recent advancements in large vision-language models have enabled highly
expressive and diverse vector sketch generation. However, state-of-the-art
methods rely on a time-consuming optimization process involving repeated
feedback from a pretrained model to determine stroke placement. Consequently,
despite producing impressive sketches, these methods are limited in practical
applications. In this work, we introduce SwiftSketch, a diffusion model for
image-conditioned vector sketch generation that can produce high-quality
sketches in less than a second. SwiftSketch operates by progressively denoising
stroke control points sampled from a Gaussian distribution. Its
transformer-decoder architecture is designed to effectively handle the discrete
nature of vector representation and capture the inherent global dependencies
between strokes. To train SwiftSketch, we construct a synthetic dataset of
image-sketch pairs, addressing the limitations of existing sketch datasets,
which are often created by non-artists and lack professional quality. For
generating these synthetic sketches, we introduce ControlSketch, a method that
enhances SDS-based techniques by incorporating precise spatial control through
a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across
diverse concepts, efficiently producing sketches that combine high fidelity
with a natural and visually appealing style.",['cs.CV'],False,,,,"Breaking the SSL-AL Barrier: A Synergistic Semi-Supervised Active
  Learning Framework for 3D Object Detection",SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation
neg-d2-971,2025-02-21,,2502.15373," Intensive research has revealed intriguing optical responses in topological
materials. This paper focuses on the optical responses in $s$-wave
superconductors with a Rashba spin-orbit coupling and a magnetic field, one of
the platforms of topological superconductivity. On the one hand, to satisfy
some conservation laws in superconducting responses, it is essential to take
into account collective excitation modes. On the other hand, the optical
response is a promising phenomenon for detecting hidden collective modes in
superconductors. In this paper, we investigate the effect of collective
excitation modes on the linear and second-order optical responses based on the
self-consistent response approximation, which is formulated using the
Kadanoff-Baym method. Our main results reveal that the Higgs mode enhances the
optical responses when the Fermi level is close to the Dirac point. The
enhancement is due to the multiband effects characterized by interband pairing.
We also demonstrate the sign reversal of the photocurrent conductivity around
the topological transition with increasing the Zeeman field. This finding
supports the prediction in our previous work without considering collective
excitation modes [H. Tanaka, et al., Phys. Rev. B 110, 014520 (2024)]. The sign
reversal phenomenon is attributed to the magnetic injection current modified by
the Higgs mode, and is proposed for a bulk probe of topological
superconductors. We also discuss the interplay of quantum geometry and
collective modes.",['cond-mat.supr-con'],2501.14876," The muon spin rotation ($\mu$SR) technique has been applied to determine the
behavior of the in-plane magnetic penetration depth ($\lambda_{ab}$) in the
vortex state of the unconventional superconductor Sr$_2$RuO$_4$ as a means of
gaining insight into its still unknown superconducting order parameter. A
recent $\mu$SR study of Sr$_2$RuO$_4$ reported a $T$-linear temperature
dependence for $\lambda_{ab}$ at low temperatures that was not identified in an
earlier $\mu$SR study. Here we show that there is no significant difference
between the data in the early and recent $\mu$SR studies and both are
compatible with the limiting low-temperature $\lambda_{ab} \sim T^2$ dependence
expected from measurements of the change in $\lambda_{ab}(T)$ in the Meissner
state by other techniques. However, we argue that at this time there is no
valid theoretical model for reliably determining the absolute value of
$\lambda_{ab}$ in Sr$_2$RuO$_4$ from $\mu$SR measurements. Instead, we identify
the formation of an unusual square vortex lattice that introduces a new
constraint on candidate superconducting order parameters for Sr$_2$RuO$_4$.",['cond-mat.supr-con'],False,,,,"Vertex correction for the linear and nonlinear optical responses in
  superconductors: multiband effect and topological superconductivity","Atypical vortex lattice and the magnetic penetration depth in
  superconducting Sr$_2$RuO$_4$ deduced by $\mu$SR"
neg-d2-972,2025-01-06,,2501.02799," Let $(\pi,V)$ be a smooth representation of a compact Lie group $G$ on a
quasi-complete locally convex complex topological vector space. We show that
the Lie algebra cohomology space $\mathrm{H} ^\bullet(\mathfrak{u}, V)$ and the
Lie algebra homology space $\mathrm{H}_\bullet(\mathfrak{u}, V)$ are both
Hausdorff, where $\mathfrak{u}$ is the nilpotent radical of a parabolic
subalgebra of the complexified Lie algebra $\mathfrak{g}$ of $G$.",['math.RT'],2501.02799," Let $(\pi,V)$ be a smooth representation of a compact Lie group $G$ on a
quasi-complete locally convex complex topological vector space. We show that
the Lie algebra cohomology space $\mathrm{H} ^\bullet(\mathfrak{u}, V)$ and the
Lie algebra homology space $\mathrm{H}_\bullet(\mathfrak{u}, V)$ are both
Hausdorff, where $\mathfrak{u}$ is the nilpotent radical of a parabolic
subalgebra of the complexified Lie algebra $\mathfrak{g}$ of $G$.",['math.RT'],False,,,,Hausdorffness of certain nilpotent cohomology spaces,Hausdorffness of certain nilpotent cohomology spaces
neg-d2-973,2025-03-20,,2503.16362," In this work, we proved the existence of a unique global mild solution of the
d-dimensional incompressible Navier-Stokes equations, for small initial data in
Besov type spaces based on mixed-Lebesgue spaces; namely, mixed-norm
Besov-Lebesgue spaces and also mixed-norm Fourier-Besov-Lebesgue spaces. The
main tools are the Bernstein's type inequalities, Bony's paraproduct to
estimate the bilinear term and a fixed point scheme in order to get the
well-posedness. Our results complement and cover previous and recents result on
(Fourier-)Besov spaces and, for instance, provide a new class of initial data
possibly not included in BMO^{-1}(R^3) but continuously included in
\dot{B}^{-1}_{\infty,infty}(R^3).",['math.AP'],2503.09323," In the present paper, we establish a multiplicity result for a following
class of nonlocal Neumann eigenvalue problems involving the fractional
p-Laplacian.
  \begin{align} \begin{cases}
  (-\Delta)^{s}_{p}u + a(x) \abs{u}^{p-2}u =\lambda h(x,u) & \text {in }
\Omega,
  \mathcal{N}_{s,p}u=0 & \text {in } \mathbb{R}^N \setminus \overline{\Omega},
  \end{cases}
  \end{align}
  Precisely, we demonstrate the existence of an open interval for positive
eigenvalues $\lambda$, for which the problem has at least three non-zero
solutions in $W^{s,p}_{\Omega}.$",['math.AP'],False,,,,"Global well-posedness for the Navier-Stokes system in new critical
  mixed-norm Besov spaces","Three non-zero solutions of a Neumann eigenvalue problems involving the
  fractional p-Laplacian"
neg-d2-974,2025-02-13,,2502.09818," Although vision-language models (VLMs) have achieved significant success in
various applications such as visual question answering, their resilience to
prompt variations remains an under-explored area. Understanding how
distractions affect VLMs is crucial for improving their real-world
applicability, as inputs could have noisy and irrelevant information in many
practical scenarios. This paper aims to assess the robustness of VLMs against
both visual and textual distractions in the context of science question
answering. Built on the ScienceQA dataset, we developed a new benchmark that
introduces distractions in both the visual and textual contexts to evaluate the
reasoning capacity of VLMs amid these distractions. Our findings reveal that
most-of-the-art VLMs, including GPT-4, are vulnerable to various types of
distractions, experiencing noticeable degradation in reasoning capabilities
when confronted with distractions. Notably, models such as InternVL2
demonstrate a higher degree of robustness to these distractions. We also found
that models exhibit greater sensitivity to textual distractions than visual
ones. Additionally, we explored various mitigation strategies, such as prompt
engineering, to counteract the impact of distractions. While these strategies
improved solution accuracy, our analysis shows that there remain significant
opportunities for improvement.",['cs.CV'],2502.08779," Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful
societal prejudices, undermining the fairness and equity of AI applications. As
LMMs grow increasingly influential, addressing and mitigating inherent biases
related to stereotypes, harmful generations, and ambiguous assumptions in
real-world scenarios has become essential. However, existing datasets
evaluating stereotype biases in LMMs often lack diversity and rely on synthetic
images, leaving a gap in bias evaluation for real-world visual contexts. To
address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most
comprehensive framework to date for assessing stereotype biases across nine
diverse categories with non-synthetic images. SB-bench rigorously evaluates
LMMs through carefully curated, visually grounded scenarios, challenging them
to reason accurately about visual stereotypes. It offers a robust evaluation
framework featuring real-world visual samples, image variations, and
multiple-choice question formats. By introducing visually grounded queries that
isolate visual biases from textual ones, SB-bench enables a precise and nuanced
assessment of a model's reasoning capabilities across varying levels of
difficulty. Through rigorous testing of state-of-the-art open-source and
closed-source LMMs, SB-bench provides a systematic approach to assessing
stereotype biases in LMMs across key social dimensions. This benchmark
represents a significant step toward fostering fairness in AI systems and
reducing harmful biases, laying the groundwork for more equitable and socially
responsible LMMs. Our code and dataset are publicly available.",['cs.CV'],False,,,,On the robustness of multimodal language model towards distractions,SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models
neg-d2-975,2025-01-27,,2501.15913," Stream-based runtime monitoring frameworks are safety assurance tools that
check the runtime behavior of a system against a formal specification. This
tutorial provides a hands-on introduction to RTLola, a real-time monitoring
toolkit for cyber-physical systems and networks. RTLola processes, evaluates,
and aggregates streams of input data, such as sensor readings, and provides a
real-time analysis in the form of comprehensive statistics and logical
assessments of the system's health. RTLola has been applied successfully in
monitoring autonomous systems such as unmanned aircraft. The tutorial guides
the reader through the development of a stream-based specification for an
autonomous drone observing other flying objects in its flight path. Each
tutorial section provides an intuitive introduction, highlighting useful
language features and specification patterns, and gives a more in-depth
explanation of technical details for the advanced reader. Finally, we discuss
how runtime monitors generated from RTLola specifications can be integrated
into a variety of systems and discuss different monitoring applications.",['cs.LO'],2503.09831," It is well-known that intersection type assignment systems can be used to
characterize strong normalization (SN). Typical proofs that typable
lambda-terms are SN in these systems rely on semantical techniques. In this
work, we study $\Lambda_\cap^e$, a variant of Coppo and Dezani's (Curry-style)
intersection type system, and we propose a syntactical proof of strong
normalization for it. We first design $\Lambda_\cap^i$, a Church-style version,
in which terms closely correspond to typing derivations. Then we prove that
typability in $\Lambda_\cap^i$ implies SN through a measure that, given a term,
produces a natural number that decreases along with reduction. Finally, the
result is extended to $\Lambda_\cap^e$, since the two systems simulate each
other.",['cs.LO'],False,,,,A Tutorial on Stream-based Monitoring,"Strong normalization through idempotent intersection types: a new
  syntactical approach"
neg-d2-976,2025-03-19,,2503.16559," This paper proposes a design scheme of reward function that constantly
evaluates both driving states and actions for applying reinforcement learning
to automated driving. In the field of reinforcement learning, reward functions
often evaluate whether the goal is achieved by assigning values such as +1 for
success and -1 for failure. This type of reward function can potentially obtain
a policy that achieves the goal, but the process by which the goal is reached
is not evaluated. However, process to reach a destination is important for
automated driving, such as keeping velocity, avoiding risk, retaining distance
from other cars, keeping comfortable for passengers. Therefore, the reward
function designed by the proposed scheme is suited for automated driving by
evaluating driving process. The effects of the proposed scheme are demonstrated
on simulated circuit driving and highway cruising. Asynchronous Advantage
Actor-Critic is used, and models are trained under some situations for
generalization. The result shows that appropriate driving positions are
obtained, such as traveling on the inside of corners, and rapid deceleration to
turn along sharp curves. In highway cruising, the ego vehicle becomes able to
change lane in an environment where there are other vehicles with suitable
deceleration to avoid catching up to a front vehicle, and acceleration so that
a rear vehicle does not catch up to the ego vehicle.",['cs.RO'],2503.14899," This study presents a robust optimization algorithm for automated highway
merge. The merging scenario is one of the challenging scenes in automated
driving, because it requires adjusting ego vehicle's speed to match other
vehicles before reaching the end point. Then, we model the speed planning
problem as a deterministic Markov decision process. The proposed scheme is able
to compute each state value of the process and reliably derive the optimal
sequence of actions. In our approach, we adopt jerk as the action of the
process to prevent a sudden change of acceleration. However, since this expands
the state space, we also consider ways to achieve a real-time operation. We
compared our scheme with a simple algorithm with the Intelligent Driver Model.
We not only evaluated the scheme in a simulation environment but also conduct a
real world testing.",['cs.RO'],False,,,,"Design of Reward Function on Reinforcement Learning for Automated
  Driving","Speed Optimization Algorithm based on Deterministic Markov Decision
  Process for Automated Highway Merge"
neg-d2-977,2025-01-11,,2501.06632," We present the first analytic-derivative-based formulation of vibrational
circular dichroism (VCD) atomic axial tensors for second-order Moller-Plesset
(MP2) perturbation theory. We compare our implementation to our recently
reported finite-difference approach and find close agreement, thus validating
the new formulation. The new approach is dramatically less computationally
expensive than the numerical-derivative method with an overall computational
scaling of $O(N^6)$. In addition, we report the first fully analytic VCD
spectrum for (S)-methyloxirane at the MP2 level of theory.",['physics.chem-ph'],2501.02939," Three-dimensional atomic force microscopy (3D-AFM) has been a powerful tool
to probe the atomic-scale structure of solid-liquid interfaces. As a nanoprobe
moves along the 3D volume of interfacial liquid, the probe-sample interaction
force is sensed and mapped, providing information on not only the solid
morphology, but also the liquid density distribution. To date 3D-AFM force maps
of a diverse set of solid-liquid interfaces have been recorded, revealing
remarkable force oscillations that are typically attributed to solvation layers
or electrical double layers. However, despite the high resolution down to
sub-angstrom level, quantitative interpretation of the 3D force maps has been
an outstanding challenge. Here we will review the technical details of 3D-AFM
and the existing approaches for quantitative data interpretation. Based on
evidences in recent literature, we conclude that the perturbation-induced AFM
force paradoxically represents the intrinsic, unperturbed liquid density
profile. We will further discuss how the oscillatory force profiles can be
attributed to the probe-modulation of the liquid configurational entropy, and
how the quantitative, atomic-scale liquid density distribution can be derived
from the force maps.",['physics.chem-ph'],False,,,,"Analytic Computation of Vibrational Circular Dichroism Spectra Using
  Second-Order M{\o}ller-Plesset Perturbation Theory","Towards Quantitative Interpretation of 3D Atomic Force Microscopy at
  Solid-Liquid Interfaces"
neg-d2-978,2025-02-19,,2502.13501," We consider four-dimensional Euclidean Yang-Mills theories quantized in the
maximal Abelian and linear covariant gauges at finite temperature.
Non-perturbatively, the Faddeev-Popov procedure must be improved to take into
account the existence of the so-called Gribov copies. Tapping on previous
results about the elimination of infinitesimal Gribov copies in maximal Abelian
and linear covariant gauges at zero temperature, we explore the interplay
between finite temperature effects and the removal of gauge copies. We focus in
a hybrid approach where the thermal masses are derived through perturbative
propagators as a stepping stone for a self-consistent treatment. The resulting
action collects the effects of the elimination of infinitesimal Gribov copies
as well as the thermal masses. We verify the existence of three different
phases for the gluonic degrees of freedom; one of complete confinement at low
temperatures, an intermediate one of partial confinement, and one of complete
deconfinement at high temperatures.",['hep-th'],2502.13501," We consider four-dimensional Euclidean Yang-Mills theories quantized in the
maximal Abelian and linear covariant gauges at finite temperature.
Non-perturbatively, the Faddeev-Popov procedure must be improved to take into
account the existence of the so-called Gribov copies. Tapping on previous
results about the elimination of infinitesimal Gribov copies in maximal Abelian
and linear covariant gauges at zero temperature, we explore the interplay
between finite temperature effects and the removal of gauge copies. We focus in
a hybrid approach where the thermal masses are derived through perturbative
propagators as a stepping stone for a self-consistent treatment. The resulting
action collects the effects of the elimination of infinitesimal Gribov copies
as well as the thermal masses. We verify the existence of three different
phases for the gluonic degrees of freedom; one of complete confinement at low
temperatures, an intermediate one of partial confinement, and one of complete
deconfinement at high temperatures.",['hep-th'],False,,,,"Yang-Mills theories at finite temperature quantized in linear covariant
  gauges: gauge copies and semi-non-perturbative effects","Yang-Mills theories at finite temperature quantized in linear covariant
  gauges: gauge copies and semi-non-perturbative effects"
neg-d2-979,2025-02-27,,2502.2062," Large language models (LLMs) can exhibit advanced reasoning yet still
generate incorrect answers. We hypothesize that such errors frequently stem
from spurious beliefs, propositions the model internally considers true but are
incorrect. To address this, we propose a method to rectify the belief space by
suppressing these spurious beliefs while simultaneously enhancing true ones,
thereby enabling more reliable inferences. Our approach first identifies the
beliefs that lead to incorrect or correct answers by prompting the model to
generate textual explanations, using our Forward-Backward Beam Search (FBBS).
We then apply unlearning to suppress the identified spurious beliefs and
enhance the true ones, effectively rectifying the model's belief space.
Empirical results on multiple QA datasets and LLMs show that our method
corrects previously misanswered questions without harming overall model
performance. Furthermore, our approach yields improved generalization on unseen
data, suggesting that rectifying a model's belief space is a promising
direction for mitigating errors and enhancing overall reliability.",['cs.CL'],2502.15401," In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be publicly available
subsequently.",['cs.CL'],False,,,,Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning,"Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs
  Complex Reasoning"
neg-d2-980,2025-03-21,,2503.17273," We study the subrank of real order-three tensors and give an upper bound to
the subrank of a real tensor given its complex subrank. Using similar arguments
to those used by Bernardi-Blekherman-Ottaviani, we show that all subranks
between the minimal typical subrank and the maximal typical subrank, which
equals the generic subrank, are also typical. We then study small tensor
formats with more than one typical subrank. In particular, we construct a $3
\times 3 \times 5$-tensor with subrank $2$ and show that the subrank of the $4
\times 4 \times 4$-quaternion multiplication tensor is $2$. Finally, we
consider the tensor associated to componentwise complex multiplication in
$\mathbb{C}^n$ and show that this tensor has real subrank $n$ - informally, no
more than $n$ real scalar multiplications can be carried out using a device
that does $n$ complex scalar multiplications. We also prove a version of this
result for other real division algebras.",['math.AG'],2503.17273," We study the subrank of real order-three tensors and give an upper bound to
the subrank of a real tensor given its complex subrank. Using similar arguments
to those used by Bernardi-Blekherman-Ottaviani, we show that all subranks
between the minimal typical subrank and the maximal typical subrank, which
equals the generic subrank, are also typical. We then study small tensor
formats with more than one typical subrank. In particular, we construct a $3
\times 3 \times 5$-tensor with subrank $2$ and show that the subrank of the $4
\times 4 \times 4$-quaternion multiplication tensor is $2$. Finally, we
consider the tensor associated to componentwise complex multiplication in
$\mathbb{C}^n$ and show that this tensor has real subrank $n$ - informally, no
more than $n$ real scalar multiplications can be carried out using a device
that does $n$ complex scalar multiplications. We also prove a version of this
result for other real division algebras.",['math.AG'],False,,,,Real subrank of order-three tensors,Real subrank of order-three tensors
neg-d2-981,2025-01-28,,2501.16987," Dielectronic recombination resonance strengths, energy-differential cross
sections, and recombination rate coefficients are calculated fully
relativistically for Fe$^{2+}$ ions. The ground-state and resonance energies
are determined using the multiconfiguration Dirac-Hartree-Fock method.
Radiative and auto-ionization rates are computed with a relativistic
configuration interaction method. For the calculation of Auger widths and
resonance strengths, the continuum electron is treated within the framework of
the relativistic distorted-wave model. Notably, the calculated level energies
for Fe$^{2+}$ not only align well with experimental results but also show
improvements compared to earlier theoretical studies. These fully relativistic
calculations provide a more accurate and comprehensive understanding of the
recombination process. This is particularly important in astrophysics and
plasma physics, especially for studying phenomena such as kilonova events.",['physics.atom-ph'],2503.01778," This work presents a novel formulation for a redefinition of the second based
on the weighted arithmetic mean of multiple normalized frequencies. We
demonstrate that it is mathematically equivalent to the previously discussed
implementation employing a geometric mean. In our reformulation, the
normalization of frequencies provides the defining constants with immediate
physical meaning, while maintaining the decoupling of assigned weights from the
frequencies of the reference transitions. We believe that a definition based on
this formulation would be significantly more accessible to both experts and
non-specialists, enhancing understanding and facilitating broader acceptance.
We hope that this approach will help overcome barriers to the adoption of a
redefinition that effectively values all state-of-the-art atomic clocks.",['physics.atom-ph'],False,,,,Dielectronic recombination studies on Fe$^{2+}$,"An Accessible Formulation for Defining the SI Second Based on Multiple
  Atomic Transitions"
neg-d2-982,2025-02-04,,2502.02212," We address the problem of solving a system of linear equations via the
Quantum Singular Value Transformation (QSVT). One drawback of the QSVT
algorithm is that it requires huge quantum resources if we want to achieve an
acceptable accuracy. To reduce the quantum cost, we propose a hybrid
quantum-classical algorithm that improves the accuracy and reduces the cost of
the QSVT by adding iterative refinement in mixed-precision A first quantum
solution is computed using the QSVT, in low precision, and then refined in
higher precision until we get a satisfactory accuracy. For this solver, we
present an error and complexity analysis, and first experiments using the
quantum software stack myQLM.",['quant-ph'],2503.03527," The equivalence between the Schr\""odinger and Heisenberg representations is a
cornerstone of quantum mechanics. However, this relationship remains unclear in
the non-Hermitian regime, particularly when the Hamiltonian is time-dependent.
In this study, we address this gap by establishing the connection between the
two representations, incorporating the metric of the Hilbert space bundle. We
not only demonstrate the consistency between the Schr\""odinger and Heisenberg
representations but also present a Heisenberg-like representation grounded in
the generalized vielbein formalism, which provides a clear and intuitive
geometric interpretation. Unlike the standard Heisenberg representation, where
the metric of the Hilbert space is encoded solely in the dual states, the
Heisenberg-like representation distributes the metric information between both
the states and the dual states. Despite this distinction, it retains the same
Heisenberg equation of motion for operators. Within this formalism, the
Hamiltonian is replaced by a Hermitian counterpart, while the ""non-Hermiticity""
is transferred to the operators. Moreover, this approach extends to regimes
with a dynamical metric (beyond the pseudo-Hermitian framework) and to systems
governed by time-dependent Hamiltonians.",['quant-ph'],False,,,,A mixed-precision quantum-classical algorithm for solving linear systems,"Heisenberg and Heisenberg-Like Representations via Hilbert Space Bundle
  Geometry in the Non-Hermitian Regime"
neg-d2-983,2025-03-12,,2503.09088," This paper studies the derivation and well-posedness of a class of high -
order water wave equations, the fifth - order Benjamin - Bona - Mahony (BBM)
equation. Low - order models have limitations in describing strong nonlinear
and high - frequency dispersion effects. Thus, it is proposed to improve the
modeling accuracy of water wave dynamics on long - time scales through high -
order correction models. By making small - parameter corrections to the
$abcd-$system, then performing approximate estimations, the fifth - order BBM
equation is finally derived.For local well - posedness, the equation is first
transformed into an equivalent integral equation form. With the help of
multilinear estimates and the contraction mapping principle, it is proved that
when $s\geq1$, for a given initial value $\eta_{0}\in H^{s}(\mathbb{R})$, the
equation has a local solution $\eta \in C([0, T];H^{s})$, and the solution
depends continuously on the initial value. Meanwhile, the maximum existence
time of the solution and its growth restriction are given.For global well -
posedness, when $s\geq2$, through energy estimates and local theory, combined
with conservation laws, it is proved that the initial - value problem of the
equation is globally well - posed in $H^{s}(\mathbb{R})$. When $1\leq s<2$, the
initial value is decomposed into a rough small part and a smooth part, and
evolution equations are established respectively. It is proved that the
corresponding integral equation is locally well - posed in $H^{2}$ and the
solution can be extended, thus concluding that the initial - value problem of
the equation is globally well - posed in $H^{s}$.",['math.AP'],2501.04893," In this paper, we consider the existence and multiplicity of prescribed mass
solutions to the following nonlinear Schr\""{o}dinger equation with general
nonlinearity: Mass super-critical case: \[\begin{cases} -\Delta u+V(x)u+\lambda
u=g(u),\\ \|u\|_2^2=\int|u|^2\mathrm{d}x=c, \end{cases} \] both on large
bounded smooth star-shaped domain $\Omega\subset\mathbb{R}^N$ and on
$\mathbb{R}^N$, where $V(x)$ is the potential and the nonlinearity $g(\cdot)$
considered here are very general and of mass super-critical. The standard
approach based on the Pohozaev identity to obtain normalized solutions is
invalid as the presence of potential $V(x)$. In addition, our study can be
considered as a complement of Bartsch-Qi-Zou (Math Ann 390, 4813--4859, 2024),
which has addressed an open problem raised in Bartsch et al. (Commun Partial
Differ Equ 46(9):1729--1756, 2021).",['math.AP'],False,,,,"Derivation and Well-Posedness Analysis of the Higher-Order
  Benjamin-Bona-Mahony Equation","Normalized Solutions on large smooth domains to the Schr\""{o}dinger
  equation with potential and general nonlinearity: Mass super-critical case"
neg-d2-984,2025-03-07,,2503.05964," Proper-orthogonal decomposition (POD) based reduced-order models (ROM) of
structurally dominant fluid flow can support a wide range of engineering
applications. Yet, although they perform well for unsteady laminar flows, their
straightforward extension to turbulent flows fails to capture the effects of
small scale eddies and often leads to divergent solutions. Several approaches
to mimic nonlinear closure terms modeling techniques within ROM frameworks have
been employed to include the effect of higher modes that are often neglected.
Recent success of neural network based models show promising results in
modeling the effects of turbulence. In this study, we augment POD-ROM with a
recurrent neural network (RNN) to develop ROM for turbulent flows. We simulate
a three dimensional flow past a circular cylinder at Reynolds number of 1000.
We first compute the POD modes and project the Navier-Stokes equations onto the
limited number of modes in a Galerkin approach to develop a conventional ROM
and LES-inspired ROM for comparison. We then develop a hybrid model by
integrating the output of Galerkin projection ROM and long short-term memory
(LSTM) RNN and term it as a physics-guided machine learning (PGML) model. The
novelty of this study is to introduce a hybrid model that integrates LES
inspired ROM and RNN to achieve more accurate and reliable predictions of
turbulent flows. The results demonstrate that PGML for higher temporal
coefficients outperforms the conventional and LES-inspired ROM.",['physics.flu-dyn'],2501.08178," Quasiperiodicity, a partially synchronous state that precedes the onset of
forced synchronization in hydrodynamic systems, exhibits distinct geometrical
patterns based on the specific route to lock-in. In this study, we explore
these dynamic behaviors using recurrence quantification analysis. Focusing on a
self-excited hydrodynamic system-a low-density jet subjected to external
acoustic forcing at varying frequencies and amplitudes. We generate recurrence
plots from unsteady velocity time traces. These recurrence plots provide
insight into the synchronization dynamics and pathways of the jet under forced
conditions. Further, we show that recurrence quantities are helpful to detect
and distinguish between different routes to lock-in.",['physics.flu-dyn'],False,,,,"Hybrid Reduced-Order Models for Turbulent Flows Using Recurrent Neural
  Architectures","Detection and analysis of synchronization routes in an axially forced
  globally unstable jet using recurrence quantification"
neg-d2-985,2025-03-09,,2503.17367," In this study, we develop and implement a specialized coupled-cluster (CC)
approach tailored for accurately describing atoms and molecules in strong
magnetic fields. Using the open-source Ghent Quantum Chemistry Package
(\texttt{GQCP}) in conjunction with the Python-based Simulations of Chemistry
Framework (\texttt{PySCF}), we calculate potential energy curves, permanent and
transient dipole moments, as well as vibrational spectra for the diatomic
molecules H$_2$, HeH$^+$ and LiH under various magnetic field strengths
adopting a fully non-perturbative treatment. The main computational
difficulties stem from the inclusion of the magnetic field in the Hamiltonian,
in particular, from the presence of the angular momentum operator, which leads
to a complication of the wave function and introduces a gauge-origin
dependence. Addressing these challenges requires advanced modifications to
existing routines, which we achieve by implementing gauge-comprising atomic
orbitals (GIAOs) by using \texttt{GQCP}, and the capabilities offered by
\texttt{PySCF}. This approach enhances the accuracy and reliability of the CC
theory, opening pathways for more comprehensive investigations in molecular
quantum chemistry at strong magnetic fields.",['physics.atom-ph'],2503.01778," This work presents a novel formulation for a redefinition of the second based
on the weighted arithmetic mean of multiple normalized frequencies. We
demonstrate that it is mathematically equivalent to the previously discussed
implementation employing a geometric mean. In our reformulation, the
normalization of frequencies provides the defining constants with immediate
physical meaning, while maintaining the decoupling of assigned weights from the
frequencies of the reference transitions. We believe that a definition based on
this formulation would be significantly more accessible to both experts and
non-specialists, enhancing understanding and facilitating broader acceptance.
We hope that this approach will help overcome barriers to the adoption of a
redefinition that effectively values all state-of-the-art atomic clocks.",['physics.atom-ph'],False,,,,"Exploring the Properties of Light Diatomic Molecules in Strong Magnetic
  Fields","An Accessible Formulation for Defining the SI Second Based on Multiple
  Atomic Transitions"
neg-d2-986,2025-03-08,,2503.06308," Computable phenotypes are used to characterize patients and identify outcomes
in studies conducted using healthcare claims and electronic health record data.
Chart review studies establish reference labels against which computable
phenotypes are compared to understand their measurement characteristics, the
quantity of interest, for instance the positive predictive value. We describe a
method to adaptively evaluate a quantity of interest over sequential samples of
charts, with the goal to minimize the number of charts reviewed. With the help
of a simultaneous confidence band, we stop the reviewing once the confidence
band meets a pre-specified stopping threshold. The contribution of this article
is threefold. First, we tested the use of an adaptive approach called Neyman's
sampling of charts versus random or stratified random sampling. Second, we
propose frequentist confidence bands and Bayesian credible intervals to
sequentially evaluate the quantity of interest. Third, we propose a tool to
predict the stopping time (defined as the number of charts reviewed) at which
the chart review would be complete. We observe that Bayesian credible intervals
proved to be tighter than its frequentist confidence band counterparts.
Moreover, we observe that simple random sampling is often performing similarly
to Neyman's sampling.",['stat.AP'],2502.10787," Excess mortality, i.e. the difference between expected and observed
mortality, is used to quantify the death toll of mortality shocks, such as
infectious disease-related epidemics and pandemics. However, predictions of
expected mortality are sensitive to model assumptions. Among three
specifications of a Serfling-Poisson regression for seasonal mortality, we
analyse which one yields the most accurate predictions. We compare the
Serfling-Poisson models with: 1) parametric effect for the trend and
seasonality (SP), 2) non-parametric effect for the trend and seasonality
(SP-STSS), also known as modulation model, and 3) non-parametric effect for the
trend and parametric effect for the seasonality (SP-STFS). Forecasting is
achieved with P-splines smoothing. The SP-STFS model resulted in more accurate
historical forecasts of monthly rates from national statistical offices in 25
European countries. An application to the COVID-19 pandemic years illustrates
how excess mortality can be used to evaluate the vulnerability of populations
and aid public health planning.",['stat.AP'],False,,,,Adaptive multi-wave sampling for efficient chart validation,Modelling and short-term forecasting of seasonal mortality
neg-d2-987,2025-02-07,,2502.04839," Algebraic Morava K-theories are defined by Sechin,Vishik and others as
quotients of algebraic cobordisms. On the other hand, the author had defined
them as some (two degrees) cohomology theories. In this paper, we compare these
theories.",['math.AT'],2502.12228," In this note, we show that there does not exist a $C_2$-ring spectrum whose
underlying ring spectrum is $\mathrm{MSpin}^c$ and whose $C_2$-fixed point
spectrum is $\mathrm{MSpin}$.",['math.AT'],False,,,,Definitions and examples of algeebraic Morava K-theories,On the nonexistence of a Green functor with values MSpin${}^c$ and MSpin
neg-d2-988,2025-03-13,,2503.10876," The proliferation of Large Language Models (LLMs) in recent years has
realized many applications in various domains. Being trained with a huge of
amount of data coming from various sources, LLMs can be deployed to solve
different tasks, including those in Software Engineering (SE). Though they have
been widely adopted, the potential of using LLMs cooperatively has not been
thoroughly investigated. In this paper, we proposed Metagente as a novel
approach to amplify the synergy of various LLMs. Metagente is a Multi-Agent
framework based on a series of LLMs to self-optimize the system through
evaluation, feedback, and cooperation among specialized agents. Such a
framework creates an environment where multiple agents iteratively refine and
optimize prompts from various perspectives. The results of these explorations
are then reviewed and aggregated by a teacher agent. To study its performance,
we evaluated Metagente with an SE task, i.e., summarization of README.MD files,
and compared it with three well-established baselines, i.e., GitSum, LLaMA-2,
and GPT-4o. The results show that our proposed approach works efficiently and
effectively, consuming a small amount of data for fine-tuning but still getting
a high accuracy, thus substantially outperforming the baselines. The
performance gain compared to GitSum, the most relevant benchmark, ranges from
27.63% to 60.43%. More importantly, compared to using only one LLM, Metagente
boots up the accuracy to multiple folds.",['cs.SE'],2503.05561," Chatbots are software typically embedded in Web and Mobile applications
designed to assist the user in a plethora of activities, from chit-chatting to
task completion. They enable diverse forms of interactions, like text and voice
commands. As any software, even chatbots are susceptible to bugs, and their
pervasiveness in our lives, as well as the underlying technological
advancements, call for tailored quality assurance techniques. However, test
case generation techniques for conversational chatbots are still limited. In
this paper, we present Chatbot Test Generator (CTG), an automated testing
technique designed for task-based chatbots. We conducted an experiment
comparing CTG with state-of-the-art BOTIUM and CHARM tools with seven chatbots,
observing that the test cases generated by CTG outperformed the competitors, in
terms of robustness and effectiveness.",['cs.SE'],False,,,,"Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD
  Summarization",Test Case Generation for Dialogflow Task-Based Chatbots
neg-d2-989,2025-01-31,,2501.18906," We solve the lifting problem for Galois representations in every dimension
and in every characteristic. That is, we determine all pairs $(n,k)$, where $n$
is a positive integer and $k$ is a field of characteristic $p>0$, such that for
every field $F$, every continuous homomorphism $\Gamma_F\to \mathrm{GL}_n(k)$
lifts to $\mathrm{GL}_n(W_2(k))$, where $\Gamma_F$ is the absolute Galois group
of $F$ and $W_2(k)$ is the ring of $p$-typical length $2$ Witt vectors of $k$.",['math.NT'],2503.03994," Let $p$ be an odd prime, and $\mathbf{Q}_{p^f}$ the unramified extension of
$\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of
constructing strongly divisible modules for $2$-dimensional semi-stable
non-crystalline representations of
$\mathrm{Gal}(\overline{\mathbf{Q}}_p/\mathbf{Q}_{p^f})$ with Hodge--Tate
weights in the Fontaine--Laffaille range to solving systems of linear equations
and inequalities. We also determine the Breuil modules corresponding to the
mod-$p$ reduction of the strongly divisible modules. We expect our method to
produce at least one Galois-stable lattice in each such representation for
general $f$. Moreover, when the mod-$p$ reduction is an extension of distinct
characters, we further expect our method to provide the two non-homothetic
lattices. As applications, we show that our approach recovers previously known
results for $f=1$ and determine the mod-$p$ reduction of the semi-stable
representations with some small Hodge--Tate weights when $f=2$.",['math.NT'],False,,,,The lifting problem for Galois representations,On families of strongly divisible modules of rank 2
neg-d2-990,2025-02-13,,2502.09248," Traditional Phase-Linking (PL) algorithms are known for their high cost,
especially with the huge volume of Synthetic Aperture Radar (SAR) images
generated by Sentinel-1 SAR missions. Recently, a COvariance Fitting
Interferometric Phase Linking (COFI-PL) approach has been proposed, which can
be seen as a generic framework for existing PL methods. Although this method is
less computationally expensive than traditional PL approaches, COFI-PL exploits
the entire covariance matrix, which poses a challenge with the increasing time
series of SAR images. However, COFI-PL, like traditional PL approaches, cannot
accommodate the efficient inclusion of newly acquired SAR images. This paper
overcomes this drawback by introducing a sequential integration of a block of
newly acquired SAR images. Specifically, we propose a method for effectively
addressing optimization problems associated with phase-only complex vectors on
the torus based on the Majorization-Minimization framework.",['stat.AP'],2501.0618," Electricity price forecasting is a critical tool for the efficient operation
of power systems and for supporting informed decision-making by market
participants. This paper explores a novel methodology aimed at improving the
accuracy of electricity price forecasts by incorporating probabilistic inputs
of fundamental variables. Traditional approaches often rely on point forecasts
of exogenous variables such as load, solar, and wind generation. Our method
proposes the integration of quantile forecasts of these fundamental variables,
providing a new set of exogenous variables that account for a more
comprehensive representation of uncertainty. We conducted empirical tests on
the German electricity market using recent data to evaluate the effectiveness
of this approach. The findings indicate that incorporating probabilistic
forecasts of load and renewable energy source generation significantly improves
the accuracy of point forecasts of electricity prices. Furthermore, the results
clearly show that the highest improvement in forecast accuracy can be achieved
with full probabilistic forecast information. This highlights the importance of
probabilistic forecasting in research and practice, particularly that the
current state-of-the-art in reporting load, wind and solar forecast is
insufficient.",['stat.AP'],False,,,,Sequential Covariance Fitting for InSAR Phase Linking,"Probabilistic Forecasts of Load, Solar and Wind for Electricity Price
  Forecasting"
neg-d2-991,2025-01-23,,2501.14031," Using the TNG100-1 cosmological simulations, we explore how galaxy
properties, such as specific star formation rate ($\rm sSFR=SFR/M_*$), gas
fraction ($\rm f_{gas} \,= \, M_{\rm H}/M_{*}$), and star formation efficiency
($\rm SFE_{H} = SFR/M_{H}$), change over the course of galaxy-galaxy
interactions. We identify 18,534 distinct encounters from the reconstructed
orbits of a sample of massive galaxies ($\rm M_{*} > 10^{10} \; \rm M_{\odot}$)
with companions within a stellar mass ratio of 0.1 to 10. Using these
encounters, we study the variation of galaxy properties over time as they
approach and move away from pericentric encounters over a redshift range of $0
\leq z < 1$. Following the closest pericentric encounters ($\leq 50$ kpc) of a
host galaxy with its companion, we find that sSFR is enhanced by a factor of
$1.6 \pm 0.1$ on average within the central stellar half-mass radius
(R\textsubscript{1/2}) compared to pre-encounter values. Our results show a
time delay between pericentre and maximum sSFR enhancement of $\sim$0.1 Gyr
with a mean galaxy separation of 75 kpc. We similarly find that $\rm f_{gas}$
is enhanced by a factor of $1.2 \pm 0.1$, and $\rm SFE_{H}$ is enhanced by a
factor of $1.4 \pm 0.1$ following the pericentre of an encounter within the
same timescale. Additionally, we find evidence of inflowing gas towards the
centre, measured by comparing the $\rm f_{gas}$ and metallicity within the
central R\textsubscript{1/2} to the galactic outskirts. We find that
approximately 70 per cent of the peak sSFR enhancement can be attributed to the
increase in $\rm SFE_{H}$, with the increase in $\rm f_{gas}$ contributing the
remaining 30 per cent.",['astro-ph.GA'],2501.1028," The hierarchical interplay among gravity, magnetic fields, and turbulent gas
flows in delivering the necessary material to form massive protostellar
clusters remains enigmatic. We have performed high-resolution (beam size
$\sim$14 arcsec $\simeq$ 0.05 pc at a distance 725 pc) 850 $\mu$m dust
polarization and C$^{18}$O molecular line observations of Cepheus A (Cep A),
the second closest massive star-forming region, using the 15-meter James Clerk
Maxwell Telescope (JCMT) along with the SCUBA-2/POL-2 and HARP instruments. Our
key analyses reveal that (i) morphologically, all three fields--gravitational
(G), magnetic (B), and kinetic (K) fields--are aligned with each other, and
(ii) energetically, they exhibit a hierarchical relationship with gravitational
($E_{\mathrm{G}}$) > magnetic ($E_{\mathrm{B}}$) > kinetic ($E_{\mathrm{K}}$).
Gravity dominates in Cep A clump and, as a primary active player, dictates the
other two agents. Consequently, gravity plays two active roles: (i) induces gas
flows and (ii) drags B-field lines toward the gravitational trough. Since
magnetic energy dominates kinetic energy, $E_{\mathrm{B}}$ > $E_{\mathrm{K}}$,
the ""dragged-in"" B-field as a secondary active player can mitigate turbulence
and instabilities, thereby regularizing gas flows into a more ordered
configuration. At the $\sim$0.60 pc clump scale, these flows deliver material
at a substantially high rate of $\sim$ 2.1$\times$10$^{-4}$ M$_{\odot}$
yr$^{-1}$ toward the cluster center. Our study, for the first time, presents
new insights into how B-fields and turbulent gas flows passively assist the
active role of gravity in the formation of a protostellar cluster, contrasting
with the standard notion that these agents primarily oppose gravitational
collapse.",['astro-ph.GA'],False,,,,"Interacting galaxies in the IllustrisTNG simulations -- VIII:
  Pericentric star formation rate enhancements are driven both by increased
  fuelling and efficiency","Evidence for the gravity-driven and magnetically-regularized gas flows
  feeding the massive protostellar cluster in Cep A"
neg-d2-992,2025-02-18,,2502.12723," This paper presents the myEye2Wheeler dataset, a unique resource of
real-world gaze behaviour of two-wheeler drivers navigating complex Indian
traffic. Most datasets are from four-wheeler drivers on well-planned roads and
homogeneous traffic. Our dataset offers a critical lens into the unique visual
attention patterns and insights into the decision-making of Indian two-wheeler
drivers. The analysis demonstrates that existing saliency models, like
TASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to
when applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),
highlighting the need for models specifically tailored to the traffic
conditions. By introducing the dataset, we not only fill a significant gap in
two-wheeler driver behaviour research in India but also emphasise the critical
need for developing context-specific saliency models. The larger aim is to
improve road safety for two-wheeler users and lane-planning to support a
cost-effective mode of transport.",['cs.CV'],2502.08377," Recently, the generation of dynamic 3D objects from a video has shown
impressive results. Existing methods directly optimize Gaussians using whole
information in frames. However, when dynamic regions are interwoven with static
regions within frames, particularly if the static regions account for a large
proportion, existing methods often overlook information in dynamic regions and
are prone to overfitting on static regions. This leads to producing results
with blurry textures. We consider that decoupling dynamic-static features to
enhance dynamic representations can alleviate this issue. Thus, we propose a
dynamic-static feature decoupling module (DSFD). Along temporal axes, it
regards the regions of current frame features that possess significant
differences relative to reference frame features as dynamic features.
Conversely, the remaining parts are the static features. Then, we acquire
decoupled features driven by dynamic features and current frame features.
Moreover, to further enhance the dynamic representation of decoupled features
from different viewpoints and ensure accurate motion prediction, we design a
temporal-spatial similarity fusion module (TSSF). Along spatial axes, it
adaptively selects similar information of dynamic regions. Hinging on the
above, we construct a novel approach, DS4D. Experimental results verify our
method achieves state-of-the-art (SOTA) results in video-to-4D. In addition,
the experiments on a real-world scenario dataset demonstrate its effectiveness
on the 4D scene. Our code will be publicly available.",['cs.CV'],False,,,,"myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking
  Dataset","Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling
  Dynamic-Static Features"
neg-d2-993,2025-02-09,,2502.05907," Completing Long-Horizon (LH) tasks in open-ended worlds is an important yet
difficult problem for embodied agents. Existing approaches suffer from two key
challenges: (1) they heavily rely on experiences obtained from human-created
data or curricula, lacking the ability to continuously update multimodal
experiences, and (2) they may encounter catastrophic forgetting issues when
faced with new tasks, lacking the ability to continuously update world
knowledge. To solve these challenges, this paper presents EvoAgent, an
autonomous-evolving agent with a continual World Model (WM), which can
autonomously complete various LH tasks across environments through
self-planning, self-control, and self-reflection, without human intervention.
Our proposed EvoAgent contains three modules, i.e., i) the memory-driven
planner which uses an LLM along with the WM and interaction memory, to convert
LH tasks into executable sub-tasks; ii) the WM-guided action controller which
leverages WM to generate low-level actions and incorporates a self-verification
mechanism to update multimodal experiences; iii) the experience-inspired
reflector which implements a two-stage curriculum learning algorithm to select
experiences for task-adaptive WM updates. Moreover, we develop a continual
World Model for EvoAgent, which can continuously update the multimodal
experience pool and world knowledge through closed-loop dynamics. We conducted
extensive experiments on Minecraft, compared with existing methods, EvoAgent
can achieve an average success rate improvement of 105% and reduce ineffective
actions by more than 6x.",['cs.RO'],2503.03192," Reliable simultaneous localization and mapping (SLAM) algorithms are
necessary for safety-critical autonomous navigation. In the
communication-constrained multi-agent setting, navigation systems increasingly
use point-to-point range sensors as they afford measurements with low bandwidth
requirements and known data association. The state estimation problem for these
systems takes the form of range-aided (RA) SLAM. However, distributed
algorithms for solving the RA-SLAM problem lack formal guarantees on the
quality of the returned estimate. To this end, we present the first distributed
algorithm for RA-SLAM that can efficiently recover certifiably globally optimal
solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),
achieves this via the Riemannian Staircase method, where computational
procedures developed for distributed certifiably correct pose graph
optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's
efficacy on real-world multi-agent datasets by achieving absolute trajectory
errors comparable to those of a state-of-the-art centralized certifiably
correct RA-SLAM algorithm. Additionally, we perform a parametric study on the
structure of the RA-SLAM problem using synthetic data, revealing how common
parameters affect DCORA's performance.",['cs.RO'],False,,,,"EvoAgent: Agent Autonomous Evolution with Continual World Model for
  Long-Horizon Tasks",Distributed Certifiably Correct Range-Aided SLAM
neg-d2-994,2025-03-11,,2503.08201," Human-centric visual perception (HVP) has recently achieved remarkable
progress due to advancements in large-scale self-supervised pretraining (SSP).
However, existing HVP models face limitations in adapting to real-world
applications, which require general visual patterns for downstream tasks while
maintaining computationally sustainable costs to ensure compatibility with edge
devices. These limitations primarily arise from two issues: 1) the pretraining
objectives focus solely on specific visual patterns, limiting the
generalizability of the learned patterns for diverse downstream tasks; and 2)
HVP models often exhibit excessively large model sizes, making them
incompatible with real-world applications. To address these limitations, we
introduce Scale-Aware Image Pretraining (SAIP), a novel SSP framework enabling
lightweight vision models to acquire general patterns for HVP. Specifically,
SAIP incorporates three learning objectives based on the principle of
cross-scale consistency: 1) Cross-scale Matching (CSM) which contrastively
learns image-level invariant patterns from multi-scale single-person images; 2)
Cross-scale Reconstruction (CSR) which learns pixel-level consistent visual
structures from multi-scale masked single-person images; and 3) Cross-scale
Search (CSS) which learns to capture diverse patterns from multi-scale
multi-person images. Three objectives complement one another, enabling
lightweight models to learn multi-scale generalizable patterns essential for
HVP downstream tasks.Extensive experiments conducted across 12 HVP datasets
demonstrate that SAIP exhibits remarkable generalization capabilities across 9
human-centric vision tasks. Moreover, it achieves significant performance
improvements over existing methods, with gains of 3%-13% in single-person
discrimination tasks, 1%-11% in dense prediction tasks, and 1%-6% in
multi-person visual understanding tasks.",['cs.CV'],2503.03613," Despite its prevalent use in image-text matching tasks in a zero-shot manner,
CLIP has been shown to be highly vulnerable to adversarial perturbations added
onto images. Recent studies propose to finetune the vision encoder of CLIP with
adversarial samples generated on the fly, and show improved robustness against
adversarial attacks on a spectrum of downstream datasets, a property termed as
zero-shot robustness. In this paper, we show that malicious perturbations that
seek to maximise the classification loss lead to `falsely stable' images, and
propose to leverage the pre-trained vision encoder of CLIP to counterattack
such adversarial images during inference to achieve robustness. Our paradigm is
simple and training-free, providing the first method to defend CLIP from
adversarial attacks at test time, which is orthogonal to existing methods
aiming to boost zero-shot adversarial robustness of CLIP. We conduct
experiments across 16 classification datasets, and demonstrate stable and
consistent gains compared to test-time defence methods adapted from existing
adversarial robustness studies that do not rely on external networks, without
noticeably impairing performance on clean images. We also show that our
paradigm can be employed on CLIP models that have been adversarially finetuned
to further enhance their robustness at test time. Our code is available
\href{https://github.com/Sxing2/CLIP-Test-time-Counterattacks}{here}.",['cs.CV'],False,,,,"Scale-Aware Pre-Training for Human-Centric Visual Perception: Enabling
  Lightweight and Generalizable Models","CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards
  Zero-shot Adversarial Robustness of CLIP"
neg-d2-995,2025-03-20,,2503.16204," The existence of the binary system SDSS J1257+5428 has been described as
paradoxical. Here we investigate under which conditions SDSS J1257+5428 could
be understood as a descendant of a cataclysmic variable with an evolved donor
star, which is a scenario that has never been explored in detail. We used the
BSE code for pre-common-envelope (CE) evolution and the MESA code for post-CE
evolution to run binary evolution simulations and searched for potential
formation pathways for SDSS J1257+5428 that lead to its observed
characteristics. For the post-CE evolution, we adopted a boosted version of the
CARB model. We find that SDSS J1257+5428 can be explained as a
post-cataclysmic-variable system if (i) the progenitor of the extremely
low-mass WD was initially a solar-type star that evolved into a subgiant before
the onset of mass transfer and underwent hydrogen shell flashes after the mass
transfer stopped, (ii) the massive WD was highly or entirely rejuvenated during
the cataclysmic variable evolution, and (iii) magnetic braking was strong
enough to make the evolution convergent. In this case, the torques due to
magnetic braking need to be stronger than those provided by the CARB model by a
factor of ${\sim100}$. We conclude that SDSS J1257+5428 can be reasonably well
explained as having originated from a cataclysmic variable that hosted an
evolved donor star and should no longer be regarded as paradoxical. If our
formation channel is correct, our findings provide further support that
stronger magnetic braking acts on progenitors of (i) close detached WD
binaries, (ii) close detached millisecond pulsar with extremely low-mass WDs,
(iii) AM CVn binaries, and (iv) ultra-compact X-ray binaries, in comparison to
the magnetic braking strength required to explain binaries hosting
main-sequence stars and single main-sequence stars.",['astro-ph.SR'],2502.19256," Observations of stars other than the Sun are sensitive to oscillations of
only low degree. Many are high-order acoustic modes. Acoustic frequencies of
main-sequence stars, for example, satisfy a well-known pattern, which some
astronomers have adopted even for red-giant stars. That is not wise, because
the internal structures of these stars can be quite different from those on the
Main Sequence, which is populated by stars whose structure is regular. Here I
report on pondering this matter, and point out two fundamental deviations from
the commonly adopted relation. There are aspects of the regular relation that
are connected in a simple way to gross properties of the star, such as the
dependence of the eigenfrequencies on the linear combination
$n+\textstyle{\frac {1}{2}}l$ of the order $n$ and degree $l$, which is
characteristic of a regular spherical acoustic cavity. That is not a feature of
red-giant frequencies, because, as experienced by the waves, red-giant stars
appear to have (phantom) singular centres, which substantially modify the
propagation of waves. That requires a generalization of the eigenfrequency
relation, which I present here. When fitted to the observed frequencies of the
Sun, the outcome is consistent with the Sun being round, with no singularity in
the core. That is hardly novel, but at least it provides some assurance that
our understanding of stellar acoustic wave dynamics is on a sound footing.",['astro-ph.SR'],False,,,,"Resolution of a paradox: SDSS J1257+5428 can be explained as a
  descendant of a cataclysmic variable with an evolved donor",Some musings on erythrogigantoacoustics
neg-d2-996,2025-02-13,,2502.09359," In the theory of integral weight harmonic Maass forms of manageable growth,
two key differential operators, the Bol operator and the shadow operator, play
a fundamental role. Harmonic Maass forms of manageable growth canonically split
into two parts, and each operator controls one of these parts. A third
operator, called the flipping operator, exchanges the role of these two parts.
Maass--Poincar\'e series (of parabolic type) form a convenient basis of
negative weight harmonic Maass forms of manageable growth, and flipping has the
effect of negating an index. Recently, there has been much interest in locally
harmonic Maass forms defined by the first author, Kane, and Kohnen. These are
lifts of Poincar\'e series of hyperbolic type, and are intimately related to
the Shimura and Shintani lifts. In this note, we prove that a similar property
holds for the flipping operator applied to these Poincar\'e series.",['math.NT'],2503.03994," Let $p$ be an odd prime, and $\mathbf{Q}_{p^f}$ the unramified extension of
$\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of
constructing strongly divisible modules for $2$-dimensional semi-stable
non-crystalline representations of
$\mathrm{Gal}(\overline{\mathbf{Q}}_p/\mathbf{Q}_{p^f})$ with Hodge--Tate
weights in the Fontaine--Laffaille range to solving systems of linear equations
and inequalities. We also determine the Breuil modules corresponding to the
mod-$p$ reduction of the strongly divisible modules. We expect our method to
produce at least one Galois-stable lattice in each such representation for
general $f$. Moreover, when the mod-$p$ reduction is an extension of distinct
characters, we further expect our method to provide the two non-homothetic
lattices. As applications, we show that our approach recovers previously known
results for $f=1$ and determine the mod-$p$ reduction of the semi-stable
representations with some small Hodge--Tate weights when $f=2$.",['math.NT'],False,,,,Flipping operators and locally harmonic Maass forms,On families of strongly divisible modules of rank 2
neg-d2-997,2025-01-08,,2501.0461," Collaborative learning in peer-to-peer networks offers the benefits of
distributed learning while mitigating the risks associated with single points
of failure inherent in centralized servers. However, adversarial workers pose
potential threats by attempting to inject malicious information into the
network. Thus, ensuring the resilience of peer-to-peer learning emerges as a
pivotal research objective. The challenge is exacerbated in the presence of
non-convex loss functions and non-iid data distributions. This paper introduces
a resilient aggregation technique tailored for such scenarios, aimed at
fostering similarity among peers' learning processes. The aggregation weights
are determined through an optimization procedure, and use the loss function
computed using the neighbor's models and individual private data, thereby
addressing concerns regarding data privacy in distributed machine learning.
Theoretical analysis demonstrates convergence of parameters with non-convex
loss functions and non-iid data distributions. Empirical evaluations across
three distinct machine learning tasks support the claims. The empirical
findings, which encompass a range of diverse attack models, also demonstrate
improved accuracy when compared to existing methodologies.",['cs.LG'],2502.06738," Recent work showed that small changes in benchmark questions can reduce LLMs'
reasoning and recall. We explore two such changes: pairing questions and adding
more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We
find that for more capable models, these predictably reduce performance,
essentially heightening the performance ceiling of a benchmark and unsaturating
it again. We suggest this approach can resurrect old benchmarks.",['cs.LG'],False,,,,Resilient Peer-to-peer Learning based on Adaptive Aggregation,Resurrecting saturated LLM benchmarks with adversarial encoding
neg-d2-998,2025-01-15,,2501.08827," The key molecules such as triphosphate (ATP), glutathione (GSH), and
homocarnosine (hCs) - central to metabolic processes in the human brain remain
elusive or challenging to detect with upfield 1H-MRSI. Traditional 3D 1H-MRSI
in vivo faces challenges, including a low signal-to-noise ratio and
magnetization transfer effects with water, leading to prolonged measurement
times and reduced resolution. To address these limitations, we propose a
downfield 3D-MRSI method aimed at measuring downfield metabolites with enhanced
spatial resolution, and speed acceptable for clinical practice at 7T. The
CHEmical-shift selective Adiabatic Pulse (CHEAP) technique was integrated into
echo-planar spectroscopic imaging (EPSI) readout sequence for downfield
metabolite and water reference 3D-MRSI. Five healthy subjects and two glioma
patients were scanned to test the feasibility. In this work, CHEAP-EPSI
technique is shown to significantly enhance spatial the resolution to 0.37 ml
while simultaneously reducing the scan time to 10.5 minutes. Its distinct
advantages include low specific absorption rate, effective suppression of water
and lipid signals, and minimal baseline distortions, making it a valuable tool
for research or potentially diagnostic purposes. CHEAP-EPSI improves the
detection sensitivity of downfield metabolites like N-acetyl-aspartate (NAA+)
and DF8.18 (ATP&GSH+), and offers new possibilities for the study of metabolism
in healthy and diseased brain.",['physics.med-ph'],2502.03475," Euler angle representation in biomechanical analysis allows straightforward
description of joints rotations. However, application of Euler angles could be
limited due to singularity called gimbal lock. Quaternions offer an alternative
way to describe rotations but they have been mostly avoided in biomechanics as
they are complex and not inherently intuitive, specifically in dynamic models
actuated by muscles. This study introduces a mathematical framework for
describing muscle actions in dynamic quaternion-based musculoskeletal
simulations. The proposed method estimates muscle torques in quaternion-based
musculoskeletal model. Its application is shown on three-dimensional
double-pendulum system actuated by muscle elements. Furthermore, transformation
of muscle moment arms obtained from muscle paths based on Euler angles into
quaternions description is presented. The proposed method is advantageous for
dynamic modeling of musculoskeletal models with complex kinematics and large
range of motion like the shoulder joint.",['physics.med-ph'],False,,,,"CHEmical-shift selective Adiabatic Pulse (CHEAP): Fast and High
  Resolution Downfield 3D 1H-MRSI at 7T","Calculation of a force effect from muscle action to a quaternion-based
  musculoskeletal model"
neg-d2-999,2025-01-03,,2501.01923," We generalize Hopf's theorem to thermostats: the total thermostat curvature
of a thermostat without conjugate points is non-positive, and vanishes only if
the thermostat curvature is identically zero. We further show that, if the
thermostat curvature is zero, then the flow has no conjugate points, and the
Green bundles collapse almost everywhere. Given a thermostat without conjugate
points, we prove that the Green bundles are transversal everywhere if and only
if it admits a dominated splitting. Finally, we provide an example showing that
Hopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is
also the first example of a thermostat with a dominated splitting which is not
Anosov.",['math.DS'],2501.11359," We consider two types of dynamical systems namely non-autonomous discrete
dynamical systems(NDDS) and generic dynamical systems(GDS). In both of them, we
study various notions of transitivity. We give many equivalent conditions for
each of these notions and present the implications among these in NDDS and GDS.
For a given NDDS, we associate a GDS and discuss whether if the given NDDS has
a particular variation of transitivity then the associated GDS also has such a
variation and vice versa.",['math.DS'],False,,,,Thermostats without conjugate points,"Various notions of topological transitivity in non-autonomous and
  generic dynamical systems"
