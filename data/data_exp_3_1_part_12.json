[
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"A ConvNet for the 2020s",
    "start_abstract":"The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry"
      ],
      "abstract":[
        "Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "From Site Response to Site-city Interaction: a Case Study in the Tokyo\n  Area",
        "Well-to-Tank Carbon Intensity Variability of Fossil Marine Fuels: A\n  Country-Level Assessment",
        "The IDEA detector concept for FCC-ee",
        "A simple magnetic field stabilization technique for atomic Bose-Einstein\n  condensate experiments",
        "Terahertz Magnon Excitations and Switching in Non-Collinear\n  Antiferromagnets",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Diffusion Approximation for Slow-Fast SDEs with State-Dependent\n  Switching",
        "Direct Nucleation of Hierarchical Nanostructures on Plasmonic Fiber\n  Optics Enables Enhanced SERS Performance",
        "Periodic Variability of the Central Stars of Planetary Nebulae Surveyed\n  through the Zwicky Transient Facility",
        "Further results on relative, divergence measures based on extropy and\n  their applications",
        "On recurrence and entropy in hyperspace of continua in dimension one",
        "Emerging Excess Consistent with a Narrow Resonance at 152 GeV in\n  High-Energy Proton-Proton Collisions",
        "Fibonacci-Modulation-Induced Multiple Topological Anderson Insulators",
        "The route of shear to Ising superconductivity in bilayer graphene",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "Large $N$ limits of supersymmetric quantum field theories: A pedagogical\n  overview",
        "A note on the differential spectrum of a class of locally APN functions",
        "Limiting distributions of generalized money exchange models",
        "Integrals of Legendre polynomials and approximations",
        "Four regimes of primary radiation damage in tungsten",
        "Manipulation of topological phase transitions and the mechanism of\n  magnetic interactions in Eu-based Zintl-phase materials",
        "Group distance magic cubic graphs",
        "An abelian formula for the quantum Weyl group action of the coroot\n  lattice",
        "PDRs4All. XII. FUV-driven formation of hydrocarbon radicals and their\n  relation with PAHs",
        "Weak-strong uniqueness for solutions to mean-field games",
        "Strong law of large numbers for a branching random walk among Bernoulli\n  traps",
        "On termination of minimal model program for log canonical pairs on\n  complex analytic spaces",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals"
      ],
      "abstract":[
        "Considering the purpose of the session relating early engineering\ndevelopments in site response and soil-structure interaction, this paper\nfocuses on the development of studies regarding site-city interaction following\nthe striking site response observations obtained in Mexico City during the 1985\nGuerrero-Michoacan event, The first part presents an overview of the\ninvestigations on multiple structure-soil-structure interaction, starting with\nMexico-city like environments with dense urbanization on soft soils, which\nlater evolved with the concept of metamaterials. Up to now, such investigations\nhave been largely relying on numerical simulations in 2D and 3D media, coupling\nsoft surface soil layers and simplified building models, including also some\ntheoretical developments using various mechanical concepts. They also relied on\na number of laboratory experiments on reduced-scale mock-ups with diverse\nvibratory sources (shaking table, acoustic devices). The latest studies coupled\nfull-scale experiments on mechanical analogs such as forests or wind turbine\nfarms involving sets of resonators with similar frequencies, and numerical\nsimulation to investigate their impact on the propagation of surface (Rayleigh)\nwaves. Almost all such studies converge in predicting lower ground motion\namplitude for sites located within the ''urbanized'' area, but none of them can\nbe considered a ''groundtruth'' proof for a real earthquake in a real city. The\nsecond part thus takes advantage of the long duration of strong motion\nobservations in the Kanto area thanks to the KiK-net, K-NET and JMA\n(Shin-dokei) networks, to investigate the possible changes in site response\nwith time. The first results obtained with the event-specific site terms\nderived from Generalized Inversion Techniques (Nakano et al., 2015) indicate a\nsystematic reduction of the low frequency (0.2 -1 Hz) site amplification, in\nthe central-south Tokyo area. As this frequency band corresponds both to the\nsite frequency (very thick deposits) and to the high-rise buildings, the\ndiscussion focuses on the possible relation with the extensive construction in\nsome areas of downtown Tokyo over the last 2 decades.",
        "The transition toward a low-carbon maritime transportation requires\nunderstanding lifecycle carbon intensity (CI) of marine fuels. While\nwell-to-tank emissions significantly contribute to total greenhouse gas\nemissions, many studies lack global perspective in accounting for upstream\noperations, transportation, refining, and distribution. This study evaluates\nwell-to-tank CI of High Sulphur Fuel Oil (HSFO) and well-to-refinery exit CI of\nLiquefied Petroleum Gas (LPG) worldwide at asset level. HSFO represents\ntraditional marine fuel, while LPG serves as potential transition fuel due to\nlower tank-to-wake emissions and compatibility with low-carbon fuels. Using\nOPGEE and PRELIM tools with R-based geospatial methods, we derive country-level\nCI values for 72 countries (HSFO) and 74 countries (LPG), covering 98% of\nglobal production. Results show significant variation in climate impacts\nglobally. HSFO upstream CI ranges 1-22.7 gCO2e\/MJ, refining CI 1.2-12.6\ngCO2e\/MJ, with global volume-weighted-average well-to-tank CI of 12.4 gCO2e\/MJ.\nUpstream and refining account for 55% and 32% of HSFO well-to-tank CI, with\nlarge exporters and intensive refining practices showing higher emissions. For\nLPG, upstream CI ranges 0.9-22.7 gCO2e\/MJ, refining CI 2.8-13.9 gCO2e\/MJ, with\nvolume-weighted-average well-to-refinery CI of 15.6 gCO2e\/MJ. Refining\ncomprises 49% of LPG well-to-refinery CI, while upstream and transport\nrepresent 44% and 6%. Major players include China, United States and Russia.\nThese findings reveal significant CI variability across countries and supply\nchains, offering opportunities for targeted emission reduction policies.",
        "A detector concept, named IDEA, optimized for the physics and running\nconditions at the FCC-ee is presented. After discussing the expected running\nconditions and the main physics drivers, a detailed description of the\nindividual sub-detectors is given. These include: a very light tracking system\nwith a powerful vertex detector inside a large drift chamber surrounded by a\nsilicon wrapper, a high resolution dual readout crystal electromagnetic\ncalorimeter, an HTS based superconducting solenoid, a dual readout fiber\ncalorimeter and three layers of muon chambers embedded in the magnet flux\nreturn yoke. Some examples of the expected detector performance, based on fast\nand full simulation, are also given.",
        "We demonstrate a simple magnetic field stabilization technique in a\nBose-Einstein condensate experiment. Our technique is based on the precise\nmeasurement of the current fluctuations in the main magnetic field coils and\namounts to their compensation using an auxiliary coil. It has the advantage of\nsimplicity as compensation is done using a low inductance coil that can be\nstraightforwardly driven at the relevant frequencies (1 kHz). The performances\nof the different components (power supply, current transducer, electronics...)\nare precisely characterized. In addition, for optimal stability the ambient\nmagnetic field is also measured and compensated. The magnetic field stability\naround 57 G is measured by Ramsey spectroscopy of magnetic field sensitive\nradiofrequency transition between two spin states of potassium 39 and the\nshot-to-shot fluctuations are reduced to 64(7) $\\mu$G rms, i.e. at the 1 x 10\n-6 level. In the context of our experiment, this result opens interesting\nprospects for the study of three-body interactions in Bose-Einstein condensate\npotassium spin mixtures.",
        "We investigate how spatiotemporal spin polarized current can lead to\nterahertz frequency excitations in non-collinear antiferromagnets. By solving\nthe Landau-Lifshitz-Gilbert equation numerically for non-collinear\nantiferromagnet, we show that the magnon frequency spectrum exhibits standing\nspin wave modes and depends on the thickness of Mn$_3$Ge in heterostructure\nFe|Au|Mn$_3$Ge. Also, we analyze the switching process of ground state as a\nfunction of a spin current. We show a switching phase diagram, which contains\nswitching and non-switching regions. Our work suggests non-collinear\nantiferromagnets as an efficient platform for terahertz magnonics and ultrafast\nmemory devices.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "In this paper, we study the diffusion approximation for slow-fast stochastic\ndifferential equations with state-dependent switching, where the slow component\n$X^{\\varepsilon}$ is the solution of a stochastic differential equation with\nadditional homogenization term, while the fast component $\\alpha^{\\varepsilon}$\nis a switching process. We first prove the weak convergence of\n$\\{X^\\varepsilon\\}_{0<\\varepsilon\\leq 1}$ to $\\bar{X}$ in the space of\ncontinuous functions, as $\\varepsilon\\rightarrow 0$. Using the martingale\nproblem approach and Poisson equation associated with a Markov chain, we\nidentify this weak limiting process as the unique solution $\\bar{X}$ of a new\nstochastic differential equation, which has new drift and diffusion terms that\ndiffer from those in the original equation. Next, we prove the order $1\/2$ of\nweak convergence of $X^{\\varepsilon}_t$ to $\\bar{X}_t$ by applying suitable\ntest functions $\\phi$, for any $t\\in [0, T]$. Additionally, we provide an\nexample to illustrate that the order we achieve is optimal.",
        "We present an innovative fabrication method to achieve bottom-up in situ\nsurface-overstructured Au nanoislands (NIs) with tunable grades of surface\ncoverage, elongation, and branching, directly on micro-optical fibers for\nsensing applications. These all-in-gold hierarchical nanostructures consist of\nNIs coated with surface protrusions of various morphologies. They are created\nin solution using a selective seeded growth approach, whereby additional gold\ngrowth is achieved over Au NIs formerly developed on the fiber facet by a\nsolid-state dewetting approach. The morphology of nanosized surface-NI\noverstructuring can be adjusted from multi-dot-decorated Au NIs to\nmulti-arm-decorated Au NIs. This engineering of optical fibers allows for\nimproved remote surface-enhanced Raman spectroscopy (SERS) molecular detection.\nBy combining solid-state dewetting and wet-chemical approaches, we achieve\nstable in-contact deposition of surface-overstructured NIs with the optical\nfiber solid substrate, alongside precise control over branching morphology and\nanisotropy extent. The fiber optic probes engineered by surface-overstructured\nNIs exhibit outstanding sensing performance in an instant and through-fiber\ndetection scheme, achieving a remarkable detection limit at 10-7 M for the R6G\naqueous solution. These engineered probes demonstrate an improved detection\nlimit by one order of magnitude and enhanced peak prominence compared to\ndevices solely decorated with pristine NIs.",
        "A consensus has been reached in recent years that binarity plays an important\nrole in the formation and evolution of a significant fraction of planetary\nnebulae (PNe). Utilizing the archived photometric data from the Zwicky\nTransient Facility survey, we conducted a comprehensive data mining in search\nfor brightness variations in a large sample of Galactic PNe. This effort leads\nto identification of 39 PNe, whose central stars exhibit periodic variation in\nlight curves. Among these objects, 20 are known binary central stars of PNe,\nwhile the remaining 19 are new discoveries. Additionally, we identified 14 PNe\nwith central stars displaying anomalous variation in light curves, as well as\neight variables based on the high-cadence photometric data. Among the new\ndiscoveries of periodicity, we found compelling evidence of binary systems at\nthe centres of two archetypal quadrupolar PNe. We also report on very peculiar\nbrightness variation observed in the central core of the compact PN NGC6833.\nSeveral PNe in our sample deserve follow-up observations, both high-dispersion\nspectroscopy and high-precision photometry, to reveal the true nature of their\ncentral binarity or even multiplicity.",
        "This study explores information measures based on extropy, introducing\ndynamic relative extropy measures for residual and past lifetimes, and\ninvestigating their various properties. Furthermore, the study analyzes the\nrelationships between extropy-based divergence with dynamic relative extropy\nand other extropy measures. A nonparametric estimator for relative extropy is\ndeveloped, and its performance is assessed through numerical simulation\nstudies. The practical applicability of the relative extropy is demonstrated\nthrough some real-life data sets.",
        "We show that if $G$ is a topological graph, and $f$ is continuous map, then\nthe induced map $\\tilde{f}$ acting on the hyperspace $C(G)$ of all connected\nsubsets of $G$ by natural formula $\\tilde{f}(C)=f(C)$ carries the same entropy\nas $f$.\n  This is well known that it does not hold on the larger hyperspace of all\ncompact subsets. Also negative examples were given for the hyperspace $C(X)$ on\nsome continua $X$, including dendrites.\n  Our work extends previous positive results obtained first for much simpler\ncase of compact interval by completely different tools.",
        "The Higgs boson discovery at the Large Hadron Collider (LHC) at CERN\nconfirmed the existence of the last missing particle of the Standard Model\n(SM). The existence of new fundamental constituents of matter beyond the SM is\nof great importance for our understanding of Nature. In this context, indirect\n(non-resonant) indications for new scalar bosons were found in the data from\nthe first run of the LHC, taken between 2010 and 2012 at CERN: an excess in the\ninvariant mass of muon-electron pairs, consistent with a new Higgs boson ($S$)\nwith a mass of $150\\pm5$ GeV. Other processes with multiple leptons in the\nfinal state, moderate missing energy, and possibly (bottom quark) jets exhibit\ndeviations from the SM predictions. These anomalies can be explained within a\nsimplified model in which a new heavy Higgs boson $H$ decays into two lighter\nHiggses $S$. This lighter Higgs $S$ subsequently decays to $W$ bosons, bottom\nquarks and has also an invisible decay mode.\n  Here, we demonstrate that using this model we can identify narrow excesses in\ndi-photon and $Z$-photon spectra around 152 GeV. By incorporating the latest\nmeasurements of di-photons in association with leptons, we obtain a combined\nglobal significance of $5.4\\sigma$. This represents the highest significance\never reported for an excess consistent with a narrow resonance beyond the SM\n(BSM) in high-energy proton-proton collision data at the LHC. Such findings\nhave the potential to usher in a new era in particle physics - the BSM epoch -\noffering crucial insights into unresolved puzzles of nature.",
        "We uncover the emergence of multiple topological Anderson insulators (TAIs)\nin a 1D spin-orbit coupled (SOC) chain driven by Fibonacci modulation,\ntransforming a trivial band structure into a cascade of topologically\nnontrivial phases. This intriguing phenomenon is marked by the appearance of\nzero-energy modes and transitions in the $\\mathcal{Z}_2$ topological quantum\nnumber. Strikingly, as the SOC amplitude decreases, the number of TAI phases\ngrows, a behavior intricately linked to the fractal structure of the energy\nspectrum induced by Fibonacci modulation. Unlike conventional TAI phases, which\nexhibit fully localized eigenstates, the wave functions in the\nFibonacci-modulated TAI phases exhibit multifractal behavior. Furthermore, this\nmodel can be experimentally realized in a Bose-Einstein condensate along the\nmomentum lattice, where its topological transitions and multifractal properties\ncan be probed through quench dynamics. Our findings open new avenues for\nexploring exotic disorder-induced topological phases and their intricate\nmultifractal nature.",
        "We show that the sheared graphene bilayers can be tuned to have flat\nlow-energy bands for sufficiently large size of their moir\\'e supercell. In\nthis regime, the interacting system becomes prone to develop broken-symmetry\nphases, with valley symmetry breaking as the dominant pattern. The strong\nsignal of symmetry breaking favors the onset of a pairing instability in which\nthe electrons with opposite spin projection in the Cooper pairs live in\ndifferent valleys. The Fermi lines become distorted due to the repulsive\nCoulomb interaction, which makes the screening highly anisotropic, leading\neasily to attraction in some of the interaction channels. We also show that the\nsheared graphene bilayers offer the possibility to realize the combined\nbreakdown of parity and valley symmetry, making them very suitable to study the\ninterplay between correlations and topology in a two-dimensional electron\nsystem.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "The different large $N$ limits of supersymmetric quantum field theories in\nthree, four, and five dimensions are reviewed. We distinguish between the\nplanar limit of SQCD theories, the M-theory limit suited in three and five\ndimensions, and the long quiver limit. The method to solve exactly the sphere\npartition functions in each type of limit is spelled out in a pedagogical way.\nAfter a comprehensive general treatment of the saddle point approximation in\nthe large $N$ limit, we present an extensive list of examples and detail the\ncalculations. The scope of this overview is to provide an entry-level,\ncomputation-oriented understanding of the techniques featured in the field\ntheory side of the AdS\/CFT correspondence.",
        "Let $\\gf_{p^n}$ denote the finite field containing $p^n$ elements, where $n$\nis a positive integer and $p$ is a prime. The function\n$f_u(x)=x^{\\frac{p^n+3}{2}}+ux^2$ over $\\gf_{p^n}[x]$ with\n$u\\in\\gf_{p^n}\\setminus\\{0,\\pm1\\}$ was recently studied by Budaghyan and Pal in\n\\cite{Budaghyan2024ArithmetizationorientedAP}, whose differential uniformity is\nat most $5$ when $p^n\\equiv3~(mod~4)$. In this paper, we study the differential\nuniformity and the differential spectrum of $f_u$ for $u=\\pm1$. We first give\nsome properties of the differential spectrum of any cryptographic function.\nMoreover, by solving some systems of equations over finite fields, we express\nthe differential spectrum of $f_{\\pm1}$ in terms of the quadratic character\nsums.",
        "The \"Money Exchange Model\" is a type of agent-based simulation model used to\nstudy how wealth distribution and inequality evolve through monetary exchanges\nbetween individuals. The primary focus of this model is to identify the\nlimiting wealth distributions that emerge at the macroscopic level, given the\nmicroscopic rules governing the exchanges among agents. In this paper, we\nformulate generalized versions of the immediate exchange model and the uniform\nsaving model both of which are types of money exchange models, as discrete-time\ninteracting particle systems and characterize their stationary distributions.\nFurthermore, we prove that under appropriate scaling, the asymptotic wealth\ndistribution converges to a Gamma distribution or an exponential distribution\nfor both models. The limiting distribution depends on the weight function that\naffects the probability distribution of the number of coins exchanged by each\nagent. In particular, our results provide a mathematically rigorous formulation\nand generalization of the assertions previously predicted in studies based on\nnumerical simulations and heuristic arguments.",
        "We derive some identities and relations and extremal problems and\nminimization and Fourier development involving of integral Legendre\npolynomials.",
        "We observe for the first time in silico the transition to a linear regime in\nthe primary damage production in tungsten. As the critical plasma-facing\nmaterial in fusion reactors, radiation damage in tungsten has been studied\nextensively in experiments and simulations. Irradiation experiments routinely\nproduce recoils in the MeV range while full atomistic modelling has been\nlimited to a few hundred keV. Here we bridge these scales with extremely\nlarge-scale and accurate machine-learning-driven molecular dynamics simulations\nwith recoil energies up to 2 MeV in systems up to one billion atoms. We reveal\nfour regimes of primary damage as a function of damage energy, with a\ntransition to a high-energy regime that deviates from all previous models.\nCuriously, the start of the high-energy regime coincides with the highest\npossible recoil energy to tungsten atoms from fusion-emitted neutrons (300\nkeV).",
        "Various topological phases, including topological insulators, topological\nsemimetals, and topological superconductors, along with the controllable\ntopological phase transitions, have attracted considerable attention due to\ntheir promising applications in spintronics and quantum computing. In this\nwork, we propose two distinct methods for manipulating topological phase\ntransitions in magnetic materials. First, by varying the strength of electron\ncorrelation effects, we induce a series of topological state transitions within\nthe EuM$_2$X$_2$ (M = Zn, Cd; X = P, As, Sb) family of Zintl materials,\nincluding magnetic topological crystalline insulators (TCIs) and magnetic Dirac\nsemimetals. Our findings indicate that strong electron correlation effects tend\nto influence the emergence of topological phases. Second, by reducing the\nelectronegativity of the pnictogen X (from P to As and Sb), we observe a\nsimilar transition from trivial insulator to magnetic Dirac semimetal or\nmagnetic TCI. This suggests that weaker electronegativity favors the emergence\nof topological phases. Furthermore, we establish a Heisenberg model to describe\nthe magnetic interactions of the EuM$_2$X$_2$ system, based on which we perform\nMonte Carlo simulations of specific heat and magnetic susceptibility, yielding\nN\\'eel temperatures that perfectly match experimental data. This suggests that\nthe local magnetic moment framework provides an accurate description of the\nmagnetization behavior in this family of materials. This work provides the\npotential for the experimental manipulation of topological phase transitions\nand their possible applications, while also enhancing the understanding of the\nmagnetic interactions within the EuM$_2$X$_2$ system and offering a theoretical\nfoundation for future applications in magnetism.",
        "A $\\Gamma$\\emph{-distance magic labeling} of a graph $G = (V, E)$ with $|V| =\nn$ is a bijection $\\ell$ from $V$ to an Abelian group $\\Gamma$ of order $n$,\nfor which there exists $\\mu \\in \\Gamma$, such that the weight $w(x) =\\sum_{y\\in\nN(x)}\\ell(y)$ of every vertex $x \\in V$ is equal to $\\mu$. In this case, the\nelement $\\mu$ is called the \\emph{magic constant of} $G$. A graph $G$ is called\na \\emph{group distance magic} if there exists a $\\Gamma$-distance magic\nlabeling of $G$ for every Abelian group $\\Gamma$ of order $n$.\n  In this paper, we focused on cubic $\\Gamma$-distance magic graphs as well as\nsome properties of such graphs.",
        "Let g be a complex simple Lie algebra and Uq(Lg) its quantum loop algebra,\nwhere q is not a root of unity. We give an explicit formula for the quantum\nWeyl group action of the coroot lattice Q of g on finite-dimensional\nrepresentations of Uq(Lg) in terms of its commuting generators. The answer is\nexpressed in terms of the Chari-Pressley series, whose evaluation on highest\nweight vectors gives rise to Drinfeld polynomials. It hinges on a strong\nrationality result for that series, which is derived in the present paper. As\nan application, we identify the action of Q on the equivariant K-theory of\nNakajima quiver varieties with that of explicitly given determinant line\nbundles.",
        "We present subarcsecond-resolution ALMA mosaics of the Orion Bar PDR in [CI]\n609um, C2H (4-3), and C18O (3-2) emission lines complemented by JWST images of\nH2 and aromatic infrared band (AIB) emission. The rim of the Bar shows very\ncorrugated structures made of small-scale H2 dissociation fronts (DFs). The\n[CI] 609 um emission peaks very close (~0.002 pc) to the main H2-emitting DFs,\nsuggesting the presence of gas density gradients. These DFs are also bright and\nremarkably similar in C2H emission, which traces \"hydrocarbon radical peaks\"\ncharacterized by very high C2H abundances, reaching up to several x10^-7. The\nhigh abundance of C2H and of related hydrocarbon radicals, such as CH3, CH2,\nand CH, can be attributed to gas-phase reactions driven by elevated\ntemperatures, the presence of C+ and C, and the reactivity of FUV-pumped H2.\nThe hydrocarbon radical peaks roughly coincide with maxima of the 3.4\/3.3 um\nAIB intensity ratio, a proxy for the aliphatic-to-aromatic content of PAHs.\nThis implies that the conditions triggering the formation of simple\nhydrocarbons also favor the formation (and survival) of PAHs with aliphatic\nside groups, potentially via the contribution of bottom-up processes in which\nabundant hydrocarbon radicals react in situ with PAHs. Ahead of the DFs, in the\natomic PDR zone (where [H]>>[H2]), the AIB emission is the brightest, but small\nPAHs and carbonaceous grains undergo photo-processing due to the stronger FUV\nfield. Our detection of trace amounts of C2H in this zone may result from the\nphotoerosion of these species. This study provides a spatially resolved view of\nthe chemical stratification of key carbon carriers in a PDR. Overall, both\nbottom-up and top-down processes appear to link simple hydrocarbon molecules\nwith PAHs in molecular clouds; however, the exact chemical pathways and their\nrelative contributions remain to be quantified.",
        "This paper addresses the crucial question of solution uniqueness in\nstationary first-order Mean-Field Games (MFGs). Despite well-established\nexistence results, establishing uniqueness, particularly for weaker solutions\nin the sense of monotone operators, remains an open challenge. Building upon\nthe framework of monotonicity methods, we introduce a linearization method that\nenables us to prove a weak-strong uniqueness result for stationary MFG systems\non the d-dimensional torus. In particular, we give explicit conditions under\nwhich this uniqueness holds.",
        "We study a $d$-dimensional branching random walk (BRW) in an i.i.d. random\nenvironment on $\\mathbb{Z}^d$ in discrete time. A Bernoulli trap field is\nattached to $\\mathbb{Z}^d$, where each site, independently of the others, is a\ntrap with a fixed probability. The interaction between the BRW and the trap\nfield is given by the hard killing rule. Given a realization of the\nenvironment, over each time step, each particle first moves according to a\nsimple symmetric random walk to a nearest neighbor, and immediately afterwards,\nsplits into two particles if the new site is not a trap or is killed instantly\nif the new site is a trap. Conditional on the ultimate survival of the BRW, we\nprove a strong law of large numbers for the total mass of the process. Our\nresult is quenched, that is, it holds in almost every environment in which the\nstarting point of the BRW is inside the infinite connected component of\ntrap-free sites.",
        "We study the termination of minimal model programs for log canonical pairs in\nthe complex analytic setting. By using the termination, we prove a relation\nbetween the minimal model theory for projective log canonical pairs and that\nfor log canonical pairs in the complex analytic setting. The minimal model\nprograms for algebraic stacks and analytic stacks are also discussed.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation."
      ]
    }
  },
  {
    "id":2412.11399,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Forecasting the inevitable: A review on the impacts of climate change on renewable energy resources",
    "start_abstract":"Understanding the relationship and quantifying impacts of climate change on energy production is key to meeting our objectives achieving a sustainable future. Here we review current state art methodologies forecast future climate, potential changes in renewable main findings regarding role renewables decarbonisation supply. Most studies used model power equations estimate output. The largest variation estimated was for long-term scenarios, with non-significant variations reported short-term. highest variability found wind followed by hydro, both long-term, overall low solar any period. Additionally, efforts point investments as one pillars reducing fossil fuel dependency. Current knowledge gaps about uncertainty modelling results combined effects resources. Future should focus increasing resolution models improving input data, well assess entire electricity system not concentrate single source, which will aid defining strategies.",
    "start_categories":[
      "Physical Geography"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "High-Resolution Image Synthesis with Latent Diffusion Models"
      ],
      "abstract":[
        "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on data and beyond. Additionally, their formulation allows for guiding mechanism to control generation without retraining. However, since these typically operate directly in pixel space, optimization powerful DMs often consumes hundreds GPU days inference is expensive due evaluations. To enable DM training limited computational resources while retaining quality flexibility, we apply them latent space pretrained autoencoders. In contrast previous work, such representation first time reach near-optimal point between complexity reduction detail preservation, greatly boosting visual fidelity. introducing cross-attention layers model architecture, turn flexible generators general conditioning inputs as text or bounding boxes high-resolution becomes possible convolutional manner. Our (LDMs) new state art scores inpainting class-conditional highly competitive performance various tasks, including unconditional generation, text-to-image synthesis, super-resolution, significantly reducing requirements compared pixel-based DMs."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "METAMON: Finding Inconsistencies between Program Documentation and\n  Behavior using Metamorphic LLM Queries",
        "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language\n  Models Tackling Knowledge-based Reasoning Tasks",
        "A Population Synthesis Study on the Formation of Cold Jupiters from\n  Truncated Planetesimal Disks",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline\n  Parallel Offloading",
        "BASIC: Bipartite Assisted Spectral-clustering for Identifying\n  Communities in Large-scale Networks",
        "Fractional differential equations of a reaction-diffusion SIR model\n  involving the Caputo-fractional time-derivative and a nonlinear diffusion\n  operator",
        "The best two-term underapproximation using numbers from Fibonacci-type\n  sequences",
        "Deep Lossless Image Compression via Masked Sampling and Coarse-to-Fine\n  Auto-Regression",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Integrated Sensing, Communication, and Powering (ISCAP) for IoT: A Joint\n  Beamforming Design",
        "Optimizing Sequence Alignment with Scored NFAs",
        "Observation of the charged-particle multiplicity dependence of\n  $\\sigma_{\\psi(2S)}\/\\sigma_{\\text{J}\/\\psi}$ in pPb collisions at 8.16 TeV",
        "Design Optimization of Musculoskeletal Humanoids with Maximization of\n  Redundancy to Compensate for Muscle Rupture",
        "Evidence for Variable Accretion onto PDS 70 c and Implications for\n  Protoplanet Detections",
        "Athermal creep deformation of ultrastable amorphous solids",
        "BiRating -- Iterative averaging on a bipartite graph of Beat Saber\n  scores, player skills, and map difficulties",
        "Gravitational form factors in the perturbative limit",
        "Ladder Operator Block-Encoding",
        "Integrally Hilbertian rings and the polynomial Schinzel hypothesis",
        "Stackelberg Game Based Performance Optimization in Digital Twin Assisted\n  Federated Learning over NOMA Networks",
        "Why a Bose-Einstein condensate cannot exist in a system of interacting\n  bosons at ultrahigh temperatures",
        "Frequency domain identification for multivariable motion control\n  systems: Applied to a prototype wafer stage",
        "Strict Erd\\H{o}s-Ko-Rado theorems for simplicial complexes",
        "Exploring Large Language Models (LLMs) through interactive Python\n  activities",
        "Multilingual Language Model Pretraining using Machine-translated Data",
        "Langevin Multiplicative Weights Update with Applications in Polynomial\n  Portfolio Management",
        "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
        "Strong positive recurrence and exponential mixing for diffeomorphisms"
      ],
      "abstract":[
        "Code documentation can, if written precisely, help developers better\nunderstand the code they accompany. However, unlike code, code documentation\ncannot be automatically verified via execution, potentially leading to\ninconsistencies between documentation and the actual behavior. While such\ninconsistencies can be harmful for the developer's understanding of the code,\nchecking and finding them remains a costly task due to the involvement of human\nengineers. This paper proposes METAMON, which uses an existing search-based\ntest generation technique to capture the current program behavior in the form\nof test cases, and subsequently uses LLM-based code reasoning to identify the\ngenerated regression test oracles that are not consistent with the program\nspecifications in the documentation. METAMON is supported in this task by\nmetamorphic testing and self-consistency. An empirical evaluation against 9,482\npairs of code documentation and code snippets, generated using five open-source\nprojects from Defects4J v2.0.1, shows that METAMON can classify the\ncode-and-documentation inconsistencies with a precision of 0.72 and a recall of\n0.48.",
        "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT.",
        "The occurrence rate of giant planets increases with orbital period and turns\nover at a location that roughly corresponds to the snow line of solar-type\nstars. Further, the density distribution of cold Jupiters (CJs) on the\nsemi-major axis - mass diagram shows a relatively steep inner boundary, shaping\nthe desert of warm Jupiters. The eccentricities of CJs show a broad\ndistribution with a decreasing number density towards the larger end. Previous\nplanet formation models fail to reproduce all these features at the same time.\nWe use a planet population synthesis (PPS) model with truncated initial\nplanetesimal distribution and compare the mass and orbital distribution of the\nsimulated planets with the observation. We show that the occurrence of CJs with\nrespect to the orbital period, the slope of the inner boundary of CJs on the\nsemi-major axis - mass diagram, and the eccentricity distribution of CJs agree\nreasonably well with observation, if CJs form from truncated planetesimal disks\nof 10 au or wider with suppressed migration. While PPS simulations generally\noverestimate the fraction of giants with eccentricity below 0.2, $N$-body\nsimulations produce a more consistent eccentricity distribution with\nobservation. While the fraction of high-eccentricity planets can be increased\nby widening the planetesimal disk or reducing the migration speed, a deficit of\ngiants with eccentricity between 0.2-0.4 exists regardless of the choices of\nparameters. Our results indicate that CJs are more likely born in truncated\ndisks near the snow line than in classical uniform disks.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities, driving advancements in real-world applications. However,\ntraining LLMs on increasingly long input sequences imposes significant\nchallenges due to high GPU memory and computational demands. Existing solutions\nface two key limitations: (1) memory reduction techniques, such as activation\nrecomputation and CPU offloading, compromise training efficiency; (2)\ndistributed parallelism strategies require excessive GPU resources, limiting\nthe scalability of input sequence length.\n  To address these gaps, we propose Adaptive Sequence Pipeline Parallel\nOffloading (SPPO), a novel LLM training framework that optimizes memory and\ncomputational resource efficiency for long-sequence training. SPPO introduces\nadaptive offloading, leveraging sequence-aware offloading, and two-level\nactivation management to reduce GPU memory consumption without degrading the\ntraining efficiency. Additionally, SPPO develops an adaptive pipeline\nscheduling approach with a heuristic solver and multiplexed sequence\npartitioning to improve computational resource efficiency. Experimental results\ndemonstrate that SPPO achieves up to 3.38x throughput improvement over\nMegatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with\nsequence lengths of up to 4M tokens on only 128 A100 GPUs.",
        "Community detection, which focuses on recovering the group structure within\nnetworks, is a crucial and fundamental task in network analysis. However, the\ndetection process can be quite challenging and unstable when community signals\nare weak. Motivated by a newly collected large-scale academic network dataset\nfrom the Web of Science, which includes multi-layer network information, we\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\nCommunities (BASIC), which incorporates the bipartite network information into\nthe community structure learning of the primary network. The accuracy and\nstability enhancement of BASIC is validated theoretically on the basis of the\ndegree-corrected stochastic block model framework, as well as numerically\nthrough extensive simulation studies. We rigorously study the convergence rate\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\nupper error bound than that based on the primary network information alone. We\nutilize the proposed BASIC method to analyze the newly collected large-scale\nacademic network dataset from statistical papers. During the author\ncollaboration network structure learning, we incorporate the bipartite network\ninformation from author-paper, author-institution, and author-region\nrelationships. From both statistical and interpretative perspectives, these\nbipartite networks greatly aid in identifying communities within the primary\ncollaboration network.",
        "The main aim of this study is to analyze a fractional parabolic SIR epidemic\nmodel of a reaction-diffusion, by using the nonlocal Caputo fractional\ntime-fractional derivative and employing the $p$-Laplacian operator. The\nimmunity is imposed through the vaccination program, which is regarded as a\ncontrol variable. Finding the optimal control pair that reduces the number of\nsick people, the associated vaccination, and treatment expenses across a\nconstrained time and space is our main study. The existence and uniqueness of\nthe nonnegative solution for the spatiotemporal SIR model are established. It\nis also demonstrated that an optimal control exists. In addition, we obtain a\ndescription of the optimal control in terms of state and adjoint functions.\nThen, the optimality system is resolved by a discrete iterative scheme that\nconverges after an appropriate test, similar to the forward-backward sweep\nmethod. Finally, numerical approximations are given to show the effectiveness\nof the proposed control program, which provides meaningful results using\ndifferent values of the fractional order and $p$, respectively the order of the\nCaputo derivative and the $p$-Laplacian operators.",
        "This paper studies the greedy two-term underapproximation of $\\theta\\in\n(0,1]$ using reciprocals of numbers from a Fibonacci-type sequence\n$(c_n)_{n=1}^\\infty$. We find the set of $\\theta$ whose greedy two-term\nunderapproximation is the best among all two-term underapproximations using\n$1\/c_n$'s. We then derive a neat description of the set when\n$(c_n)_{n=1}^\\infty$ is the Fibonacci sequence or the Lucas sequence.",
        "Learning-based lossless image compression employs pixel-based or\nsubimage-based auto-regression for probability estimation, which achieves\ndesirable performances. However, the existing works only consider context\ndependencies in one direction, namely, those symbols that appear before the\ncurrent symbol in raster order. We believe that the dependencies between the\ncurrent and future symbols should be further considered. In this work, we\npropose a deep lossless image compression via masked sampling and\ncoarse-to-fine auto-regression. It combines lossy reconstruction and\nprogressive residual compression, which fuses contexts from various directions\nand is more consistent with human perception. Specifically,\n  the residuals are decomposed via $T$ iterative masked sampling, and each\nsampling consists of three steps: 1) probability estimation, 2) mask\ncomputation, and 3) arithmetic coding. The iterative process progressively\nrefines our prediction and gradually presents a real image. Extensive\nexperimental results show that compared with the existing traditional and\nlearned lossless compression, our method achieves comparable compression\nperformance on extensive datasets with competitive coding speed and more\nflexibility.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "This paper studies Integrated Sensing, Communication, and Powering (ISCAP) as\na novel framework designed to enhance Internet of Things (IoT) applications\nwithin sixth-generation wireless networks. In these applications, in addition\nto IoT devices requiring an energy supply and receiving information or control\ndata to perform their tasks, the base station serving them must sense the\ndevices and their environment to localize them, thereby improving data\ntransmission and enabling simultaneous power delivery. In our multi-node ISCAP\nIoT system, we optimize base station beamforming alongside the receiver's\npower-splitting factor to maximize energy harvesting while adhering to strict\ncommunication and sensing constraints. To effectively tackle this non-convex\noptimization problem, we decompose it into three manageable subproblems and\nemploy several techniques such as semidefinite relaxation and Rayleigh quotient\nmethods to find an efficient solution. Simulation results demonstrate the\neffectiveness of the proposed design, highlighting performance trade-offs among\nsensing accuracy, communication reliability, and power transfer efficiency.",
        "The rapid increase in symbolic data has underscored the significance of\npattern matching and regular expression processing. While nondeterministic\nfinite automata (NFA) are commonly used for these tasks, they are limited to\ndetecting matches without determining the optimal one. This research expands on\nthe NAPOLY pattern-matching accelerator by introducing NAPOLY+, which adds\nregisters to each processing element to store variables like scores, weights,\nor edge costs. This enhancement allows NAPOLY+ to identify the highest score\ncorresponding to the best match in sequence alignment tasks through the\nnew-added arithmetic unit in each processor element. The design was evaluated\nagainst the original NAPOLY, with results showing that NAPOLY+ offers superior\nfunctionality and improved performance in identifying the best match. The\ndesign was implemented and tested on zynq102 and zynq104 FPGA devices, with\nperformance metrics compared across array sizes from 1K to 64K processing\nelements. The results showed that memory usage increased proportionally with\narray size with Fmax decreasing as the array size grew on both platforms. The\nreported findings focus specifically on the core array, excluding the impact of\nbuffers and DRAMs.",
        "Bound states of charm and anticharm quarks, known as charmonia, have a rich\nspectroscopic structure that can be used to probe the dynamics of hadron\nproduction in high-energy hadron collisions. Here, the cross section ratio of\nexcited ($\\psi$(2S)) and ground state (J\/$\\psi$) vector mesons is measured as a\nfunction of the charged-particle multiplicity in proton-lead (pPb) collisions\nat a center-of-mass (CM) energy per nucleon pair of 8.16 TeV. The data\ncorresponding to an integrated luminosity of 175 nb$^{-1}$ were collected using\nthe CMS detector. The ratio is measured separately for prompt and nonprompt\ncharmonia in the transverse momentum range 6.5 $\\lt$ $p_\\text{T}$ $\\lt$ 30 GeV\nand in four rapidity ranges spanning $-$2.865 $\\lt$ $y_\\text{CM}$ $\\lt$ 1.935.\nFor the first time, a statistically significant multiplicity dependence of the\nprompt cross section ratio is observed in proton-nucleus collisions. There is\nno clear rapidity dependence in the ratio. The prompt measurements are compared\nwith a theoretical model which includes interactions with nearby particles\nduring the evolution of the system. These results provide additional\nconstraints on hadronization models of heavy quarks in nuclear collisions.",
        "Musculoskeletal humanoids have various biomimetic advantages, and the\nredundant muscle arrangement allowing for variable stiffness control is one of\nthe most important. In this study, we focus on one feature of the redundancy,\nwhich enables the humanoid to keep moving even if one of its muscles breaks, an\nadvantage that has not been dealt with in many studies. In order to make the\nmost of this advantage, the design of muscle arrangement is optimized by\nconsidering the maximization of minimum available torque that can be exerted\nwhen one muscle breaks. This method is applied to the elbow of a\nmusculoskeletal humanoid Musashi with simulations, the design policy is\nextracted from the optimization results, and its effectiveness is confirmed\nwith the actual robot.",
        "Understanding the processes of planet formation and accretion in young\nsystems is essential to unraveling the initial conditions of planetary systems.\nThe PDS 70 system, which hosts two directly imaged protoplanets, provides a\nunique laboratory for studying these phenomena, particularly through H-alpha\nemission a commonly used accretion tracer. We present multi-epoch observations\nand examine the variability in accretion signatures within this system,\nfocusing on PDS 70 b and c. Using Hubble Space Telescope narrowband H-alpha\nimaging from 2020 and 2024, we achieve high signal-to-noise ratio detections of\nthese planets and reveal significant changes in H-alpha flux. For PDS 70 c, the\nH-alpha flux more than doubled between 2020 and 2024. The trend is consistent\nwith the one identified in recently published MagAO-X data, further confirming\nthat PDS 70 c has become significantly brighter in H between 2023 March and\n2024 May. The observed variability suggests dynamic accretion processes,\npossibly modulated by circumplanetary disk properties or transient accretion\nbursts. High-amplitude variability in PDS 70 c motivates simultaneous\nmonitoring of multiple accretion tracers to probe the mechanisms of mass growth\nof gas giant planets. We quantify the impact of variability on the\ndetectability of protoplanets in imaging surveys and emphasize the need for\ncontinued and regular monitoring to accurately assess the occurrence and\ncharacteristics of young, forming planets.",
        "We numerically investigate the athermal creep deformation of amorphous\nmaterials having a wide range of stability. The imposed shear stress serves as\nthe control parameter, allowing us to examine the time-dependent transient\nresponse through both the macroscopic strain and microscopic observables. Least\nstable samples exhibit monotonicity in the transient strain rate versus time,\nwhile more stable samples display a pronounced non-monotonic S-shaped curve,\ncorresponding to failure by sharp shear band formation. We identify a diverging\ntimescale associated with the fluidization process and extract the\ncorresponding critical exponents. Our results are compared with predictions\nfrom existing scaling theories relevant to soft matter systems. The numerical\nfindings for stable, brittle-like materials represent a challenge for\ntheoretical descriptions. We monitor the microscopic initiation of shear bands\nduring creep responses. Our study encompasses creep deformation across a\nvariety of materials ranging from ductile soft matter to brittle metallic and\noxide glasses, all within the same numerical framework.",
        "Difficulty estimation of Beat Saber maps is an interesting data analysis\nproblem and valuable to the Beat Saber competitive scene. We present a simple\nalgorithm that iteratively averages player skill and map difficulty estimations\nin a bipartite graph of players and maps, connected by scores, using scores\nonly as input. This approach simultaneously estimates player skills and map\ndifficulties, exploiting each of them to improve the estimation of the other,\nexploitng the relation of multiple scores by different players on the same map,\nor on different maps by the same player. While we have been unable to prove or\ncharacterize theoretical convergence, the implementation exhibits convergent\nbehaviour to low estimation error in all instances, producing accurate results.\nAn informal qualitative evaluation involving experienced Beat Saber community\nmembers was carried out, comparing the difficulty estimations output by our\nalgorithm with their personal perspectives on the difficulties of different\nmaps. There was a significant alignment with player perceived perceptions of\ndifficulty and with other existing methods for estimating difficulty. Our\napproach showed significant improvement over existing methods in certain known\nproblematic maps that are not typically accurately estimated, but also produces\nproblematic estimations for certain families of maps where the assumptions on\nthe meaning of scores were inadequate (e.g. not enough scores, or scores over\noptimized by players). The algorithm has important limitations, related to data\nquality and meaningfulness, assumptions on the domain problem, and theoretical\nconvergence of the algorithm. Future work would significantly benefit from a\nbetter understanding of adequate ways to quantify map difficulty in Beat Saber,\nincluding multidimensionality of skill and difficulty, and the systematic\nbiases present in score data.",
        "The generalized distribution amplitudes (GDAs) have been paid attention in\nrecent years because of their relation with the energy momentum tensor (EMT)\nform factors (FFs). The GDAs can be experimentally accessed through the study\nof amplitudes in $\\gamma^{\\ast} \\gamma \\to M_1 M_2$ and $\\gamma^{\\ast} \\to M_1\nM_2 \\gamma$, where $M_1M_2$ is a pseudoscalar meson pair such as $\\pi \\eta $\nand $\\eta \\eta^{\\prime}$. In this paper we calculate these amplitudes in the\nperturbative limit where the $M_1M_2$ GDAs are expressed in terms of meson\ndistribution amplitudes which have been constrained in the past experiments.\nOur explicit calculation verifies the existence of a new EMT FF which breaks\nthe conservation law of EMT for each quark flavor in its hadronic matrix\nelement. In addition, our result shows that the $M_1M_2$ GDAs are identical\nbetween $\\gamma^{\\ast} \\gamma \\to M_1 M_2$ and $\\gamma^{\\ast} \\to M_1 M_2\n\\gamma$, which confirms the universality of GDAs. In future, the GDAs and EMT\nFFs studied in this paper can be investigated at Belle II. Our study enhances\nthe accessibility to the $P$-wave GDAs in $\\gamma^{\\ast} \\gamma \\to M_1 M_2$\nand $\\gamma^{\\ast} \\to M_1 M_2 \\gamma$ and gives a promising way to search for\nexotic hybrid mesons in the future experiment.",
        "We describe and analyze LOBE (Ladder Operator Block-Encoding), a framework\nfor block-encoding second-quantized ladder operators that act upon fermionic\nand bosonic modes. We numerically benchmark these constructions using models\narising in quantum field theories including the quartic oscillator, and phi4\nand Yukawa Hamiltonians on the light front. These benchmarks show that LOBE\nproduces block-encodings with fewer non-Clifford operations, fewer\nblock-encoding ancillae and overall number of qubits, and lower rescaling\nfactors for various second-quantized operators as compared to block-encoding\nframeworks that expand the ladder operators in the Pauli basis. The LOBE\nconstructions also demonstrate favorable scaling with respect to key\nparameters, including the maximum occupation of bosonic modes, the total number\nof fermionic and bosonic modes, and the locality of the operators. LOBE is\nimplemented as an open source python package to enable further applications.",
        "We prove that all rings of integers of number fields are ``integrally\nHilbertian''. That is, i.e. they satisfy an ``integral'' version of the\nclassical Hilbert irreducibility property, to the effect that, under\nappropriate irreducibility assumptions on given multivariate polynomials with\ncoefficients in such a ring ${\\mathcal Z}$, one can specialize in the ring\n${\\mathcal Z}$ some of the variables in such a way that irreducibility over the\nring ${\\mathcal Z}$ is preserved for the specialized polynomials. We identify\nan intermediate type of ring, which we call ``Schinzel ring'', that is central\nin the study of this new Hilbert property. Dedekind domains and Unique\nFactorization Domains are shown to be Schinzel rings, leading to other examples\nof integrally Hilbertian rings. A main application is a polynomial version, for\nthe ring ${\\mathcal Z}[Y_1,\\ldots,Y_n]$, of the Schinzel Hypothesis about\nprimes in value sets of polynomials. Some consequences for the ring of integers\nitself are also deduced.",
        "Despite the advantage of preserving data privacy, federated learning (FL)\nstill suffers from the straggler issue due to the limited computing resources\nof distributed clients and the unreliable wireless communication environment.\nBy effectively imitating the distributed resources, digital twin (DT) shows\ngreat potential in alleviating this issue. In this paper, we leverage DT in the\nFL framework over non-orthogonal multiple access (NOMA) network to assist FL\ntraining process, considering malicious attacks on model updates from clients.\nA reputationbased client selection scheme is proposed, which accounts for\nclient heterogeneity in multiple aspects and effectively mitigates the risks of\npoisoning attacks in FL systems. To minimize the total latency and energy\nconsumption in the proposed system, we then formulate a Stackelberg game by\nconsidering clients and the server as the leader and the follower,\nrespectively. Specifically, the leader aims to minimize the energy consumption\nwhile the objective of the follower is to minimize the total latency during FL\ntraining. The Stackelberg equilibrium is achieved to obtain the optimal\nsolutions. We first derive the strategies for the followerlevel problem and\ninclude them in the leader-level problem which is then solved via problem\ndecomposition. Simulation results verify the superior performance of the\nproposed scheme.",
        "It is well known that a Bose-Einstein (BE) condensate of atoms exists in a\nsystem of interacting Bose atoms at $T\\lesssim T^{(i)}_{c}$, where\n$T^{(i)}_{c}$ is the BE condensation temperature of an ideal gas. It is also\ngenerally accepted that BE condensation is impossible at ``ultrahigh''\ntemperatures $T\\gg T^{(i)}_{c}$. While the latter property has been\ntheoretically proven for an ideal gas, no such proof exists for an interacting\nsystem, to our knowledge. In this paper, we propose an approximate mathematical\nproof for a finite, nonrelativistic, periodic system of $N$ spinless\ninteracting bosons. The key point is that, at $T\\gg T^{(i)}_{c}$, the main\ncontribution to the occupation number\n$N_{0}=\\frac{1}{Z}\\sum_{\\wp}e^{-E_{\\wp}\/k_{B}T}\\langle\n\\Psi_{\\wp}|\\hat{a}^{+}_{\\mathbf{0}}\\hat{a}_{\\mathbf{0}}|\\Psi_{\\wp}\\rangle$,\ncorresponding to atoms with zero momentum, originates from the states\ncontaining $N$ elementary quasiparticles. These states do not contain the BE\ncondensate of zero-momentum atoms, implying that an ultrahigh temperature\nshould ``blur'' such a condensate.",
        "Multivariable parametric models are essential for optimizing the performance\nof high-tech systems. The main objective of this paper is to develop an\nidentification strategy that provides accurate parametric models for complex\nmultivariable systems. To achieve this, an additive model structure is adopted,\noffering advantages over traditional black-box model structures when\nconsidering physical systems. The introduced method minimizes a weighted\nleast-squares criterion and uses an iterative linear regression algorithm to\nsolve the estimation problem, achieving local optimality upon convergence.\nExperimental validation is conducted on a prototype wafer-stage system,\nfeaturing a large number of spatially distributed actuators and sensors and\nexhibiting complex flexible dynamic behavior, to evaluate performance and\ndemonstrate the effectiveness of the proposed method.",
        "We show that if a simplicial complex is a near-cone of sufficiently high\ndepth, then the only maximum families of small pairwise intersecting faces are\nthose with a common intersection. Thus, near-cones of sufficiently high depth\nsatisfy the strict Erd\\H{o}s-Ko-Rado property conjectured by Holroyd and Talbot\nand by Borg. One consequence is a strict Erd\\H{o}s-Ko-Rado theorem for\nindependence complexes of chordal graphs with an isolated vertex. Under\nstronger shiftedness conditions, we prove a sharper stability theorem of\nHilton-Milner type, as well as two cross-intersecting theorems.",
        "This paper presents an approach to introduce physics students to the basic\nconcepts of Large Language Models (LLMs) using Python-based activities in\nGoogle Colab. The teaching strategy integrates active learning strategies and\ncombines theoretical ideas with practical, physics-related examples. Students\nengage with key technical concepts, such as word embeddings, through hands-on\nexploration of the Word2Vec neural network and GPT-2 - an LLM that gained a lot\nof attention in 2019 for its ability to generate coherent and plausible text\nfrom simple prompts.\n  The activities highlight how words acquire meaning and how LLMs predict\nsubsequent tokens by simulating simplified scenarios related to physics. By\nfocusing on Word2Vec and GPT-2, the exercises illustrate fundamental principles\nunderlying modern LLMs, such as semantic representation and contextual\nprediction. Through interactive experimenting in Google Colab, students observe\nthe relationship between model parameters (such as temperature) in GPT-2 and\noutput behaviour, understand scaling laws relating data quantity to model\nperformance, and gain practical insights into the predictive capabilities of\nLLMs. This approach allows students to begin to understand how these systems\nwork by linking them to physics concepts - systems that will shape their\nacademic studies, professional careers and roles in society.",
        "High-resource languages such as English, enables the pretraining of\nhigh-quality large language models (LLMs). The same can not be said for most\nother languages as LLMs still underperform for non-English languages, likely\ndue to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated texts from a\nsingle high-quality source language can contribute significantly to the\npretraining quality of multilingual LLMs. We translate FineWeb-Edu, a\nhigh-quality English web dataset, into nine languages, resulting in a\n1.7-trillion-token dataset, which we call TransWebEdu and pretrain a\n1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine\nnon-English reasoning tasks, we show that TransWebLLM matches or outperforms\nstate-of-the-art multilingual models trained using closed data, such as\nLlama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We\ndemonstrate that adding less than 5% of TransWebEdu as domain-specific\npretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian,\nSwahili, and Welsh understanding and commonsense reasoning tasks. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nOpen Source Initiative-approved licenses.",
        "We consider nonconvex optimization problem over simplex, and more generally,\na product of simplices. We provide an algorithm, Langevin Multiplicative\nWeights Update (LMWU) for solving global optimization problems by adding a\nnoise scaling with the non-Euclidean geometry in the simplex. Non-convex\noptimization has been extensively studied by machine learning community due to\nits application in various scenarios such as neural network approximation and\nfinding Nash equilibrium. Despite recent progresses on provable guarantee of\nescaping and avoiding saddle point (convergence to local minima) and global\nconvergence of Langevin gradient based method without constraints, the global\noptimization with constraints is less studied. We show that LMWU algorithm is\nprovably convergent to interior global minima with a non-asymptotic convergence\nanalysis. We verify the efficiency of the proposed algorithm in real data set\nfrom polynomial portfolio management, where optimization of a highly non-linear\nobjective function plays a crucial role.",
        "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps:\/\/github.com\/Gen-Verse\/WideRange4D",
        "We introduce the strong positive recurrence (SPR) property for\ndiffeomorphisms on closed manifolds with arbitrary dimension, and show that it\nhas many consequences and holds in many cases. SPR diffeomorphisms can be coded\nby countable state Markov shifts whose transition matrices act with a spectral\ngap on a large Banach space, and this implies exponential decay of\ncorrelations, almost sure invariance principle, large deviations, among other\nproperties of the ergodic measures of maximal entropy. Any $C^\\infty$ smooth\nsurface diffeomorphism with positive entropy is SPR, and there are many other\nexamples with lesser regularity, or in higher dimension."
      ]
    }
  },
  {
    "id":2412.11399,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"High-Resolution Image Synthesis with Latent Diffusion Models",
    "start_abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on data and beyond. Additionally, their formulation allows for guiding mechanism to control generation without retraining. However, since these typically operate directly in pixel space, optimization powerful DMs often consumes hundreds GPU days inference is expensive due evaluations. To enable DM training limited computational resources while retaining quality flexibility, we apply them latent space pretrained autoencoders. In contrast previous work, such representation first time reach near-optimal point between complexity reduction detail preservation, greatly boosting visual fidelity. introducing cross-attention layers model architecture, turn flexible generators general conditioning inputs as text or bounding boxes high-resolution becomes possible convolutional manner. Our (LDMs) new state art scores inpainting class-conditional highly competitive performance various tasks, including unconditional generation, text-to-image synthesis, super-resolution, significantly reducing requirements compared pixel-based DMs.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Forecasting the inevitable: A review on the impacts of climate change on renewable energy resources"
      ],
      "abstract":[
        "Understanding the relationship and quantifying impacts of climate change on energy production is key to meeting our objectives achieving a sustainable future. Here we review current state art methodologies forecast future climate, potential changes in renewable main findings regarding role renewables decarbonisation supply. Most studies used model power equations estimate output. The largest variation estimated was for long-term scenarios, with non-significant variations reported short-term. highest variability found wind followed by hydro, both long-term, overall low solar any period. Additionally, efforts point investments as one pillars reducing fossil fuel dependency. Current knowledge gaps about uncertainty modelling results combined effects resources. Future should focus increasing resolution models improving input data, well assess entire electricity system not concentrate single source, which will aid defining strategies."
      ],
      "categories":[
        "Physical Geography"
      ]
    },
    "list":{
      "title":[
        "Computing theta-dependent mass spectrum of the 2-flavor Schwinger model\n  in the Hamiltonian formalism",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Polarization-induced Quantum Spin Hall Insulator and Topological Devices\n  in InAs Quantum Wells",
        "On the module of derivations of a line arrangement",
        "Quantum Entanglement Response to Step-like Gate Modulation",
        "Graph Counterfactual Explainable AI via Latent Space Traversal",
        "Galaxy populations of ProtoClusters in cosmological hydrodynamical\n  simulations",
        "Flipping operators and locally harmonic Maass forms",
        "Convergence of body-orders in linear atomic cluster expansions",
        "Thermal Stability of Skyrmion Tubes in Nanostructured Cuboids",
        "Direct derivation of anisotropic atomic displacement parameters from\n  molecular dynamics simulations demonstrated in thermoelectric materials",
        "Kontsevich graphs act on Nambu--Poisson brackets, IV. Resilience of the\n  graph calculus in the dimensional shift $d\\mapsto d+1$",
        "Optimal multi-time-scale estimates for diluted autocatalytic chemical\n  networks. (1) Introduction and $\\sigma^*$-dominant case",
        "Siamese Foundation Models for Crystal Structure Prediction",
        "A variational approach to the analysis of the continuous space-time FEM\n  for the wave equation",
        "Homoclinic orbits, Reeb chords and nice Birkhoff sections for Reeb flows\n  in 3D",
        "Line-of-sight shear in SLACS strong lenses",
        "Duality of Selmer groups of an abelian variety over a number field",
        "The Kaluza-Klein AdS Virasoro-Shapiro Amplitude near Flat Space",
        "Dynamics of Supersolid state: normal fluid, superfluid, and supersolid\n  velocities",
        "Self-assembly of Dipolar Crystals from Magnetic Colloids",
        "Combinatorial Calabi flow for ideal circle pattern",
        "Statistical Uncertainties of Limit Cycle Systems in Langevin Bath",
        "Wave Decay with Singular Damping",
        "Modulation of superconductivity across a Lifshitz transition in\n  alternating-angle twisted quadrilayer graphene",
        "Twisting $\\mathcal{O}$-operators by $(2,3)$-Cocycle of Hom-Lie-Yamaguti\n  Algebras with Representations",
        "An automated geometric space curve approach for designing dynamically\n  corrected gates",
        "State-Agnostic Approach to Certifying Electron-Photon Entanglement in\n  Electron Microscopy",
        "On the linear independence condition for the Bobkov-Tanaka first\n  eigenvalue of the double-phase operator"
      ],
      "abstract":[
        "We compute the $\\theta$-dependent mass spectrum of the 2-flavor Schwingr\nmodel using the tensor network (DMRG) in the Hamiltonian formalism. The pion\nand the sigma meson are identified as stable particles of the model for nonzero\n$\\theta$ whereas the eta meson becomes unstable. The meson masses are obtained\nfrom the one-point functions, using the meson operators defined by\ndiagonalizing the correlation matrix to deal with the operator mixing. We also\ncompute the dispersion relation directly by measuring the energy and momentum\nof the excited states, where the mesons are distinguished by the isospin\nquantum number. We confirmed that the meson masses computed by these methods\nagree with each other and are consistent with the calculation by the bosonized\nmodel. Our methods are free from the sign problem and show a significant\nimprovement in accuracy compared to the conventional Monte Carlo methods.\nFurthermore, at the critical point $\\theta = \\pi$, the mesons become almost\nmassless, and the one-point functions reproduce the expected CFT-like behavior.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "In this work, we predict the emergence of a quantum spin Hall insulator\n(QSHI) in conventional semiconductors, specifically InAs quantum wells, driven\nby a built-in polarization field. We propose QSHI InAs quantum wells as a\nplatform to engineer topological field effect devices. More precisely, we first\npresent a novel topological logic device that operates without a topological\nphase transition. Subsequently, we design a high-performance topological\ntransistor due to the presence of edge states. Our approach provides a\npotential framework for harnessing the unique features of QSHI in device\ndesign, paving the way for future topological devices.",
        "To each multiple point $p$ in a line arrangement $\\mathcal A$ in the complex\nprojective plane we associate a derivation $\\tilde D_p \\in D_0( \\mathcal A)$.\nWe show first that these derivations span a large subspace of $D_0(\\mathcal\nA)$. To each such derivation $\\tilde D_p \\in D_0(\\mathcal A)$ we associate a\npolynomial $g_p$ which seems to play a key role in the characterization of the\nfreeness of $\\mathcal A$, as well as in the study of the position of the\nmultiple points of $\\mathcal A$ with respect to unions of lines.",
        "We examine the influence of a step-like gate voltage on the entanglement\nformation of two interacting charge qubits, where charge is injected on demand\ninto the qubits. The gate voltage modulates the tunnel coupling between the\nqubits and two electronic reservoirs (leads), which supply the initial charges\nto the system. The qubits interact capacitively through Coulomb repulsion, and\nthe interplay between Coulomb interactions and hopping processes leads to the\nformation of entangled states. Our analysis focuses on how the physical\nparameters of the gate pulse affect the degree of entanglement. In pursuit of\nthis aim, we calculate fidelity, linear entropy, and negativity within the\nframework of density matrix formalism. Our analysis demonstrate how to optimize\nthe gate pulse to reach a ``sweet spot'' that maximizes entanglement, even in\nthe presence of additional dephasing sources. These results could contribute to\nthe future experimental realization of entanglement in interacting charge\nqubits.",
        "Explaining the predictions of a deep neural network is a nontrivial task, yet\nhigh-quality explanations for predictions are often a prerequisite for\npractitioners to trust these models. Counterfactual explanations aim to explain\npredictions by finding the ''nearest'' in-distribution alternative input whose\nprediction changes in a pre-specified way. However, it remains an open question\nhow to define this nearest alternative input, whose solution depends on both\nthe domain (e.g. images, graphs, tabular data, etc.) and the specific\napplication considered. For graphs, this problem is complicated i) by their\ndiscrete nature, as opposed to the continuous nature of state-of-the-art graph\nclassifiers; and ii) by the node permutation group acting on the graphs. We\npropose a method to generate counterfactual explanations for any differentiable\nblack-box graph classifier, utilizing a case-specific permutation equivariant\ngraph variational autoencoder. We generate counterfactual explanations in a\ncontinuous fashion by traversing the latent space of the autoencoder across the\nclassification boundary of the classifier, allowing for seamless integration of\ndiscrete graph structure and continuous graph attributes. We empirically\nvalidate the approach on three graph datasets, showing that our model is\nconsistently high-performing and more robust than the baselines.",
        "The study of protoclusters at cosmic noon is essential to understand the\nimpact on galaxies of the environment and of the transformational processes\noccurring in this epoch. This work tests the predictions of the DIANOGA\ncosmological hydrodynamical simulations of cluster progenitors at z=2.2,\ncomparing them with observations, and investigates the environmental effects on\ngalaxies by comparing protoclusters with an average volume of the Universe. We\nanalyze 14 protoclusters and a cosmological box of 49 cMpc\/h per side. We\ncompare predictions and observations of the galaxy properties, including colors\nof galaxies obtained with radiative transfer, to analyze UVJ diagrams. We\nshowed that the DIANOGA simulations produce a galaxy stellar mass function in\nbroad agreement with observations, with a higher fraction of high-mass galaxies\n($M_{\\ast}>10^{10} \\ M_{\\odot}$) in massive halos in protoclusters, compared to\nthe box. The same signal, with lower significance, is also observed in the\nwide-field protocluster structures, indicating an accelerated evolution of\ngalaxies before their infall into massive halos. Our simulations underestimate\nSFRs of galaxies both in protoclusters and in the box, compared to\nobservations, due to low gas reservoirs. We find a weak suppression of SFRs in\nprotocluster galaxies (~0.05 dex), compared to the box, increasing up to ~0.25\ndex in massive halos. The quenched galaxy fraction varies significantly across\ndifferent protocluster halos, consistent with observations. The simulations\nshow a strong dependence of quenched fractions on halo mass and an excess of\nquenched galaxies in the wide-field protocluster region, compared to the\ncosmological box. UVJ diagram analysis shows qualitative agreement with\nobserved color distributions of star-forming and quenched galaxies, except for\nfew massive galaxies with steeper reddening vectors than typically assumed in\nobservations.",
        "In the theory of integral weight harmonic Maass forms of manageable growth,\ntwo key differential operators, the Bol operator and the shadow operator, play\na fundamental role. Harmonic Maass forms of manageable growth canonically split\ninto two parts, and each operator controls one of these parts. A third\noperator, called the flipping operator, exchanges the role of these two parts.\nMaass--Poincar\\'e series (of parabolic type) form a convenient basis of\nnegative weight harmonic Maass forms of manageable growth, and flipping has the\neffect of negating an index. Recently, there has been much interest in locally\nharmonic Maass forms defined by the first author, Kane, and Kohnen. These are\nlifts of Poincar\\'e series of hyperbolic type, and are intimately related to\nthe Shimura and Shintani lifts. In this note, we prove that a similar property\nholds for the flipping operator applied to these Poincar\\'e series.",
        "We study the convergence of a linear atomic cluster expansion (ACE) potential\nwith respect to its basis functions, in terms of the effective two-body\ninteractions of elemental Carbon systems. We build ACE potentials with\ndescriptor sets truncated at body-orders $K=2,3,4$ trained on pure dimers, or\non large datasets of different diversities but without any dimers. Potentials\ntrained on a more diverse dataset fare better in validation and result in a\nnontrivial dimer curve, but still very far from the theoretical two-body\ninteraction calculated by DFT. Moreover, dimer curves between descriptor sets\nclipped at different $K$ do not seem to converge to a universal function for a\ngiven dataset. We conclude that machine learning potentials employing atomic\ncluster expansions optimize losses at low $K$ but fail to generalize and\nconverge properties described by two-body interactions.",
        "Magnetic skyrmions in bulk materials are typically regarded as\ntwo-dimensional structures. However, they also exhibit three-dimensional\nconfigurations, known as skyrmion tubes, which elongate and extend in-depth.\nUnderstanding the configurations and stabilization mechanism of skyrmion tubes\nis crucial for the development of advanced spintronic devices. However, the\ngeneration and annihilation of skyrmion tubes in confined geometries are still\nrarely reported. Here, we present direct imaging of skyrmion tubes in\nnanostructured cuboids of a chiral magnet FeGe using Lorentz transmission\nelectronic microscopy (TEM), while applying an in-plane magnetic field. It is\nobserved that skyrmion tubes stabilize in a narrow field-temperature region\nnear the Curie temperature (Tc). Through a field cooling process, metastable\nskyrmion tubes can exist in a larger region of the field-temperature diagram.\nCombining these experimental findings with micromagnetic simulations, we\nattribute these phenomena to energy differences and thermal fluctuations. Our\nresults could promote topological spintronic devices based on skyrmion tubes.",
        "Atomic displacement parameters (ADPs) are crystallographic information that\ndescribe the statistical distribution of atoms around an atom site. Direct\nderivation of anisotropic ADPs by atom from molecular dynamics (MD)\nsimulations, where the (co)valences of atom positions are taken over recordings\nat different time steps in a single MD simulation, was demonstrated on three\nthermoelectric materials, Ag8SnSe6, Na2In2Sn4, and BaCu1.14In0.86P2. Unlike the\nvery frequently used lattice dynamics approach, the MD approach can obtain ADPs\nin disordered crystals and at finite temperature, but not under conditions\nwhere atoms migrate in the crystal. ADPs from MD simulations would act as a\ntool complementing experimental efforts to understand the crystal structure\nincluding the distribution of atoms around atom sites.",
        "We examine whether the Kontsevich flows $\\dot{P}=Q^\\gamma_d(P)$ of\nNambu--Poisson structures $P$ on $\\mathbb{R}^d$ are Poisson coboundaries, for\n$\\gamma$ some suitable cocycle in the Kontsevich graph complex. That is, we\ninspect the existence of a vector field $\\vec{X}^\\gamma_d(P)$ such that\n$Q^\\gamma_d(P)=[[ P,\\vec{X}^\\gamma_d(P)]]$, where $[[\\cdot,\\cdot]]$ is the\nSchouten bracket of multivector fields (the generalised Lie bracket). To tackle\nthis class of problems in dimensions $d\\geq3$, we introduced a series of\nsimplications in paper II (arXiv:2409.12555); here, we present a series of\nresults regarding the down-up behaviour of solutions $\\vec{X}^\\gamma_d(P)$ and\nvanishing micro-graphs in the course of dimension shift $d\\mapsto d+1$.",
        "Autocatalytic chemical networks are dynamical systems whose linearization\naround zero has a positive Lyapunov exponent; this exponent gives the growth\nrate of the system in the diluted regime, i.e. for near-zero concentrations.\nThe generator of the dynamics in the kinetic limit is then a Perron-Frobenius\nmatrix, suggesting the use of Markov chain techniques to get long-time\nasymptotics. This series of works introduces a new, general procedure providing\nprecise quantitative information about such asymptotics, based on estimates for\nthe Lyapunov eigenvalue and eigenvector. The algorithm, inspired from Wilson's\nrenormalization group method in quantum field theory, is based on a downward\nrecursion on kinetic scales, starting from the fastest, and terminating with\nthe slowest rates. Estimates take on the form of simple rational functions of\nkinetic rates. They are accurate under a separation of scales hypothesis,\nloosely stating that kinetic rates span many orders of magnitude. We provide\nhere a brief general motivation and introduction to the method, present some\nsimple examples, and derive a number of preliminary results, in particular the\nestimation of Lyapunov data for a subclass of so-called $\\sigma^*$-dominant\ngraphs.",
        "Crystal Structure Prediction (CSP), which aims to generate stable crystal\nstructures from compositions, represents a critical pathway for discovering\nnovel materials. While structure prediction tasks in other domains, such as\nproteins, have seen remarkable progress, CSP remains a relatively underexplored\narea due to the more complex geometries inherent in crystal structures. In this\npaper, we propose Siamese foundation models specifically designed to address\nCSP. Our pretrain-finetune framework, named DAO, comprises two complementary\nfoundation models: DAO-G for structure generation and DAO-P for energy\nprediction. Experiments on CSP benchmarks (MP-20 and MPTS-52) demonstrate that\nour DAO-G significantly surpasses state-of-the-art (SOTA) methods across all\nmetrics. Extensive ablation studies further confirm that DAO-G excels in\ngenerating diverse polymorphic structures, and the dataset relaxation and\nenergy guidance provided by DAO-P are essential for enhancing DAO-G's\nperformance. When applied to three real-world superconductors\n($\\text{CsV}_3\\text{Sb}_5$, $ \\text{Zr}_{16}\\text{Rh}_8\\text{O}_4$ and\n$\\text{Zr}_{16}\\text{Pd}_8\\text{O}_4$) that are known to be challenging to\nanalyze, our foundation models achieve accurate critical temperature\npredictions and structure generations. For instance, on\n$\\text{CsV}_3\\text{Sb}_5$, DAO-G generates a structure close to the\nexperimental one with an RMSE of 0.0085; DAO-P predicts the $T_c$ value with\nhigh accuracy (2.26 K vs. the ground-truth value of 2.30 K). In contrast,\nconventional DFT calculators like Quantum Espresso only successfully derive the\nstructure of the first superconductor within an acceptable time, while the RMSE\nis nearly 8 times larger, and the computation speed is more than 1000 times\nslower. These compelling results collectively highlight the potential of our\napproach for advancing materials science research and development.",
        "We present a stability and convergence analysis of the space-time continuous\nfinite element method for the Hamiltonian formulation of the wave equation.\nMore precisely, we prove a continuous dependence of the discrete solution on\nthe data in a $C^0([0, T]; X)$-type energy norm, which does not require any\nrestriction on the meshsize or the time steps. Such stability estimates are\nthen used to derive a priori error estimates with quasi-optimal convergence\nrates, where a suitable treatment of possible nonhomogeneous Dirichlet boundary\nconditions is pivotal to avoid loss of accuracy. Moreover, based on the\nproperties of a postprocessed approximation, we derive a constant-free,\nreliable a posteriori error estimate in the $C^0([0, T]; L^2(\\Omega))$-norm for\nthe semidiscrete-in-time formulation. Several numerical experiments are\npresented to validate our theoretical findings.",
        "We prove that for a $C^\\infty$-generic contact form defining a given\nco-oriented contact structure on a closed $3$-manifold, every hyperbolic\nperiodic Reeb orbit admits a transverse homoclinic connection in each of the\nbranches of its stable and unstable manifolds. We exploit this result to prove\nthat for a $C^\\infty$-generic contact form defining a given co-oriented contact\nstructure, given any finite collection $\\Gamma$ of periodic Reeb orbits and any\nLegendrian link $L$, there exists a global surface of section (embedded\nBirkhoff section) for the Reeb flow that contains $\\Gamma$ in its boundary, and\nthat contains in its interior a Legendrian link that is Legendrian isotopic to\n$L$ by a $C^0$-small isotopy. Finally we prove that if the Reeb vector field\nadmits a $\\partial$-strong Birkhoff section then every Legendrian knot has\ninfinitely many geometrically distinct Reeb chords, except possibly when the\nambient manifold is a lens space or the sphere and the Reeb flow has exactly\ntwo periodic orbits. In particular, $C^\\infty$-generically on the contact form\nthere are infinitely many geometrically distinct Reeb chords for every\nLegendrian knot. In the case of geodesic flows, every Legendrian knot has\ninfinitely many disjoint chords, without any further assumptions.",
        "Inhomogeneities along the line of sight in strong gravitational lensing\ndistort the images produced, in an effect called shear. If measurable, this\nshear may provide independent constraints on cosmological parameters,\ncomplementary to traditional cosmic shear. We model 50 strong gravitational\nlenses from the Sloan Lens ACS (SLACS) catalogue with the aim of measuring the\nline-of-sight (LOS) shear for the first time. We use the `minimal model' for\nthe LOS shear, which has been shown to be theoretically safe from degeneracies\nwith lens model parameters, a finding which has been confirmed using mock data.\nWe use the dolphin automated modelling pipeline, which uses the lenstronomy\nsoftware as a modelling engine, to model our selected lenses. We model the main\ndeflector with an elliptical power law profile, the lens light with elliptical\nS\\'ersic profiles and the source with a basis set of shapelets and an\nelliptical S\\'ersic profile. We successfully obtain a line-of-sight shear\nmeasurement from 18 of the 50 lenses. We find that these LOS shear measurements\nare consistent with external shears measured in recent works using a simpler\nshear model, which are larger than those expected from weak lensing. Neglecting\nthe post-Born correction to the potential of the main deflector due to\nforeground shear leads to a propagation of degeneracies to the LOS shear\nmeasurement, and the same effect is seen if a prior is used to connect the lens\nmass and light ellipticities. The inclusion of an octupole moment in the lens\nmass profile does not lead to shear measurements that are in better agreement\nwith the expectations from weak lensing.",
        "Let $A$ be an abelian variety defined over a number field $K$ and let\n$A^{\\vee}$ be the dual abelian variety. For an odd prime $p$, we consider two\nSelmer groups attached to $A[p]$ and relate the orders of these groups along\nwith those of their corresponding duals to the order of the component groups of\n$A^{\\vee}$ at primes $v$.",
        "We bootstrap the first-order correction in the curvature expansion of the\nVirasoro-Shapiro amplitude in AdS spacetime, for arbitrary Kaluza-Klein charges\nof external operators. By constructing a universal ansatz based on\nsingle-valued multiple polylogarithms as well as an AdS$\\times$S formalism, and\nmatching it with the low-lying result, we derive a unified formula in terms of\nworld-sheet integrals. Our result predicts an infinite number of Wilson\ncoefficients that were not available in previous literature.",
        "Landau's excitation-based argument for superfluids -- that at temperature\n$T=0$ the normal fluid density $\\rho_{n}$ is zero -- should also apply to\nsupersolids. Further, for a total mass density $\\rho$, Leggett argues that the\nsuperfluid fraction $\\rho_{s}\/\\rho<1$. These arguments imply that there is a\nmissing mass. We attribute this to a supersolid density $\\rho_{L}$, with\n$\\rho_{L}\\equiv \\rho-\\rho_{s}-\\rho_{n}$, and a momentum-bearing supersolid\nvelocity $v_{Li}$. Using Onsager's irreversible thermodynamics we derive the\nmacroscopic dynamical equations for this system. We find that $v_{Li}$ is\nsubject to the force of elasticity, to the negative gradient of the chemical\npotential per mass $\\mu$ (as for the superfluid velocity $v_{si}$), and to drag\nagainst the normal fluid (leading to the interpretation of $L$ as lattice).\nThus both the superfluid and supersolid components are associated with the\nground state. The normal modes for such a system have a crossover in frequency,\nabove which the normal fluid velocity $v_{ni}$ is an independent variable and\nbelow which it is locked to $v_{Li}$. For an isotropic lattice we study both\nthe transverse response and longitudinal response. The ring geometry for atomic\ngas supersolid states may provide a geometry for testing these predictions.",
        "We study the self-assembly of magnetic colloids using the Stockmayer (SM)\nmodel characterized by short-range Lennard-Jones interactions and long-range\ndipole-dipole interactions. Using molecular dynamics simulations, we design\ncooling protocols that yield perfectly assembled single-domain magnetic\ncrystals. We identify cooling rates at which the system transforms from an\namorphous glass to a crystal, where magnetic ordering promotes crystalline\norder. Remarkably, we observe that the latter develops via a spontaneous\ntransition rather than through the traditional nucleation and growth mechanism.\nFor a weakly dipolar fluid ($\\mu=1$), this self-assembly results in a\nface-centered cubic (FCC) colloidal crystal with dipole moments chained along\nthe (111) direction. For fluids with higher dipole moment ($\\mu = 2.5$), the\ncrystal structure shifts towards a body-centered orthorhombic (BCO) arrangement\ndue to the compression of chains from strong dipolar attractions. These results\nprovide valuable insights into the mechanisms driving crystallization in\nmagnetic fluids, opening new avenues for understanding the formation of\nmagnetically responsive colloidal magnetic crystals with promising\napplications.",
        "We study the combinatorial Calabi flow for ideal circle patterns in both\nhyperbolic and Euclidean background geometry. We prove that the flow exists for\nall time and converges exponentially fast to an ideal circle pattern metric on\nsurfaces with prescribed attainable curvatures. As a consequence, we provide an\nalgorithm to find the desired ideal circle patterns.",
        "We show that limit cycle systems in Langevin bath exhibit uncertainty in\nobservables that define the limit-cycle plane, and maintain a positive lower\nbound. The uncertainty-bound depends on the parameters that determine the shape\nand periodicity of the limit cycle. In one dimension, we use the framework of\ncanonical dissipative systems to construct the limit cycle, whereas in two\ndimensions, particle in central potentials with radial-dissipation provide us\nnatural examples. We show that, the position-momenta uncertainty of particle in\na central potential is larger than half the magnitude of the angular momentum\n(conserved) of the particle. We also investigate how uncertainties, which are\nabsent in deterministic systems, increase with time when the systems are\nattached to a bath and eventually cross the lower bound before reaching the\nsteady state.",
        "We consider the stabilization problem on a manifold with boundary for a wave\nequation with measure-valued linear damping. For a wide class of measures,\ncontaining Dirac masses on hypersurfaces as well as measures with fractal\nsupport, we establish an abstract energy decay result.",
        "We report electric field-controlled modulation of the Fermi surface topology\nand explore its effects on the superconducting state in alternating-angle\ntwisted quadrilayer graphene (TQG). The unique combination of flat and\ndispersive bands in TQG allows us to simultaneously tune the band structure\nthrough carrier density, $n$, and displacement field, $D$. From\ndensity-dependent Shubnikov-de Haas quantum oscillations and Hall measurements,\nwe quantify the $D$-dependent bandwidth of the flat and dispersive bands and\ntheir hybridization. In the high $|D|$ regime, the increased bandwidth favors\nthe single particle bands, which coincides exactly with the vanishing of the\nsuperconducting transition temperature $T_c$, showing that superconductivity in\nTQG is strongly bound to the symmetry-broken state. For a range of lower $|D|$\nvalues, a Lifshitz transition occurs when the flat and dispersive band Fermi\nsurfaces merge within the $\\nu=+2$ symmetry-broken state. The superconducting\nstate correspondingly shows an enhanced $T_c$, suggesting that the\nsuperconducting condensate is strongly dependent on the Fermi surface topology\nand density of states within this symmetry-broken state.",
        "In this paper, we first introduce the notion of twisted $\\mathcal\nO$-operators on a Hom-Lie-Yamaguti algebra by a given $(2,3)$-cocycle with\ncoefficients in a representation. We show that a twisted $\\mathcal O$-operator\ninduces a Hom-Lie-Yamaguti structure. We also introduce the notion of a\nweighted Reynolds operator on a Hom-Lie-Yamaguti algebra, which can serve as a\nspecial case of twisted $\\mathcal O$-operators on Hom-Lie-Yamaguti algebras.\nThen, we define a cohomology of twisted $\\mathcal O$-operator on\nHom-Lie-Yamaguiti algebras with coefficients in a representation. Furthermore,\nwe introduce and study the Hom-NS-Lie-Yamaguti algebras as the underlying\nstructure of the twisted $\\mathcal O$-operator on Hom-Lie-Yamaguti algebras.\nFinally, we investigate the twisted $\\mathcal O$-operator on Hom-Lie-Yamaguti\nalgebras induced by the twisted $\\mathcal O$-operator on a Hom-Lie algebras.",
        "The noisy nature of quantum hardware necessitates the implementation of\nhigh-fidelity quantum gates in a noise-insensitive manner. While there exist\nmany powerful methods for designing dynamically corrected gates, they typically\ninvolve an exploration across a large-dimensional landscape filled with\nsolutions that are only locally optimal, making it challenging to find globally\noptimal ones. Moreover, these methods often use a single cost function to try\nto accomplish the two disparate goals of achieving a target gate and\nsuppressing noise, and this can lead to unnecessary tradeoffs between the two\nand, consequently, lower fidelities. Here, we present a method for designing\ndynamically corrected gates called B\\'ezier Ansatz for Robust Quantum (BARQ)\ncontrol to address these challenges. Rather than numerically optimizing the\ncontrols directly, BARQ instead makes use of the Space Curve Quantum Control\nformalism in which the quantum evolution is mapped to a geometric space curve.\nIn the formulation used by BARQ, the boundary conditions of the space curve\ndetermine the target gate, while its shape determines the noise robustness of\nthe corresponding gate. This allows the target gate to be fixed upfront, so\nthat numerical optimization is only needed to achieve noise-robustness, and\nthis is performed efficiently using a control-point parameterization of the\nspace curve. In this way, BARQ eliminates the gate-fixing and noise-robustness\ntradeoff while also providing a global perspective into the control landscape,\nand allows for ample freedom to design experimentally friendly and robust\ncontrol pulses. The pulse design is facilitated through the developed software\npackage qurveros.",
        "Transmission electron microscopes (TEMs) enable atomic-scale imaging and\ncharacterisation, driving advances across fields from materials science to\nbiology. Quantum correlations, specifically entanglement, may provide a basis\nfor novel hybrid sensing techniques to make TEMs compatible with sensitive\nsamples prone to radiation damage. We present a protocol to certify\nentanglement between electrons and photons naturally arising from certain\ncoherent cathodoluminescence processes. Using mutually unbiased bases in\nposition and momentum, our method allows robust, state-agnostic entanglement\nverification and provides a lower bound on the entanglement of formation,\nenabling quantitative comparisons across platforms. Simulations under\nexperiment-inspired conditions and preliminary experimental data highlight the\nfeasibility of implementing this approach in modern TEM systems with optical\nspecimen access. Our work integrates photonic quantum information techniques\nwith electron microscopy. It establishes a foundation for entanglement-based\nimaging at the atomic scale, offering a potential pathway to reduce radiation\nexposure.",
        "The paper investigates a pivotal condition for the Bobkov-Tanaka type\nspectrum for double-phase operators. This condition is satisfied if either the\nweight $w$ driving the double-phase operator is strictly positive in the whole\ndomain or the domain is convex and fulfils a suitable symmetry condition."
      ]
    }
  },
  {
    "id":2412.18649,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A Review of Surface Haptics: Enabling Tactile Effects on Touch Surfaces",
    "start_abstract":"In this article, we review the current technology underlying surface haptics that converts passive touch surfaces to active ones (machine haptics), our perception of tactile stimuli displayed through (human their potential applications (human-machine interaction), and finally, challenges ahead us in making them available commercial systems. This article primarily covers interactions human fingers or hands with surface-haptics displays by focusing on three most popular actuation methods: vibrotactile, electrostatic, ultrasonic.",
    "start_categories":[
      "cs.HC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Biophysical properties of the human finger for touch comprehension: influences of ageing and gender"
      ],
      "abstract":[
        "The human finger plays an extremely important role in tactile perception, but little is known about how age and gender affect its biophysical properties their perception. We combined studies on contact characteristics, mechanical surface topography to understand effects the finger. values obtained regarding characteristics (i.e. adhesive force) were significantly higher for women than men. As Young's modulus E), a significant positive correlation with was observed found be women. A between arithmetic mean of roughness However, inverse effect highlighted have never been reported previously literature. These results open new perspectives understanding weakening perception across ages it differs men"
      ],
      "categories":[
        "Biodynamic"
      ]
    },
    "list":{
      "title":[
        "Spatiotemporal steering of non-diffracting wavepackets",
        "Regularizing effect of the spatially homogeneous Landau equation with\n  soft potential",
        "A Note on Strongly $\\pi$-Regular Elements",
        "Foundations of Digital Circuits: Denotation, Operational, and Algebraic\n  Semantics",
        "Looking for the {\\gamma}-Ray Cascades of the KM3-230213A Neutrino Source",
        "Optimizing Input Data Collection for Ranking and Selection",
        "Modeling cell differentiation in neuroblastoma: insights into\n  development, malignancy, and treatment relapse",
        "Metasurface Dome for Above-the-Horizon Grating Lobes Reduction in 5G-NR\n  Systems",
        "A multi-wavelength investigation of spiral structures in $z > 1$\n  galaxies with JWST",
        "A note on goodness of fit testing for the Poisson distribution",
        "Interface conditions for Maxwell's equations by homogenization of thin\n  inclusions: transmission, reflection or polarization",
        "Utilizing Quantum Fingerprints in Plant Cells to Evaluate Plant\n  productivity",
        "Galaxies in the simulated cosmic web: I. Filament identification and\n  their properties",
        "Topological Casimir effect for fermionic condensate in AdS spacetime\n  with compact dimensions",
        "Smoothing properties of the Wilson flow and the topological charge",
        "Controllability and Displacement Analysis of a Three-Link Elastic\n  Microswimmer: A Geometric Control Approach",
        "LU Decomposition and Generalized Autoone-Takagi Decomposition of Dual\n  Matrices and their Applications",
        "New constraints on the galactic ionizing efficiency and escape fraction\n  at 2.5 < z < 6 based on quasar absorption spectra",
        "Optimal ANOVA-based emulators of models with(out) derivatives",
        "Model-independent forecasts for the cosmological anisotropic stress",
        "Cubic norm pairs and hermitian cubic norm structures",
        "Weak electronic correlations in the cobalt oxychalcogenide\n  superconductor Na2CoSe2O",
        "Multi-photon enhanced resolution for Superconducting Nanowire\n  Single-Photon Detector-based Time-of-Flight lidar systems",
        "Bikernels by monochromatic paths",
        "Hard Lefschetz Condition on symplectic non-K\\\"ahler solvmanifolds",
        "Stories that (are) Move(d by) Markets: A Causal Exploration of Market\n  Shocks and Semantic Shifts across Different Partisan Groups",
        "A convenient characterisation of convergent upper transition operators",
        "Decay estimates for solutions to non-autonomous critical p-Laplace\n  problems",
        "Parselets: An Abstraction for Fast, General-Purpose Algorithmic\n  Information Calculus"
      ],
      "abstract":[
        "We study the dynamics of space-time non-diffracting wavepackets, commonly\nknown as light bullets, in a spatiotemporally varying medium. We show that by\nspatiotemporal refraction, a monochromatic focused beam can be converted to a\nlight bullet that propagates at a given velocity. By further designing the\nindex profile of the spatiotemporal boundary, the group velocity and the\npropagation direction of the light bullet can be engineered in a programmable\nway. All effects mentioned above cannot be achieved by spatial or temporal\nboundaries, and are only possible with spatiotemporal boundaries. These\nfindings provide unique ways to engineer the dynamics of electromagnetic\nwavepackets in space-time. Such wavepackets with engineered spacetime\ntrajectories may find potential applications in the spatiotemporal control of\nmaterial properties or particles, or for use as a way to emulate relativistic\nphysics in the laboratory.",
        "This paper investigates the Cauchy problem of the spatially homogeneous\nLandau equation with soft potential under the perturbation framework to global\nequilibrium. We prove that the solution to the Cauchy problem exhibits\nanalyticity in the time variable and the Gelfand-Shilov regularizing effect in\nthe velocity variables.",
        "Let $A \\in \\mathbb{M}_m(S)$, where $S$ is a commutative ring and $m>1$. For a\npositive integer $n$ if $A^n \\mathbb{M}_m(S)= A^{n+1}\\mathbb{M}_m(S)$, then we\nprove that $\\mathbb{M}_m(S)A^n = \\mathbb{M}_m(S)A^{n+1}$. For $n = 1$ this\nvalidates a recent conjecture [3, Conjecture 3.7].",
        "This thesis details a project to define a fully compositional theory of\nsynchronous sequential circuits built from primitive components, motivated by\napplying techniques successfully used in programming languages to hardware.\n  The first part of the thesis defines the syntactic foundations of sequential\ncircuit morphisms, and then builds three different semantic theories:\ndenotational, operational and algebraic. We characterise the denotational\nsemantics of sequential circuits as certain causal stream functions, as well as\nproviding a link to existing circuit methodologies by mapping between circuit\nmorphisms, stream functions and Mealy machines. The operational semantics is\ndefined as a strategy for applying some global transformations followed by\nlocal reductions to demonstrate how a circuit processes a value, leading to a\nnotion of observational equivalence. The algebraic semantics consists of\nequations for bringing circuits into a pseudo-normal form, and then encoding\nbetween different state sets. This part of the thesis concludes with a\ndiscussion of some novel applications, such as those for using partial\nevaluation for digital circuits.\n  While mathematically rigorous, the categorical string diagram formalism is\nnot suited for reasoning computationally. The second part of this thesis\ndetails an extension of string diagram rewriting with hypergraphs so that it is\ncompatible with the traced comonoid structure present in the category of\ndigital circuits. We identify the properties that characterise cospans of\nhypergraphs corresponding to traced comonoid terms, and demonstrate how to\nidentify rewriting contexts valid for rewriting modulo traced comonoid\nstructure. We apply the graph rewriting framework to fixed point operators as\nwell as the operational semantics from the first part, and present a new\nhardware description language based on these theoretical developments.",
        "The extreme energy of the KM3-230213A event could transform our understanding\nof the most energetic sources in the Universe. However, it also reveals an\ninconsistency between the KM3NeT detection and strong IceCube constraints on\nthe ultra-high energy neutrino flux. The most congruous explanation for the\nKM3NeT and IceCube data requires KM3-230213A to be produced by a (potentially\ntransient) source fortuitously located in a region where the KM3NeT acceptance\nis maximized. In hadronic models of ultra-high-energy neutrino production, such\na source would also produce a bright {\\gamma}-ray signal, which would cascade\nto GeV--TeV energies due to interactions with extragalactic background light.\nWe utilize the {\\gamma}-Cascade package to model the spectrum, spatial\nextension, and time-delay of such a source, and scan a region surrounding the\nKM3NeT event to search for a consistent {\\gamma}-ray signal. We find no\nconvincing evidence for a comparable \\textit{Fermi}-LAT source and place\nconstraints on a combination of the source redshift and the intergalactic\nmagnetic field strength between the source and Earth.",
        "We study a ranking and selection (R&S) problem when all solutions share\ncommon parametric Bayesian input models updated with the data collected from\nmultiple independent data-generating sources. Our objective is to identify the\nbest system by designing a sequential sampling algorithm that collects input\nand simulation data given a budget. We adopt the most probable best (MPB) as\nthe estimator of the optimum and show that its posterior probability of\noptimality converges to one at an exponential rate as the sampling budget\nincreases. Assuming that the input parameters belong to a finite set, we\ncharacterize the $\\epsilon$-optimal static sampling ratios for input and\nsimulation data that maximize the convergence rate. Using these ratios as\nguidance, we propose the optimal sampling algorithm for R&S (OSAR) that\nachieves the $\\epsilon$-optimal ratios almost surely in the limit. We further\nextend OSAR by adopting the kernel ridge regression to improve the simulation\noutput mean prediction. This not only improves OSAR's finite-sample\nperformance, but also lets us tackle the case where the input parameters lie in\na continuous space with a strong consistency guarantee for finding the optimum.\nWe numerically demonstrate that OSAR outperforms a state-of-the-art competitor.",
        "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
        "The use of 5G New Radio (NR) spectrum around 26 GHz is currently raising the\nquest on its compatibility with the well-established Earth\nExploration-Satellite Service (EESS), which may be blinded by the spurious\nradiation emitted Above-the-Horizon (AtH) by Base Station (BS) antennas.\nIndeed, AtH grating lobes are often present during cell scanning due to the\nlarge inter-element spacing in BS array antennas for achieving higher gains\nwith a reduced number of RF chains. In this letter, we propose an approach\nbased on an electrically thin metasurface-based dome for the reduction of AtH\ngrating lobes in 5G-NR BS antennas. The proposed scanning range shifting\napproach exploits the natural lower amplitude of the grating lobes when the\nantenna array scans in an angular region closer to the broadside direction. The\ngrating lobe reduction is here demonstrated considering a 1x4 phased linear\nantenna array operating under dual-liner 45deg-slant polarization. A simple\ndesign procedure for designing the metasurface dome is reported, together with\nthe antenna performances, evaluated through a proper set of numerical\nexperiments. It is shown that the grating lobe radiation towards the satellite\nregion is significantly reduced, whereas the overall insertion loss is\nmoderate.",
        "Recent JWST observations have revealed the prevalence of spiral structures at\n$z > 1$. Unlike in the local Universe, the origin and the consequence of\nspirals at this epoch remain unexplored. We use public JWST\/NIRCam data from\nthe COSMOS-Web survey to map spiral structures in eight massive ($>\n10^{10.5}\\,\\rm M_{\\odot}$) star-forming galaxies at $z_{\\rm spec} \\sim 1.5$. We\npresent a method for systematically quantifying spiral arms at $z>1$, enabling\ndirect measurements of flux distributions. Using rest-frame near-IR images, we\nconstruct morphological models accurately tracing spiral arms. We detect\noffsets ($\\sim 0.2 - 0.8\\,\\rm kpc$) between the rest-frame optical and near-IR\nflux distributions across most arms. Drawing parallels to the local Universe,\nwe conclude that these offsets reflect the presence of density waves. For nine\nout of eighteen arms, the offsets indicate spiral shocks triggered by density\nwaves. Five arms have offsets in the opposite direction and are likely\nassociated with tidal interactions. For the remaining cases with no detected\noffsets, we suggest that stochastic 'clumpy' star formation is the primary\ndriver of their formation. In conclusion, we find a multi-faceted nature of\nspiral arms at $z > 1$, similar to that in the local Universe.",
        "Since its introduction in 1950, Fisher's dispersion test has become a\nstandard means of deciding whether or not count data follow the Poisson\ndistribution. The test is based on a characteristic property of the Poisson\ndistribution, and discriminates well between the Poisson and the natural\nalternative hypotheses of binomial and negative binomial distributions.\n  While the test is commonly used to test for general deviations from\nPoissonity, its performance against more general alternatives has not been\nwidely investigated. This paper presents realistic alternative hypotheses for\nwhich general goodness of fit tests perform much better than the Fisher\ndispersion test.",
        "We consider the time-harmonic Maxwell equations in a complex geometry. We are\ninterested in geometries that model polarization filters or Faraday cages. We\nstudy the situation that the underlying domain contains perfectly conducting\ninclusions, the inclusions are distributed in a periodic fashion along a\nsurface. The periodicity is $\\eta>0$ and the typical scale of the inclusion is\n$\\eta$, but we allow also the presence of even smaller scales, e.g. when thin\nwires are analyzed. We are interested in the limit $\\eta\\to 0$ and in effective\nequations. Depending on geometric properties of the inclusions, the effective\nsystem can imply perfect transmission, perfect reflection or polarization.",
        "Overcoming the strong chlorophyll background poses a significant challenge\nfor measuring and optimizing plant growth. This research investigates the novel\napplication of specialized quantum light emitters introduced into intact leaves\nof tobacco (Nicotiana tabacum), a well-characterized model plant system for\nstudies of plant health and productivity. Leaves were harvested from plants\ncultivated under two distinct conditions: low light (LL), representing\nunhealthy leaves with reduced photosynthesis. and high light (HL), representing\nhealthy leaves with highly active photosynthesis. Higher-order correlation data\nwere collected and analyzed using machine learning (ML) techniques,\nspecifically a Convolutional Neural Network (CNN), to classify the photon\nemitter states. This CNN efficiently identified unique patterns and created\ndistinct fingerprints for Nicotiana leaves grown under LL and HL, demonstrating\nsignificantly different quantum profiles between the two conditions. These\nquantum fingerprints serve as a foundation for a novel unified analysis of\nplant growth parameters associated with different photosynthetic states. By\nemploying CNN, the emitter profiles were able to reproducibly classify the\nleaves as healthy or unhealthy. This model achieved high probability values for\neach classification, confirming its accuracy and reliability. The findings of\nthis study pave the way for broader applications, including the application of\nadvanced quantum and machine learning technologies in plant health monitoring\nsystems.",
        "As the environment harbouring the majority of galaxies, filaments are thought\nto play a key role in the co-evolution of galaxies and the cosmic web. In this\nfirst part of a series to understand the link between galaxies and filaments\nthrough cosmological simulations, we address two major current obstacles on\nthis path: the difficulty of meaningful filament identification, and their\npoorly constrained properties and internal structure. We use the public EAGLE\nand TNG100 simulations to build physically motivated filament catalogues with\nthe DisPerSE algorithm, based on the dark matter (DM) field at redshift z = 0\nand z = 2, explicitly accounting for the multi-scale nature of filaments and\nwith careful validation of results. Filament widths, lengths, and densities\nvary by factors ~5-100 in both simulations, highlighting the heterogeneous\nnature of filaments as a cosmic environment. All filaments are relatively thin,\nwith overdensity profiles of galaxies, DM, and gas dropping to the cosmic mean\nwithin <3 Mpc from their spines. Contrary to groups and clusters, filament\ncores are highly substructure dominated, by as much as ~80 per cent. Filament\ngas maps reveal rich temperature and density structures that limit the\napplicability of simple cylindrically symmetric models. EAGLE and TNG100 agree\nthat z = 2 filament spines are traced by overdense cool gas in pressure\nequilibrium with a >10x hotter envelope. However, significant differences in\ndetail between their predicted gas property maps imply that individual\nsimulations cannot yet describe the baryon structure of filaments with\ncertainty. Finally, we compare our fiducial filament network to one constructed\nfrom galaxies. The two differ in many aspects, but the distance of a galaxy to\nits nearest galaxy-based filament still serves as a statistical proxy for its\ntrue environment.",
        "We investigate combined effects of gravitational field and spatial topology\non the fermionic condensate (FC) for a massive Dirac field in locally anti-de\nSitter (AdS) spacetime with a part of spatial dimensions compactified to a\ntorus. For general phases in the periodicity conditions along compact\ndimensions the topological Casimir contribution is explicitly extracted and the\nrenormalization is reduced to the one for purely AdS spacetime. The FC is an\neven periodic function of the magnetic flux enclosed by compact dimension with\nthe period of flux quantum. The topological contribution vanishes on the AdS\nboundary and dominates in the total FC in the region near the AdS horizon. For\nproper lengths of compact dimensions smaller than the AdS curvature radius the\ninfluence of the gravitational field is weak and the leading term in the\ncorresponding expansion coincides with the FC for a locally Minkowski spacetime\nwith compact dimensions. For large proper lengths the decay of the topological\nFC follows a power law for both massless and massive field, in contrast to an\nexponential decay in Minkowski bulk for massive fields. Applications are\ndiscussed for 2D Dirac materials.",
        "We study SU$(N_C)$ gauge theories with a single fermion in the two-index\nantisymmetric representation to predict the mesonic spectrum of supersymmetric\n$\\mathcal{N}=1$ SYM theories. Using gradient flow methods, we investigate\nfractional topological charges in $N_C = 4$ ensembles with varying lattice\nspacings. We show that the use of overimproved gauge actions (specifically the\nDBW2 action) in the smearing kernel stabilises the values of the topological\ncharge already at moderate values of the flow time, while this is not the case\nfor the standard Wilson flow.",
        "This study investigates the dynamics and controllability of a Purcell\nthree-link microswimmer equipped with passive elastic torsional coils at its\njoints. By controlling the spontaneous curvature, we analyse the swimmers\nmotion using both linear and weakly nonlinear approaches. Linear analysis\nreveals steady harmonic solutions for small-amplitude controls but does not\npredict any net displacement, whereas weakly nonlinear analysis predicts\ntranslation along the orientation of the central link. Using geometric control\ntheory, we prove that the system is small time locally controllable near\nequilibrium and derive displacement estimates for periodic piecewise constant\ncontrols, which are validated through numerical simulations. These findings\nindicate that oscillatory controls can enable motion in all directions near\nequilibrium. This work offers foundational insights into the controllability of\nelastic microswimmers, paving the way for advanced motion planning and control\nstrategies.",
        "This paper uses matrix transformations to provide the Autoone-Takagi\ndecomposition of dual complex symmetric matrices and extends it to dual\nquaternion $\\eta$-Hermitian matrices. The LU decomposition of dual matrices is\ngiven using the general solution of the Sylvester equation, and its equivalence\nto the existence of rank-k decomposition and dual Moore-Penrose generalized\ninverse (DMPGI) is proved. Similar methods are then used to provide the\nCholesky decomposition of dual real symmetric positive definite matrices. Both\nof our decompositions are driven by applications in numerical linear algebra.",
        "Measurements of the ionization state of the intergalactic medium (IGM) can\nprobe the sources of the extragalactic ionizing background. We provide new\nmeasurements of the ionizing emissivity of galaxies using measurements of the\nionizing background and ionizing photon mean free path from high-redshift\nquasar spectra at $2.5 < z < 6$. Unlike most prior works, we account for\nradiative-transfer effects and possible neutral islands from the tail of\nreionization at $z > 5$. We combine our results with measurements of the UV\nluminosity function to constrain the average escaping ionizing efficiency of\ngalaxies, $\\langle f_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}$. Assuming\ngalaxies with $M_{\\rm UV} < -11$ emit ionizing photons, we find $\\log (\\langle\nf_{\\rm esc} \\xi_{\\rm ion}\\rangle_{L_{\\rm UV}}\/{\\rm erg^{-1}Hz}) =\n24.47_{-0.17}^{+0.09}$ and $24.75_{-0.28}^{+0.15}$ at $z=5$ and $6$, and\n$1\\sigma$ upper limits of $24.48$ and $24.31$ at $z = 2.5$ and $4$,\nrespectively. We also estimate the population-averaged $f_{\\rm esc}$ using\nmeasurements of intrinsic ionizing efficiency from JWST. We find $\\langle\nf_{\\rm esc} \\rangle = 0.126_{-0.041}^{+0.034}$ and $0.224_{-0.108}^{+0.098}$ at\n$z=5$ and $6$, and $1\\sigma$ upper limits of $f_{\\rm esc}< 0.138$ and $0.096$\nat $z=2.5$ and $4$, respectively, for $M_{\\rm UV} < -11$. Our findings are\nconsistent with prior measurements of $f_{\\rm esc} \\lesssim 10\\%$ at $z \\leq\n4$, but indicate a factor of several increase between $z = 4$ and $6$. The\nsteepness of this evolution is sensitive to the highly uncertain mean free path\nand ionizing background intensity at $z>5$. Lastly, we find\n$1.10^{+0.21}_{-0.39}$ photons per H atom are emitted into the IGM between\n$z=6$ and $=5.3$. This is $\\approx 4\\times$ more than needed to complete the\nlast $20\\%$ of reionization absent recombinations, suggesting that\nreionization's end was likely absorption-dominated.",
        "This paper proposes new ANOVA-based approximations of functions and emulators\nof high-dimensional models using either available derivatives or local\nstochastic evaluations of such models. Our approach makes use of sensitivity\nindices to design adequate structures of emulators. For high-dimensional models\nwith available derivatives, our derivative-based emulators reach dimension-free\nmean squared errors (MSEs) and parametric rate of convergence (i.e.,\n$\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model\n(without available derivatives) by deriving global emulators that account for\nthe local properties of models or simulators. Such generic emulators enjoy\ndimension-free biases, parametric rates of convergence and MSEs that depend on\nthe dimensionality. Dimension-free MSEs are obtained for high-dimensional\nmodels with particular inputs' distributions. Our emulators are also\ncompetitive in dealing with different distributions of the input variables and\nfor selecting inputs and interactions. Simulations show the efficiency of our\napproach.",
        "The effective anisotropic stress $\\eta$ is a key variable in the\ncharacterization of many classes of modified gravity theories, as it allows the\ntesting for a long-range force additional to gravity. In this paper we forecast\nthe precision with which future large surveys can determine $\\eta$ in a way\nthat only relies on directly observable quantities obtained from the\nspectroscopic measurements of the clustering of galaxies and the photometric\nbased observation of the projected lensing and galaxy clustering correlations\nand their cross signal. Our method does not require further assumptions about\nthe initial power spectrum, the modified gravity model, the expansion rate, or\nthe bias. We consider various cases: $\\eta$ free to vary in space and time, or\nwith only redshift dependence, or constant. We take as a reference\nspecifications that approximate a Euclid-like photometric or a combined one\nwith a DESI-like spectroscopic survey. Among our results, we find that a future\nlarge-scale lensing and clustering survey can constrain $\\eta$ to at least 30\\%\nif $z$, $k$ independent, and to less than 10\\% on average for the $z$\ndependence only, to finally reach 5\\% values in the constant case.",
        "We generalize cubic norm structures to cubic norm pairs and extend hermitian\ncubic norm structures to arbitrary commutative unital rings. For the associated\n``skew dimension one structurable algebra\" of these pairs, we construct a\ncorresponding Lie algebra and a group of automorphisms of the Lie algebra.\nUsing the structure of this automorphism group, we also prove that each\nhermitian cubic norm structure induces an operator Kantor pair.",
        "Motivated by the newly discovered Co-based superconductor Na2CoSe2O, we\nperformed systematic calculations of its electronic band structures using the\ndensity functional theory (DFT) plus the dynamical mean-field theory (DMFT)\napproaches. Our comparative studies reveal weakly correlated and itinerant\nnature of the Co-3d electrons and show no sign of fluctuating local moments as\nexpected in many other unconventional superconductors, although the Co eg\norbitals are close to half filling. These suggest that Na2CoSe2O is a normal\nparamagnetic metal and its superconductivity might not be of strongly\ncorrelated nature, contrary to the initial speculation. We suggest future\ninvestigations of electron-phonon interactions to clarify its pairing\nmechanism.",
        "Superconducting nanowire single photon detectors (SNSPDs) emerged in the last\ndecade as a disruptive technology that features performance characteristics,\nsuch as high sensitivity, dynamic range and temporal accuracy, which are\nideally suited for light detection and ranging (lidar) applications. Here, we\nreport a time-of-flight (TOF) lidar system based on waveguide-integrated SNSPDs\nthat excels in temporal accuracy, which translates into high range resolution.\nFor single-shot measurements, we find resolution in the millimeter regime,\nresulting from the jitter of the time-of-flight signal of 21$\\,$ps for low\nphoton numbers. We further decrease this signal jitter to 11$\\,$ps by driving\nthe SNSPD into a multiphoton detection regime, utilizing laser pulses of higher\nintensity, thus improving range resolution. For multi-shot measurements we find\nsub-millimeter range-accuracy of 0.75$\\,$mm and reveal additional surface\ninformation of scanned objects by visualizing the number of reflected photons\nand their temporal spread with the acquired range data in a combined\nrepresentation. Our realization of a lidar receiver exploits favorable timing\naccuracy of waveguide-integrated SNSPDs and extends their operation to the\nmultiphoton regime, which benefits a wide range of remote sensing applications.",
        "In this paper, we introduce the concept of bikernel by monochromatic paths of\na bicolored digraph. This concept is strongly motivated by the existing notions\nof kernels, kernels by monochromatic paths, and double stable augmented\ncategories. We establish sufficient and necessary conditions for several\nfamilies of bicolored digraphs to have a bikernel by monochromatic paths. Also,\nwe characterize bicolored digraphs without monochromatic cycles that possess a\nbikernel by monochromatic paths. Similarly, we characterize bicolored digraphs\nwith monochromatic cycles that also have a bikernel by monochromatic paths.\nFurthermore, we prove sufficient and necessary conditions for some families of\ndigraphs to be $BK$-colorable. This means that a bicoloration of the digraph\nexists where the resulting bicolored digraph has a bikernel.",
        "We provide new families of compact complex manifolds with no K\\\"ahler\nstructure carrying symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. These examples are obtained as compact quotients of the solvable\nLie group $\\mathbb{C}^{2n} \\ltimes_{\\rho} \\mathbb{C}^{2m}$, for which we\nconstruct explicit lattices. By cohomological computations we prove that such\nmanifolds carry symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. Furthermore, we compute the Kodaira dimension of an almost-K\\\"ahler\nstructure and generators for the de Rham and Dolbeault cohomologies.",
        "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.",
        "Motivated by its connection to the limit behaviour of imprecise Markov\nchains, we introduce and study the so-called convergence of upper transition\noperators: the condition that for any function, the orbit resulting from\niterated application of this operator converges. In contrast, the existing\nnotion of `ergodicity' requires convergence of the orbit to a constant. We\nderive a very general (and practically verifiable) sufficient condition for\nconvergence in terms of accessibility and lower reachability, and prove that\nthis sufficient condition is also necessary whenever (i) all transient states\nare absorbed or (ii) the upper transition operator is finitely generated.",
        "We prove optimal decay estimates for positive solutions to elliptic\np-Laplacian problems in the entire Euclidean space, when a critical\nnonlinearity with a decaying source term is considered. Also gradient decay\nestimates are furnished. Our results extend previous theorems in the\nliterature, in which a purely critical reaction is treated. The technique is\nbased on a priori estimates, regularity results, and rescaling arguments,\ncombined with the doubling lemma.",
        "This work describes the principled design of a theoretical framework leading\nto fast and accurate algorithmic information measures on finite multisets of\nfinite strings by means of compression. One distinctive feature of our approach\nis to manipulate {\\em reified}, explicit representations of the very entities\nand quantities of the theory itself: compressed strings, models,\nrate-distortion states, minimal sufficient models, joint and relative\ncomplexity. To do so, a programmable, recursive data structure called a {\\em\nparselet} essentially provides modeling of a string as a concatenation of\nparameterized instantiations from sets of finite strings that encode the\nregular part of the data. This supports another distinctive feature of this\nwork, which is the native embodiment of Epicurus' Principle on top of Occam's\nRazor, so as to produce both a most-significant and most-general explicit model\nfor the data. This model is iteratively evolved through the Principle of\nMinimal Change to reach the so-called minimal sufficient model of the data.\nParselets may also be used to compute a compression score to any arbitrary\nhypothesis about the data. A lossless, rate-distortion oriented, compressed\nrepresentation is proposed, that allows immediate reusability of the costly\ncomputations stored on disk for their fast merging as our core routine for\ninformation calculus. Two information measures are deduced: one is exact\nbecause it is purely combinatorial, and the other may occasionally incur slight\nnumerical inaccuracies because it is an approximation of the Kolmogorov\ncomplexity of the minimal sufficient model. Symmetry of information is enforced\nat the bit level. Whenever possible, parselets are compared with off-the-shelf\ncompressors on real data. Some other applications just get enabled by\nparselets."
      ]
    }
  },
  {
    "id":2412.18649,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Biophysical properties of the human finger for touch comprehension: influences of ageing and gender",
    "start_abstract":"The human finger plays an extremely important role in tactile perception, but little is known about how age and gender affect its biophysical properties their perception. We combined studies on contact characteristics, mechanical surface topography to understand effects the finger. values obtained regarding characteristics (i.e. adhesive force) were significantly higher for women than men. As Young's modulus E), a significant positive correlation with was observed found be women. A between arithmetic mean of roughness However, inverse effect highlighted have never been reported previously literature. These results open new perspectives understanding weakening perception across ages it differs men",
    "start_categories":[
      "Biodynamic"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A Review of Surface Haptics: Enabling Tactile Effects on Touch Surfaces"
      ],
      "abstract":[
        "In this article, we review the current technology underlying surface haptics that converts passive touch surfaces to active ones (machine haptics), our perception of tactile stimuli displayed through (human their potential applications (human-machine interaction), and finally, challenges ahead us in making them available commercial systems. This article primarily covers interactions human fingers or hands with surface-haptics displays by focusing on three most popular actuation methods: vibrotactile, electrostatic, ultrasonic."
      ],
      "categories":[
        "cs.HC"
      ]
    },
    "list":{
      "title":[
        "Exploring Typographic Visual Prompts Injection Threats in Cross-Modality\n  Generation Models",
        "Reasoning is All You Need for Video Generalization: A Counterfactual\n  Benchmark with Sub-question Evaluation",
        "All-dry pick-up and transfer method for quantum emitter arrays in\n  hexagonal boron nitride",
        "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction",
        "Data-driven methods to discover stable linear models of the helicity\n  injectors on HIT-SIU",
        "Make yourself comfortable: Nudging urban heat and noise mitigation with\n  smartwatch-based Just-in-time Adaptive Interventions (JITAI)",
        "Aug3D: Augmenting large scale outdoor datasets for Generalizable Novel\n  View Synthesis",
        "A universal total anomalous dissipator",
        "Autocatalysis due to combinatorial enhancement",
        "W-class states-identification and quantification of Bell-CHSH\n  inequalities' violation",
        "Illuminating Youth: Decades of Mid-Infrared Variability and Color\n  Evolution of Young Stellar Objects",
        "Effective Two-Stage Double Auction for Dynamic Resource Trading in Edge\n  Networks via Overbooking",
        "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear\n  Algebra",
        "SoK: Understanding Vulnerabilities in the Large Language Model Supply\n  Chain",
        "When Unsupervised Domain Adaptation meets One-class Anomaly Detection:\n  Addressing the Two-fold Unsupervised Curse by Leveraging Anomaly Scarcity",
        "Quadrotor Neural Dead Reckoning in Periodic Trajectories",
        "Gensors: Authoring Personalized Visual Sensors with Multimodal\n  Foundation Models and Reasoning",
        "Low-overhead Magic State Circuits with Transversal CNOTs",
        "DocVideoQA: Towards Comprehensive Understanding of Document-Centric\n  Videos through Question Answering",
        "On quasisymmetric mappings between ultrametric spaces",
        "Template Matching in Images using Segmented Normalized Cross-Correlation",
        "The Quest for Visual Understanding: A Journey Through the Evolution of\n  Visual Question Answering",
        "How manipulable are prediction markets?",
        "Quantitative analysis of vectorial torques in thin 3d Co ferromagnet\n  using orbital-spin conversion",
        "Rigidity of Higson coronas",
        "Semi-Lie arithmetic fundamental lemma for the full spherical Hecke\n  algebra",
        "The Liabilities of Robots.txt",
        "Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic\n  Pick-and-Place Setups",
        "Label Distribution Learning with Biased Annotations by Learning\n  Multi-Label Representation"
      ],
      "abstract":[
        "Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-vision,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks\nhave attracted significant attention. Large Vision Language Models (LVLMs) and\nI2I GMs are employed to handle VLP and I2I tasks, respectively. Previous\nresearch indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to generate disruptive outputs\nsemantically related to those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of VLP tasks when injected into images. In this paper, we\ncomprehensively investigate the performance impact induced by Typographic\nVisual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe\nperformance modifications and characteristics of this threat, we also introduce\nthe TVPI Dataset. Through extensive explorations, we deepen the understanding\nof the underlying causes of the TVPI threat in various GMs and offer valuable\ninsights into its potential origins.",
        "Counterfactual reasoning is crucial for robust video understanding but\nremains underexplored in existing multimodal benchmarks. In this paper, we\nintroduce \\textbf{COVER} (\\textbf{\\underline{CO}}unterfactual\n\\textbf{\\underline{V}}id\\textbf{\\underline{E}}o\n\\textbf{\\underline{R}}easoning), a multidimensional multimodal benchmark that\nsystematically evaluates MLLMs across the abstract-concrete and\nperception-cognition dimensions. Beyond prior multimodal benchmarks, COVER\ndecomposes complex queries into structured sub-questions, enabling fine-grained\nreasoning analysis. Experiments on commercial and open-source models reveal a\nstrong correlation between sub-question accuracy and counterfactual reasoning\nperformance, highlighting the role of structured inference in video\nunderstanding. Furthermore, our results suggest a key insight: enhancing the\nreasoning capability of models is essential for improving the robustness of\nvideo understanding. COVER establishes a new standard for assessing MLLMs'\nlogical reasoning abilities in dynamic environments.",
        "Single photon emitters in hexagonal boron nitride are based on fluorescent\npoint-like defects. These defects typically have exceptional photophysical\nproperties and therefore been the focus of extensive research due to their\npotential to advance photonic quantum technologies. However, achieving scalable\nintegration of these emitters to arbitrary platforms with high yield while\nretaining their characteristics remains a significant challenge, particularly\nwhen the target substrate is not compatible with the fabrication method. In\nthis work, we introduce an all-dry transfer method aimed at addressing these\nchallenges with improved effectiveness compared to existing techniques. This\npolymer stamp-assisted transfer method maintains high output and preserves the\nfundamental characteristics of the emitters while eliminating wet chemical\nprocesses. A comprehensive post-transfer characterization verified not only the\nmaintenance of the defining characteristic of a single photon emitter, the\nsecond-order correlation function $g^{(2)}(0)$, but also showed improvement by\nabout 46%. In contrast, the lifetime, emission spectrum, and the photostability\nshowed only negligible change, demonstrating that the characteristics of the\nemitters were retained during the transfer process. This transfer technique has\nsuccess rate of 81.8%, determined by the proportion of single photon emitters\nthat retain their optical and preserve physical structure post-transfer. This\nhigh success rate shows the potential to scale the integration of single photon\nemitters across diverse platforms. We expect that this process contributes to\nthe applications of boron nitride defects in quantum technologies.",
        "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks.",
        "Accurate and efficient circuit models are necessary to control the power\nelectronic circuits found on plasma physics experiments. Tuning and controlling\nthe behavior of these circuits is inextricably linked to plasma performance.\nLinear models are greatly preferred for control applications due to their\nwell-established performance guarantees, but they typically fail to capture\nnonlinear dynamics and changes in experimental parameters. Data-driven system\nidentification can help mitigate these shortcomings by learning interpretable\nand accurate reduced-order models of a complex system, in this case the\ninjector circuits of the Helicity Injected Torus - Steady Inductive Upgrade\n(HIT-SIU) experiment. Specifically, the Bagging Optimized Dynamic Mode\nDecomposition (BOP-DMD), is leveraged to learn stable, reduced order models of\nthe interaction between the spheromak plasma formed in the confinement volume,\nand the injector circuits of the device. BOP-DMD is trained and evaluated on an\nanalytic model of the vacuum dynamics of the injector circuits of HIT-SIU, as\nwell as an analytic linear reduced-order model for the injector dynamics when a\nplasma is present. BOP-DMD is then fit on experimental data, both on shots with\nand without a plasma in the confinement volume. In doing so, we demonstrate the\ncapability of data-driven methods to produce stable, linear models for control\nand uncertainty quantification in plasma experiments.",
        "Humans can play a more active role in improving their comfort in the built\nenvironment if given the right information at the right place and time. This\npaper outlines the use of Just-in-Time Adaptive Interventions (JITAI)\nimplemented in the context of the built environment to provide information that\nhelps humans minimize the impact of heat and noise on their daily lives. This\nframework builds upon the open-source Cozie iOS smartwatch platform. It\nincludes data collection through micro-surveys and intervention messages\ntriggered by environmental, contextual, and personal history conditions. An\neight-month deployment of the method was completed in Singapore with 103\nparticipants who submitted over 12,000 micro-surveys and delivered over 3,600\nJITAI intervention messages. A weekly survey conducted during two deployment\nphases revealed an overall increase in perceived usefulness ranging from 8-19%\nover the first three weeks of data collection. For noise-related interventions,\nparticipants showed an overall increase in location changes ranging from 4-11%\nand a 2-17% increase in earphone use to mitigate noise distractions. For\nthermal comfort-related interventions, participants demonstrated a 3-13%\nincrease in adjustments to their location or thermostat to feel more\ncomfortable. The analysis found evidence that personality traits (such as\nconscientiousness), gender, and environmental preferences could be factors in\ndetermining the perceived helpfulness of JITAIs and influencing behavior\nchange. These findings underscore the importance of tailoring intervention\nstrategies to individual traits and environmental conditions, setting the stage\nfor future research to refine the delivery, timing, and content of intervention\nmessages.",
        "Recent photorealistic Novel View Synthesis (NVS) advances have increasingly\ngained attention. However, these approaches remain constrained to small indoor\nscenes. While optimization-based NVS models have attempted to address this,\ngeneralizable feed-forward methods, offering significant advantages, remain\nunderexplored. In this work, we train PixelNeRF, a feed-forward NVS model, on\nthe large-scale UrbanScene3D dataset. We propose four training strategies to\ncluster and train on this dataset, highlighting that performance is hindered by\nlimited view overlap. To address this, we introduce Aug3D, an augmentation\ntechnique that leverages reconstructed scenes using traditional\nStructure-from-Motion (SfM). Aug3D generates well-conditioned novel views\nthrough grid and semantic sampling to enhance feed-forward NVS model learning.\nOur experiments reveal that reducing the number of views per cluster from 20 to\n10 improves PSNR by 10%, but the performance remains suboptimal. Aug3D further\naddresses this by combining the newly generated novel views with the original\ndataset, demonstrating its effectiveness in improving the model's ability to\npredict novel views.",
        "For all $\\alpha\\in(0,1)$, we construct an explicit divergence-free vector\nfield $V\\in L^\\infty_tC^\\alpha_x \\cap C^{\\frac{\\alpha}{1-\\alpha}}_t L^\\infty_x$\nso that the solutions to the drift-diffusion equations\n$$\\partial_t\\theta^\\kappa-\\kappa\\Delta\\theta^\\kappa+V\\cdot\\nabla\\theta^\\kappa=0$$\nexhibit asymptotic total dissipation for all mean-zero initial data:\n$\\lim_{\\kappa\\rightarrow 0}\\|\\theta^\\kappa(1,\\cdot)\\|_{L^2}=0$. Additionally,\nwe give explicit rates in $\\kappa$ and uniform dependence on initial data.",
        "We demonstrate that autocatalytic reactions, where a product catalyzes its\nown formation, can be significantly accelerated when the product molecules are\nindistinguishable from each other. This ``combinatorial enhancement,\" analogous\nto the driving force of osmotic pressure, arises from the increased\nmultiplicity of microscopic configurations. We quantify this effect with a\nfree-energy gain, Fgain, and validate our theoretical predictions using\nmolecular dynamics simulations. We also propose an experiment to directly test\nthis phenomenon, potentially providing new insights into self-assembly,\nbiomolecular binding, and other cooperative processes.",
        "We discuss a family of W-class states describing three-qubit systems. For\nsuch systems, we analyze the relations between the entanglement measures and\nthe nonlocality parameter for a two-mode mixed state related to the two-qubit\nsubsystem. We find the conditions determining the boundary values of the\nnegativity, parameterized by concurrence, for violating the Bell-CHSH\ninequality. Additionally, we derive the value ranges of the mixedness measure,\nparameterized by concurrence and negativity for the qubit--qubit mixed state,\nguaranteeing the violation and non-violation of the Bell-CHSH inequality.",
        "The variability of Young Stellar Objects (YSOs) is a crucial tool for\nunderstanding the mechanisms driving flux changes. In this study, we present an\ninfrared variability analysis of a large sample of over 20,000 candidate YSOs,\nusing data from the ALLWISE and NEOWISE surveys, which span around a decade\nwith a 6-month cadence. We applied Lomb-Scargle Periodogram (LSP) analysis and\nlinear fitting to the light curves, classifying them into distinct categories:\n{\\it Secular} ({\\it Linear}, {\\it Curved}, and {\\it Periodic}) and {\\it\nStochastic} ({\\it Burst}, {\\it Drop}, and {\\it Irregular}). Our findings show\nthat 5,467 (26.2$\\pm$0.3\\%) of the sources exhibit variability, with most\n(19.7$\\pm$0.3\\%) showing {\\it Irregular} variations, followed by {\\it Curved}\nand {\\it Periodic} variations. In addition, 235 sources of {\\it Bursts} and 122\n{\\it Drop} sources were identified. Variability is more pronounced in Class I\nsources with a higher fraction of variables (36.3$\\pm$0.6\\%) compared to Class\nII (22.1$\\pm$0.4\\%) and Class III (22.5$\\pm$1.0\\%) sources. The color (W1 $-$\nW2) versus magnitude analysis (W2) using linear fitting shows that the trend\n``redder-when-brighter\" (RWB) is more prevalent (85.4$\\pm$0.5\\%) among YSOs. In\ncontrast, the trend ``bluer-when-brighter\" (BWB) is more common in younger\nsources compared to more evolved ones, having a BWB fraction of 29.0$\\pm$1.1\\%\nfor Class I to 4.0$\\pm$0.9\\% for Class III.",
        "To facilitate responsive and cost-effective computing resource scheduling and\nservice delivery over edge-assisted mobile networks, this paper investigates a\nnovel two-stage double auction methodology via utilizing an interesting idea of\nresource overbooking to overcome dynamic and uncertain nature from edge servers\n(sellers) and demand from mobile devices (as buyers). The proposed auction\nintegrates multiple essential factors such as social welfare maximization and\ndecision-making latency (e.g., the time for determining winning seller-buyer\npairs) reduction, by introducing a stagewise strategy: an overbooking-driven\npre-double auction (OPDAuction) for determining long-term cooperations between\nsellers and buyers before practical resource transactions as Stage I, and a\nreal-time backup double auction (RBDAuction) for handling residual resource\ndemands during actual transactions. In particular, by applying a proper\noverbooking rate, OPDAuction helps with facilitating trading contracts between\nappropriate sellers and buyers as guidance for future transactions, by allowing\nthe booked resources to exceed supply. Then, since pre-auctions may cause\nrisks, our RBDAuction adjusts to real-time market changes, further enhancing\nthe overall social welfare. More importantly, we offer an interesting view to\nshow that our proposed two-stage auction can support significant design\nproperties such as truthfulness, individual rationality, and budget balance.\nThrough extensive experiments, we demonstrate good performance in social\nwelfare, time efficiency, and computational scalability, outstripping\nconventional methods in dynamic edge computing settings.",
        "K-means is a popular clustering algorithm with significant applications in\nnumerous scientific and engineering areas. One drawback of K-means is its\ninability to identify non-linearly separable clusters, which may lead to\ninaccurate solutions in certain cases. Kernel K-means is a variant of classical\nK-means that can find non-linearly separable clusters. However, it scales\nquadratically with respect to the size of the dataset, taking several minutes\nto cluster even medium-sized datasets on traditional CPU-based machines. In\nthis paper, we present a formulation of Kernel K-means using sparse-dense\nmatrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV),\nand we show that our formulation enables the rapid implementation of a fast\nGPU-based version of Kernel K-means with little programming effort. Our\nimplementation, named Popcorn, is the first open-source GPU-based\nimplementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x\nover a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a\nGPU implementation of Kernel K-means that does not use sparse matrix\ncomputations. Our results support the effectiveness of sparse matrices as tools\nfor efficient parallel programming.",
        "Large Language Models (LLMs) transform artificial intelligence, driving\nadvancements in natural language understanding, text generation, and autonomous\nsystems. The increasing complexity of their development and deployment\nintroduces significant security challenges, particularly within the LLM supply\nchain. However, existing research primarily focuses on content safety, such as\nadversarial attacks, jailbreaking, and backdoor attacks, while overlooking\nsecurity vulnerabilities in the underlying software systems. To address this\ngap, this study systematically analyzes 529 vulnerabilities reported across 75\nprominent projects spanning 13 lifecycle stages. The findings show that\nvulnerabilities are concentrated in the application (50.3%) and model (42.7%)\nlayers, with improper resource control (45.7%) and improper neutralization\n(25.1%) identified as the leading root causes. Additionally, while 56.7% of the\nvulnerabilities have available fixes, 8% of these patches are ineffective,\nresulting in recurring vulnerabilities. This study underscores the challenges\nof securing the LLM ecosystem and provides actionable insights to guide future\nresearch and mitigation strategies.",
        "This paper introduces the first fully unsupervised domain adaptation (UDA)\nframework for unsupervised anomaly detection (UAD). The performance of UAD\ntechniques degrades significantly in the presence of a domain shift, difficult\nto avoid in a real-world setting. While UDA has contributed to solving this\nissue in binary and multi-class classification, such a strategy is ill-posed in\nUAD. This might be explained by the unsupervised nature of the two tasks,\nnamely, domain adaptation and anomaly detection. Herein, we first formulate\nthis problem that we call the two-fold unsupervised curse. Then, we propose a\npioneering solution to this curse, considered intractable so far, by assuming\nthat anomalies are rare. Specifically, we leverage clustering techniques to\nidentify a dominant cluster in the target feature space. Posed as the normal\ncluster, the latter is aligned with the source normal features. Concretely,\ngiven a one-class source set and an unlabeled target set composed mostly of\nnormal data and some anomalies, we fit the source features within a hypersphere\nwhile jointly aligning them with the features of the dominant cluster from the\ntarget set. The paper provides extensive experiments and analysis on common\nadaptation benchmarks for anomaly detection, demonstrating the relevance of\nboth the newly introduced paradigm and the proposed approach. The code will be\nmade publicly available.",
        "In real world scenarios, due to environmental or hardware constraints, the\nquadrotor is forced to navigate in pure inertial navigation mode while\noperating indoors or outdoors. To mitigate inertial drift, end-to-end neural\nnetwork approaches combined with quadrotor periodic trajectories were\nsuggested. There, the quadrotor distance is regressed and combined with\ninertial model-based heading estimation, the quadrotor position vector is\nestimated. To further enhance positioning performance, in this paper we propose\na quadrotor neural dead reckoning approach for quadrotors flying on periodic\ntrajectories. In this case, the inertial readings are fed into a simple and\nefficient network to directly estimate the quadrotor position vector. Our\napproach was evaluated on two different quadrotors, one operating indoors while\nthe other outdoors. Our approach improves the positioning accuracy of other\ndeep-learning approaches, achieving an average 27% reduction in error outdoors\nand an average 79% reduction indoors, while requiring only software\nmodifications. With the improved positioning accuracy achieved by our method,\nthe quadrotor can seamlessly perform its tasks.",
        "Multimodal large language models (MLLMs), with their expansive world\nknowledge and reasoning capabilities, present a unique opportunity for\nend-users to create personalized AI sensors capable of reasoning about complex\nsituations. A user could describe a desired sensing task in natural language\n(e.g., \"alert if my toddler is getting into mischief\"), with the MLLM analyzing\nthe camera feed and responding within seconds. In a formative study, we found\nthat users saw substantial value in defining their own sensors, yet struggled\nto articulate their unique personal requirements and debug the sensors through\nprompting alone. To address these challenges, we developed Gensors, a system\nthat empowers users to define customized sensors supported by the reasoning\ncapabilities of MLLMs. Gensors 1) assists users in eliciting requirements\nthrough both automatically-generated and manually created sensor criteria, 2)\nfacilitates debugging by allowing users to isolate and test individual criteria\nin parallel, 3) suggests additional criteria based on user-provided images, and\n4) proposes test cases to help users \"stress test\" sensors on potentially\nunforeseen scenarios. In a user study, participants reported significantly\ngreater sense of control, understanding, and ease of communication when\ndefining sensors using Gensors. Beyond addressing model limitations, Gensors\nsupported users in debugging, eliciting requirements, and expressing unique\npersonal requirements to the sensor through criteria-based reasoning; it also\nhelped uncover users' \"blind spots\" by exposing overlooked criteria and\nrevealing unanticipated failure modes. Finally, we discuss how unique\ncharacteristics of MLLMs--such as hallucinations and inconsistent\nresponses--can impact the sensor-creation process. These findings contribute to\nthe design of future intelligent sensing systems that are intuitive and\ncustomizable by everyday users.",
        "With the successful demonstration of transversal CNOTs in many recent\nexperiments, it is the right moment to examine its implications on one of the\nmost critical parts of fault-tolerant computation -- magic state preparation.\nUsing an algorithm that can recompile and simplify a circuit of consecutive\nmulti-qubit phase rotations, we manage to construct fault-tolerant circuits for\nCCZ, CS and T states with minimal T-depth and also much lower CNOT depths and\nqubit counts than before. These circuits can play crucial roles in\nfault-tolerant computation with transversal CNOTs, and we hope that the\nalgorithms and methods developed in this paper can be used to further simplify\nother protocols in similar contexts.",
        "Remote work and online courses have become important methods of knowledge\ndissemination, leading to a large number of document-based instructional\nvideos. Unlike traditional video datasets, these videos mainly feature\nrich-text images and audio that are densely packed with information closely\ntied to the visual content, requiring advanced multimodal understanding\ncapabilities. However, this domain remains underexplored due to dataset\navailability and its inherent complexity. In this paper, we introduce the\nDocVideoQA task and dataset for the first time, comprising 1454 videos across\n23 categories with a total duration of about 828 hours. The dataset is\nannotated with 154k question-answer pairs generated manually and via GPT,\nassessing models' comprehension, temporal awareness, and modality integration\ncapabilities. Initially, we establish a baseline using open-source MLLMs.\nRecognizing the challenges in modality comprehension for document-centric\nvideos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances\nunimodal feature extraction with diverse instruction-tuning data and employs\ncontrastive learning to strengthen modality integration. Through fine-tuning,\nthe LLM is equipped with audio-visual capabilities, leading to significant\nimprovements in document-centric video understanding. Extensive testing on the\nDocVideoQA dataset shows that DV-LLaMA significantly outperforms existing\nmodels. We'll release the code and dataset to facilitate future research.",
        "In 1980 P. Tukia and J. V\\\"{a}is\\\"{a}l\\\"{a} in seminal paper [P. Tukia and J.\nV\\\"ais\\\"al\\\"a, Quasisymmetric embeddings of metric spaces, Ann. Acad. Sci.\nFenn., Ser. A I, Math. 5, 97--114 (1980)] extended a concept of quasisymmetric\nmapping known from the theory of quasiconformal mappings to the case of general\nmetric spaces. They also found an estimation for the ratio of diameters of two\nsubsets which are images of two bounded subsets of a metric space under a\nquasisymmetric mapping. We improve this estimation for the case of ultrametric\nspaces. It was also shown that the image of an ultrametric space under an\n$\\eta$-quasisymmetric mapping with $\\eta(1)=1$ is again an ultrametric space.\nIn the case of finite ultrametric spaces it is proved that such mappings are\nball-preserving.",
        "In this paper, a new variant of an algorithm for normalized cross-correlation\n(NCC) is proposed in the context of template matching in images. The proposed\nalgorithm is based on the precomputation of a template image approximation,\nenabling more efficient calculation of approximate NCC with the source image\nthan using the original template for exact NCC calculation. The approximate\ntemplate is precomputed from the template image by a split-and-merge approach,\nresulting in a decomposition to axis-aligned rectangular segments, whose sizes\ndepend on per-segment pixel intensity variance. In the approximate template,\neach segment is assigned the mean grayscale value of the corresponding pixels\nfrom the original template. The proposed algorithm achieves superior\ncomputational performance with negligible NCC approximation errors compared to\nthe well-known Fast Fourier Transform (FFT)-based NCC algorithm, when applied\non less visually complex and\/or smaller template images. In other cases, the\nproposed algorithm can maintain either computational performance or NCC\napproximation error within the range of the FFT-based algorithm, but not both.",
        "Visual Question Answering (VQA) is an interdisciplinary field that bridges\nthe gap between computer vision (CV) and natural language processing(NLP),\nenabling Artificial Intelligence(AI) systems to answer questions about images.\nSince its inception in 2015, VQA has rapidly evolved, driven by advances in\ndeep learning, attention mechanisms, and transformer-based models. This survey\ntraces the journey of VQA from its early days, through major breakthroughs,\nsuch as attention mechanisms, compositional reasoning, and the rise of\nvision-language pre-training methods. We highlight key models, datasets, and\ntechniques that shaped the development of VQA systems, emphasizing the pivotal\nrole of transformer architectures and multimodal pre-training in driving recent\nprogress. Additionally, we explore specialized applications of VQA in domains\nlike healthcare and discuss ongoing challenges, such as dataset bias, model\ninterpretability, and the need for common-sense reasoning. Lastly, we discuss\nthe emerging trends in large multimodal language models and the integration of\nexternal knowledge, offering insights into the future directions of VQA. This\npaper aims to provide a comprehensive overview of the evolution of VQA,\nhighlighting both its current state and potential advancements.",
        "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
        "Recent findings in orbitronics pointed out large current-induced torques\noriginating, in the current understanding, from incident orbital currents.\nThese are generated by orbital Rashba-Edelstein effect (OREE) produced at the\ninterface between some light metal and oxides films e.g. by naturally oxidized\ncopper layer (Cu*). In the present work, by using second harmonic Hall\ntechniques, we determine the ratio of orbital vs spin currents exerting torques\non thin transition metals Co ferromagnet in systems using an orbit-to-spin Pt\nconverter as interlayer with Cu*. Our results quantifying damping like torques\nshow that both orbital and spin currents are enhanced in these systems.\nMoreover, the experimental determination of the decoherence length in a sample\nseries with varying Co thickness clearly demonstrates the interfacial\ngeneration of the orbital currents in Cu* by Orbital Rashba-Edelstein effects\n(REE) leading to subsequent magnetic torque in Co over a typical lengthscale of\nseveral nanometers",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "As an analog to the Jacquet-Rallis fundamental lemma that appears in the\nrelative trace formula approach to the Gan-Gross-Prasad conjectures, the\narithmetic fundamental lemma was proposed by Wei Zhang and used in an approach\nto the arithmetic Gan-Gross-Prasad conjectures. The Jacquet-Rallis fundamental\nlemma was recently generalized by Spencer Leslie to a statement holding for the\nfull spherical Hecke algebra. In the same spirit, Li, Rapoport, and Zhang have\nrecently formulated a conjectural generalization of the arithmetic fundamental\nlemma to the full spherical Hecke algebra. This paper formulates another\nanalogous conjecture for the semi-Lie version of the arithmetic fundamental\nlemma proposed by Yifeng Liu. Then this paper produces explicit formulas for\nparticular cases of the weighted orbital integrals in the two conjectures\nmentioned above, and proves the first non-trivial case of the conjecture.",
        "The robots.txt file, introduced as part of the Robots Exclusion Protocol in\n1994, provides webmasters with a mechanism to communicate access permissions to\nautomated bots. While broadly adopted as a community standard, the legal\nliabilities associated with violating robots.txt remain ambiguous. The rapid\nrise of large language models, which depend on extensive datasets for training,\nhas amplified these challenges, prompting webmasters to increasingly use\nrobots.txt to restrict the activities of bots engaged in large-scale data\ncollection. This paper clarifies the liabilities associated with robots.txt\nwithin the contexts of contract, copyright, and tort law. Drawing on key cases,\nlegal principles, and scholarly discourse, it proposes a legal framework for\nweb scraping disputes. It also addresses the growing fragmentation of the\ninternet, as restrictive practices by webmasters threaten the principles of\nopenness and collaboration. Through balancing innovation with accountability,\nthis paper offers insights to ensure that robots.txt remains an equitable\nprotocol for the internet and thus contributes to digital governance in the age\nof AI.",
        "Simulating object dynamics from real-world perception shows great promise for\ndigital twins and robotic manipulation but often demands labor-intensive\nmeasurements and expertise. We present a fully automated Real2Sim pipeline that\ngenerates simulation-ready assets for real-world objects through robotic\ninteraction. Using only a robot's joint torque sensors and an external camera,\nthe pipeline identifies visual geometry, collision geometry, and physical\nproperties such as inertial parameters. Our approach introduces a general\nmethod for extracting high-quality, object-centric meshes from photometric\nreconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing\nalpha-transparent training while explicitly distinguishing foreground\nocclusions from background subtraction. We validate the full pipeline through\nextensive experiments, demonstrating its effectiveness across diverse objects.\nBy eliminating the need for manual intervention or environment modifications,\nour pipeline can be integrated directly into existing pick-and-place setups,\nenabling scalable and efficient dataset creation.",
        "Multi-label learning (MLL) has gained attention for its ability to represent\nreal-world data. Label Distribution Learning (LDL), an extension of MLL to\nlearning from label distributions, faces challenges in collecting accurate\nlabel distributions. To address the issue of biased annotations, based on the\nlow-rank assumption, existing works recover true distributions from biased\nobservations by exploring the label correlations. However, recent evidence\nshows that the label distribution tends to be full-rank, and naive apply of\nlow-rank approximation on biased observation leads to inaccurate recovery and\nperformance degradation. In this paper, we address the LDL with biased\nannotations problem from a novel perspective, where we first degenerate the\nsoft label distribution into a hard multi-hot label and then recover the true\nlabel information for each instance. This idea stems from an insight that\nassigning hard multi-hot labels is often easier than assigning a soft label\ndistribution, and it shows stronger immunity to noise disturbances, leading to\nsmaller label bias. Moreover, assuming that the multi-label space for\npredicting label distributions is low-rank offers a more reasonable approach to\ncapturing label correlations. Theoretical analysis and experiments confirm the\neffectiveness and robustness of our method on real-world datasets."
      ]
    }
  },
  {
    "id":2411.0656,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Tracing Power With Circuit Theory",
    "start_abstract":"Power tracing is the task of disaggregating power injection a generator (or load) into sum constituent components that can unambiguously be attributed to loads (generators) and losses. Applications range broad spectrum of: transmission services pricing, loss allocation in distribution networks, fixed-cost allocation, modelling bilateral transactions, financial storage rights. This paper develops an analytical approach leveraging elementary circuit laws. The method rigorous from system-theoretic vantage point, it yields unambiguous results are consistent with constitutive principles describe steady-state behaviour networks. Moreover, implemented limited computational burden, applies networks arbitrary topologies, preserves coupling between activeand reactive-power injections. Numerical experiments indicate given solved power-flow solution, disaggregations computed for test system 2383 buses, 327 generators, 2056 4.34 s on personal computer, hence establishing scalability. Furthermore, applications demonstrated case studies focused quantifying impact distributed generation extracting nodal contributions respectively.",
    "start_categories":[
      "Digital Circuits"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Carbon-Aware Optimal Power Flow"
      ],
      "abstract":[
        "To facilitate effective decarbonization of the electric power sector, this paper introduces generic Carbon-aware Optimal Power Flow (C-OPF) method for system decision-making that considers demand-side carbon accounting and emission management. Built upon classic optimal flow (OPF) model, C-OPF incorporates equations constraints, as well carbon-related objectives, to jointly optimize flow. In particular, establishes invertibility matrix proposes modeling linearization techniques address issues undetermined directions bilinear terms in model. Additionally, two novel models, together with schemes, energy storage systems are developed integrated into Numerical simulations demonstrate characteristics effectiveness method, comparison OPF solutions."
      ],
      "categories":[
        "physics.soc-ph"
      ]
    },
    "list":{
      "title":[
        "DUAL: Diversity and Uncertainty Active Learning for Text Summarization",
        "The global representation fibered ring",
        "Analysis of Niobium Electropolishing Using a Generalized Distribution of\n  Relaxation Times Method",
        "Rota-Baxter operators of nonzero weight on the split Cayley-Dickson\n  algebra",
        "Quantum Neural Networks for Cloud Cover Parameterizations in Climate\n  Models",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Molecular Pseudorotation in Phthalocyanines as a Tool for Magnetic Field\n  Control at the Nanoscale",
        "Advanced Deep Learning Techniques for Analyzing Earnings Call\n  Transcripts: Methodologies and Applications",
        "Ill-Posedness of the Incompressible Euler--Maxwell Equations in the\n  Yudovich Class",
        "Pessimistic bilevel optimization approach for decision-focused learning",
        "Beyond Interpolation: Extrapolative Reasoning with Reinforcement\n  Learning and Graph Neural Networks",
        "Diffusion-aware Censored Gaussian Processes for Demand Modelling",
        "Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization",
        "Strategy Coopetition Explains the Emergence and Transience of In-Context\n  Learning",
        "A deep learning approach to inverse medium scattering: Learning\n  regularizers from a direct imaging method",
        "Quantum oscillation studies of the nodal line semimetal Ni3In2S2-xSex",
        "Proximity-Induced Nodal Metal in an Extremely Underdoped CuO$_2$ Plane\n  in Triple-Layer Cuprates",
        "Infrared spectroscopy of astrophysical ice analogs at oblique angles",
        "Hamiltonian Lattice Gauge Theories: emergent properties from Tensor\n  Network methods",
        "Distributed LLMs and Multimodal Large Language Models: A Survey on\n  Advances, Challenges, and Future Directions",
        "Advancing Singlish Understanding: Bridging the Gap with Datasets and\n  Multimodal Models",
        "Optical conductivity of the topologically-nontrivial MXenes,\n  Mo$_2$HfC$_2$O$_2$ and W$_2$HfC$_2$O$_2$: first-principles calculation and\n  effective model analysis",
        "Low rank MSO",
        "Countably compact inverse semigroups and Nyikos problem",
        "In Specs we Trust? Conformance-Analysis of Implementation to\n  Specifications in Node-RED and Associated Security Risks",
        "RNN-DAS: A New Deep Learning Approach for Detection and Real-Time\n  Monitoring of Volcano-Tectonic Events Using Distributed Acoustic Sensing",
        "Stability of multi-state configurations of fuzzy dark matter",
        "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
        "Adaptive Noise-Tolerant Network for Image Segmentation"
      ],
      "abstract":[
        "With the rise of large language models, neural text summarization has\nadvanced significantly in recent years. However, even state-of-the-art models\ncontinue to rely heavily on high-quality human-annotated data for training and\nevaluation. Active learning is frequently used as an effective way to collect\nsuch datasets, especially when annotation resources are scarce. Active learning\nmethods typically prioritize either uncertainty or diversity but have shown\nlimited effectiveness in summarization, often being outperformed by random\nsampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel\nalgorithm that combines uncertainty and diversity to iteratively select and\nannotate samples that are both representative of the data distribution and\nchallenging for the current model. DUAL addresses the selection of noisy\nsamples in uncertainty-based methods and the limited exploration scope of\ndiversity-based methods. Through extensive experiments with different\nsummarization models and benchmark datasets, we demonstrate that DUAL\nconsistently matches or outperforms the best performing strategies. Using\nvisualizations and quantitative metrics, we provide valuable insights into the\neffectiveness and robustness of different active learning strategies, in an\nattempt to understand why these strategies haven't performed consistently in\ntext summarization. Finally, we show that DUAL strikes a good balance between\ndiversity and robustness.",
        "In this paper, we combine the concepts of the fibered Burnside ring and the\ncharacter ring, viewing them as fibered biset functors, into what we call the\nglobal representation fibered ring of a finite group. We compute all ring\nhomomorphisms from this ring to the complex numbers, determine its spectrum and\nits connected components, and identify the primitive idempotents of this ring\ntensor with $\\mathbb{Q}$ and its conductors.",
        "Using electrochemical impedance spectroscopy, we have devised a method of\nsensing the microscopic surface conditions on the surface of niobium as it is\nundergoing an electrochemical polishing (EP) treatment. The method uses\nelectrochemical impedance spectroscopy (EIS) to gather information on the\nsurface state of the electrode without disrupting the polishing reaction. The\nEIS data is analyzed using a so-called distribution of relaxation times (DRT)\nmethod. Using DRT, the EIS data can be deconvolved into discrete relaxation\ntime peaks without any a priori knowledge of the electrode dynamics. By\nanalyzing the relaxation time peaks, we are able to distinguish two distinct\nmodes of the EP reaction. As the polishing voltage is increased, the electrode\ntransitions from the low voltage EP mode, characterized by a single relaxation\ntime peaks, to the high voltage EP mode, characterized by two relaxation time\npeaks. We theorize that this second peak is caused by the formation of an oxide\nlayer on the electrode. We also find that this oxide induced peak transitions\nfrom to a negative relaxation time, which is indicative of a blocking electrode\nprocess. By analyzing EPed samples, we show that samples polished in the low\nvoltage mode have significantly higher surface roughness due to grain etching\nand faceting. We find that the surface roughness of the samples only improves\nwhen the oxide film peak is present and in the negative relaxation time region.\nThis shows that EIS combined with DRT analysis can be used to predict etching\non EPed Nb. This method can also be performed before or during the EP, which\ncould allow for adjustment of polishing parameters to guarantee a smooth cavity\nsurface finish.",
        "We describe Rota-Baxter operators on split octonions. It turns out that up to\nsome transformations there exists exactly one such non-splitting operator over\nany field. We also obtain a description of all decompositions of split\noctonions over a quadratically closed field of characteristic different from 2\ninto a sum of two subalgebras, which describes the splitting Rota-Baxter\noperators. It completes the classification of Rota-Baxter operators on\ncomposition algebras of any weight.",
        "Long-term climate projections require running global Earth system models on\ntimescales of hundreds of years and have relatively coarse resolution (from 40\nto 160 km in the horizontal) due to their high computational costs. Unresolved\nsubgrid-scale processes, such as clouds, are described in a semi-empirical\nmanner by so called parameterizations, which are a major source of uncertainty\nin climate projections. Machine learning models trained on short\nhigh-resolution climate simulations are promising candidates to replace\nconventional parameterizations. In this work, we explore the potential of\nquantum machine learning (QML), and in particular quantum neural networks\n(QNNs), to develop cloud cover parameterizations. QNNs differ from their\nclassical counterparts, and their potentially high expressivity turns them into\npromising tools for accurate data-driven schemes to be used in climate models.\nWe perform an extensive comparative analysis between several QNNs and classical\nneural networks (NNs), by training both ansatzes on data coming from\nhigh-resolution simulations with the ICOsahedral Non-hydrostatic weather and\nclimate model (ICON). Our results show that the overall performance of the\ninvestigated QNNs is comparable to that of classical NNs of similar size, i.e.,\nwith the same number of trainable parameters, with both ansatzes outperforming\nstandard parameterizations used in climate models. Our study includes an\nanalysis of the generalization ability of the models as well as the geometrical\nproperties of their optimization landscape. We also investigate the effects of\nfinite sampling noise, and show that the training and the predictions of the\nQNNs are stable even in this noisy setting. These results demonstrate the\napplicability of QML to learn meaningful patterns in climate data, and are thus\nrelevant for a broad range of problems within the climate modeling community.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Metal phthalocyanines, a highly versatile class of aromatic, planar,\nmacrocyclic molecules with a chelated central metal ion, are topical objects of\nongoing research and particularly interesting due to their magnetic properties.\nHowever, while current focus lies almost exclusively on spin-Zeeman-related\neffects, the high symmetry of the molecule and its circular shape suggests the\nexploitation of light-induced excitation of twofold degenerate vibrational\nstates in order to generate, switch and manipulate magnetic fields at the\nnanoscale. The underlying mechanism is a molecular pseudorotation that can be\ntriggered by infrared pulses and gives rise to a quantized, small but\ncontrollable magnetic dipole moment. We investigate the optical stimulation of\nvibrationally-induced molecular magnetism and estimate changes in the magnetic\nshielding constants for confirmation by future experiments.",
        "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
        "It was shown recently by Ars\\'enio and the author that the two-dimensional\nincompressible Euler--Maxwell system is globally well-posed in the Yudovich\nclass, provided that the electromagnetic field enjoys appropriate conditions,\nincluding the Normal Structure. In this paper, we prove that this assumption is\nsharp, in the sense that the Euler--Maxwell system becomes ill-posed in the\nYudovich class for initial data that do not obey the Normal Structure\ncondition. The proof applies to both the whole plane and the two-dimensional\ntorus, and holds for any value of the speed of light $c\\in (0,\\infty)$. This is\nachieved by expanding the magnetic field around a horizontal background and\nshowing that the Lorentz force can be decomposed into two parts: the first is\nin the form of a singular operator acting on the vorticity, and the second, a\n\"remainder\", is of lower order when analyzed in a specific time regime.",
        "The recent interest in contextual optimization problems, where randomness is\nassociated with side information, has led to two primary strategies for\nformulation and solution. The first, estimate-then-optimize, separates the\nestimation of the problem's parameters from the optimization process. The\nsecond, decision-focused optimization, integrates the optimization problem's\nstructure directly into the prediction procedure. In this work, we propose a\npessimistic bilevel approach for solving general decision-focused formulations\nof combinatorial optimization problems. Our method solves an\n$\\varepsilon$-approximation of the pessimistic bilevel problem using a\nspecialized cut generation algorithm. We benchmark its performance on the 0-1\nknapsack problem against estimate-then-optimize and decision-focused methods,\nincluding the popular SPO+ approach. Computational experiments highlight the\nproposed method's advantages, particularly in reducing out-of-sample regret.",
        "Despite incredible progress, many neural architectures fail to properly\ngeneralize beyond their training distribution. As such, learning to reason in a\ncorrect and generalizable way is one of the current fundamental challenges in\nmachine learning. In this respect, logic puzzles provide a great testbed, as we\ncan fully understand and control the learning environment. Thus, they allow to\nevaluate performance on previously unseen, larger and more difficult puzzles\nthat follow the same underlying rules. Since traditional approaches often\nstruggle to represent such scalable logical structures, we propose to model\nthese puzzles using a graph-based approach. Then, we investigate the key\nfactors enabling the proposed models to learn generalizable solutions in a\nreinforcement learning setting. Our study focuses on the impact of the\ninductive bias of the architecture, different reward systems and the role of\nrecurrent modeling in enabling sequential reasoning. Through extensive\nexperiments, we demonstrate how these elements contribute to successful\nextrapolation on increasingly complex puzzles.These insights and frameworks\noffer a systematic way to design learning-based systems capable of\ngeneralizable reasoning beyond interpolation.",
        "Inferring the true demand for a product or a service from aggregate data is\noften challenging due to the limited available supply, thus resulting in\nobservations that are censored and correspond to the realized demand, thereby\nnot accounting for the unsatisfied demand. Censored regression models are able\nto account for the effect of censoring due to the limited supply, but they\ndon't consider the effect of substitutions, which may cause the demand for\nsimilar alternative products or services to increase. This paper proposes\nDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with a\ngraph diffusion process in order to model the latent process of transfer of\nunsatisfied demand between similar products or services. We instantiate this\nnew class of models under the framework of GPs and, based on both simulated and\nreal-world data for modeling sales, bike-sharing demand, and EV charging\ndemand, demonstrate its ability to better recover the true demand and produce\nmore accurate out-of-sample predictions.",
        "The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1\/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE.",
        "In-context learning (ICL) is a powerful ability that emerges in transformer\nmodels, enabling them to learn from context without weight updates. Recent work\nhas established emergent ICL as a transient phenomenon that can sometimes\ndisappear after long training times. In this work, we sought a mechanistic\nunderstanding of these transient dynamics. Firstly, we find that, after the\ndisappearance of ICL, the asymptotic strategy is a remarkable hybrid between\nin-weights and in-context learning, which we term \"context-constrained\nin-weights learning\" (CIWL). CIWL is in competition with ICL, and eventually\nreplaces it as the dominant strategy of the model (thus leading to ICL\ntransience). However, we also find that the two competing strategies actually\nshare sub-circuits, which gives rise to cooperative dynamics as well. For\nexample, in our setup, ICL is unable to emerge quickly on its own, and can only\nbe enabled through the simultaneous slow development of asymptotic CIWL. CIWL\nthus both cooperates and competes with ICL, a phenomenon we term \"strategy\ncoopetition.\" We propose a minimal mathematical model that reproduces these key\ndynamics and interactions. Informed by this model, we were able to identify a\nsetup where ICL is truly emergent and persistent.",
        "This paper aims to solve numerically the two-dimensional inverse medium\nscattering problem with far-field data. This is a challenging task due to the\nsevere ill-posedness and strong nonlinearity of the inverse problem. As already\nknown, it is necessary but also difficult numerically to employ an appropriate\nregularization strategy which effectively incorporates certain a priori\ninformation of the unknown scatterer to overcome the severe ill-posedness of\nthe inverse problem. In this paper, we propose to use a deep learning approach\nto learn the a priori information of the support of the unknown scatterer from\na direct imaging method. Based on the learned a priori information, we propose\ntwo inversion algorithms for solving the inverse problem. In the first one, the\nlearned a priori information is incorporated into the projected Landweber\nmethod. In the second one, the learned a priori information is used to design\nthe regularization functional for the regularized variational formulation of\nthe inverse problem which is then solved with a traditional iteration\nalgorithm. Extensive numerical experiments show that our inversion algorithms\nprovide good reconstruction results even for the high contrast case and have a\nsatisfactory generalization ability.",
        "Ternary shandite compounds with the general formula T3M2X2 (T = Ni, Co, Rh or\nPd; M = Sn, In or Pb and X = S or Se) have emerged as a large pool of\ntopological semimetals. This family of compounds hosts different topological\nphases for various combinations of T, M and X. This paper reports the\nobservation of quantum oscillations under the high magnetic fields in\nNi3In2S2-xSex single crystals. Angular dependence of oscillation frequency\nsuggests an evolution of the Fermi surface from three-dimensional to\ntwo-dimensional on Se substitution for S in Ni3In2S2. The effective mass\nobtained for each composition by fitting the oscillation amplitude with the\nLifshitz-Kosevich formula, shows no significant change, suggesting that the\ntopological phase might be relatively robust against enhanced SOC upon Se\ndoping in Ni3In2S2.",
        "ARPES studies have established that the high-$T_c$ cuprates with single and\ndouble CuO$_2$ layers evolve from the Mott insulator to the pseudogap state\nwith a Fermi arc, on which the superconducting (SC) gap opens. In four- to\nsix-layer cuprates, on the other hand, small hole Fermi pockets are formed in\nthe innermost CuO$_2$ planes, indicating antiferromagnetism. Here, we performed\nARPES studies on the triple-layer Bi$_2$Sr$_2$Ca$_2$Cu$_3$O$_{10+\\delta}$ over\na wide doping range, and found that, although the doping level of the inner\nCuO$_2$ plane was extremely low in underdoped samples, the $d$-wave SC gap was\nenhanced to the unprecedentedly large value of $\\Delta_0\\sim$100 meV at the\nantinode and persisted well above $T_{{c}}$ without the appearance of a Fermi\narc, indicating a robust ``nodal metal''. We attribute the nodal metallic\nbehavior to the unique local environment of the inner clean CuO$_2$ plane in\nthe triple-layer cuprates, sandwiched by nearly optimally-doped two outer\nCuO$_2$ planes and hence subject to strong proximity effect from both sides. In\nthe nodal metal, quasiparticle peaks showed electron-hole symmetry, suggesting\n$d$-wave pairing fluctuations. Thus the proximity effect on the innermost\nCuO${_2}$ plane is the strongest in the triple-layer cuprates, which explains\nwhy the $T_c$ reaches the maximum at the layer number of three in every\nmulti-layer cuprate family.",
        "In astrochemical exploration, infrared (IR) spectroscopy is vital for\nunderstanding the composition and structure of ice in various space\nenvironments. This article explores the impact of incident angles on IR\nspectroscopy, focusing on molecular components present in interstellar and\ncircumstellar ice mantles such as CO, CO$_2$, H$_2$O, CH$_3$OH, NH$_3$, CH$_4$,\nH$_2$S. The experiment involves changing the angle at which the infrared beam\nhits the surface used for ice deposition. It is important to measure the\ndensity of the ice layer accurately, especially for experiments that involve\nusing different angles in infrared spectroscopy. Furthermore, the experimental\nmethodology allowed us to derive the {\\it effective} refraction index values in\nthe infrared range for each ice component. Existing corrections typically\nconsider geometric configurations but overlook the refractive index of the ice\n($n$), a factor dependent on ice composition. The study reveals that the\nincident angle and the refractive index, determine the pathlength of the IR\nbeam across the ice sample. This insight challenges conventional corrections,\nimpacting the integrated absorption values of the IR bands and column\ndensities. In addition, for certain ice components, variations in the incidence\nangle affect the longitudinal (LO) and transverse (TO) optical modes of the\nice, leading to observable changes in the IR band profiles that provide\ninformation on the amorphous or crystalline structure of the ice. The practical\nimplications of this work apply to experimental setups where normal IR\nmeasurements are unfeasible. Researchers using, for example, the standard\n45$^{\\circ}$ angle for IR spectroscopy, will benefit from a more accurate\nestimation of ice column density.",
        "This thesis develops advanced Tensor Network (TN) methods to address\nHamiltonian Lattice Gauge Theories (LGTs), overcoming limitations in real-time\ndynamics and finite-density regimes. A novel dressed-site formalism is\nintroduced, enabling efficient truncation of gauge fields while preserving\ngauge invariance for both Abelian and non-Abelian theories. This formalism is\nsuccessfully applied to SU(2) Yang-Mills LGTs in two dimensions, providing the\nfirst TN simulations of this system and revealing critical aspects of its phase\ndiagram and non-equilibrium behavior, such as a Quantum Many-Body (QMB)\nscarring dynamics. A generalization of the dressed-site formalism is proposed\nthrough a new fermion-to-qubit mapping for general lattice fermion theories,\nrevealing powerful for classical and quantum simulations. Numerical\ninnovations, including the use of optimal space-filling curves such as the\nHilbert curve to preserve locality in high-dimensional simulations, further\nenhance the efficiency of these methods. Together with high-performance\ncomputing techniques, these advances open current and future development\npathways toward optimized, efficient, and faster simulations on scales\ncomparable to Monte Carlo state-of-the-art.",
        "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.",
        "Singlish, a Creole language rooted in English, is a key focus in linguistic\nresearch within multilingual and multicultural contexts. However, its spoken\nform remains underexplored, limiting insights into its linguistic structure and\napplications. To address this gap, we standardize and annotate the largest\nspoken Singlish corpus, introducing the Multitask National Speech Corpus\n(MNSC). These datasets support diverse tasks, including Automatic Speech\nRecognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue\nSummarization (SDS), and Paralinguistic Question Answering (PQA). We release\nstandardized splits and a human-verified test set to facilitate further\nresearch. Additionally, we propose SingAudioLLM, a multi-task multimodal model\nleveraging multimodal large language models to handle these tasks concurrently.\nExperiments reveal our models adaptability to Singlish context, achieving\nstate-of-the-art performance and outperforming prior models by 10-30% in\ncomparison with other AudioLLMs and cascaded solutions.",
        "The optical conductivity and the relevant electronic excitation processes are\ninvestigated in topologically-nontrivial MXenes, Mo$_2$HfC$_2$O$_2$ and\nW$_2$HfC$_2$O$_2$, utilizing first-principles calculation and effective model\nanalysis. The numerical calculation based on the first-principles band\nstructure reveals the presence of several characteristic features in the\nspectrum of optical conductivity as a function of photon energy. The drastic\ndependence on the photon polarization angle is also presented in terms of\napparent features. In this paper, an effective model is also generated\nreferring to the crystal symmetries and applied to reveal the microscopic\norigin of the characteristics. Then, it is shown that some features are\nstrongly related to parity inversion between the conduction and valence bands,\nthe key signature in electronic structures of topologically nontrivial\ninsulators.",
        "We introduce a new logic for describing properties of graphs, which we call\nlow rank MSO. This is the fragment of monadic second-order logic in which set\nquantification is restricted to vertex sets of bounded cutrank. We prove the\nfollowing statements about the expressive power of low rank MSO.\n  - Over any class of graphs that is weakly sparse, low rank MSO has the same\nexpressive power as separator logic. This equivalence does not hold over all\ngraphs.\n  - Over any class of graphs that has bounded VC dimension, low rank MSO has\nthe same expressive power as flip-connectivity logic. This equivalence does not\nhold over all graphs.\n  - Over all graphs, low rank MSO has the same expressive power as\nflip-reachability logic.\n  Here, separator logic is an extension of first-order logic by basic\npredicates for checking connectivity, which was proposed by Boja\\'nczyk [ArXiv\n2107.13953] and by Schirrmacher, Siebertz, and Vigny [ACM ToCL 2023].\nFlip-connectivity logic and flip-reachability logic are analogues of separator\nlogic suited for non-sparse graphs, which we propose in this work. In\nparticular, the last statement above implies that every property of undirected\ngraphs expressible in low rank MSO can be decided in polynomial time.",
        "A regular separable first-countable countably compact space is called a {\\em\nNyikos} space. In this paper, we give a partial solution to an old problem of\nNyikos by showing that each locally compact Nyikos inverse topological\nsemigroup is compact. Also, we show that a topological semigroup $S$ that\ncontains a dense inverse subsemigroup is a topological inverse semigroup,\nprovided (i) $S$ is compact, or (ii) $S$ is countably compact and sequential.\nThe latter result solves a problem of Banakh and Pastukhova and provides the\nautomatic continuity of inversion in certain compact-like inverse semigroups.",
        "Low-code development frameworks for IoT platforms offer a simple\ndrag-and-drop mechanism to create applications for the billions of existing IoT\ndevices without the need for extensive programming knowledge. The security of\nsuch software is crucial given the close integration of IoT devices in many\nhighly sensitive areas such as healthcare or home automation. Node-RED is such\na framework, where applications are built from nodes that are contributed by\nopen-source developers. Its reliance on unvetted open-source contributions and\nlack of security checks raises the concern that the applications could be\nvulnerable to attacks, thereby imposing a security risk to end users. The\nlow-code approach suggests, that many users could lack the technical knowledge\nto mitigate, understand, or even realize such security concerns. This paper\nfocuses on \"hidden\" information flows in Node-RED nodes, meaning flows that are\nnot captured by the specifications. They could (unknowingly or with malicious\nintent) cause leaks of sensitive information to unauthorized entities. We\nreport the results of a conformance analysis of all nodes in the Node-RED\nframework, for which we compared the numbers of specified inputs and outputs of\neach node against the number of sources and sinks detected with CodeQL. The\nresults show, that 55% of all nodes exhibit more possible flows than are\nspecified. A risk assessment of a subset of the nodes showed, that 28% of them\nare associated with a high severity and 36% with a medium severity rating.",
        "In this article, we present a novel Deep Learning model based on Recurrent\nNeural Networks (RNNs) with Long Short-Term Memory (LSTM) cells, designed as a\nreal-time Volcano-seismic Signal Recognition (VSR) system for Distributed\nAcoustic Sensing (DAS) measurements. The model was trained on an extensive\ndatabase of Volcano-Tectonic (VT) events derived from the co-eruptive\nseismicity of the 2021 La Palma eruption, recorded by a High-fidelity submarine\nDistributed Acoustic Sensing array (HDAS) near the eruption site. The features\nused for supervised model training, based on signal energy average in frequency\nbands, effectively enable the model to leverage spatial contextual information\nand the temporal evolution of volcano-seismic signals provided by the DAS\ntechnique. The proposed model not only detects the presence of VT events but\nalso analyzes their temporal evolution, selecting and classifying their\ncomplete waveforms with an accuracy of approximately 97% for correctly detected\nand classified VT events. Furthermore, the model has demonstrated robust\nperformance in generalizing to other time intervals and volcanoes, enabling\ncontinuous real-time monitoring of seismicity. Such results highlight the\npotential of using RNN-based approaches with LSTM cells for application to\nother active volcanoes, enabling fast, automatic analysis with low\ncomputational requirements and the need of minimal retraining, for the creation\nof labeled seismic catalogs directly from DAS measurements. This represents a\nsignificant advancement in the use of DAS technology as a viable tool to study\nactive volcanoes and their seismic activity.",
        "We study the stability properties of multi-state configurations of the\nSchr\\\"odinger-Poisson system without self-interaction, with monopolar and first\ndipolar components $(1,0,0)$+$(2,1,0)$. We show these configurations studied\nare stable using numerical simulations, and using criteria of stationarity,\nunitarity and time dependence consistency. The study covers a range of states\nwith monopolar to dipolar mass ratio between 47 and 0.17. The astrophysical\nimplication of this result is that this type of configurations is at least\nstable and can be considered physically sound in multi-state ultralight bosonic\ndark matter.",
        "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
        "Unlike image classification and annotation, for which deep network models\nhave achieved dominating superior performances compared to traditional computer\nvision algorithms, deep learning for automatic image segmentation still faces\ncritical challenges. One of such hurdles is to obtain ground-truth\nsegmentations as the training labels for deep network training. Especially when\nwe study biomedical images, such as histopathological images (histo-images), it\nis unrealistic to ask for manual segmentation labels as the ground truth for\ntraining due to the fine image resolution as well as the large image size and\ncomplexity. In this paper, instead of relying on clean segmentation labels, we\nstudy whether and how integrating imperfect or noisy segmentation results from\noff-the-shelf segmentation algorithms may help achieve better segmentation\nresults through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend\nthe noisy label deep learning to image segmentation with two novel aspects: (1)\nmultiple noisy labels can be integrated into one deep learning model; (2) noisy\nsegmentation modeling, including probabilistic parameters, is adaptive,\ndepending on the given testing image appearance. Implementation of the new ANTN\nmodel on both the synthetic data and real-world histo-images demonstrates its\neffectiveness and superiority over off-the-shelf and other existing\ndeep-learning-based image segmentation algorithms."
      ]
    }
  },
  {
    "id":2411.0656,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Carbon-Aware Optimal Power Flow",
    "start_abstract":"To facilitate effective decarbonization of the electric power sector, this paper introduces generic Carbon-aware Optimal Power Flow (C-OPF) method for system decision-making that considers demand-side carbon accounting and emission management. Built upon classic optimal flow (OPF) model, C-OPF incorporates equations constraints, as well carbon-related objectives, to jointly optimize flow. In particular, establishes invertibility matrix proposes modeling linearization techniques address issues undetermined directions bilinear terms in model. Additionally, two novel models, together with schemes, energy storage systems are developed integrated into Numerical simulations demonstrate characteristics effectiveness method, comparison OPF solutions.",
    "start_categories":[
      "physics.soc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Tracing Power With Circuit Theory"
      ],
      "abstract":[
        "Power tracing is the task of disaggregating power injection a generator (or load) into sum constituent components that can unambiguously be attributed to loads (generators) and losses. Applications range broad spectrum of: transmission services pricing, loss allocation in distribution networks, fixed-cost allocation, modelling bilateral transactions, financial storage rights. This paper develops an analytical approach leveraging elementary circuit laws. The method rigorous from system-theoretic vantage point, it yields unambiguous results are consistent with constitutive principles describe steady-state behaviour networks. Moreover, implemented limited computational burden, applies networks arbitrary topologies, preserves coupling between activeand reactive-power injections. Numerical experiments indicate given solved power-flow solution, disaggregations computed for test system 2383 buses, 327 generators, 2056 4.34 s on personal computer, hence establishing scalability. Furthermore, applications demonstrated case studies focused quantifying impact distributed generation extracting nodal contributions respectively."
      ],
      "categories":[
        "Digital Circuits"
      ]
    },
    "list":{
      "title":[
        "On the Jucys-Murphy method and fusion procedure for the Sergeev\n  superalgebra",
        "Designing Telepresence Robots to Support Place Attachment",
        "Large Neighborhood Search and Bitmask Dynamic Programming for Wireless\n  Mobile Charging Electric Vehicle Routing Problems in Medical Transportation",
        "Maritime Mission Planning for Unmanned Surface Vessel using Large\n  Language Model",
        "Nonlinear multidomain model for nerve bundles with random structure",
        "Predicting Cognitive Decline: A Multimodal AI Approach to Dementia\n  Screening from Speech",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe\n  Medication Recommendation",
        "Ego vs. Exo and Active vs. Passive: Investigating the Effects of\n  Viewpoint and Navigation on Spatial Immersion and Understanding in Immersive\n  Storytelling",
        "Assessing Autonomous Inspection Regimes: Active Versus Passive Satellite\n  Inspection",
        "Survey of image processing settings used for mammography systems in the\n  United Kingdom: how variable is it?",
        "R-equivalence classes of $\\mathrm{Rot} \\mathbb{E}^{2}$-colorings of\n  torus knots",
        "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More",
        "Dual Arm Steering of Deformable Linear Objects in 2-D and 3-D\n  Environments Using Euler's Elastica Solutions",
        "ClinText-SP and RigoBERTa Clinical: a new set of open resources for\n  Spanish Clinical NLP",
        "Unified Few-shot Crack Segmentation and its Precise 3D Automatic\n  Measurement in Concrete Structures",
        "Diffusion Adversarial Post-Training for One-Step Video Generation",
        "Exploring Interpretability for Visual Prompt Tuning with Hierarchical\n  Concepts",
        "Diagnostic tools for exploring differences in distributional properties\n  between two samples: nonparametric approach",
        "Quantum Fourier transform computational accuracy analysis",
        "Adaptive Meta-learning-based Adversarial Training for Robust Automatic\n  Modulation Classification",
        "MOFA: Discovering Materials for Carbon Capture with a GenAI- and\n  Simulation-Based Workflow",
        "Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as\n  an Adaptive Feature Model: Generalization and Adaptivity",
        "On the emergence and properties of weird quasiperiodic attractors",
        "Deep Learning within Tabular Data: Foundations, Challenges, Advances and\n  Future Directions",
        "Deep Distance Map Regression Network with Shape-aware Loss for\n  Imbalanced Medical Image Segmentation",
        "Model Tampering Attacks Enable More Rigorous Evaluations of LLM\n  Capabilities",
        "Causal Inference for Qualitative Outcomes",
        "Estimating unknown dynamics and cost as a bilinear system with\n  Koopman-based Inverse Optimal Control"
      ],
      "abstract":[
        "We use the Jucys-Murphy elements to construct a complete set of primitive\nidempotents for the Sergeev superalgebra ${\\mathcal S}_n$. We produce\nseminormal forms for the simple modules over ${\\mathcal S}_n$ and over the spin\nsymmetric group algebra with explicit constructions of basis vectors. We show\nthat the idempotents can also be obtained from a new version of the fusion\nprocedure.",
        "People feel attached to places that are meaningful to them, which\npsychological research calls \"place attachment.\" Place attachment is associated\nwith self-identity, self-continuity, and psychological well-being. Even small\ncues, including videos, images, sounds, and scents, can facilitate feelings of\nconnection and belonging to a place. Telepresence robots that allow people to\nsee, hear, and interact with a remote place have the potential to establish and\nmaintain a connection with places and support place attachment. In this paper,\nwe explore the design space of robotic telepresence to promote place\nattachment, including how users might be guided in a remote place and whether\nthey experience the environment individually or with others. We prototyped a\ntelepresence robot that allows one or more remote users to visit a place and be\nguided by a local human guide or a conversational agent. Participants were 38\nuniversity alumni who visited their alma mater via the telepresence robot. Our\nfindings uncovered four distinct user personas in the remote experience and\nhighlighted the need for social participation to enhance place attachment. We\ngenerated design implications for future telepresence robot design to support\npeople's connections with places of personal significance.",
        "The transition to electric vehicles (EVs) is critical to achieving\nsustainable transportation, but challenges such as limited driving range and\ninsufficient charging infrastructure have hindered the widespread adoption of\nEVs, especially in time-sensitive logistics such as medical transportation.\nThis paper presents a new model to break through this barrier by combining\nwireless mobile charging technology with optimization. We propose the Wireless\nMobile Charging Electric Vehicle Routing Problem (WMC-EVRP), which enables\nMedical Transportation Electric Vehicles (MTEVs) to be charged while traveling\nvia Mobile Charging Carts (MCTs). This eliminates the time wastage of stopping\nfor charging and ensures uninterrupted operation of MTEVs for such\ntime-sensitive transportation problems. However, in this problem, the decisions\nof these two types of heterogeneous vehicles are coupled with each other, which\ngreatly increases the difficulty of vehicle routing optimizations. To address\nthis complex problem, we develop a mathematical model and a tailored\nmeta-heuristic algorithm that combines Bit Mask Dynamic Programming (BDP) and\nLarge Neighborhood Search (LNS). The BDP approach efficiently optimizes\ncharging strategies, while the LNS framework utilizes custom operators to\noptimize the MTEV routes under capacity and synchronization constraints. Our\napproach outperforms traditional solvers in providing solutions for medium and\nlarge instances. Using actual hospital locations in Singapore as data, we\nvalidated the practical applicability of the model through extensive\nexperiments and provided important insights into minimizing costs and ensuring\nthe timely delivery of healthcare services.",
        "Unmanned Surface Vessels (USVs) are essential for various maritime\noperations. USV mission planning approach offers autonomous solutions for\nmonitoring, surveillance, and logistics. Existing approaches, which are based\non static methods, struggle to adapt to dynamic environments, leading to\nsuboptimal performance, higher costs, and increased risk of failure. This paper\nintroduces a novel mission planning framework that uses Large Language Models\n(LLMs), such as GPT-4, to address these challenges. LLMs are proficient at\nunderstanding natural language commands, executing symbolic reasoning, and\nflexibly adjusting to changing situations. Our approach integrates LLMs into\nmaritime mission planning to bridge the gap between high-level human\ninstructions and executable plans, allowing real-time adaptation to\nenvironmental changes and unforeseen obstacles. In addition, feedback from\nlow-level controllers is utilized to refine symbolic mission plans, ensuring\nrobustness and adaptability. This framework improves the robustness and\neffectiveness of USV operations by integrating the power of symbolic planning\nwith the reasoning abilities of LLMs. In addition, it simplifies the mission\nspecification, allowing operators to focus on high-level objectives without\nrequiring complex programming. The simulation results validate the proposed\napproach, demonstrating its ability to optimize mission execution while\nseamlessly adapting to dynamic maritime conditions.",
        "We present a derivation of a multidomain model for the electric potential in\nbundles of randomly distributed axons with different radii. The FitzHugh-Nagumo\ndynamics is assumed on the axons' membrane, and the conductivity depends\nnonlinearly on the electric field. Under ergodicity conditions, we study the\nasymptotic behavior of the potential in the bundle when the number of the axons\nin the bundle is sufficiently large and derive a macroscopic multidomain model\ndescribing the electrical activity of the bundle. Due to the randomness of\ngeometry, the effective intracellular potential is not deterministic but is\nshown to be a stationary function with realizations that are constant on axons'\ncross sections. The technique combines the stochastic two-scale convergence and\nthe method of monotone operators.",
        "Recent progress has been made in detecting early stage dementia entirely\nthrough recordings of patient speech. Multimodal speech analysis methods were\napplied to the PROCESS challenge, which requires participants to use audio\nrecordings of clinical interviews to predict patients as healthy control, mild\ncognitive impairment (MCI), or dementia and regress the patient's Mini-Mental\nState Exam (MMSE) scores. The approach implemented in this work combines\nacoustic features (eGeMAPS and Prosody) with embeddings from Whisper and\nRoBERTa models, achieving competitive results in both regression (RMSE: 2.7666)\nand classification (Macro-F1 score: 0.5774) tasks. Additionally, a novel\ntwo-tiered classification setup is utilized to better differentiate between MCI\nand dementia. Our approach achieved strong results on the test set, ranking\nseventh on regression and eleventh on classification out of thirty-seven teams,\nexceeding the baseline results.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Medication Recommendation (MR) is a promising research topic which booms\ndiverse applications in the healthcare and clinical domains. However, existing\nmethods mainly rely on sequential modeling and static graphs for representation\nlearning, which ignore the dynamic correlations in diverse medical events of a\npatient's temporal visits, leading to insufficient global structural\nexploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is\nanother issue determining the utility of the MR systems. To address the\nchallenges mentioned above, this paper proposes a novel MR method with the\nintegration of dynamic networks and multi-view drug representations (DNMDR).\nSpecifically, weighted snapshot sequences for dynamic heterogeneous networks\nare constructed based on discrete visits in temporal EHRs, and all the dynamic\nnetworks are jointly trained to gain both structural correlations in diverse\nmedical events and temporal dependency in historical health conditions, for\nachieving comprehensive patient representations with both semantic features and\nstructural relationships. Moreover, combining the drug co-occurrences and\nadverse drug-drug interactions (DDIs) in internal view of drug molecule\nstructure and interactive view of drug pairs, the safe drug representations are\navailable to obtain high-quality medication combination recommendation.\nFinally, extensive experiments on real world datasets are conducted for\nperformance evaluation, and the experimental results demonstrate that the\nproposed DNMDR method outperforms the state-of-the-art baseline models with a\nlarge margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.",
        "Visual storytelling combines visuals and narratives to communicate important\ninsights. While web-based visual storytelling is well-established, leveraging\nthe next generation of digital technologies for visual storytelling,\nspecifically immersive technologies, remains underexplored. We investigated the\nimpact of the story viewpoint (from the audience's perspective) and navigation\n(when progressing through the story) on spatial immersion and understanding.\nFirst, we collected web-based 3D stories and elicited design considerations\nfrom three VR developers. We then adapted four selected web-based stories to an\nimmersive format. Finally, we conducted a user study (N=24) to examine\negocentric and exocentric viewpoints, active and passive navigation, and the\ncombinations they form. Our results indicated significantly higher preferences\nfor egocentric+active (higher agency and engagement) and exocentric+passive\n(higher focus on content). We also found a marginal significance of viewpoints\non story understanding and a strong significance of navigation on spatial\nimmersion.",
        "This paper addresses the problem of satellite inspection, where one or more\nsatellites (inspectors) are tasked with imaging or inspecting a resident space\nobject (RSO) due to potential malfunctions or anomalies. Inspection strategies\nare often reduced to a discretized action space with predefined waypoints,\nfacilitating tractability in both classical optimization and machine learning\nbased approaches. However, this discretization can lead to suboptimal guidance\nin certain scenarios. This study presents a comparative simulation to explore\nthe tradeoffs of passive versus active strategies in multi-agent missions. Key\nfactors considered include RSO dynamic mode, state uncertainty, unmodeled\nentrance criteria, and inspector motion types. The evaluation is conducted with\na focus on fuel utilization and surface coverage. Building on a Monte-Carlo\nbased evaluator of passive strategies and a reinforcement learning framework\nfor training active inspection policies, this study investigates conditions\nunder which passive strategies, such as Natural Motion Circumnavigation (NMC),\nmay perform comparably to active strategies like Reinforcement Learning based\nwaypoint transfers.",
        "The aim was to undertake a national survey of the setup of mammography\nimaging systems in the UK, we were particularly interested in image processing\nand software version. We created a program that can extract selected tags from\nthe DICOM header. 28 medical physics departments used the program on processed\nimages of the TORMAM phantom acquired since 2023 and this produced data for 497\nsystems. We received data for 7 different models of mammography systems. We\nfound that currently in use each model had between 2 and 7 different versions\nof software for the acquisition workstation. Each of the systems had multiple\nversions of image processing settings, a preliminary investigation with TORMAM\ndemonstrated large differences in the appearance of the image for the same\nX-ray model. The Fujifilm, GE and Siemens systems showed differences in the\nsetup of the dose levels. In addition to these settings there were differences\nin the paddles used and grid type. Our snapshot of system set up showed that\nthere is a potential for the images to appear differently according to the\nsettings seen in the headers. These differences may affect the outcomes of AI\nand also human readers. Thus the introduction of AI must take these differences\ninto consideration and the inevitably changes of settings in the future. There\nare responsibilities on AI suppliers, physics, mammographic equipment\nmanufacturers, and breast-screening units to manage the use of AI and ensure\nthe outcomes of breast screening are not adversely affected by the set-up of\nequipment.",
        "We introduce a new equivalence relation, named R-equivalence relation, on the\nset of colorings of an oriented knot diagram by a quandle. We determine the\nR-equivalence classes of colorings of a diagram of a torus knot by a quandle,\ncalled $\\mathrm{Rot} \\mathbb{E}^{2}$, under a certain condition.",
        "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.",
        "This paper describes a method for steering deformable linear objects using\ntwo robot hands in environments populated by sparsely spaced obstacles. The\napproach involves manipulating an elastic inextensible rod by varying the\ngripping endpoint positions and tangents. Closed form solutions that describe\nthe flexible linear object shape in planar environments, Euler's elastica, are\ndescribed. The paper uses these solutions to formulate criteria for non\nself-intersection, stability and obstacle avoidance. These criteria are\nformulated as constraints in the flexible object six-dimensional configuration\nspace that represents the robot gripping endpoint positions and tangents. In\nparticular, this paper introduces a novel criterion that ensures the flexible\nobject stability during steering. All safety criteria are integrated into a\nscheme for steering flexible linear objects in planar environments, which is\nlifted into a steering scheme in three-dimensional environments populated by\nsparsely spaced obstacles. Experiments with a dual-arm robot demonstrate the\nmethod.",
        "We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications.",
        "Visual-Spatial Systems has become increasingly essential in concrete crack\ninspection. However, existing methods often lacks adaptability to diverse\nscenarios, exhibits limited robustness in image-based approaches, and struggles\nwith curved or complex geometries. To address these limitations, an innovative\nframework for two-dimensional (2D) crack detection, three-dimensional (3D)\nreconstruction, and 3D automatic crack measurement was proposed by integrating\ncomputer vision technologies and multi-modal Simultaneous localization and\nmapping (SLAM) in this study. Firstly, building on a base DeepLabv3+\nsegmentation model, and incorporating specific refinements utilizing foundation\nmodel Segment Anything Model (SAM), we developed a crack segmentation method\nwith strong generalization across unfamiliar scenarios, enabling the generation\nof precise 2D crack masks. To enhance the accuracy and robustness of 3D\nreconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized\ntogether with image data and segmentation masks. By leveraging both image- and\nLiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that\nproduces dense, colorized point clouds, effectively capturing crack semantics\nat a 3D real-world scale. Furthermore, the crack geometric attributions were\nmeasured automatically and directly within 3D dense point cloud space,\nsurpassing the limitations of conventional 2D image-based measurements. This\nadvancement makes the method suitable for structural components with curved and\ncomplex 3D geometries. Experimental results across various concrete structures\nhighlight the significant improvements and unique advantages of the proposed\nmethod, demonstrating its effectiveness, accuracy, and robustness in real-world\napplications.",
        "The diffusion models are widely used for image and video generation, but\ntheir iterative generation process is slow and expansive. While existing\ndistillation approaches have demonstrated the potential for one-step generation\nin the image domain, they still suffer from significant quality degradation. In\nthis work, we propose Adversarial Post-Training (APT) against real data\nfollowing diffusion pre-training for one-step video generation. To improve the\ntraining stability and quality, we introduce several improvements to the model\narchitecture and training procedures, along with an approximated R1\nregularization objective. Empirically, our experiments show that our\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\n24fps videos in real time using a single forward evaluation step. Additionally,\nour model is capable of generating 1024px images in a single step, achieving\nquality comparable to state-of-the-art methods.",
        "Visual prompt tuning offers significant advantages for adapting pre-trained\nvisual foundation models to specific tasks. However, current research provides\nlimited insight into the interpretability of this approach, which is essential\nfor enhancing AI reliability and enabling AI-driven knowledge discovery. In\nthis paper, rather than learning abstract prompt embeddings, we propose the\nfirst framework, named Interpretable Visual Prompt Tuning (IVPT), to explore\ninterpretability for visual prompts, by introducing hierarchical concept\nprototypes. Specifically, visual prompts are linked to human-understandable\nsemantic concepts, represented as a set of category-agnostic prototypes, each\ncorresponding to a specific region of the image. Then, IVPT aggregates features\nfrom these regions to generate interpretable prompts, which are structured\nhierarchically to explain visual prompts at different granularities.\nComprehensive qualitative and quantitative evaluations on fine-grained\nclassification benchmarks show its superior interpretability and performance\nover conventional visual prompt tuning methods and existing interpretable\nmethods.",
        "This paper reconsiders the problem of testing the equality of two unspecified\ncontinuous distributions. The framework, which we propose, allows for readable\nand insightful data visualisation and helps to understand and quantify how two\ngroups of data differ. We consider a useful weighted rank empirical process on\n(0,1) and utilise a grid-based approach, based on diadic partitions of (0,1),\nto discretize the continuous process and construct local simultaneous\nacceptance regions. These regions help to identify statistically significant\ndeviations from the null model. In addition, the form of the process and its\ndicretization lead to a highly interpretable visualisation of distributional\ndifferences. We also introduce a new two-sample test, explicitly related to the\nvisualisation. Numerical studies show that the new test procedure performs very\nwell. We illustrate the use and diagnostic capabilities of our approach by an\napplication to a known set of neuroscience data.",
        "In this work, we present a rigorous accuracy analysis of the quantum Fourier\ntransform (QFT), that identifies three natural sources of accuracy degeneracy:\n(i) discretization accuracy inherited from classical sampling theory, (ii)\naccuracy degeneracy due to limited resolution in eigenvalue (phase) estimation,\nand (iii) accuracy degeneracy resulting from finite quantum resources. We\nformalize these accuracy degradation sources by proving two theorems that\nrelate the minimal amplitude and eigenvalue resolution to the number of qubits.\nIn addition, we describe a gate-level implementation of the QFT and present\nsimulation results on small-scale quantum systems that illustrate our\ntheoretical findings. Our results clarify the interplay between classical\nsignal discretization limits and quantum hardware limitations, and they provide\nguidelines for the resource requirements needed to achieve a desired precision.",
        "DL-based automatic modulation classification (AMC) models are highly\nsusceptible to adversarial attacks, where even minimal input perturbations can\ncause severe misclassifications. While adversarially training an AMC model\nbased on an adversarial attack significantly increases its robustness against\nthat attack, the AMC model will still be defenseless against other adversarial\nattacks. The theoretically infinite possibilities for adversarial perturbations\nmean that an AMC model will inevitably encounter new unseen adversarial attacks\nif it is ever to be deployed to a real-world communication system. Moreover,\nthe computational limitations and challenges of obtaining new data in real-time\nwill not allow a full training process for the AMC model to adapt to the new\nattack when it is online. To this end, we propose a meta-learning-based\nadversarial training framework for AMC models that substantially enhances\nrobustness against unseen adversarial attacks and enables fast adaptation to\nthese attacks using just a few new training samples, if any are available. Our\nresults demonstrate that this training framework provides superior robustness\nand accuracy with much less online training time than conventional adversarial\ntraining of AMC models, making it highly efficient for real-world deployment.",
        "We present MOFA, an open-source generative AI (GenAI) plus simulation\nworkflow for high-throughput generation of metal-organic frameworks (MOFs) on\nlarge-scale high-performance computing (HPC) systems. MOFA addresses key\nchallenges in integrating GPU-accelerated computing for GPU-intensive GenAI\ntasks, including distributed training and inference, alongside CPU- and\nGPU-optimized tasks for screening and filtering AI-generated MOFs using\nmolecular dynamics, density functional theory, and Monte Carlo simulations.\nThese heterogeneous tasks are unified within an online learning framework that\noptimizes the utilization of available CPU and GPU resources across HPC\nsystems. Performance metrics from a 450-node (14,400 AMD Zen 3 CPUs + 1800\nNVIDIA A100 GPUs) supercomputer run demonstrate that MOFA achieves\nhigh-throughput generation of novel MOF structures, with CO$_2$ adsorption\ncapacities ranking among the top 10 in the hypothetical MOF (hMOF) dataset.\nFurthermore, the production of high-quality MOFs exhibits a linear relationship\nwith the number of nodes utilized. The modular architecture of MOFA will\nfacilitate its integration into other scientific applications that dynamically\ncombine GenAI with large-scale simulations.",
        "This paper introduces a diagonal adaptive kernel model that dynamically\nlearns kernel eigenvalues and output coefficients simultaneously during\ntraining. Unlike fixed-kernel methods tied to the neural tangent kernel theory,\nthe diagonal adaptive kernel model adapts to the structure of the truth\nfunction, significantly improving generalization over fixed-kernel methods,\nespecially when the initial kernel is misaligned with the target. Moreover, we\nshow that the adaptivity comes from learning the right eigenvalues during\ntraining, showing a feature learning behavior. By extending to deeper\nparameterization, we further show how extra depth enhances adaptability and\ngeneralization. This study combines the insights from feature learning and\nimplicit regularization and provides new perspective into the adaptivity and\ngeneralization potential of neural networks beyond the kernel regime.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance.",
        "Tabular data remains one of the most prevalent data types across a wide range\nof real-world applications, yet effective representation learning for this\ndomain poses unique challenges due to its irregular patterns, heterogeneous\nfeature distributions, and complex inter-column dependencies. This survey\nprovides a comprehensive review of state-of-the-art techniques in tabular data\nrepresentation learning, structured around three foundational design elements:\ntraining data, neural architectures, and learning objectives. Unlike prior\nsurveys that focus primarily on either architecture design or learning\nstrategies, we adopt a holistic perspective that emphasizes the universality\nand robustness of representation learning methods across diverse downstream\ntasks. We examine recent advances in data augmentation and generation,\nspecialized neural network architectures tailored to tabular data, and\ninnovative learning objectives that enhance representation quality.\nAdditionally, we highlight the growing influence of self-supervised learning\nand the adaptation of transformer-based foundation models for tabular data. Our\nreview is based on a systematic literature search using rigorous inclusion\ncriteria, encompassing 127 papers published since 2020 in top-tier conferences\nand journals. Through detailed analysis and comparison, we identify emerging\ntrends, critical gaps, and promising directions for future research, aiming to\nguide the development of more generalizable and effective tabular data\nrepresentation methods.",
        "Small object segmentation, like tumor segmentation, is a difficult and\ncritical task in the field of medical image analysis. Although deep learning\nbased methods have achieved promising performance, they are restricted to the\nuse of binary segmentation mask. Inspired by the rigorous mapping between\nbinary segmentation mask and distance map, we adopt distance map as a novel\nground truth and employ a network to fulfill the computation of distance map.\nSpecially, we propose a new segmentation framework that incorporates the\nexisting binary segmentation network and a light weight regression network\n(dubbed as LR-Net). Thus, the LR-Net can convert the distance map computation\ninto a regression task and leverage the rich information of distance maps.\nAdditionally, we derive a shape-aware loss by employing distance maps as\npenalty map to infer the complete shape of an object. We evaluated our approach\non MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and a clinical\ndataset. Experimental results show that our approach outperforms the\nclassification-based methods as well as other existing state-of-the-arts.",
        "Evaluations of large language model (LLM) risks and capabilities are\nincreasingly being incorporated into AI risk management and governance\nframeworks. Currently, most risk evaluations are conducted by designing inputs\nthat elicit harmful behaviors from the system. However, a fundamental\nlimitation of this approach is that the harmfulness of the behaviors identified\nduring any particular evaluation can only lower bound the model's\nworst-possible-case behavior. As a complementary method for eliciting harmful\nbehaviors, we propose evaluating LLMs with model tampering attacks which allow\nfor modifications to latent activations or weights. We pit state-of-the-art\ntechniques for removing harmful LLM capabilities against a suite of 5\ninput-space and 6 model tampering attacks. In addition to benchmarking these\nmethods against each other, we show that (1) model resilience to capability\nelicitation attacks lies on a low-dimensional robustness subspace; (2) the\nattack success rate of model tampering attacks can empirically predict and\noffer conservative estimates for the success of held-out input-space attacks;\nand (3) state-of-the-art unlearning methods can easily be undone within 16\nsteps of fine-tuning. Together these results highlight the difficulty of\nremoving harmful LLM capabilities and show that model tampering attacks enable\nsubstantially more rigorous evaluations than input-space attacks alone. We\nrelease models at https:\/\/huggingface.co\/LLM-GAT",
        "Causal inference methods such as instrumental variables, regression\ndiscontinuity, and difference-in-differences are widely used to estimate\ntreatment effects. However, their application to qualitative outcomes poses\nfundamental challenges, as standard causal estimands are ill-defined in this\ncontext. This paper highlights these issues and introduces an alternative\nframework that focuses on well-defined and interpretable estimands that\nquantify how treatment affects the probability distribution over outcome\ncategories. We establish that standard identification assumptions are\nsufficient for identification and propose simple, intuitive estimation\nstrategies that remain fully compatible with conventional econometric methods.\nTo facilitate implementation, we provide an open-source R package,\n$\\texttt{causalQual}$, which is publicly available on GitHub.",
        "In this work, we address the challenge of approximating unknown system\ndynamics and costs by representing them as a bilinear system using\nKoopman-based Inverse Optimal Control (IOC). Using optimal trajectories, we\nconstruct a bilinear control system in transformed state variables through a\nmodified Extended Dynamic Mode Decomposition with control (EDMDc) that\nmaintains exact dynamical equivalence with the original nonlinear system. We\nderive Pontryagin's Maximum Principle (PMP) optimality conditions for this\nsystem, which closely resemble those of the inverse Linear Quadratic Regulator\n(LQR) problem due to the consistent control input and state independence from\nthe control. This similarity allows us to apply modified inverse LQR theory,\noffering a more tractable and robust alternative to nonlinear Inverse Optimal\nControl methods, especially when dealing with unknown dynamics. Our approach\nalso benefits from the extensive analytical properties of bilinear control\nsystems, providing a solid foundation for further analysis and application. We\ndemonstrate the effectiveness of the proposed method through theoretical\nanalysis, simulation studies and a robotic experiment, highlighting its\npotential for broader applications in the approximation and design of control\nsystems."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association",
    "start_abstract":"We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.",
    "start_categories":[
      "cs.DC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "A Global Geometric Framework for Nonlinear Dimensionality Reduction"
      ],
      "abstract":[
        "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure."
      ],
      "categories":[
        "math.ST"
      ]
    },
    "list":{
      "title":[
        "Efficient inference of rankings from multi-body comparisons",
        "GWSkyNet-Multi II: an updated deep learning model for rapid\n  classification of gravitational-wave events",
        "On the components of random geometric graphs in the dense limit",
        "Infinitely many saturated travelling waves for epidemic models with\n  distributed-contacts",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Chance-Constrained Covariance Steering for Discrete-Time Markov Jump\n  Linear Systems",
        "Noncommutative Novikov bialgebras and differential antisymmetric\n  infinitesimal bialgebras with weight",
        "The second-order intrinsic Wiedemann-Franz law",
        "A model calculation of the CKM matrix",
        "A Unified Blockwise Measurement Design for Learning Quantum Channels and\n  Lindbladians via Low-Rank Matrix Sensing",
        "Generalized Recurrence Criteria for Classes of Open Quantum Walks",
        "Survival Concept-Based Learning Models",
        "Is a phonon excitation of a superfluid Bose gas a Goldstone boson?",
        "Multiple Horn problems for planar networks and invertible matrices",
        "Torsion in Magnitude homology theories",
        "Primordial black holes as cosmic expansion accelerators",
        "On de Bruijn Array Codes Part II: Linear Codes",
        "Event-Based Limit Order Book Simulation under a Neural Hawkes Process:\n  Application in Market-Making",
        "Propagation of extreme events in multiplex neuronal networks",
        "Diagnosing the solar atmosphere through the Mg I b$_2$ 5173 \\AA\\ line.\n  II. Morphological classification of the intensity and circular polarization\n  profiles",
        "Partial separability of the Schroedinger equation combined with a\n  Jastrow factor",
        "Transfer Learning Analysis of Variational Quantum Circuits",
        "Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow\n  in Shallow Linear Networks",
        "On the approaching geodesics property",
        "Designing Illumination Patterns for Single-Pixel Imaging Using Lattice\n  Models",
        "Relativistic Gas Accretion onto Supermassive Black Hole Binaries from\n  Inspiral through Merger",
        "A Guide to Molecular Properties from the Bethe-Salpeter Equation",
        "From non-equilibrium Green functions to Lattice Wigner: A toy model for\n  quantum nanofluidics simulations",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions"
      ],
      "abstract":[
        "Many of the existing approaches to assess and predict the performance of\nplayers, teams or products in competitive contests rely on the assumption that\ncomparisons occur between pairs of such entities. There are, however, several\nreal contests where more than two entities are part of each comparison, e.g.,\nsports tournaments,multiplayer board and card games, and preference surveys.\nThe Plackett-Luce (PL) model provides a principled approach to infer the\nranking of entities involved in such contests characterized by multi-body\ncomparisons. Unfortunately, traditional algorithms used to compute PL rankings\nsuffer from slow convergence limiting the application of the PL model to\nrelatively small-scale systems. We present here an alternative implementation\nthat allows for significant speed-ups and validate its efficiency in both\nsynthetic and real-world sets of data. Further, we perform systematic\ncross-validation tests concerning the ability of the PL model to predict\nunobserved comparisons. We find that a PL model trained on a set composed of\nmulti-body comparisons is more predictive than a PL model trained on a set of\nprojected pairwise comparisons derived from the very same training set,\nemphasizing the need of properly accounting for the true multi-body nature of\nreal-world systems whenever such an information is available.",
        "Multi-messenger observations of gravitational waves and electromagnetic\nemission from compact object mergers offer unique insights into the structure\nof neutron stars, the formation of heavy elements, and the expansion rate of\nthe Universe. With the LIGO-Virgo-KAGRA (LVK) gravitational-wave detectors\ncurrently in their fourth observing run (O4), it is an exciting time for\ndetecting these mergers. However, assessing whether to follow up a candidate\ngravitational-wave event given limited telescope time and resources is\nchallenging; the candidate can be a false alert due to detector glitches, or\nmay not have any detectable electromagnetic counterpart even if it is real.\nGWSkyNet-Multi is a deep learning model developed to facilitate follow-up\ndecisions by providing real-time classification of candidate events, using\nlocalization information released in LVK rapid public alerts. Here we introduce\nGWSkyNet-Multi II, an updated model targeted towards providing more robust and\ninformative predictions during O4 and beyond. Specifically, the model now\nprovides normalized probability scores and associated uncertainties for each of\nthe four corresponding source categories released by the LVK: glitch, binary\nblack hole, neutron star-black hole, and binary neutron star. Informed by\nexplainability studies of the original model, the updated model architecture is\nalso significantly simplified, including replacing input images with intuitive\nsummary values, making it more interpretable. For significant O4 event alerts\nissued between May 2023 and December 2024, GWSkyNet-Multi II produces a\nprediction that is consistent with the updated LVK classification for 93% of\nevents. The updated model can be used by the community to help make\ntime-critical follow-up decisions.",
        "Consider the geometric graph on $n$ independent uniform random points in a\nconnected compact region $A$ of ${\\bf R}^d, d \\geq 2$ with $C^2$ boundary, or\nin the unit square, with distance parameter $r_n$. Let $K_n$ be the number of\ncomponents of this graph, and $R_n$ the number of vertices not in the giant\ncomponent. Let $S_n$ be the number of isolated vertices. We show that if $r_n$\nis chosen so that $nr_n^d$ tends to infinity but slowly enough that ${\\bf\nE}[S_n]$ also tends to infinity, then $K_n$, $R_n$ and $S_n$ are all asymptotic\nto $\\mu_n$ in probability as $n \\to \\infty$ where (with $|A|$, $\\theta_d$ and\n$|\\partial A|$ denoting the volume of $A$, of the unit $d$-ball, and the\nperimeter of $A$ respectively) $\\mu_n := ne^{-\\pi n r_n^d\/|A|}$ if $d=2$ and\n$\\mu_n := ne^{-\\theta_d n r_n^d\/|A|} + \\theta_{d-1}^{-1} |\\partial A| r_n^{1-d}\ne^{- \\theta_d n r_n^d\/(2|A|)}$ if $d\\geq 3$. We also give variance asymptotics\nand central limit theorems for $K_n$ and $R_n$ in this limiting regime when $d\n\\geq 3$, and for Poisson input with $d \\geq 2$. We extend these results\n(substituting ${\\bf E}[S_n]$ for $\\mu_n$) to a class of non-uniform\ndistributions on $A$.",
        "We consider an epidemic model with distributed-contacts. When the contact\nkernel concentrates, one formally reaches a very degenerate Fisher-KPP equation\nwith a diffusion term that is not in divergence form. We make an exhaustive\nstudy of its travelling waves. For every admissible speed, there exists not\nonly a non-saturated (smooth) wave but also infinitely many saturated (sharp)\nones. Furthermore their tails may differ from what is usually expected. These\nresults are thus in sharp contrast with their counterparts on related models.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "In this paper, we propose a novel convex optimization framework to solve the\noptimal covariance steering problem for discrete-time Markov Jump Linear\nSystems (MJLS) with chance constraints. We derive the analytical expressions\nfor the mean and covariance trajectories of time-varying discrete-time MJLS and\nshow that they cannot be separated even without chance constraints, unlike the\nsingle-mode dynamics case. To solve the covariance steering problem, we propose\na two-step convex optimization framework, which optimizes the mean and\ncovariance subproblems sequentially. Further, we use Gaussian approximations to\nincorporate chance constraints and propose an iterative optimization framework\nto solve the chance-constrained covariance steering problem. Both problems are\noriginally nonconvex, and we derive convex relaxations which are proved to be\nlossless at optimality using the Karush-Kuhn-Tucker (KKT) conditions. Numerical\nsimulations demonstrate the proposed method by achieving target covariances\nwhile respecting chance constraints under Gaussian noise and Markovian jump\ndynamics.",
        "This paper first develops a bialgebra theory for a noncommutative Novikov\nalgebra, called a noncommutative Novikov bialgebra, which is further\ncharacterized by matched pairs and Manin triples of noncommutative Novikov\nalgebras. The classical Yang-Baxter type equation, $\\mathcal{O}$-operators, and\nnoncommutative pre-Novikov algebras are introduced to study noncommutative\nNovikov bialgebra. As an application, noncommutative pre-Novikov algebras are\nobtained from differential dendriform algebras. Next, to generalize Gelfand's\nclassical construction of a Novikov algebra from a commutative differential\nalgebra to the bialgebra context in the noncommutative case, we establish\nantisymmetric infinitesimal (ASI) bialgebras for (noncommutative) differential\nalgebras, and obtain the condition under which a differential ASI bialgebra\ninduces a noncommutative Novikov bialgebra.",
        "In recent years, the nonlinear anomalous thermal Hall effect has attracted\nsubstantial attention. In this paper, we carry out a theoretical exploration of\nthe intrinsic anomalous thermal Hall and Nernst effect that is induced by the\nthermal Berry connection polarizability. This effect is independent of the\nrelaxation time and can be present in antiferromagnets possessing PT symmetry.\nAdditionally, we put forward a second-order intrinsic Wiedemann-Franz law,\nwhich represents the ratio of the second-order intrinsic thermal conductivity\ncoefficient to the second-order intrinsic electrical conductivity coefficient .\nWhen analyzed within a four-band PT symmetric Dirac model, we observe that the\nsecond-order intrinsic thermal conductivity coefficient is linearly\nproportional to the second-order intrinsic electrical conductivity coefficient\n, and the second-order intrinsic Wiedemann-Franz law is characterized by the\nchemical potential $\\mu$ in the low-temperature regime. These findings provide\nsignificant implications for experimental verification.",
        "We propose a strategy to compute the CKM matrix based on the conjecture,\nrecently put forward in the literature, according to which elementary particle\nmasses are not generated like in the standard Higgs scenario, but emerge from a\nnon-perturbative mechanism triggered by the presence in the fundamental\nLagrangian of ``irrelevant'' chiral breaking operators of the Wilson type of\ndimension $d\\geq 6$ scaled by $d-4$ powers of the UV cutoff. Non-perturbatively\ngenerated quark masses have the form $m_q\\sim C_q(\\alpha) \\Lambda_{RGI}$ where\n$\\Lambda_{RGI}$ is the RGI scale of the theory and $C_q(\\alpha)$ is a function\nof the gauge couplings. For the (elementary) fermion $q$ the $C_q(\\alpha)$\nleading behaviour is $C_q(\\alpha)={\\mbox{O}}(\\alpha^{1+(d_q-4)\/2})$. The\ndependence of the gauge coupling power behaviour from the dimension $d_q$ of\nthe Wilson-like operators associated with the fermion $q$ can be exploited to\nconstruct hierarchically organized up and down ''proto-mass matrices'' for\n''proto-flavours'', the diagonalization of which yields flavoured quarks with\ndefinite masses and a first principle construction of the CKM matrix.",
        "Quantum superoperator learning is a pivotal task in quantum information\nscience, enabling accurate reconstruction of unknown quantum operations from\nmeasurement data. We propose a robust approach based on the matrix sensing\ntechniques for quantum superoperator learning that extends beyond the positive\nsemidefinite case, encompassing both quantum channels and Lindbladians. We\nfirst introduce a randomized measurement design using a near-optimal number of\nmeasurements. By leveraging the restricted isometry property (RIP), we provide\ntheoretical guarantees for the identifiability and recovery of low-rank\nsuperoperators in the presence of noise. Additionally, we propose a blockwise\nmeasurement design that restricts the tomography to the sub-blocks,\nsignificantly enhancing performance while maintaining a comparable scale of\nmeasurements. We also provide a performance guarantee for this setup. Our\napproach employs alternating least squares (ALS) with acceleration for\noptimization in matrix sensing. Numerical experiments validate the efficiency\nand scalability of the proposed methods.",
        "In this paper, we study the recurrence of open quantum walks (OQWs) induced\nby finite-dimensional coins $(L,B,R)$. The focus is on homogeneous OQWs with a\nset of vertices $\\mathbb{Z}$, the set of integers. We present three distinct\nrecurrence criteria, each adapted to different types of coins. The first\ncriterion was developed for a class of Lazy OQWs in any finite dimension, where\nthe presented criterion is associated with an auxiliary map and its only\ninvariant state, resulting in the first recurrence criterion for Lazy OQWs. The\nsecond one is restricted to Lazy OQWs of dimension 2, where we provide a\ncomplete characterization of the recurrence for this lower dimension. Finally,\nwe present a general criterion for finite-dimensional coins in the non-lazy\ncase $(B=0)$, which generalizes many of the previously known results. This new\ncriterion holds for irreducible and reducible OQWs through a decomposition of\nthe Hilbert space where our quantum states act.",
        "Concept-based learning enhances prediction accuracy and interpretability by\nleveraging high-level, human-understandable concepts. However, existing CBL\nframeworks do not address survival analysis tasks, which involve predicting\nevent times in the presence of censored data -- a common scenario in fields\nlike medicine and reliability analysis. To bridge this gap, we propose two\nnovel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM\n(Survival Regularized Concept-based Model), which integrate concept-based\nlearning with survival analysis to handle censored event time data. The models\nemploy the Cox proportional hazards model and the Beran estimator. SurvCBM is\nbased on the architecture of the well-known concept bottleneck model, offering\ninterpretable predictions through concept-based explanations. SurvRCM uses\nconcepts as regularization to enhance accuracy. Both models are trained\nend-to-end and provide interpretable predictions in terms of concepts. Two\ninterpretability approaches are proposed: one leveraging the linear\nrelationship in the Cox model and another using an instance-based explanation\nframework with the Beran estimator. Numerical experiments demonstrate that\nSurvCBM outperforms SurvRCM and traditional survival models, underscoring the\nimportance and advantages of incorporating concept information. The code for\nthe proposed algorithms is publicly available.",
        "It is generally accepted that phonons in a superfluid Bose gas are Goldstone\nbosons. This is justified by spontaneous symmetry breaking (SSB), which is\nusually defined as follows: the Hamiltonian of the system is invariant under\nthe $U(1)$ transformation $\\hat{\\Psi}(\\mathbf{r},t)\\rightarrow e^{i\\alpha}%\n\\hat{\\Psi}(\\mathbf{r},t)$, whereas the order parameter $\\Psi(\\mathbf{r},t)$ is\nnot. However, the strict definition of SSB is different: the Hamiltonian and\nthe boundary conditions are invariant under a symmetry transformation, while\nthe \\emph{ground state} is not. Based on the latter criterion, we study a\nfinite system of spinless, weakly interacting bosons using three approaches:\nthe standard Bogoliubov method, the particle-number-conserving Bogoliubov\nmethod, and the approach based on the exact ground-state wave function. Our\nresults show that the answer to the question in the title is \\textquotedblleft\nno\\textquotedblright. Thus, phonons in a real-world (finite) superfluid Bose\ngas are similar to sound in a classical gas: they are not Goldstone bosons, but\nquantised collective vibrational modes arising from the interaction between\natoms. In the case of an infinite Bose gas, however, the picture becomes\nparadoxical: the ground state can be regarded as either infinitely degenerate\nor non-degenerate, making the phonon both similar to a Goldstone boson and\ndifferent from it.",
        "The multiplicative multiple Horn problem is asking to determine possible\nsingular values of the combinations $AB, BC$ and $ABC$ for a triple of\ninvertible matrices $A,B,C$ with given singular values. There are similar\nproblems for eigenvalues of sums of Hermitian matrices (the additive problem),\nand for maximal weights of multi-paths in concatenations of planar networks\n(the tropical problem).\n  For the planar network multiple Horn problem, we establish necessary\nconditions, and we conjecture that for large enough networks they are also\nsufficient. These conditions are given by the trace equalities and rhombus\ninequalities (familiar from the hive description of the classical Horn\nproblem), and by the new set of tetrahedron equalities. Furthermore, if one\nimposes Gelfand-Zeitlin conditions on weights of planar networks, tetrahedron\nequalities turn into the octahedron recurrence from the theory of crystals. We\ngive a geometric interpretation of our results in terms of positive varieties\nwith potential. In this approach, rhombus inequalities follow from the\ninequality $\\Phi^t \\leqslant 0$ for the tropicalized potential, and tetrahedron\nequalities are obtained as tropicalization of certain Pl\\\"ucker relations.\n  For the multiplicative problem, we introduce a scaling parameter $s$, and we\nshow that for $s$ large enough (corresponding to exponentially large\/small\nsingular values) the Duistermaat-Heckman measure associated to the\nmultiplicative problem concentrates in a small neighborhood of the octahedron\nrecurrence locus.",
        "In this article, we analyze the structure and relationships between magnitude\nhomology and Eulerian magnitude homology of finite graphs. Building on the work\nof Kaneta and Yoshinaga, Sazdanovic and Summers, and Asao and Izumihara, we\nprovide two proofs of the existence of torsion in Eulerian magnitude homology,\noffer insights into the types and orders of torsion, and present explicit\ncomputations for various classes of graphs.",
        "We propose a novel and natural mechanism for cosmic acceleration driven by\nprimordial black holes (PBHs) exhibiting repulsive behavior. Using a new\n``Swiss Cheese'' cosmological approach, we demonstrate that this cosmic\nacceleration mechanism is a general phenomenon by examining three regular black\nhole spacetimes - namely the Hayward, Bardeen and Dymnikova spacetimes - as\nwell as the singular de Sitter-Schwarzschild spacetime. Interestingly, by\nmatching these black hole spacetimes with an isotropic and homogeneous\nexpanding Universe, we obtain a phase of cosmic acceleration that ends at an\nenergy scale characteristic to the black hole parameters or due to black hole\nevaporation. This cosmic acceleration mechanism can be relevant either to an\ninflationary phase with a graceful exit and reheating or to an early dark\nenergy type of contribution pertinent to the Hubble tension. Remarkably, we\nfind that ultra-light PBHs with masses $m<5\\times 10^8\\mathrm{g}$ dominating\nthe energy content of the Univese before Big Bang Nucleosynthesis, can drive a\nsuccessful inflationary expansion era without the use of an inflaton field.\nAdditionally, PBHs with masses $m \\sim 10^{12}\\mathrm{g}$ and abundances $0.107\n< \\Omega^\\mathrm{eq}_\\mathrm{PBH}< 0.5$, slightly before matter-radiation\nequality, can produce a substantial amount of early dark energy, helping to\nalleviate the $H_0$ tension.",
        "An M-sequence generated by a primitive polynomial has many interesting and\ndesirable properties. A pseudo-random array is the two-dimensional\ngeneralization of an M-sequence. Similarly to primitive polynomials, there are\nirreducible and reducible polynomials whose all nonzero sequences have the same\nlength. In this paper, a two-dimensional generalization for such sequences is\ngiven. This generalization is for a pseudo-random array code which is a set of\n$r_1 \\times r_2$ arrays in which each $n_1 \\times n_2$ nonzero matrix is\ncontained exactly once as a window in one of the arrays. Moreover, these arrays\nhave the shift-and-add property, i.e., the bitwise addition of two arrays (or a\nnontrivial shift of such arrays) is another array (or a shift of another array)\nfrom the code. All the known arrays can be formed by folding sequences\ngenerated from an irreducible polynomial or a reducible polynomial whose\nfactors have the same degree and the same exponent. Two proof techniques are\nused to prove the parameters of the constructed arrays. The first one is based\non another method for constructing some of these arrays. The second one is a\ngeneralization of a known proof technique. This generalization enables to\npresent pseudo-random arrays with parameters not known before and also a\nvariety of pseudo-random array codes which cannot be generated by the first\nmethod. The two techniques also suggest two different hierarchies between\npseudo-random array codes. Finally, a method to verify whether a folding of\nsequences, generated by these polynomials, yields a pseudo-random array or a\npseudo-random array code, will be presented.",
        "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "The Mg I b$_2$ line at 5173 \\r{A} is primarily magnetically sensitive to\nheights between the mid photosphere and the low chromosphere, a region that has\nnot been sufficiently explored in the solar atmosphere but is crucial for\nunderstanding the magnetic coupling between the two layers. New generation\nsolar observatories are now performing polarimetric observations of this\nspectral line, enabling simultaneous measurements with multiple spectral lines.\nThis allows for detailed studies of the magnetism around the temperature\nminimum region at high spatial, temporal, and spectral resolutions. We present\na morphological classification of the Stokes $I$ and $V$ profiles of the Mg I\nb$_2$ line using the Euclidean distance method on high spatial resolution\nobservations from the Swedish 1-m Solar Telescope. The physical properties of\nthe resulting classes were analyzed using classical inference methods.\nAdditionally, we present a two-line full-Stokes inversion of the representative\nprofiles in which the Mg I b$_2$ line is treated fully under non-local\nthermodynamic equilibrium (NLTE) conditions, while the Fe I 6173 \\r{A} line is\nsimultaneously inverted under LTE assumptions to provide photospheric\nconstraints. This approach offers insights into the temperature stratification\nand other physical gradients involved in the formation of the different profile\nmorphologies. We found nine classes of Stokes $V$ profiles and 16 classes of\nStokes $I$ profiles in our Mg I b$_2$ dataset. These classes can be further\ngrouped into families based on shared characteristics, physical properties, and\nlocation. Our classification provides important information on the different\nenvironments and processes occurring in the solar atmosphere around the\ntemperature minimum region. It is also relevant for improving the performance\nof NLTE inversions.",
        "Describing the Coulomb interactions between electrons in atomic or molecular\nsystems is an important step to help us obtain accurate results for the\ndifferent observables in the system. One convenient approach is to separate the\ndynamic electronic correlation, i.e., Coulomb electron-electron repulsion, from\nthe motion of the electrons in the nuclei electric field. The wave function is\nwritten as the product of two terms, one accounting for the electron-electron\ninteractions, which is symmetric under identical particle exchange; the other\nis antisymmetric and represents the dynamics and exchange of electrons within\nthe nuclear electric field. In this work, we present a novel computational\nscheme based on this idea that leads to an expression of the energy as the sum\nof two terms. To illustrate the method, we look into few-body Coulombic\nsystems, H2, H3+ and Li(1s2,2s), and discuss the possible extension to larger\nsystems. A simple correlation factor, based on the Jastrow exponential term, is\nemployed to represent the dynamics of the electron pairs leading to simple\nanalytical forms and accurate results. We also present and illustrate a\ndifferent approach with the Li atom based on the partial separability applied\nto a portion of the atom.",
        "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
        "We study the gradient descent (GD) dynamics of a depth-2 linear neural\nnetwork with a single input and output. We show that GD converges at an\nexplicit linear rate to a global minimum of the training loss, even with a\nlarge stepsize -- about $2\/\\textrm{sharpness}$. It still converges for even\nlarger stepsizes, but may do so very slowly. We also characterize the solution\nto which GD converges, which has lower norm and sharpness than the gradient\nflow solution. Our analysis reveals a trade off between the speed of\nconvergence and the magnitude of implicit regularization. This sheds light on\nthe benefits of training at the ``Edge of Stability'', which induces additional\nregularization by delaying convergence and may have implications for training\nmore complex models.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Single-pixel imaging leverages a single-pixel detector and structured\nillumination patterns to reconstruct images, offering a cost-effective solution\nfor imaging across a wide range of wavelengths, such as x-ray and terahertz.\nHowever, the technique faces challenges in efficiency due to the need for\nnumerous patterns to achieve high-quality image reconstruction. In this study,\nwe explore the use of spin lattice models from statistical mechanics to design\nillumination patterns for single-pixel imaging. By employing models like Ising,\nPotts, XY, and Heisenberg, we generate structured patterns that are adaptable\nfor binary, grayscale, and color imaging. This work creates a direct connection\nbetween lattice models and imaging applications, providing a systematic\napproach to pattern generation that can enhance single-pixel imaging\nefficiency.",
        "Accreting supermassive black hole binaries are powerful multimessenger\nsources emitting both gravitational and electromagnetic (EM) radiation.\nUnderstanding the accretion dynamics of these systems and predicting their\ndistinctive EM signals is crucial to informing and guiding upcoming efforts\naimed at detecting gravitational waves produced by these binaries. To this end,\naccurate numerical modeling is required to describe both the spacetime and the\nmagnetized gas around the black holes. In this paper, we present two key\nadvances in this field of research.\n  First, we have developed a novel 3D general relativistic magnetohydrodynamics\n(GRMHD) framework that combines multiple numerical codes to simulate the\ninspiral and merger of supermassive black hole binaries starting from realistic\ninitial data and running all the way through merger. Throughout the evolution,\nwe adopt a simple but functional prescription to account for gas cooling\nthrough the emission of photons.\n  Next, we have applied our new computational method to follow the time\nevolution of a circular, equal-mass, non-spinning black hole binary for\n${\\sim\\!200}$ orbits starting from a separation of ${20\\,r_g}$ and reaching the\npost-merger evolutionary stage of the system. We have identified how and when\nthe minidisks dissolve as the binary compresses. We also show that even when\nthe binary ``decouples'' from its surrounding disk, its luminosity decreases by\nonly a factor of a few and abruptly increases by ${\\sim\\!50\\%}$ at the time of\nmerger, accompanied by an equally abrupt change in spectrum. Finally, the\nmagnetic flux brought to the spin-parameter ${\\sim\\!0.68}$ merger remnant is\nable to drive a relativistic, Poynting-flux-dominated jet.",
        "The Bethe-Salpeter equation (BSE) combined with the Green's function GW\nmethod has successfully transformed into a robust computational tool to\ndescribe light-matter interactions and excitation spectra for molecules,\nsolids, and materials from first principles. Thanks to its ability to\naccurately describe charge-transfer and Rydberg excitations, the GW-BSE already\nforms an established and cost-efficient alternative to time-dependent density\nfunctional theory. This raises the question whether the GW-BSE approach can\nbecome a more general framework for molecular properties beyond excitation\nenergies. In this mini-review, we recapitulate recent endeavors along this\npoint in terms of both theoretical and practical developments for quantum\nchemistry, physical chemistry, and related fields. In doing so, we provide\nguidelines for current applications to chemical challenges in collaboration\nwith experimentalists as well as to future developments to extended the GW-BSE\ntoolkit.",
        "Recent experiments of fluid transport in nano-channels have shown evidence of\na dramatic reduction of friction due to the coupling between\ncharge-fluctuations in polar fluids and electronic excitations in graphene\nsolids, a phenomenon dubbed \"negative quantum friction\". In this paper, we\npresent a semi-classical mesoscale Boltzmann-Wigner lattice kinetic model of\nquantum-nanoscale transport and perform a numerical study of the effects of the\nquantum interactions on the evolution of a one-dimensional nano-fluid subject\nto a periodic external potential. It is shown that the effects of quantum\nfluctuations become visible once the quantum length scale (Fermi wavelength) of\nthe quasiparticles becomes comparable to the wavelength of the external\npotential. Under such conditions, quantum fluctuations are mostly felt on the\nodd kinetic moments, while the even ones remain nearly unaffected because they\nare \"protected\" by thermal fluctuations. It is hoped that the present\nBoltzmann-Wigner lattice model and extensions thereof may offer a useful tool\nfor the computer simulation of quantum-nanofluidic transport phenomena.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"A Global Geometric Framework for Nonlinear Dimensionality Reduction",
    "start_abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure.",
    "start_categories":[
      "math.ST"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association"
      ],
      "abstract":[
        "We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure."
      ],
      "categories":[
        "cs.DC"
      ]
    },
    "list":{
      "title":[
        "The effect of accretion on scalar superradiant instability",
        "Scaling of the elastic proton-proton cross-section",
        "Feedback-enhanced squeezing or cooling of fluctuations in a parametric\n  resonator",
        "Ising superconductivity in noncentrosymmetric bulk NbSe2",
        "Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "Selection Function of Clusters in Dark Energy Survey Year 3 Data from\n  Cross-Matching with South Pole Telescope Detections",
        "The Complexity of Local Stoquastic Hamiltonians on 2D Lattices",
        "Neural cyberattacks applied to the vision under realistic visual stimuli",
        "Proof-Producing Translation of Functional Programs into a Time \\& Space\n  Reasonable Model",
        "Neural Radiance Fields for the Real World: A Survey",
        "Israel-Hamas war through Telegram, Reddit and Twitter",
        "Roles of the $N(1535)$ and $a_0(980)$ in the process $\\Lambda_c^+ \\to\n  \\pi^+\\eta n$",
        "Resurgence of the Tilted Cusp Anomalous Dimension",
        "New Frontiers in Fighting Misinformation",
        "Hybrid sub- and superradiant states in emitter arrays with quantized\n  motion",
        "Probing new hadronic forces with heavy exotic atoms",
        "Modeling Feature Maps for Quantum Machine Learning",
        "Efficient Deployment of Large Language Models on Resource-constrained\n  Devices",
        "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor\n  Segmentation in PET-CT Images",
        "Two-stage Incomplete Utterance Rewriting on Editing Operation",
        "Elucidating the high compliance mechanism by which the urinary bladder\n  fills under low pressures",
        "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
        "Growing black-hole hair in nonminimally coupled biscalar gravity",
        "Federated Learning in NTNs: Design, Architecture and Challenges",
        "Statistically validated projection of bipartite signed networks",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Operator Spreading in Random Unitary Circuits with Unitary-invariant\n  Gate Distributions",
        "Data efficiency and long-term prediction capabilities for neural\n  operator surrogate models of edge plasma simulations"
      ],
      "abstract":[
        "Superradiance can lead to the formation of a black hole (BH) condensate\nsystem. We thoroughly investigate the accretion effect on the evolution of this\nsystem, and the gravitational wave signals it emits in the presence of multiple\nsuperradiance modes. Assuming the multiplication of the BH mass and scalar mass\nas a small number, we obtain the analytical approximations of all important\nquantities, which can be directly applied to phenomenological studies. In\naddition, we confirm that accretion could significantly enhance the\ngravitational wave (GW) emission and reduce its duration, and show that the GW\nbeat signature is similarly modified.",
        "We discuss scaling properties of the elastic $pp$ cross-section both at the\nISR and the LHC. We observe that the ratio of bump to dip positions of the\ndifferential cross-section $d\\sigma_{\\rm el}\/dt$ is constant over a wide energy\nrange. We next study the consequences of this property, including geometric\nscaling at the ISR and new scaling laws at the LHC.",
        "Here we analyse ways to achieve deep subthreshold parametric squeezing of\nfluctuations beyond the $-6$~dB limit of single degree-of-freedom parametric\nresonators. One way of accomplishing this is via a lock-in amplifier feedback\nloop. Initially, we calculate the phase-dependent parametric amplification with\nfeedback of an added ac signal. In one approach, we use the averaging method to\nobtain the amplification gain, while in the second approach, we obtain the ac\nresponse of the parametric amplifier with feedback using the harmonic balance\nmethod. In this latter approach, the feedback is proportional to an integral\nterm that emulates the cosine quadrature output of a lock-in amplifier\nmultiplied by a sine at the same tone of the lock-in. We find that the gain\nobtained via these two methods are the same whenever the integration time span\nof the integral is a multiple of the tone period. When this is not the case, we\ncan obtain considerable deamplification. Finally, we analyse the response of\nthe parametric resonator with feedback, described by this integro-differential\nmodel, to an added white noise in the frequency domain. Using this model we\nwere able to calculate, in addition to squeezing, the noise spectral density in\nthis resonator with feedback. Very strong squeezing or cooling can be obtained.",
        "Ising superconductivity allows in-plane upper critical magnetic fields to\nvastly surpass Pauli limit by locking the antiparallel electron spins of Cooper\npairs in the out-of-plane direction. It was first explicitly demonstrated in\nfully two-dimensional monolayers of transition metal dichalcogenides with large\nspin-orbit coupling and broken inversion symmetry. Since then, several studies\nhave shown that it can be present in layered bulk materials, too. In our\nprevious study, we have clarified the underlying microscopic mechanism of Ising\nsuperconductivity in bulk, based on a reduced electronic coupling between\nsuperconducting layers due to intercalation by insulating layers and restricted\ninversion symmetry. But earlier studies suggest that in some transition metal\ndichalcogenide polytypes Pauli paramagnetic limit is violated even without\nintercalation. Here, using heat capacity measurements we unambiguously\ndemonstrate, that the pristine noncentrosymmetric bulk 4Ha-NbSe2 polytype\nsignificantly violates the Pauli limit. The band structure parameters obtained\nfrom ab initio calculations using the experimentally determined crystal\nstructure are used in the theoretical model which provides the microscopic\nmechanism of the Ising protection based solely on broken inversion symmetry.",
        "Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Galaxy clusters selected based on overdensities of galaxies in photometric\nsurveys provide the largest cluster samples. Yet modeling the selection\nfunction of such samples is complicated by non-cluster members projected along\nthe line of sight (projection effects) and the potential detection of\nunvirialized objects (contamination). We empirically constrain the magnitude of\nthese effects by cross-matching galaxy clusters selected in the Dark Energy\nsurvey data with the \\rdmpr$\\,$ algorithm with significant detections in three\nSouth Pole Telescope surveys (SZ, pol-ECS, pol-500d). For matched clusters, we\naugment the \\rdmpr$\\,$catalog by the SPT detection significance. For unmatched\nobjects we use the SPT detection threshold as an upper limit on the SZe\nsignature. Using a Bayesian population model applied to the collected\nmulti-wavelength data, we explore various physically motivated models to\ndescribe the relationship between observed richness and halo mass. Our analysis\nreveals the limitations of a simple lognormal scatter model in describing the\ndata. We rule out significant contamination by unvirialized objects at the\nhigh-richness end of the sample. While dedicated simulations offer a\nwell-fitting calibration of projection effects, our findings suggest the\npresence of redshift-dependent trends that these simulations may not have\ncaptured. Our findings highlight that modeling the selection function of\noptically detected clusters remains a complicated challenge, requiring a\ncombination of simulation and data-driven approaches.",
        "We show the 2-Local Stoquastic Hamiltonian problem on a 2D square lattice is\nStoqMA-complete. We achieve this by extending the spatially sparse circuit\nconstruction of Oliveira and Terhal, as well as the perturbative gadgets of\nBravyi, DiVincenzo, Oliveira, and Terhal. Our main contributions demonstrate\nStoqMA circuits can be made spatially sparse and that geometrical,\nstoquastic-preserving, perturbative gadgets can be constructed.",
        "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine\nand designed to interact with the brain to record or stimulate neurons. Despite\ntheir benefits, the literature has demonstrated that invasive BCIs focused on\nneurostimulation present vulnerabilities allowing attackers to gain control. In\nthis context, neural cyberattacks emerged as threats able to disrupt\nspontaneous neural activity by performing neural overstimulation or inhibition.\nPrevious work validated these attacks in small-scale simulations with a reduced\nnumber of neurons, lacking real-world complexity. Thus, this work tackles this\nlimitation by analyzing the impact of two existing neural attacks, Neuronal\nFlooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of\nthe primary visual cortex of mice consisting of approximately 230,000 neurons,\ntested on three realistic visual stimuli: flash effect, movie, and drifting\ngratings. Each attack was evaluated over three relevant events per stimulus,\nalso testing the impact of attacking 25% and 50% of the neurons. The results,\nbased on the number of spikes and shift percentages metrics, showed that the\nattacks caused the greatest impact on the movie, while dark and fixed events\nare the most robust. Although both attacks can significantly affect neural\nactivity, JAM was generally more damaging, producing longer temporal delays,\nand had a larger prevalence. Finally, JAM did not require to alter many neurons\nto significantly affect neural activity, while the impact in FLO increased with\nthe number of neurons attacked.",
        "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle\/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.",
        "Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.",
        "The Israeli-Palestinian conflict started on 7 October 2023, have resulted\nthus far to over 48,000 people killed including more than 17,000 children with\na majority from Gaza, more than 30,000 people injured, over 10,000 missing, and\nover 1 million people displaced, fleeing conflict zones. The infrastructure\ndamage includes the 87\\% of housing units, 80\\% of public buildings and 60\\% of\ncropland 17 out of 36 hospitals, 68\\% of road networks and 87\\% of school\nbuildings damaged. This conflict has as well launched an online discussion\nacross various social media platforms. Telegram was no exception due to its\nencrypted communication and highly involved audience. The current study will\ncover an analysis of the related discussion in relation to different\nparticipants of the conflict and sentiment represented in those discussion. To\nthis end, we prepared a dataset of 125K messages shared on channels in Telegram\nspanning from 23 October 2025 until today. Additionally, we apply the same\nanalysis in two publicly available datasets from Twitter containing 2001 tweets\nand from Reddit containing 2M opinions. We apply a volume analysis across the\nthree datasets, entity extraction and then proceed to BERT topic analysis in\norder to extract common themes or topics. Next, we apply sentiment analysis to\nanalyze the emotional tone of the discussions. Our findings hint at polarized\nnarratives as the hallmark of how political factions and outsiders mold public\nopinion. We also analyze the sentiment-topic prevalence relationship, detailing\nthe trends that may show manipulation and attempts of propaganda by the\ninvolved parties. This will give a better understanding of the online discourse\non the Israel-Palestine conflict and contribute to the knowledge on the\ndynamics of social media communication during geopolitical crises.",
        "We have investigated the process $\\Lambda_c^+ \\to \\pi^+\\eta n$ by taking into\naccount the contributions from the nucleon resonance $N(1535)$ and the scalar\nmeson $a_0(980)$, which could be dynamically generated by the interaction of\nthe $S$-wave pseudosalar meson-octet baryon and the $S$-wave pseudosalar\nmeson-pseudosalar meson, respectively. Our results show that, in $\\eta n$\ninvariant mass distribution, there is a significant near-threshold enhancement\nstructure, which could be associated with $N(1535)$. On the other hand, one can\nfind a clear cusp structure of $a_0(980)$ in $\\pi^+\\eta$ invariant mass\ndistribution. We further estimate the ratio $R$ = $\\mathcal{B}(\\Lambda_c^+ \\to\na_0(980)^+ n)\/\\mathcal{B}(\\Lambda_c^+ \\to \\pi^+\\eta n)\\approx 0.313$. Our\nresults can be tested by BESIII, Belle~II, and the proposed Super Tau-Charm\nFacility experiments in the future.",
        "We use resurgent extrapolation and continuation methods to extract detailed\nanalytic information about the tilted cusp anomalous dimension solely from its\nweak coupling and strong coupling expansions. This enables accurate and smooth\ninterpolation between the weak and strong coupling limits, and identifies the\nrelevant singularities governing the finite radius of convergence of the weak\ncoupling expansion and the asymptotic nature of the strong coupling expansion.\nThe input data is purely perturbative, generated from the BES equations, and\nthese resurgent methods extract accurate non-perturbative information which\nmatches the underlying physical structure.",
        "Despite extensive research and development of tools and technologies for\nmisinformation tracking and detection, we often find ourselves largely on the\nlosing side of the battle against misinformation. In an era where\nmisinformation poses a substantial threat to public discourse, trust in\ninformation sources, and societal and political stability, it is imperative\nthat we regularly revisit and reorient our work strategies. While we have made\nsignificant strides in understanding how and why misinformation spreads, we\nmust now broaden our focus and explore how technology can help realise new\napproaches to address this complex challenge more efficiently.",
        "Ensembles of dipolar emitters which couple collectively to the radiation\nfield display sub- and superradiance. These terms refer to a reduction or an\nenhancement of photon emission rates due to the interference of emission\nchannels. Arrays of trapped neutral atoms constitute a promising platform for\nharnessing this phenomenon in technological applications, e.g. for excitation\nstorage, single-photon switches and mirrors. However, vibrational motion of the\natoms within their traps leads to position fluctuations that entangle the\nmotion and the internal atomic degrees of freedom, which is expected to affect\nthe collective photon emission. We develop here a theory for collective\natom-light coupling in the presence of this quantized motion within the\nLamb-Dicke limit. We show the existence of sub- and superradiant states, which\nare hybrids of electronic and vibrational excitations and explore their\nproperties for analytically and numerically efficiently solvable cases.",
        "We explore the potential of precision spectroscopy of heavy exotic atoms\nwhere electrons are substituted by negative hadrons to detect new force\ncarriers with hadronic couplings. The selected transitions are unaffected by\nnuclear contact terms, thus enabling highly accurate calculations using\nbound-state QED, provided that the nuclear polarization is under control.\nAlternatively, we demonstrate that the dipole polarizability, a fundamental\nproperty of nuclei, can be extracted from the spectroscopy of exotic atoms in a\nnovel way by combining two transitions while maintaining high sensitivity to\nnew physics. Based on existing data, we extracted world-leading bounds on\nmediator masses ranging from $0.1\\,$MeV to $10\\,$MeV for two benchmark models\nand show that forthcoming experiments could enhance the sensitivity to new\nphysics by two orders of magnitude.",
        "Quantum Machine Learning (QML) offers significant potential for complex tasks\nlike genome sequence classification, but quantum noise on Noisy\nIntermediate-Scale Quantum (NISQ) devices poses practical challenges. This\nstudy systematically evaluates how various quantum noise models including\ndephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and\nphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature\nmapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results\nindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are\nmore sensitive, particularly to depolarizing and amplitude-damping noise. The\nPauliFeatureMap is especially vulnerable, highlighting difficulties in\nmaintaining accurate classification under noisy conditions. These findings\nunderscore the critical importance of feature map selection and noise\nmitigation strategies in optimizing QML for genomic classification, with\npromising implications for personalized medicine.",
        "Deploying Large Language Models (LLMs) on resource-constrained (or weak)\ndevices presents significant challenges due to limited resources and\nheterogeneous data distribution. To address the data concern, it is necessary\nto fine-tune LLMs using on-device private data for various downstream tasks.\nWhile Federated Learning (FL) offers a promising privacy-preserving solution,\nexisting fine-tuning methods retain the original LLM size, leaving issues of\nhigh inference latency and excessive memory demands unresolved. Hence, we\ndesign FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning\n(PEFT) with structured pruning for efficient deployment of LLMs on\nresource-constrained devices. Specifically, FedSpine introduces an iterative\nprocess to prune and tune the parameters of LLMs. To mitigate the impact of\ndevice heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed\nto adaptively determine different pruning ratios and LoRA ranks for\nheterogeneous devices without any prior knowledge of their computing and\ncommunication capabilities. As a result, FedSpine maintains higher inference\naccuracy while improving fine-tuning efficiency. Experimental results conducted\non a physical platform with 80 devices demonstrate that FedSpine can speed up\nfine-tuning by 1.4$\\times$-6.9$\\times$ and improve final accuracy by 0.4%-4.5%\nunder the same sparsity level compared to other baselines.",
        "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps:\/\/github.com\/mj129\/CIPA.",
        "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.",
        "The high compliance of the urinary bladder during filling is essential for\nits proper function, enabling it to accommodate significant volumetric\nincreases with minimal rise in transmural pressure. This study aimed to\nelucidate the physical mechanisms underlying this phenomenon by analyzing the\nex vivo filling process in rat from a fully voided state to complete\ndistension, without preconditioning, using three complementary imaging\nmodalities. High-resolution micro-CT at 10.8 {\\mu}m resolution was used to\ngenerate detailed 3D reconstructions of the bladder lumen, revealing a 62 fold\nincrease in bladder volume during filling. Pressure-volume studies of whole\nbladder delineated three mechanical filling regimes: an initial high-compliance\nphase, a transitional phase, and a final high-pressure phase. While prior\nstudies conjectured small mucosal rugae (450 {\\mu}m) are responsible for the\nhigh compliance phase, multiphoton microscopy (MPM) of the dome of the voided\nbladder revealed large folds an order of magnitude larger than these rugae.\nBladder imaging during the inflation process demonstrated flattening of these\nlarge scale folds is responsible for volume increases in the initial high\ncompliance phase. The 3D reconstructions of the bladder lumen in the filled and\nvoided state revealed a high voiding efficiency of 97.13%. The MPM imaging\nresults suggest the large scale folds in the dome enable this high voiding\nfraction by driving urine toward the bladder outlet. These insights are vital\nfor computational models of bladder biomechanics and understanding changes to\nbladder function due to pathological conditions such as bladder outlet\nobstruction and age-related dysfunction.",
        "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.",
        "Black holes offer a unique laboratory for fundamental physics and are crucial\nfor probing theories beyond Einstein's theory of General Relativity. In this\npaper, we consider 4D effective field theories with scalar fields. We focus on\naxi-dilaton gravity, a quadratic gravity theory with two kinetically coupled\nscalar fields, an axion and a dilaton. To evolve these fields around black\nholes, we introduce Canuda-AxiDil, the first open-source, parameterized\nnumerical relativity code for quadratic and bi-scalar gravity. Using this code,\nwe perform single black hole simulations to show the dynamical formation of\naxion and dilaton hairs. Through these simulations, we measure the impact of\nblack-hole spin and curvature coupling strength on the axion and dilaton, and\nshow that a kinetic coupling between the fields increases the observed\ndeviations from General Relativity. Furthermore, we simulate the axion and\ndilaton fields around a binary black hole coalescence demonstrating the growth\nof axion hair during the inspiral and the production of radiative modes for\nboth fields.",
        "Non-terrestrial networks (NTNs) are emerging as a core component of future 6G\ncommunication systems, providing global connectivity and supporting\ndata-intensive applications. In this paper, we propose a distributed\nhierarchical federated learning (HFL) framework within the NTN architecture,\nleveraging a high altitude platform station (HAPS) constellation as\nintermediate distributed FL servers. Our framework integrates both low-Earth\norbit (LEO) satellites and ground clients in the FL training process while\nutilizing geostationary orbit (GEO) and medium-Earth orbit (MEO) satellites as\nrelays to exchange FL global models across other HAPS constellations worldwide,\nenabling seamless, global-scale learning. The proposed framework offers several\nkey benefits: (i) enhanced privacy through the decentralization of the FL\nmechanism by leveraging the HAPS constellation, (ii) improved model accuracy\nand reduced training loss while balancing latency, (iii) increased scalability\nof FL systems through ubiquitous connectivity by utilizing MEO and GEO\nsatellites, and (iv) the ability to use FL data, such as resource utilization\nmetrics, to further optimize the NTN architecture from a network management\nperspective. A numerical study demonstrates the proposed framework's\neffectiveness, with improved model accuracy, reduced training loss, and\nefficient latency management. The article also includes a brief review of FL in\nNTNs and highlights key challenges and future research directions.",
        "Bipartite networks provide a major insight into the organisation of many\nreal-world systems, describing the mechanisms that drive interactions between\ndistinct groups of nodes. Of particular interest are two-mode networks whose\nedges admit a sign: examples are provided by human interactions with entities\nsuch as products, where agents either cast either a positive or negative vote\nor abstain from voting at all. One of the most relevant issues encountered when\nmodelling a bipartite network is devising a way to obtain a monopartite\nprojection onto the layer of interest that preserves the information encoded\ninto the original structure as much as possible. In the present contribution,\nwe propose an unsupervised algorithm to obtain statistically validated\nprojections of bipartite signed networks, according to which any two nodes\nsharing a statistically significant number of concordant (discordant)\nrelationships are connected by a positive (negative) edge. More precisely, we\npropose two variants of it, according to the way ambivalent patterns and\nmissing ties are treated. Since assessing the statistical significance of any\ntwo nodes similarity requires a proper benchmark, here we consider four\ndifferent Exponential Random Graphs, defined by global as well as local\nconstraints, either leaving the topology free or keeping the topology fixed.\nOur algorithm outputs a matrix of link-specific $p-$values, from which a\nvalidated projection can be obtained upon running a multiple-hypothesis testing\nprocedure. After testing our method on synthetic configurations output by a\nfully controllable generative model, we apply it to several real-world\nconfigurations: in all cases, non-trivial mesoscopic structures, induced by\nrelationships that cannot be traced back to the constraints defining the\nemployed benchmarks, are detected.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "Random unitary circuits have become a model system to investigate information\nscrambling in quantum systems. In the literature, mostly random circuits with\nHaar-distributed gate operations have been considered. In this work, we\ninvestigate operator spreading in random unitary circuits in which the\nelementary gate operations are drawn from general unitary-invariant ensembles,\nwhich include the well-studied Haar-distributed random unitary circuits as a\nspecial case. Similar to the Haar-distributed case, the long-time behavior of\noperator spreading with the more general unitary-invariant gate distribution is\ngoverned by drift-diffusion equations characterized by the butterfly velocity\n$v_{\\rm B}$ and a diffusion constant $\\mathcal{D}$. Differences with the\nHaar-random case are (i) that it takes a finite time $\\tau_{\\rm b}$ until\nensemble-averaged Pauli-string weights take a ``binary'' form, in which they\ndepend only on whether Pauli operators inside the support of the Pauli strong\nare equal to the identity matrix, and (ii) that the operator spreading is\ncharacterized by a finite ``domain-wall width'' $n_{\\rm DW}$ separating regions\nwith a random-matrix-like Pauli-string distribution. To illustrate these\nfindings, we perform explicit calculations for random unitary circuits\ndistributed according to the Poisson kernel, which interpolates between the\ntrivial and Haar-distributed circuits.",
        "Modelling of plasma dynamics is fundamental to ensure appropriate diverter\nand core performance, and is desirable for both interpreting the current\ngeneration of experiments and informing the next generation devices like ITER\n\\cite{Loarte2007Chapter4P,Eich2013ScalingOT}. Yet the computational expense of\nmany plasma simulations makes them unsuitable for real-time applications or\niterative design workflows. Neural operator surrogate models of JOREK\n\\cite{Hoelzl_2021} and STORM \\cite{Walkden2016-ys} are evaluated, investigating\ntheir capability to replicate plasma dynamics accurately whilst reducing\ncomputational cost. It is found that the accuracy of the surrogate models will\ndegrade for long term predictions, and that physics considerations are\nimportant in assessing the performance of the surrogates. Surrogates trained on\none dataset can be effectively fine tuned with only a few simulations from a\ntarget domain. This is particularly effective where the source domain is a low\nfidelity physics model and the target domain is a high fidelity model, with an\norder of magnitude improvement in performance for a small dataset and a short\nrollout."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "start_abstract":"When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Symbolic Transfer Entropy"
      ],
      "abstract":[
        "We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "A domain decomposition strategy for natural imposition of mixed boundary\n  conditions in port-Hamiltonian systems",
        "Non-reciprocal interactions drive emergent chiral crystallites",
        "Reduced Basis Model for Compressible Flow",
        "Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy",
        "Application of resolved low-J multi-CO line modeling with RADEX to\n  constrain the molecular gas properties in the starburst M82",
        "A regional implementation of a mixed finite-element, semi-implicit\n  dynamical core",
        "1\/f and Random Telegraph Noise of Single-Layer Graphene Devices with\n  Interdigitated Electrodes",
        "Multivariate Distribution-Free Nonparametric Testing: Generalizing\n  Wilcoxon's Tests via Optimal Transport",
        "Spectrum management and the EVN",
        "Multipole generalization of the Witten effect in Mie-resonant photonics",
        "Congruence properties of prime sums and Bernoulli polynomials",
        "Determination of unscaled blood input for human dynamic FDG brain PET",
        "Harnessing Hybrid Frequency-Entangled Qudits through Quantum\n  Interference",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Quantum Maslov classes",
        "Triangulations of the `magic manifold' and families of census knots",
        "Positive Polytopes with Few Facets in the Grassmannian",
        "Speeding up Lindblad dynamics via time-rescaling engineering",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Chemical abundance ratios for the bulge of M31",
        "Simulation of current-driven magnetisation switching in nanopillars with\n  Perpendicular Shape Anisotropy",
        "Non-perturbative corrections in the semi-classical limit of\n  double-scaled SYK",
        "Proof-theoretic dilator and intermediate pointclasses",
        "On the Minimax Regret of Sequential Probability Assignment via\n  Square-Root Entropy",
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "Preparing for the 2061 return of Halley's comet -- A rendezvous mission\n  with an innovative imaging system",
        "Certified Inductive Synthesis for Online Mixed-Integer Optimization",
        "A general form of Newton-Maclaurin type inequalities",
        "Relating elliptic curve point-counting and solutions of quadratic forms\n  with congruence conditions"
      ],
      "abstract":[
        "In this contribution, a finite element scheme to impose mixed boundary\nconditions without introducing Lagrange multipliers is presented for wave\npropagation phenomena described as port-Hamiltonian systems. The strategy\nrelies on finite element exterior calculus and a domain decomposition to\ninterconnect two systems with different causalities. The spatial domain is\nsplit into two parts by introducing an arbitrary interface. Each subdomain is\ndiscretized with a mixed finite element formulation that introduces a uniform\nboundary condition in a natural way as the input. In each subdomain the spaces\nare selected from a finite element subcomplex to obtain a stable\ndiscretization. The two systems are then interconnected together by making use\nof a feedback interconnection. This is achieved by discretizing the boundary\ninputs using appropriate spaces that couple the two formulations. The final\nsystems includes all boundary conditions explicitly and does not contain any\nLagrange multiplier. Each subdomain is integrated using an implicit midpoint\nscheme in an uncoupled way from the other by means of a leapfrog scheme. The\nproposed strategy is tested on three different examples: the Euler-Bernoulli\nbeam, the wave equation and the Maxwell equations. Numerical tests assess the\nconservation properties of the scheme and the effectiveness of the methodology.",
        "We study a new type of 2D active material that exhibits macroscopic phases\nwith two emergent broken symmetries: self-propelled achiral particles that form\ndense hexatic clusters, which spontaneously rotate. We experimentally realise\nactive colloids that self-organise into both polar and hexatic crystallites,\nexhibiting exotic emergent phenomena. This is accompanied by a field theory of\ncoupled order parameters formulated on symmetry principles, including\nnon-reciprocity, to capture the non-equilibrium dynamics. We find that the\npresence of two interacting broken symmetry fields leads to the emergence of\nnovel chiral phases built from (2D) achiral active colloids (here Quincke\nrollers). These phases are characterised by the presence of both clockwise and\ncounterclockwise rotating clusters. We thus show that spontaneous rotation can\nemerge in non-equilibrium systems, even when the building blocks are achiral,\ndue to non-reciprocally coupled broken symmetries. This interplay leads to\nself-organized stirring through counter-rotating vortices in confined colloidal\nsystems, with cluster size controlled by external electric fields.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients\/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.",
        "The distribution and physical conditions of molecular gas are closely linked\nto star formation and the subsequent evolution of galaxies. Emission from\ncarbon monoxide (CO) and its isotopologues traces the bulk of molecular gas and\nprovides constraints on the physical conditions through their line ratios.\nHowever, comprehensive understanding on how the particular choice of line\nmodeling approach impacts derived molecular properties remain incomplete. Here,\nwe study the nearby starburst galaxy M82, known for its intense star formation\nand molecular emission, using the large set of available multi-CO line\nobservations. We present high-resolution (${\\sim}85$ pc) emission of seven CO\nisotopologue lines, including $^{12}$CO, $^{13}$CO, and C$^{18}$O from the $J =\n1-0$, $2-1$ and $3-2$ transitions. Using \\texttt{RADEX} for radiative transfer\nmodeling, we analyze M82\\textsc{\\char39}s molecular properties with (i) a\none-zone model and (ii) a variable density model, comparing observed and\nsimulated emissions via a minimum $\\chi^2$ analysis. We find that inferred gas\nconditions -- kinetic temperature and density -- are consistent across models,\nwith minimal statistical differences. However, due to their low critical\ndensities (${<}10^{4}$ cm$^{-3}$), low-$J$ CO isotopologue lines do not\neffectively probe higher density gas prevalent in starburst environments like\nthat of M82. Our results further imply that this limitation extends to\nhigh-redshift ($z{\\gtrapprox}1$) galaxies with similar conditions, where\nlow-$J$ CO lines are inadequate for density constraints. Future studies of\nextreme star-forming regions like M82 will require higher-$J$ CO lines or\nalternative molecular tracers with higher critical densities.",
        "This paper explores how to adapt a new dynamical core to enable its use in\none-way nested regional weather and climate models, where lateral boundary\nconditions (LBCs) are provided by a lower-resolution driving model. The\ndynamical core has recently been developed by the Met Office and uses an\niterated-semi-implicit time discretisation and mixed finite-element spatial\ndiscretisation.\n  The essential part of the adaptation is the addition of the LBCs to the\nright-hand-side of the linear system which solves for pressure and momentum\nsimultaneously. The impacts on the associated Helmholtz preconditioner and\nmultigrid techniques are also described.\n  The regional version of the dynamical core is validated through big-brother\nexperiments based on idealised dynamical core tests. These experiments\ndemonstrate that the subdomain results are consistent with those from the full\ndomain, confirming the correct application of LBCs. Inconsistencies arise in\ncases where the LBCs are not perfect, but it is shown that the application of\nblending can be used to overcome these problems.",
        "Single-layer Graphene (SLG) is a promising material for sensing applications.\nHigh performance graphene sensors can be achieved when Interdigitated\nElectrodes (IDE) are used. In this research work, we fabricated SLG\nmicro-ribbon (GMR) devices with IDE having different geometric parameters. 1\/f\nnoise behavior was observed in all of the examined devices, and in some cases\nrandom telegraph noise (RTN) signals suggesting that carrier\ntrapping\/de-trapping is taking place. Our experimental results indicate that\nthe geometrical characteristics can have a crucial impact on device\nperformance, due to the direct area dependence of the noise level.",
        "This paper reviews recent advancements in the application of optimal\ntransport (OT) to multivariate distribution-free nonparametric testing.\nInspired by classical rank-based methods, such as Wilcoxon's rank-sum and\nsigned-rank tests, we explore how OT-based ranks and signs generalize these\nconcepts to multivariate settings, while preserving key properties, including\ndistribution-freeness, robustness, and efficiency. Using the framework of\nasymptotic relative efficiency (ARE), we compare the power of the proposed\n(generalized Wilcoxon) tests against the Hotelling's $T^2$ test. The ARE lower\nbounds reveal the Hodges-Lehmann and Chernoff-Savage phenomena in the context\nof multivariate location testing, underscoring the high power and efficiency of\nthe proposed methods. We also demonstrate how OT-based ranks and signs can be\nseamlessly integrated with more modern techniques, such as kernel methods, to\ndevelop universally consistent, distribution-free tests. Additionally, we\npresent novel results on the construction of consistent and distribution-free\nkernel-based tests for multivariate symmetry, leveraging OT-based ranks and\nsigns.",
        "In recent years, the utilisation of the radio spectrum has dramatically\nincreased. Digital telecommunication applications, be it terrestrial cell-phone\nnetworks or new-space low-earth orbit satellite constellations, have not only\nacquired unprecedented amounts of spectrum but also use their frequencies\neverywhere on Earth. The consequences for radio astronomy and other scientific\nradio services are severe. A single cell-phone tower within hundreds of\nkilometers around a radio telescope can blind us and there is no place on Earth\nto escape the ubiquitous transmissions of satellite megaconstellations.\n  Since 1988, the Committee on Radio Astronomy Frequencies (CRAF) is advocating\nfor astronomers' rights to use the spectrum. CRAF does this by participation in\nthe national and international regulatory frameworks. Hundreds if not thousands\nof documents need to be processed every year. CRAF not only contributes to\nregulatory texts, but even more importantly, performs spectrum compatibility\ncalculations. In this contribution, CRAF's latest activities are summarized\nwith a focus on matters relevant to EVN operations.",
        "We present a generalization of the Witten effect on the case of oscillating\nmultipole sources exciting nonreciprocal sphere with effective axion response.\nWe find that the fields outside of the sphere are presented as a superposition\nof electric and magnetic multipoles. In addition to appearance of\ncross-polarized component in the radiation, Mie resonances of the system\nhybridize with each other, exhibiting characteristic double peaks in Mie\nspectra observed especially clearly for higher-order multipole resonances. This\ncharacteristic feature may provide a sensitive probe of axion-type\nnonreciprocal responses in Mie-resonant photonics.",
        "In this article, we derive a congruence property of particular sum rules\ninvolving prime numbers. The resulting expression involves Bernoulli numbers\nand polynomials, for which we obtain, as a consequence, a general congruence\nrelation as well.",
        "Objectives: Many existing techniques for the non-invasive quantification of\nthe blood input function in dynamic FDG-PET imaging require strong historical\ninformation or user input. The technique proposed in this work utilizes the\nassumption that a dynamic PET scan can be modeled by the Patlak plot to\ndetermine an unscaled blood input function. Materials and Methods: The time\nactivity curve (TAC) for each voxel in a dynamic image can be considered as an\nn-dimensional vector. In this context, a TAC follows the Patlak plot if and\nonly if the TAC is a linear combination of the blood input function and the\nintegral of the blood input function. Given a set of TACs which follow the\nPatlak plot, we can thus use PCA to determine a basis which spans the same\nvector space as the blood input function and the integral of the blood input\nfunction. We then seek to find two TACs in this vector space which best satisfy\nthat the estimated anti-derivative of one of the TACs is close to the other\nTAC; such TACs are candidates for the blood input function and the integral of\nthe blood input function. We were able to construct a low (2) dimensional\noptimization problem to find such TACs. Results: We applied our results to\nobtain predicted blood input functions and Ki maps for twelve normal subjects.\nScaling the predicted blood input function to best match the ground truth, we\nachieved an average SSE of $0.042 \\pm 0.032$ and an average DTW distance of\n$0.141 \\pm 0.053$. Matching the means of the predicted and ground truth Ki\nmaps, we achieved an average MAPE of $2.539 \\pm 0.928$ and an average SSIM of\n$0.991 \\pm 0.005$. Conclusion: While not often viewed as such, the assumption\nthat some dynamic data follows a kinetic model gives strong prior information.\nIn the case of the Patlak plot, we can use this assumption to estimate an\nunscaled blood input function and unscaled Ki map.",
        "High-dimensional (HD) quantum entanglement expands the Hilbert space,\noffering a robust framework for quantum information processing with enhanced\ncapacity and error resilience. In this work, we present a novel HD\nfrequency-domain entangled state, the hybrid frequency-entangled qudit (HFEQ),\ngenerated via Hong-Ou-Mandel (HOM) interference, exhibiting both\ndiscrete-variable (DV) and continuous-variable (CV) characteristics. By tuning\nHOM interference, we generate and control HFEQs with dimensions $D=5,7,9,11$,\nconfirming their DV nature. Franson interferometry confirms the global\nfrequency correlations with visibility exceeding 98% and verifies the CV\nentanglement within individual frequency modes with visibility greater than\n95%. Our findings provide deeper insight into the physical nature of\nfrequency-entangled qudits generated by quantum interference and introduce a\nnovel resource for HD time-frequency quantum information processing.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "We give a construction of ``quantum Maslov characteristic classes'',\ngeneralizing to higher dimensional cycles the Hu-Lalonde-Seidel morphism. We\nalso state a conjecture extending this to an $A _{\\infty}$ functor from the\nexact path category of the space of monotone Lagrangian branes to the Fukaya\ncategory. Quantum Maslov classes are used here for the study of Hofer geometry\nof Lagrangian equators in $S ^{2}$, giving a rigidity phenomenon for the Hofer\nmetric 2-systole, which stands in contrast to the flexibility phenomenon of the\nclosely related Hofer metric girth studied by Rauch ~\\cite{cite_Itamar}, in the\nsame context of Lagrangian equators of $S ^{2}$. More applications appear in\n~\\cite{cite_SavelyevGlobalFukayacategoryII}.",
        "We describe five ideal triangulations of the 3-cusped hyperbolic `magic\nmanifold' that are each compatible with well-established techniques for\ntriangulating Dehn fillings. Using these techniques, we construct\nlow-complexity triangulations for all partial fillings of the magic manifold,\nand in particular, recover minimal triangulations for 229 of the hyperbolic\ncensus knots. Along the way, these census knots are sorted into 42 families\nrelated by twisting that can be extended indefinitely, with each member of each\ninfinite family inheriting an upper bound on its triangulation complexity.\nThese triangulations are conjectured to be minimal for all 42 families.",
        "In this article we study adjoint hypersurfaces of geometric objects obtained\nby intersecting simple polytopes with few facets in $\\mathbb{P}^5$ with the\nGrassmannian $\\mathrm{Gr}(2,4)$. These generalize the positive Grassmannian,\nwhich is the intersection of $\\mathrm{Gr}(2,4)$ with the simplex. We show that\nif the resulting object has five facets, it is a positive geometry and the\nadjoint hypersurface is unique. For the case of six facets we show that the\nadjoint hypersurface is not necessarily unique and give an upper bound on the\ndimension of the family of adjoints. We illustrate our results with a range of\nexamples. In particular, we show that even if the adjoint is not unique, a\npositive hexahedron can still be a positive geometry.",
        "We introduce a universal method for accelerating Lindblad dynamics that\npreserves the original trajectory. The technique provides exact fast processes\nanalytically, which are Markovian with time-independent Lindblad operators, by\ntime-rescaling a reference dynamics. In particular, the engineered control\nprotocols are based only on local interactions, and no additional control\nfields are required compared to the reference protocol. We demonstrate the\nscheme with two examples: a driven two-level system in an amplitude damping\nchannel and the dissipative transverse field Ising model. Our approach can help\nadvance techniques for quantum control and computation towards more complex\nnoisy systems.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "We present abundance ratio estimates of individual elements, namely C, N, Na,\nand the so-called alpha elements, Mg, O, Si, Ca, and Ti, for the bulge of M31.\nThe analysis is based on long-slit, high-quality spectroscopy of the bulge,\ntaken with the OSIRIS spectrograph at the Gran Telescopio CANARIAS (GTC).\nAbundance ratios, [X\/Fe]s, are inferred by comparing radially binned spectra of\nM31 with different state-of-the-art stellar population models, averaging out\nresults from various methods, namely full-spectral, full-index, and\nline-strength fitting, respectively. For the bulk of the bulge, we find that O,\nN, and Na are significantly enhanced compared to Fe, with abundances of about\n0.3dex, followed by C, Mg, and Si, with [X\/Fe] about 0.2dex, and lastly, Ti and\nCa, mostly tracking Fe ([X\/Fe]<0.1dex), within the error bars. Performing the\nsame analysis on SDSS stacked spectra of early-type galaxies with different\nvelocity dispersion, we find that the abundance pattern of the M31 bulge is\nvery similar to that of most massive galaxies, supporting a scenario where most\nof the bulge formed in a fast and intense episode of star-formation.",
        "The Perpendicular Shape Anisotropy Spin Transfer Torque Magnetic Random\nAccess Memory (PSA-STT-MRAM) is a recent concept proposed to maintain the\nthermal stability of standard MRAM at small diameters, considering thick\nvertical pillars as the free layer. In order to explore the specific physics of\nPSA-STT-MRAMs expected in relation with their three-dimensional nature, we have\nperformed simulations combining a micromagnetic model coupled self-consistently\nwith spin-dependent transport equations. The 3D shape induces flower states at\nthe upper and lower surfaces. Besides, the field-like component of STT is found\nto be larger than in standard MRAMs, suggesting that it needs to be considered.\nThe combination of both effects leads to the excitation of high-order 3D\nferromagnetic resonance modes, playing a key role in magnetisation reversal.\nThese results highlight features of 3D nanomagnetic systems, largely\ndisregarded so far, which need to be considered to optimise PSA-STT-MRAM to be\na competitive solution for technological implementation.",
        "We study the disk partition function of double-scaled SYK model (DSSYK) in\nthe small $\\lambda$ limit, where $\\lambda=-\\log q$ is the coupling of DSSYK. We\nfind that the partition function receives non-perturbative corrections in\n$\\lambda$, which can be resummed by the cubic power of the Dedekind eta\nfunction in a certain low temperature limit. We also discuss a possible bulk\ninterpretation of our findings.",
        "There are two major generalizations of the standard ordinal analysis: One is\nGirard's $\\Pi^1_2$-proof theory in which dilators are assigned to theories\ninstead of ordinals. The other is Pohlers' generalized ordinal analysis with\nSpector classes, where ordinals greater than $\\omega_1^{\\mathsf{CK}}$ are\nassigned to theories. In this paper, we show that these two are systematically\nentangled, and $\\Sigma^1_2$-proof theoretic analysis has a critical role in\nconnecting these two.",
        "We study the problem of sequential probability assignment under logarithmic\nloss, both with and without side information. Our objective is to analyze the\nminimax regret -- a notion extensively studied in the literature -- in terms of\ngeometric quantities, such as covering numbers and scale-sensitive dimensions.\nWe show that the minimax regret for the case of no side information\n(equivalently, the Shtarkov sum) can be upper bounded in terms of sequential\nsquare-root entropy, a notion closely related to Hellinger distance. For the\nproblem of sequential probability assignment with side information, we develop\nboth upper and lower bounds based on the aforementioned entropy. The lower\nbound matches the upper bound, up to log factors, for classes in the Donsker\nregime (according to our definition of entropy).",
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "The return of Comet 1P\/Halley will promote a wide interest for ground and\nspace observations of a celestial body of outstanding scientific and cultural\ninterest. In addition to remote observations, space will open the possibility\nof in situ science similarly to the passage of 1986. In this paper, we first\ndiscuss the scientific motivations for a rendezvous mission, capable to\novercome the limitations of the flyby missions that took place at that time. In\nthe second part, we describe an example of a rendezvous trajectory that can be\ncarried out with existing power and propulsion technologies. The transfer is\nmade possible by the gravitational assistance of a giant planet. The resulting\nmission will be capable to reach the comet beyond the distance of Saturn, when\nthe sublimation of super-volatile species will be ongoing, and well before the\nonset of the sublimation of water (4 AU). After rendezvous, the spacecraft will\naccompany the comet for several years before, around and after perihelion (July\n2061). Our concept mission does not foresee the implementation of solar panels.\nIn this way, operations can occur even inside the dense dust coma at short\ndistance from the nucleus. In the third part of the paper, an innovative\nimaging system is proposed, with a very large field of view (100{\\deg}) capable\nto record on the same frame details on the surface and the surrounding space,\nin order to follow for several degrees the trajectories of chunks and clouds\nejected by pits or fractures, crucial to the understanding of the cometary\nactivity. A concerted effort is needed in the current decade to plan and\napprove a rendezvous mission to 1P. Indeed, the scenario here described\nrequires launching before 2040, less than 15 years from now. Later launches\nimply a severe loss of scientific knowledge, because the spacecraft will not be\nable to reach the comet before the onset of water sublimation.",
        "In fields such as autonomous and safety-critical systems, online optimization\nplays a crucial role in control and decision-making processes, often requiring\nthe integration of continuous and discrete variables. These tasks are\nfrequently modeled as mixed-integer programming (MIP) problems, where feedback\ndata are incorporated as parameters. However, solving MIPs within strict time\nconstraints is challenging due to their $\\mathcal{NP}$-complete nature. A\npromising solution to this challenge involves leveraging the largely invariant\nstructure of these problems to perform most computations offline, thus enabling\nefficient online solving even on platforms with limited hardware capabilities.\nIn this paper we present a novel implementation of this strategy that uses\ncounterexample-guided inductive synthesis to split the MIP solution process\ninto two stages. In the offline phase, we construct a mapping that provides\nfeasible assignments for binary variables based on parameter values within a\nspecified range. In the online phase, we solve the remaining continuous part of\nthe problem by fixing the binary variables to the values predicted by this\nmapping. Our numerical evaluation demonstrates the efficiency and solution\nquality of this approach compared to standard mixed-integer solvers,\nhighlighting its potential for real-time applications in resource-constrained\nenvironments.",
        "In this paper, we extend the classical Newton-Maclaurin inequalities to\nfunctions $S_{k;s}(x)=E_k(x)+\\dsum_{i=1}^s \\al_i E_{k-i}(x)$, which are formed\nby linear combinations of multiple basic symmetric mean. We proved that when\nthe coefficients $\\al_1,\\al_2,\\cdots,\\al_s$ satisfy the condition that the\npolynomial $$t^s+\\al_1 t^{s-1}+\\al_2 t^{s-2}+\\cdots+\\al_s $$ has only real\nroots, the Newton-Maclaurin type inequalities hold for $S_{k;s}(x)$.",
        "In this paper, we analyze the theta series associated to the quadratic form\n$Q(\\vec{x}) = x_1^2+x_2^2+x_3^2+x_4^2$ with congruence conditions on $x_i$\nmodulo $2,3,4$ and $6$. By employing special operators on modular,\nnon-holomorphic Eisenstein series of weight 2, we construct a basis for\nEisenstein space for levels $2^k, k\\leq 7$, $3^{\\ell}, \\ell\\leq 3$ and $p$, for\nodd prime $p$. By analyzing cusp form part of theta series corresponding to\n$Q(\\vec{x})$, we prove a linear relation between the number of integer\nsolutions to the equation $Q(\\vec{x}) = p$ under the congruence condition $x_i\n\\equiv 1 \\pmod{3}$ and the number of $\\mathbb{F}_p$-rational points on the\nelliptic curve $y^2=x^3+1$ for primes $p \\equiv 1 \\pmod{6}$."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Symbolic Transfer Entropy",
    "start_abstract":"We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
      ],
      "abstract":[
        "When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth",
        "Predictive Target-to-User Association in Complex Scenarios via\n  Hybrid-Field ISAC Signaling",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Structure and Context of Retweet Coordination in the 2022 U.S. Midterm\n  Elections",
        "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN\n  Model",
        "Ro-To-Go! Robust Reactive Control with Signal Temporal Logic",
        "Inverse Intersections for Boolean Satisfiability Problems",
        "Electromagnetic Side-Channel Analysis of PRESENT Lightweight Cipher",
        "Local well-posedness for nonlinear Schr\\\"odinger equations on compact\n  product manifolds",
        "Spectrum of L\\'evy-Ornstein-Uhlenbeck semigroups on $\\mathbb{R}^d$",
        "Spike-and-Slab Posterior Sampling in High Dimensions",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review",
        "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
        "Complexity of approximate conflict-free, linearly-ordered, and\n  nonmonochromatic hypergraph colourings",
        "Test-Time Compute: from System-1 Thinking to System-2 Thinking",
        "AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit\n  Guidance System in both Novice and Expert Pilots",
        "Dagger Behind Smile: Fool LLMs with a Happy Ending Story",
        "Knudsen boundary layer equations with incoming boundary condition: full\n  range of cutoff collision kernels and Mach numbers of the far field",
        "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation",
        "Drift: Decoding-time Personalized Alignments with Implicit User\n  Preferences",
        "Open-Source Tool for Evaluating Human-Generated vs. AI-Generated Medical\n  Notes Using the PDQI-9 Framework",
        "Scalable Decision-Making in Stochastic Environments through Learned\n  Temporal Abstraction",
        "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
        "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark\n  Challenging to Frontier LLMs",
        "Improving Adaptive Density Control for 3D Gaussian Splatting",
        "A Multi-Agent Framework for Automated Vulnerability Detection and Repair\n  in Solidity and Move Smart Contracts",
        "Deep Learning-Driven Malware Classification with API Call Sequence\n  Analysis and Concept Drift Handling",
        "InfoFusion Controller: Informed TRRT Star with Mutual Information based\n  on Fusion of Pure Pursuit and MPC for Enhanced Path Planning"
      ],
      "abstract":[
        "We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.",
        "This paper presents a novel and robust target-to-user (T2U) association\nframework to support reliable vehicle-to-infrastructure (V2I) networks that\npotentially operate within the hybrid field (near-field and far-field). To\naddress the challenges posed by complex vehicle maneuvers and user association\nambiguity, an interacting multiple-model filtering scheme is developed, which\ncombines coordinated turn and constant velocity models for predictive\nbeamforming. Building upon this foundation, a lightweight association scheme\nleverages user-specific integrated sensing and communication (ISAC) signaling\nwhile employing probabilistic data association to manage clutter measurements\nin dense traffic. Numerical results validate that the proposed framework\nsignificantly outperforms conventional methods in terms of both tracking\naccuracy and association reliability.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "The ability to detect coordinated activity in communication networks is an\nongoing challenge. Prior approaches emphasize considering any activity\nexceeding a specific threshold of similarity to be coordinated. However,\nidentifying such a threshold is often arbitrary and can be difficult to\ndistinguish from grassroots organized behavior. In this paper, we investigate a\nset of Twitter retweeting data collected around the 2022 US midterm elections,\nusing a latent sharing-space model, in which we identify the main components of\nan association network, thresholded with a k-nearest neighbor criterion. This\napproach identifies a distribution of association values with different roles\nin the network at different ranges, where the shape of the distribution\nsuggests a natural place to threshold for coordinated user candidates. We find\ncoordination candidates belonging to two broad categories, one involving music\nawards and promotion of Korean pop or Taylor Swift, the other being users\nengaged in political mobilization. In addition, the latent space suggests\ncommon motivations for different coordinated groups otherwise fragmented by\nusing an appropriately high threshold criterion for coordination.",
        "The Graphical User Interface (GUI) plays a critical role in the interaction\nbetween users and mobile applications (apps), aiming at facilitating the\noperation process. However, due to the variety of functions and\nnon-standardized design, GUIs might have many accessibility issues, like the\nsize of components being too small or their intervals being narrow. These\nissues would hinder the operation of low vision users, preventing them from\nobtaining information accurately and conveniently. Although several\ntechnologies and methods have been proposed to address these issues, they are\ntypically confined to issue identification, leaving the resolution in the hands\nof developers. Moreover, it can be challenging to ensure that the color, size,\nand interval of the fixed GUIs are appropriately compared to the original ones.\nIn this work, we propose a novel approach named AccessFixer, which utilizes the\nRelational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix\nthree kinds of accessibility issues, including small sizes, narrow intervals,\nand low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a\nconsistent color palette, uniform intervals, and adequate size changes achieved\nthrough coordinated adjustments to the attributes of related components. Our\nexperiments demonstrate the effectiveness and usefulness of AccessFixer in\nfixing GUI accessibility issues. After fixing 30 real-world apps, our approach\nsolves an average of 81.2% of their accessibility issues. Also, we apply\nAccessFixer to 10 open-source apps by submitting the fixed results with pull\nrequests (PRs) on GitHub. The results demonstrate that developers approve of\nour submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study\nexamines that low vision users host a positive attitude toward the GUIs fixed\nby our method.",
        "Signal Temporal Logic (STL) robustness is a common objective for optimal\nrobot control, but its dependence on history limits the robot's decision-making\ncapabilities when used in Model Predictive Control (MPC) approaches. In this\nwork, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new\nquantitative semantics for the logic that isolates the contributions of suffix\ntrajectories. We prove its relationship to formula progression for Metric\nTemporal Logic, and show that the robustness-to-go depends only on the suffix\ntrajectory and progressed formula. We implement robustness-to-go as the\nobjective in an MPC algorithm and use formula progression to efficiently\nevaluate it online. We test the algorithm in simulation and compare it to MPC\nusing traditional STL robustness. Our experiments show that using\nrobustness-to-go results in a higher success rate.",
        "Boolean Satisfiability (SAT) problems are expressed as mathematical formulas.\nThis paper presents an alternative matrix representation for any type of these\nSAT problems. It shows how to use this matrix representation to get the full\nset of valid assignments. It proves that this is the full set of answers for\nthe given problem, and it shows that this is exponential in size, relative to\nthe matrix. It then presents an algorithm that utilizes the inverses of the\nclauses in this matrix for faster searching through this set of answers. It\nshows that this algorithm is both correct and polynomial.",
        "Side-channel vulnerabilities pose an increasing threat to cryptographically\nprotected devices. Consequently, it is crucial to observe information leakages\nthrough physical parameters such as power consumption and electromagnetic (EM)\nradiation to reduce susceptibility during interactions with cryptographic\nfunctions. EM side-channel attacks are becoming more prevalent. PRESENT is a\npromising lightweight cryptographic algorithm expected to be incorporated into\nInternet-of-Things (IoT) devices in the future. This research investigates the\nEM side-channel robustness of PRESENT using a correlation attack model. This\nwork extends our previous Correlation EM Analysis (CEMA) of PRESENT with\nimproved results. The attack targets the Substitution box (S-box) and can\nretrieve 8 bytes of the 10-byte encryption key with a minimum of 256 EM\nwaveforms. This paper presents the process of EM attack modelling, encompassing\nboth simple and correlation attacks, followed by a critical analysis.",
        "We prove new local well-posedness results for nonlinear Schr\\\"odinger\nequations posed on a general product of spheres and tori, by the standard\napproach of multi-linear Strichartz estimates. To prove these estimates, we\nestablish and utilize multi-linear bounds for the joint spectral projector\nassociated to the Laplace--Beltrami operators on the individual sphere factors\nof the product manifold. To treat the particular case of the cubic NLS on a\nproduct of two spheres at critical regularity, we prove a sharp\n$L^\\infty_xL^p_t$ estimate of the solution to the linear Schr\\\"odinger equation\non the two-torus.",
        "We investigate the spectral properties of Markov semigroups associated with\nOrnstein-Uhlenbeck (OU) processes driven by L\\'evy processes. These semigroups\nare generated by non-local, non-self-adjoint operators. In the special case\nwhere the driving L\\'evy process is Brownian motion, one recovers the classical\ndiffusion OU semigroup, whose spectral properties have been extensively studied\nover past few decades. Our main results establish that, under suitable\nconditions on the L\\'evy process, the spectrum of the L\\'evy-OU semigroup in\nthe $L^p$-space weighted with the invariant distribution coincides with that of\nthe diffusion OU semigroup. Furthermore, when the drift matrix $B$ is\ndiagonalizable with real eigenvalues, we derive explicit formulas for\neigenfunctions and co-eigenfunctions--an observation that, to the best of our\nknowledge, has not appeared in the literature. We also show that the\nmultiplicities of the eigenvalues remain independent of the choice of the\nL\\'evy process. A key ingredient in our approach is intertwining relationship:\nwe prove that every L\\'evy-OU semigroup is intertwined with a diffusion OU\nsemigroup. Additionally, we examine the compactness properties of these\nsemigroups and provide examples of non-compact L\\'evy-OU semigroups.",
        "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.",
        "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.",
        "Using the algebraic approach to promise constraint satisfaction problems, we\nestablish complexity classifications of three natural variants of hypergraph\ncolourings: standard nonmonochromatic colourings, conflict-free colourings, and\nlinearly-ordered colourings.\n  Firstly, we show that finding an $\\ell$-colouring of a $k$-colourable\n$r$-uniform hypergraph is NP-hard for all constant $2\\leq k\\leq \\ell$ and\n$r\\geq 3$. This provides a shorter proof of a celebrated result by Dinur et al.\n[FOCS'02\/Combinatorica'05].\n  Secondly, we show that finding an $\\ell$-conflict-free colouring of an\n$r$-uniform hypergraph that admits a $k$-conflict-free colouring is NP-hard for\nall constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, except for $r=4$ and $k=2$ (and\nany $\\ell$); this case is solvable in polynomial time. The case of $r=3$ is the\nstandard nonmonochromatic colouring, and the case of $r=2$ is the notoriously\ndifficult open problem of approximate graph colouring.\n  Thirdly, we show that finding an $\\ell$-linearly-ordered colouring of an\n$r$-uniform hypergraph that admits a $k$-linearly-ordered colouring is NP-hard\nfor all constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, thus improving on the results\nof Nakajima and \\v{Z}ivn\\'y~[ICALP'22\/ACM TocT'23].",
        "The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out a few possible future\ndirections.",
        "Pilots operating modern cockpits often face high cognitive demands due to\ncomplex interfaces and multitasking requirements, which can lead to overload\nand decreased performance. This study introduces AdaptiveCoPilot, a\nneuroadaptive guidance system that adapts visual, auditory, and textual cues in\nreal time based on the pilot's cognitive workload, measured via functional\nNear-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3)\nidentified adaptive rules for modality switching and information load\nadjustments during preflight tasks. These insights informed the design of\nAdaptiveCoPilot, which integrates cognitive state assessments, behavioral data,\nand adaptive strategies within a context-aware Large Language Model (LLM). The\nsystem was evaluated in a virtual reality (VR) simulated cockpit with licensed\npilots (N=8), comparing its performance against baseline and random feedback\nconditions. The results indicate that the pilots using AdaptiveCoPilot\nexhibited higher rates of optimal cognitive load states on the facets of\nworking memory and perception, along with reduced task completion times. Based\non the formative study, experimental findings, qualitative interviews, we\npropose a set of strategies for future development of neuroadaptive pilot\nguidance systems and highlight the potential of neuroadaptive systems to\nenhance pilot performance and safety in aviation environments.",
        "The wide adoption of Large Language Models (LLMs) has attracted significant\nattention from $\\textit{jailbreak}$ attacks, where adversarial prompts crafted\nthrough optimization or manual design exploit LLMs to generate malicious\ncontents. However, optimization-based attacks have limited efficiency and\ntransferability, while existing manual designs are either easily detectable or\ndemand intricate interactions with LLMs. In this paper, we first point out a\nnovel perspective for jailbreak attacks: LLMs are more responsive to\n$\\textit{positive}$ prompts. Based on this, we deploy Happy Ending Attack (HEA)\nto wrap up a malicious request in a scenario template involving a positive\nprompt formed mainly via a $\\textit{happy ending}$, it thus fools LLMs into\njailbreaking either immediately or at a follow-up malicious request.This has\nmade HEA both efficient and effective, as it requires only up to two turns to\nfully jailbreak LLMs. Extensive experiments show that our HEA can successfully\njailbreak on state-of-the-art LLMs, including GPT-4o, Llama3-70b, Gemini-pro,\nand achieves 88.79\\% attack success rate on average. We also provide\nquantitative explanations for the success of HEA.",
        "This paper establishes tahe existence and uniqueness of the nonlinear Knudsen\nlayer equation with incoming boundary conditions. It is well-known that the\nsolvability conditions of the problem vary with the Mach number of the far\nMaxwellian $\\mathcal{M}^\\infty$. We consider full ranges of cutoff collision\nkernels (i.e., $- 3 < \\gamma \\leq 1$) and all the Mach numbers of the far field\nin the $L^\\infty_{x,v}$ framework. Additionally, the solution exhibits\nexponential decay $\\exp \\{- c x^\\frac{2}{3 - \\gamma} - c |v|^2 \\}$ for some $c\n> 0$. To address the general angular cutoff collision kernel, we introduce a\n$(x,v)$-mixed weight $\\sigma$. The proof is essentially bsed on adding an\nartificial damping term.",
        "We introduce a novel non-cooperative game to analyse opinion formation and\nresistance, incorporating principles from social psychology such as\nconfirmation bias, resource constraints, and influence penalties. Our\nsimulation features Large Language Model (LLM) agents competing to influence a\npopulation, with penalties imposed for generating messages that propagate or\ncounter misinformation. This framework integrates resource optimisation into\nthe agents' decision-making process. Our findings demonstrate that while higher\nconfirmation bias strengthens opinion alignment within groups, it also\nexacerbates overall polarisation. Conversely, lower confirmation bias leads to\nfragmented opinions and limited shifts in individual beliefs. Investing heavily\nin a high-resource debunking strategy can initially align the population with\nthe debunking agent, but risks rapid resource depletion and diminished\nlong-term influence.",
        "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable.",
        "Background: The increasing use of artificial intelligence (AI) in healthcare\ndocumentation necessitates robust methods for evaluating the quality of\nAI-generated medical notes compared to those written by humans. This paper\nintroduces an open-source tool, the Human Notes Evaluator, designed to assess\nclinical note quality and differentiate between human and AI authorship.\nMethods: The Human Notes Evaluator is a Flask-based web application implemented\non Hugging Face Spaces. It employs the Physician Documentation Quality\nInstrument (PDQI-9), a validated 9-item rubric, to evaluate notes across\ndimensions such as accuracy, thoroughness, clarity, and more. The tool allows\nusers to upload clinical notes in CSV format and systematically score each note\nagainst the PDQI-9 criteria, as well as assess the perceived origin (human, AI,\nor undetermined). Results: The Human Notes Evaluator provides a user-friendly\ninterface for standardized note assessment. It outputs comprehensive results,\nincluding individual PDQI-9 scores for each criterion, origin assessments, and\noverall quality metrics. Exportable data facilitates comparative analyses\nbetween human and AI-generated notes, identification of quality trends, and\nareas for documentation improvement. The tool is available online at\nhttps:\/\/huggingface.co\/spaces\/iyadsultan\/human_evaluator . Discussion: This\nopen-source tool offers a valuable resource for researchers, healthcare\nprofessionals, and AI developers to rigorously evaluate and compare the quality\nof medical notes. By leveraging the PDQI-9 framework, it provides a structured\nand reliable approach to assess clinical documentation, contributing to the\nresponsible integration of AI in healthcare. The tool's availability on Hugging\nFace promotes accessibility and collaborative development in the field of\nAI-driven medical documentation.",
        "Sequential decision-making in high-dimensional continuous action spaces,\nparticularly in stochastic environments, faces significant computational\nchallenges. We explore this challenge in the traditional offline RL setting,\nwhere an agent must learn how to make decisions based on data collected through\na stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),\nwhich addresses this challenge by learning a set of temporally extended\nmacro-actions through a state-conditional Vector Quantized Variational\nAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs\na (separate) learned prior model that acts as a latent transition model and\nallows efficient sampling of plausible actions. During planning, our approach\naccounts for stochasticity in both the environment and the behavior policy by\nusing Monte Carlo tree search (MCTS). In offline RL settings, including\nstochastic continuous control tasks, L-MAP efficiently searches over discrete\nlatent actions to yield high expected returns. Empirical results demonstrate\nthat L-MAP maintains low decision latency despite increased action\ndimensionality. Notably, across tasks ranging from continuous control with\ninherently stochastic dynamics to high-dimensional robotic hand manipulation,\nL-MAP significantly outperforms existing model-based methods and performs\non-par with strong model-free actor-critic baselines, highlighting the\neffectiveness of the proposed approach in planning in complex and stochastic\nenvironments with high-dimensional action spaces.",
        "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https:\/\/github.com\/Ivan-Tang-3D\/ENEL",
        "We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.",
        "3D Gaussian Splatting (3DGS) has become one of the most influential works in\nthe past year. Due to its efficient and high-quality novel view synthesis\ncapabilities, it has been widely adopted in many research fields and\napplications. Nevertheless, 3DGS still faces challenges to properly manage the\nnumber of Gaussian primitives that are used during scene reconstruction.\nFollowing the adaptive density control (ADC) mechanism of 3D Gaussian\nSplatting, new Gaussians in under-reconstructed regions are created, while\nGaussians that do not contribute to the rendering quality are pruned. We\nobserve that those criteria for densifying and pruning Gaussians can sometimes\nlead to worse rendering by introducing artifacts. We especially observe\nunder-reconstructed background or overfitted foreground regions. To encounter\nboth problems, we propose three new improvements to the adaptive density\ncontrol mechanism. Those include a correction for the scene extent calculation\nthat does not only rely on camera positions, an exponentially ascending\ngradient threshold to improve training convergence, and significance-aware\npruning strategy to avoid background artifacts. With these adaptions, we show\nthat the rendering quality improves while using the same number of Gaussians\nprimitives. Furthermore, with our improvements, the training converges\nconsiderably faster, allowing for more than twice as fast training times while\nyielding better quality than 3DGS. Finally, our contributions are easily\ncompatible with most existing derivative works of 3DGS making them relevant for\nfuture works.",
        "The rapid growth of the blockchain ecosystem and the increasing value locked\nin smart contracts necessitate robust security measures. While languages like\nSolidity and Move aim to improve smart contract security, vulnerabilities\npersist. This paper presents Smartify, a novel multi-agent framework leveraging\nLarge Language Models (LLMs) to automatically detect and repair vulnerabilities\nin Solidity and Move smart contracts. Unlike traditional methods that rely\nsolely on vast pre-training datasets, Smartify employs a team of specialized\nagents working on different specially fine-tuned LLMs to analyze code based on\nunderlying programming concepts and language-specific security principles. We\nevaluated Smartify on a dataset for Solidity and a curated dataset for Move,\ndemonstrating its effectiveness in fixing a wide range of vulnerabilities. Our\nresults show that Smartify (Gemma2+codegemma) achieves state-of-the-art\nperformance, surpassing existing LLMs and enhancing general-purpose models'\ncapabilities, such as Llama 3.1. Notably, Smartify can incorporate\nlanguage-specific knowledge, such as the nuances of Move, without requiring\nmassive language-specific pre-training datasets. This work offers a detailed\nanalysis of various LLMs' performance on smart contract repair, highlighting\nthe strengths of our multi-agent approach and providing a blueprint for\ndeveloping more secure and reliable decentralized applications in the growing\nblockchain landscape. We also provide a detailed recipe for extending this to\nother similar use cases.",
        "Malware classification in dynamic environments presents a significant\nchallenge due to concept drift, where the statistical properties of malware\ndata evolve over time, complicating detection efforts. To address this issue,\nwe propose a deep learning framework enhanced with a genetic algorithm to\nimprove malware classification accuracy and adaptability. Our approach\nincorporates mutation operations and fitness score evaluations within genetic\nalgorithms to continuously refine the deep learning model, ensuring robustness\nagainst evolving malware threats. Experimental results demonstrate that this\nhybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach\noffers a promising solution for real-time malware classification in\never-changing cybersecurity landscapes.",
        "In this paper, we propose the InfoFusion Controller, an advanced path\nplanning algorithm that integrates both global and local planning strategies to\nenhance autonomous driving in complex urban environments. The global planner\nutilizes the informed Theta-Rapidly-exploring Random Tree Star (Informed-TRRT*)\nalgorithm to generate an optimal reference path, while the local planner\ncombines Model Predictive Control (MPC) and Pure Pursuit algorithms. Mutual\nInformation (MI) is employed to fuse the outputs of the MPC and Pure Pursuit\ncontrollers, effectively balancing their strengths and compensating for their\nweaknesses. The proposed method addresses the challenges of navigating in\ndynamic environments with unpredictable obstacles by reducing uncertainty in\nlocal path planning and improving dynamic obstacle avoidance capabilities.\nExperimental results demonstrate that the InfoFusion Controller outperforms\ntraditional methods in terms of safety, stability, and efficiency across\nvarious scenarios, including complex maps generated using SLAM techniques.\n  The code for the InfoFusion Controller is available at https:\n\/\/github.com\/DrawingProcess\/InfoFusionController."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging",
    "start_abstract":"Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Magnetic resonance fingerprinting"
      ],
      "abstract":[
        "Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Integrating UX Design in Astronomical Software Development: A Case Study",
        "Bibliometric Analysis of Scientific Production on the COVID-19 Effect in\n  Information Sciences",
        "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed\n  Sensing Imaging",
        "Hybrid Channel- and Coding-Based Challenge-Response Physical-Layer\n  Authentication",
        "Thermoelectric properties of magic angle twisted bilayer\n  graphene-superconductor hetero-junction: effect of valley polarization and\n  trigonal warping",
        "Low-Eddington ratio, changing-look active galactic nuclei: the case of\n  NGC 4614",
        "Impacto del Enfoque Matematicas en Tres Actos en la Educacion Matematica",
        "Molecular Weight-Dependent Evaporation Dynamics and Morphology of PEG\n  Sessile Drops on Hydrophobic Substrates",
        "Quantized crystalline-electromagnetic responses in insulators",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Evolution of SMBHs in light of PTA measurements: implications for growth\n  by mergers and accretion",
        "Optimizing Portfolios with Pakistan-Exposed ETFs: Risk and Performance\n  Insight",
        "Quantum locally recoverable code with intersecting recovery sets",
        "How many unseen species are in multiple areas?",
        "Thermal investigation of bistability in high index doped silica\n  integrated ring resonators",
        "Tensor Cross Interpolation of Purities in Quantum Many-Body Systems",
        "Disks around young free-floating planetary-mass objects: Ultradeep\n  Spitzer imaging of IC348",
        "The quantum relative entropy of the Schwarzschild black-hole and the\n  area law",
        "Global hyperbolicity and manifold topology from the Lorentzian distance",
        "Compact Accelerator-Based Production of Carrier-free $^{177}$Lu From 18\n  MeV $D^+$ on [$^{176}$Yb]Yb$_2$O$_3$",
        "Individual Confidential Computing of Polynomials over Non-Uniform\n  Information",
        "The singlet scalar state in a chiral ensemble in $SU(2)$ with two\n  fundamental flavours",
        "Towards Accurate Mixed Quantum Classical Simulations of Vibrational\n  Polaritonic Chemistry",
        "Convergence Guarantees for Unmixing PSFs over a Manifold with Non-Convex\n  Optimization",
        "The Forestry of Adversarial Totient Iterations",
        "The r-Dynamic Chromatic Number is Bounded in the Strong 2-Coloring\n  Number",
        "Ancestral lineages and sampling in populations with density-dependent\n  interactions",
        "JADES: Average Nitrogen Enhancement in High-Redshift Broad-Line Active\n  Galactic Nuclei"
      ],
      "abstract":[
        "In 2023, ASTRON took the step of incorporating a dedicated User Experience\n(UX) designer into its software development process. This decision aimed to\nenhance the accessibility and usability of services providing access to the\ndata holdings from the telescopes we are developing.\n  The field of astronomical software development has historically under\nemphasized UX design. ASTRON's initiative not only improves our own tools, but\ncan also be used to demonstrate to the broader community the value of\nintegrating UX expertise into development teams.\n  We discuss how we integrate the UX designer at the start of our software\ndevelopment lifecycle. We end with providing some considerations on how other\nprojects could make use of UX knowledge in their development process.",
        "This paper analyzes the scientific production on the COVID-19 effect in the\narea of Information Sciences from a bibliometric perspective. The objectives\nfocused on: 1) determining the most productive authors, countries, institutions\nand journals; 2) identifying the sources that constitute the core of scientific\nproduction; 3) examining the manuscripts with the greatest impact; and 4)\nvisualizing the thematic and conceptual structure of the scientific domain\nanalyzed. Bibliometric indicators and factor analysis techniques were used for\ndata analysis. A total of 1,175 publications indexed in the Web of Science\n(WoS) core collection from 2020 to 2022 were retrieved. The results showed that\nthe most relevant countries were the United States, United Kingdom, China and\nSpain. The core of the scientific production was formed by the publications:\nJournal of the American Medical Informatics Association, Information\nProfessional, Scientometrics and Journal of Health Communication. The papers\nwith the greatest impact were concentrated in those dedicated to the analysis\nof the role of telemedicine in medical care. The conceptual structure showed\nthe main research fronts, such as the role of telehealth, academic libraries\nand digital literacy in the fight against the pandemic, the role of social\nnetworks in the health crisis, as well as the problem of misinformation and\nfake news",
        "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
        "This letter proposes a new physical layer authentication mechanism operating\nat the physical layer of a communication system where the receiver has partial\ncontrol of the channel conditions (e.g., using an intelligent reflecting\nsurface). We aim to exploit both instantaneous channel state information (CSI)\nand a secret shared key for authentication. This is achieved by both\ntransmitting an identifying key by wiretap coding (to conceal the key from the\nattacker) and checking that the instantaneous CSI corresponds to the channel\nconfiguration randomly selected by the receiver. We investigate the trade-off\nbetween the pilot signals used for CSI estimation and the coding rate (or key\nlength) to improve the overall security of the authentication procedure.",
        "We theoretically investigate the thermoelectric properties (electronic\ncontribution) of a normal-superconductor (NS) hybrid junction, where the normal\nregion consists of magic-angle twisted bilayer graphene (MATBG). The\nsuperconducting region is characterized by a common $s$-wave superconductor\nclosely proximitized to the MATBG. We compute various thermoelectric\ncoefficients, including thermal conductance, thermopower, and the figure of\nmerit ($zT$), using the scattering matrix formalism. These results are further\nsupported by calculations based on a lattice-regularized version of the\neffective Hamiltonian. Additionally, we explore the impact of trigonal warping\nand valley polarization on the thermoelectric coefficients. Notably, we find a\nsignificant variation in $zT$ as a function of these parameters, reaching\nvalues as high as 2.5. Interestingly, we observe a violation of the\nWiedemann-Franz law near the charge neutrality point with the superconducting\ncorrelation, indicating that MATBG electrons behave as slow Dirac fermions in\nthis regime. This observation is further confirmed by the damped oscillatory\nbehavior of the thermal conductance as a function of the barrier strength when\nan insulating barrier is modelled at the interface of the NS junction. Beyond\ntheoretical insights, our findings suggest new possibilities for thermoelectric\napplications using MATBG based NS junctions.",
        "Active galactic nuclei (AGN) are known to be variable sources across the\nentire electromagnetic spectrum, in particular at optical\/ultraviolet and X-ray\nenergies. Over the past decades, a growing number of AGN have displayed type\ntransitions: from type 1 to type 2 or viceversa within a few years or even\nseveral months. These galaxies have been commonly referred to as changing-look\nAGN (CLAGN). Here we report on a new CLAGN, NGC 4614, which transitioned from a\ntype 1.9 to a type 2 state. NGC 4614 is a nearly face-on barred galaxy at\nredshift $z = 0.016$, classified as a low-luminosity AGN. Its central black\nhole has a mass of about $1.6\\times 10^7 M_\\odot$ and an Eddington ratio around\n1 percent. We recently acquired optical spectra of NGC 4614 at the Telescopio\nNazionale Galileo and the data clearly suggest that the broad H$\\alpha$\ncomponent has strongly dimmed, if not disappeared. A very recent Swift\nobservation confirmed our current optical data, with the AGN weakened by almost\na factor of 10 with respect to previous X-ray observations. Indeed, NGC 4614\nhad been also observed by Swift\/XRT 6 times in 2011, when the source was\nclearly detected in all observations. By fitting the stack of the 2011 Swift\nobservations we obtain a photon index of $\\Gamma=1.3\\pm0.3$ and an equivalent\nhydrogen column density of $N_{\\rm H}$=$1.2\\pm0.3$ $\\times$10$^{22}$ cm$^{-2}$,\nindicating that NGC 4614 can be moderately absorbed in the X-rays. Although a\nsignificant change in the foreground gas absorption that may have obscured the\nbroad line region cannot be entirely ruled out, the most likely explanation for\nour optical and X-ray data is that NGC 4614 is experiencing a change in the\naccretion state that reduces the radiative efficiency of the X-ray corona.",
        "The \"Mathematics in Three Acts\" approach, proposed by Dan Meyer, aims to\ntransform the teaching of mathematics through a model that encourages active\nstudent participation, fostering creativity, problem-solving, and\nmetacognition. This study explores the implementation of this approach in a\nmathematics contest for secondary school students, evaluating its impact on\nvarious key competencies. Aspects such as mathematical creativity,\nproblem-solving skills, metacognitive abilities, and students' perceptions of\nmathematics are examined. The results show that the approach contributes to the\ndevelopment of creative skills, improves understanding and problem-solving\nabilities, and increases student motivation and confidence. However, areas for\nimprovement are also identified, particularly in the justification of\nprocedures and cognitive flexibility. This study highlights the effectiveness\nof the \"Mathematics in Three Acts\" approach as an innovative methodology that\nfosters more meaningful, reflective, and autonomous learning, suggesting its\npotential to transform mathematics teaching in diverse educational contexts.",
        "The evaporation dynamics of sessile drops are crucial for material deposition\nin applications like inkjet printing and pharmaceutical development. However,\nthe evaporation behavior of high molecular weight polymer solutions and their\nimpact on deposit morphology and flow fields are not well understood. This\nstudy investigates the evaporation dynamics and deposit morphology of\npolyethylene glycol (PEG) solution drops on hydrophobic substrates, with\nmolecular weights ranging from 200 to 1000k g\/mol, covering five orders of\nmagnitude. The results show that vapor diffusion dominates the evaporation\nprocess across all PEG molecular weights. Using image analysis and\nmicro-particle image velocimetry ($\\mu$-PIV), we reveal that molecular weight\naffects contact line dynamics and internal flow, leading to diverse deposit\nmorphologies, including spherical caps, pillars, pool-shaped disks, and flat\ndisks. Transient divergence and P\\'eclet number calculations further confirm\nthe role of hydrodynamics in deposit formation. These findings provide insights\ninto the hydrodynamic and thermodynamic factors governing evaporation in\npolymeric sessile drops, with implications for material fabrication and the\ndevelopment of inkjet printing and coating techniques.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "We study the growth of supermassive black holes accounting for both accretion\nand mergers. The former is informed by observations of the quasar luminosity\nfunction (QLF) and the latter by the gravitational wave-background (GWB)\nrecently detected by PTAs, while estimates of the present-day black hole mass\nfunction provide a boundary condition. The GWB is dominated by the most massive\nblack holes ($\\gtrsim10^{9}M_{\\odot}$). We show that their evolution can be\nsimplified into a two-step process: mergers dominate at $z\\leq1$, while\naccretion peaks at $1.4\\leq z\\leq2$. The large amplitude of the observed GWB\nsuggests a significant number of mergers. We show that this generically implies\na higher average Eddington ratio for quasars relative to a scenario in which\nmergers are negligible. In the absence of mergers, matching local estimates of\nBH abundance to the QLF implies a radiative efficiency $\\epsilon_r=0.12$ and\nEddington ratio $\\lambda=0.2$. With mergers, a progenitor of mass $M_i$ is\nboosted to a final total mass $M_f$ and there is a direct relation between the\nmass gained in mergers and the average Eddington ratio of the quasar\npopulation, given by $M_f\/M_i\\sim\\lambda\/0.2$. There is thus a tension between\nthe observed GWB, quasar properties, and the BH mass function: estimates of the\nmass function consistent with Eddington ratios inferred in quasars and\n$\\epsilon_r\\sim0.1$ underpredict the GWB; multiple\/equal mass mergers can boost\nthe GWB, but lead to a high Eddington ratio. If the local mass function is on\nthe high end of current estimates, the GWB is more readily explained, but\nrequires low efficiencies $\\epsilon_r\\sim10^{-2}$ not expected in standard\nluminous accretion models. The significant merger rate implied by the GWB also\nstrongly suggests that the most massive BHs in the local universe have\nsignificant spin due to the orbital angular momentum from mergers, perhaps\n$a\\sim0.5$.",
        "This study examines the investment landscape of Pakistan as an emerging and\nfrontier market, focusing on implications for international investors,\nparticularly those in the United States, through exchange-traded funds (ETFs)\nwith exposure to Pakistan. The analysis encompasses 30 ETFs with varying\ndegrees of exposure to Pakistan, covering the period from January 1, 2016, to\nFebruary 2024. This research highlights the potential benefits and risks\nassociated with investing in these ETFs, emphasizing the importance of thorough\nrisk assessments and portfolio performance comparisons. By providing\ndescriptive statistics and performance metrics based on historical\noptimization, this paper aims to equip investors with the necessary insights to\nmake informed decisions when optimizing their portfolios with Pakistan-exposed\nETFs. The second part of the paper introduces and assesses dynamic optimization\nmethodologies. This section is designed to explore the adaptability and\nperformance metrics of dynamic optimization techniques in comparison with\nconventional historical optimization methods. By integrating dynamic\noptimization into the investigation, this research aims to offer insights into\nthe efficacy of these contrasting methodologies in the context of\nPakistan-exposed ETFs. The findings underscore the significance of Pakistan's\nmarket dynamics within the broader context of emerging markets, offering a\npathway for diversification and potential growth in investment strategies.",
        "We introduce the concept of quantum locally recoverable codes (qLRCs) with\nintersecting recovery sets. We derive a singleton-like bound for these codes by\nleveraging the additional information provided by the intersecting recovery\nsets. Furthermore, we provide a construction for qLRCs with intersecting\nrecovery sets by introducing a variation of the hypergraph product. Finally, we\napply our qLRC methods to obtain improved results for classical LRCs. These\nresults may provide new insights into the locality of quantum error correction\ncode.",
        "In ecology, the description of species composition and biodiversity calls for\nstatistical methods that involve estimating features of interest in unobserved\nsamples based on an observed one. In the last decade, the Bayesian\nnonparametrics literature has thoroughly investigated the case where data arise\nfrom a homogeneous population. In this work, we propose a novel framework to\naddress heterogeneous populations, specifically dealing with scenarios where\ndata arise from two areas. This setting significantly increases the\nmathematical complexity of the problem and, as a consequence, it received\nlimited attention in the literature. While early approaches leverage on\ncomputational methods, we provide a distributional theory for the in-sample\nanalysis of any observed sample and we enable out-of-sample prediction for the\nnumber of unseen distinct and shared species in additional samples of arbitrary\nsizes. The latter also extends the frequentist estimators which solely deal\nwith the one-step ahead prediction. Furthermore, our results can be applied to\naddress the sample size determination in sampling problems aimed at detecting\nshared species. Our results are illustrated in a real-world dataset concerning\na population of ants in the city of Trieste.",
        "The utilization and engineering of thermo-optic effects have found broad\napplications in integrated photonic devices, facilitating efficient light\nmanipulation to achieve various functionalities. Here, we perform both an\nexperimental characterization and theoretical analysis of these effects in\nintegrated micro-ring resonators in high index doped silica (HIDS), which has\nhad many applications in integrated photonics and nonlinear optics. By fitting\nthe experimental results with theory, we obtain fundamental parameters that\ncharacterize their thermo-optic performance, including the thermo-optic\ncoefficient, the efficiency for the optically induced thermo-optic process, and\nthe thermal conductivity. The characteristics of these parameters are compared\nto those of other materials commonly used for integrated photonic platforms,\nsuch as silicon, silicon nitride, and silica. These results offer a\ncomprehensive insight into the thermo-optic properties of HIDS based devices.\nUnderstanding these properties is essential for efficiently controlling and\nengineering them in many practical applications.",
        "A defining feature of quantum many-body systems is the exponential scaling of\nthe Hilbert space with the number of degrees of freedom. This exponential\ncomplexity na\\\"ively renders a complete state characterization, for instance\nvia the complete set of bipartite Renyi entropies for all disjoint regions, a\nchallenging task. Recently, a compact way of storing subregions' purities by\nencoding them as amplitudes of a fictitious quantum wave function, known as\nentanglement feature, was proposed. Notably, the entanglement feature can be a\nsimple object even for highly entangled quantum states. However the complexity\nand practical usage of the entanglement feature for general quantum states has\nnot been explored. In this work, we demonstrate that the entanglement feature\ncan be efficiently learned using only a polynomial amount of samples in the\nnumber of degrees of freedom through the so-called tensor cross interpolation\n(TCI) algorithm, assuming it is expressible as a finite bond dimension MPS. We\nbenchmark this learning process on Haar and random MPS states, confirming\nanalytic expectations. Applying the TCI algorithm to quantum eigenstates of\nvarious one dimensional quantum systems, we identify cases where eigenstates\nhave entanglement feature learnable with TCI. We conclude with possible\napplications of the learned entanglement feature, such as quantifying the\ndistance between different entanglement patterns and finding the optimal\none-dimensional ordering of physical indices in a given state, highlighting the\npotential utility of the proposed purity interpolation method.",
        "Protoplanetary disks have been found around free-floating objects with masses\ncomparable to those of giant planets. The frequency and properties of these\ndisks around planetary-mass objects are still debated. Here we present\nultradeep mid-infrared images for the young cluster IC348, obtained through\nstacking of time series images from Spitzer. We measure fluxes at 3.6 and 4.5\nmicrons for known free-floating planetary-mass objects (FFPMOs, spectral type\nM9 or later) in this cluster. By comparing the observed infrared spectral\nenergy distributions with photospheric templates, we identify six\nplanetary-mass objects with disks, plus three which may or may not have a disk.\nThis corresponds to a disk fraction of 46% (34-59%). The disk fraction among\nplanetary-mass objects is comparable to more massive brown dwarfs. We show the\ndisk fraction among free-floating planetary-mass objects as a function of age,\ndemonstrating that these objects retain disks for several million years,\nsimilar to low-mass stars and brown dwarfs.",
        "The area law obeyed by the thermodynamic entropy of black holes is one of the\nfundamental results relating gravity to statistical mechanics. In this work we\nprovide a derivation of the area law for the quantum relative entropy of the\nSchwarzschild black-hole for arbitrary Schwarzschild radius. The quantum\nrelative entropy between the metric of the manifold and the metric induced by\nthe geometry and the matter field has been proposed in G. Bianconi \"Gravity\nfrom entropy\", Phys. Rev. D (2025) as the action for entropic quantum gravity\nleading to modified Einstein equations. The quantum relative entropy\ngeneralizes Araki entropy and treats the metrics between zero-forms, one-forms,\nand two-forms as quantum operators. Although the Schwarzschild metric is not an\nexact solution of the modified Einstein equations of the entropic quantum\ngravity, it is an approximate solution valid in the low coupling, small\ncurvature limit. Here we show that the quantum relative entropy associated to\nthe Schwarzschild metric obeys the area law for large Schwarzschild radius. We\nprovide a full statistical mechanics interpretation of the results.",
        "In this work, we seek characterizations of global hyperbolicity in smooth\nLorentzian manifolds that do not rely on the manifold topology and that are\ninspired by metric geometry. In particular, strong causality is not assumed, so\npart of the problem is precisely that of recovering the manifold topology so as\nto make sense of it also in rough frameworks. After verifying that known\nstandard characterizations do not meet this requirement, we propose two\npossible formulations. The first is based solely on chronological diamonds and\nis interesting due to its analogies with the Hopf-Rinow theorem. The second\nuses only properties of the Lorentzian distance function and it is suitable for\nextension to abstract `Lorentzian metric' frameworks. It turns out to be\nequivalent to the definition of `Lorentzian metric space' proposed in our\nprevious joint work with S. Suhr, up to slightly strengthening weak\n$d$-distinction to `future or past $d$-distinction'. The role of a new property\nwhich we term `$d$-reflectivity' is also discussed. We then investigate\ncontinuity properties of the Lorentzian distance and the property of\n$d$-reflectivity in non-smooth frameworks. Finally, we establish a result of\nbroader interest: the exponential map of a smooth spray is $C^{1,1}$ (smooth\noutside the zero section). Additionally, we derive a Lorentz-Finsler version of\nthe Busemann-Mayer formula and demonstrate that, in strongly causal smooth\nFinsler spacetimes, the Finsler fundamental function can be reconstructed from\nthe distance. As a consequence, distance-preserving bijections are shown to be\nLorentz-Finsler isometries in the conventional smooth sense.",
        "We use experimental and simulated excitation functions to estimate the yield\nof deuteron activations on a [$^{176}$Yb]Yb$_2$O$_3$ target enriched to 99%.\nSubsequent calculations are used to determine the production of\nradiotherapeutic $^{177}$Lu according to a 10 mA, 18 MeV $D^+$ compact linear\naccelerator. The design comprises a single radio-frequency quadrupole\naccelerator (RFQ) and seven drift tube linacs (DTLs) that achieve a beam\nefficiency of 99.5% over a length of $12\\,\\text{m}$. Our results show that a\n5-day irradiation can yield more than $1$ mg of $^{177}$Lu, exceeding $4.4$\nTBq. After a 2 day processing period, it is estimated that the sample will have\na radiopurity greater than 99.8% (carrier-free). Given recent EMA and FDA\napprovals of $^{177}$Lu-DOTATATE and $^{177}$Lu-PSMA-617, our results confirm\nthe viability of accelerator-based $^{177}$Lu production and provide a\npromising clinical alternative to reactor-based methods.",
        "In this paper, we address the problem of secure distributed computation in\nscenarios where user data is not uniformly distributed, extending existing\nframeworks that assume uniformity, an assumption that is challenging to enforce\nin data for computation. Motivated by the pervasive reliance on single service\nproviders for data storage and computation, we propose a privacy-preserving\nscheme that achieves information-theoretic security guarantees for computing\npolynomials over non-uniform data distributions. Our framework builds upon the\nconcept of perfect subset privacy and employs linear hashing techniques to\ntransform non-uniform data into approximately uniform distributions, enabling\nrobust and secure computation. We derive leakage bounds and demonstrate that\ninformation leakage of any subset of user data to untrusted service providers,\ni.e., not only to colluding workers but also (and more importantly) to the\nadmin, remains negligible under the proposed scheme.",
        "Composite Higgs models are a class of Beyond the Standard Model (BSM) models\nproposed to address the hierarchy and naturalness problems associated with the\nStandard Model (SM) Higgs. A new QCD-like strongly interacting sector based on\n$SU(2)$ with two fundamental flavours can be used to build a composite Higgs\nmodel which is not yet ruled out by experiment. The role of the singlet scalar\nresonance will affect Higgs phenomenology at the LHC. In this project our goal\nis to understand the properties of the singlet scalar state in the new strongly\ninteracting sector in isolation as a first step to understanding the role of\nthis state in composite Higgs models. We present here the first lattice results\nfor the mass of the $\\sigma$ in $SU(2)$ with two fundamental flavours using\nexponential clover Wilson fermions.",
        "Interest in vibrational polaritonic chemistry, where ground-state chemical\nkinetics are modified via confined optical modes in a cavity, has surged in\nrecent years. Although models have been developed to understand cavity-modified\nreactions, fully quantum mechanical simulations remain out of reach for the\ncollective regime that involves many molecules, a critical aspect of the\nphenomenon. Mixed quantum-classical (MQC) simulations offer a scalable\nalternative, but their accuracy requires testing and potential improvements\neven in the single-molecule limit. In this work, we take this step by first\nintroducing the mapping approach to surface hopping (MASH) to address the\nlimitations of traditional MQC methods. Second, we incorporate a quantum\ntreatment of the cavity mode, moving beyond the classical approximations often\nemployed in previous studies. Results for a single-molecule model of\nvibrational polaritonic chemistry show that combining MASH with a quantum\ncavity mode yields the most accurate rates. However, this scheme may produce\ndifferent long-time population dynamics at zero coupling depending on whether\nthe cavity mode is quantized; a problem known as size-inconsistency in MASH. We\naddress this problem proposing the $\\epsilon$-MASH approach, which forbids\nhopping between states with negligible nonadiabatic couplings (NACs). Combining\nMASH with a quantum cavity mode thus provides a promising approach for scalable\nand accurate MQC simulations in the collective regime.",
        "The problem of recovering the parameters of a mixture of spike signals\nconvolved with different PSFs is considered. Herein, the spike support is\nassumed to be known, while the PSFs lie on a manifold. A non-linear least\nsquares estimator of the mixture parameters is formulated. In the absence of\nnoise, a lower bound on the radius of the strong basin of attraction i.e., the\nregion of convergence, is derived. Key to the analysis is the introduction of\ncoherence and interference functions, which capture the conditioning of the PSF\nmanifold in terms of the minimal separation of the support. Numerical\nexperiments validate the theoretical findings. Finally, the practicality and\nefficacy of the non-linear least squares approach are showcased on spectral\ndata from laser-induced breakdown spectroscopy.",
        "We give a closed-form expression for\n$\\varphi(1+\\varphi(2+\\varphi(3+...+\\varphi(n)$, where $\\varphi$ is Euler's\ntotient function. More generally, for an integer sequence $A=\\{a_j\\}$ we study\nthe value of\n$A^\\varphi(n)=\\varphi(a_1+\\varphi(a_2+\\varphi(a_3+...+\\varphi(a_n)$ when $A$ is\nthe perfect squares or the perfect cubes. We show $A^\\varphi(n)$ is bounded for\nall sequences considered. We also present the Arboreal Algorithm which can\nsometimes determine a closed form of $A^\\varphi(n)$ using tree-like structures.",
        "A proper vertex-coloring of a graph is $r$-dynamic if the neighbors of each\nvertex $v$ receive at least $\\min(r, \\mathrm{deg}(v))$ different colors. In\nthis note, we prove that if $G$ has a strong $2$-coloring number at most $k$,\nthen $G$ admits an $r$-dynamic coloring with no more than $(k-1)r+1$ colors. As\na consequence, for every class of graphs of bounded expansion, the $r$-dynamic\nchromatic number is bounded by a linear function in $r$. We give a concrete\nupper bound for graphs of bounded row-treewidth, which includes for example all\nplanar graphs.",
        "We study a density-dependent Markov jump process describing a population\nwhere each individual is characterized by a type, and reproduces at rates\ndepending both on its type and on the population type distribution. First,\nusing an appropriate change in probability, we exhibit a time-inhomogeneous\nMarkov process, the auxiliary process, which allows to capture the behavior of\na sampled lineage in the population process. This is achieved through a\nmany-to-one formula, which relates the average of a function over ancestral\nlineages sampled in the population processes to its average over the auxiliary\nprocess, yielding a direct interpretation of the underlying survivorship bias.\nIn addition, this construction allows for more general sampling procedures than\nwhat was previously obtained in the literature, such as sampling restricted to\nsubpopulations. Second, we consider the large population regime, when the\npopulation size grows to infinity. Under classical assumptions, the population\ntype distribution can then be approached by a diffusion approximation, which\ncaptures the fluctuations of the population process around its deterministic\nlarge population limit. We establish a many-to-one formula allowing to sample\nin the diffusion approximation, and quantify the associated approximation\nerror.",
        "The unexpectedly high nitrogen-to-oxygen (N\/O) ratios observed in\nhigh-redshift (z) galaxies have challenged our understanding of early star\nformation. Notably, many of these nitrogen-rich galaxies show signatures of\nactive galactic nuclei (AGNs), suggesting a possible connection between black\nhole formation and nitrogen enrichment. To explore this connection, we analyse\nstacked spectra of z=4-7 broad-line and narrow-line AGNs using deep NIRSpec\ndata from the JADES survey. We identify a significant Niii] quintuplet and a\nhigh electron density ($\\sim10^{4}\\,\\mathrm{cm^{-3}}$) only in the broad-line\nAGN stack, indicating nitrogen-rich ($\\log(\\mathrm{N\/C})\\simeq0.5$,\n$\\log(\\mathrm{N\/O})>-0.6$) and dense gas similar to the high-z nitrogen-rich\ngalaxies. Our findings suggest that dense nuclear star formation may trap\nnitrogen-rich gas in proto-globular clusters, in line with the high N\/O\nobserved in local globular clusters; associated runaway stellar collisions\ncould produce intermediate-mass black hole seeds, as predicted by some models\nand simulations, whose accretion results into AGN signatures. These findings\nsupport scenarios connecting the early black hole seeding and growth to merging\nprocesses within and between proto-globular clusters in primeval galaxies."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Magnetic resonance fingerprinting",
    "start_abstract":"Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging"
      ],
      "abstract":[
        "Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Eigenvalue conditions implying edge-disjoint spanning trees and a forest\n  with constraints",
        "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws",
        "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "Personalized Interpolation: An Efficient Method to Tame Flexible\n  Optimization Window Estimation",
        "Logarithmic Width Suffices for Robust Memorization",
        "On the stress transit function",
        "Time Series Language Model for Descriptive Caption Generation",
        "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent\n  Collaborative Field Coverage",
        "TULIP: Towards Unified Language-Image Pretraining",
        "Towards Generalizable Trajectory Prediction Using Dual-Level\n  Representation Learning And Adaptive Prompting",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "The Ball-Proximal (=\"Broximal\") Point Method: a New Algorithm,\n  Convergence Theory, and Applications",
        "Further Results for the Capacity Statistic Distribution on Compositions\n  of 1's and 2's",
        "3D Point Cloud Generation via Autoregressive Up-sampling",
        "Rerailing Automata",
        "Global Group Fairness in Federated Learning via Function Tracking",
        "Revisiting gender bias research in bibliometrics: Standardizing\n  methodological variability using Scholarly Data Analysis (SoDA) Cards",
        "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
        "Uncovering Utility Functions from Observed Outcomes",
        "Media and responsible AI governance: a game-theoretic and LLM analysis",
        "Anderson localization for the multi-frequency quasi-periodic CMV\n  matrices and quantum walks",
        "Supervaluations, truth, and intuitionistic logic",
        "Cookie cutters: Bisections with fixed shapes",
        "A Stochastic Linear-Quadratic Leader-Follower Differential Game with\n  Elephant Memory",
        "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via\n  LLM Fusion",
        "Strong Low Degree Hardness for Stable Local Optima in Spin Glasses",
        "DuCos: Duality Constrained Depth Super-Resolution via Foundation Model",
        "Improving Similar Case Retrieval Ranking Performance By Revisiting\n  RankSVM",
        "From Analog to Digital -- Successful Implementation of IoT Solutions in\n  the Petrochemical Industry"
      ],
      "abstract":[
        "Let $G$ be a nontrivial graph with minimum degree $\\delta$ and $k$ an integer\nwith $k\\ge 2$. In the literature, there are eigenvalue conditions that imply\n$G$ contains $k$ edge-disjoint spanning trees. We give eigenvalue conditions\nthat imply $G$ contains $k$ edge-disjoint spanning trees and another forest $F$\nwith $|E(F)|>\\frac{\\delta-1}{\\delta}(|V(G)|-1)$, and if $F$ is not a spanning\ntree, then $F$ has a component with at least $\\delta$ edges.",
        "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.",
        "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
        "In the realm of online advertising, optimizing conversions is crucial for\ndelivering relevant products to users and enhancing business outcomes.\nPredicting conversion events is challenging due to variable delays between user\ninteractions, such as impressions or clicks, and the actual conversions. These\ndelays differ significantly across various advertisers and products,\nnecessitating distinct optimization time windows for targeted conversions. To\naddress this, we introduce a novel approach named the \\textit{Personalized\nInterpolation} method, which innovatively builds upon existing fixed conversion\nwindow models to estimate flexible conversion windows. This method allows for\nthe accurate estimation of conversions across a variety of delay ranges, thus\nmeeting the diverse needs of advertisers without increasing system complexity.\nTo validate the efficacy of our proposed method, we conducted comprehensive\nexperiments using ads conversion model. Our experiments demonstrate that this\nmethod not only achieves high prediction accuracy but also does so more\nefficiently than other existing solutions. This validation underscores the\npotential of our Personalized Interpolation method to significantly enhance\nconversion optimization in real-world online advertising systems, promising\nimproved targeting and effectiveness in advertising strategies.",
        "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$).",
        "The stress interval $S(u,v)$ between $u,v\\in V(G)$ is the set of all vertices\nin a graph $G$ that lie on every shortest $u,v$-path. A set $U \\subseteq V(G)$\nis stress convex if $S(u,v) \\subseteq U$ for any $u,v\\in U$. A vertex $v \\in\nV(G)$ is s-extreme if $V(G)-v$ is a stress convex set in $G$. The stress number\n$sn(G)$ of $G$ is the minimum cardinality of a set $U$ where $\\bigcup_{u,v \\in\nU}S(u,v)=V(G)$. The stress hull number $sh(G)$ of $G$ is the minimum\ncardinality of a set whose stress convex hull is $V(G)$. In this paper, we\npresent many basic properties of stress intervals. We characterize s-extreme\nvertices of a graph $G$ and construct graphs $G$ with arbitrarily large\ndifference between the number of s-extreme vertices, $sh(G)$ and $sn(G)$. Then\nwe study these three invariants for some special graph families, such as graph\nproducts, split graphs, and block graphs. We show that in any split graph $G$,\n$sh(G)=sn(G)=|Ext_s(G)|$, where $Ext_s(G)$ is the set of s-extreme vertices of\n$G$. Finally, we show that for $k \\in \\mathbb{N}$, deciding whether $sn(G) \\leq\nk$ is NP-complete problem, even when restricted to bipartite graphs.",
        "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
        "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.",
        "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image\/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code\/checkpoints are available at\nhttps:\/\/tulip-berkeley.github.io",
        "Existing vehicle trajectory prediction models struggle with generalizability,\nprediction uncertainties, and handling complex interactions. It is often due to\nlimitations like complex architectures customized for a specific dataset and\ninefficient multimodal handling. We propose Perceiver with Register queries\n(PerReg+), a novel trajectory prediction framework that introduces: (1)\nDual-Level Representation Learning via Self-Distillation (SD) and Masked\nReconstruction (MR), capturing global context and fine-grained details.\nAdditionally, our approach of reconstructing segmentlevel trajectories and lane\nsegments from masked inputs with query drop, enables effective use of\ncontextual information and improves generalization; (2) Enhanced Multimodality\nusing register-based queries and pretraining, eliminating the need for\nclustering and suppression; and (3) Adaptive Prompt Tuning during fine-tuning,\nfreezing the main architecture and optimizing a small number of prompts for\nefficient adaptation. PerReg+ sets a new state-of-the-art performance on\nnuScenes [1], Argoverse 2 [2], and Waymo Open Motion Dataset (WOMD) [3].\nRemarkable, our pretrained model reduces the error by 6.8% on smaller datasets,\nand multi-dataset training enhances generalization. In cross-domain tests,\nPerReg+ reduces B-FDE by 11.8% compared to its non-pretrained variant.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "Non-smooth and non-convex global optimization poses significant challenges\nacross various applications, where standard gradient-based methods often\nstruggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or\nBall Point Method (BPM) for short - a novel algorithmic framework inspired by\nthe classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we\nshow, sheds new light on several foundational optimization paradigms and\nphenomena, including non-convex and non-smooth optimization, acceleration,\nsmoothing, adaptive stepsize selection, and trust-region methods. At the core\nof BPM lies the ball-proximal (\"broximal\") operator, which arises from the\nclassical proximal operator by replacing the quadratic distance penalty by a\nball constraint. Surprisingly, and in sharp contrast with the sublinear rate of\nPPM in the nonsmooth convex regime, we prove that BPM converges linearly and in\na finite number of steps in the same regime. Furthermore, by introducing the\nconcept of ball-convexity, we prove that BPM retains the same global\nconvergence guarantees under weaker assumptions, making it a powerful tool for\na broader class of potentially non-convex optimization problems. Just like PPM\nplays the role of a conceptual method inspiring the development of practically\nefficient algorithms and algorithmic elements, e.g., gradient descent, adaptive\nstep sizes, acceleration (Ahn & Sra, 2020), and \"W\" in AdamW (Zhuang et al.,\n2022), we believe that BPM should be understood in the same manner: as a\nblueprint and inspiration for further development.",
        "In this paper, we study additional aspects of the capacity distribution on\nthe set $\\mathcal{B}_n$ of compositions of $n$ consisting of $1$'s and $2$'s.\nAmong our results are further recurrences for this distribution as well as\nformulas for the total capacity and sign balance on $\\mathcal{B}_n$. We provide\nalgebraic and combinatorial proofs of our results. We also give combinatorial\nexplanations of some prior results where such a proof was requested. Finally,\nthe joint distribution of the capacity statistic with two further parameters on\n$\\mathcal{B}_n$ is briefly considered.",
        "We introduce a pioneering autoregressive generative model for 3D point cloud\ngeneration. Inspired by visual autoregressive modeling (VAR), we conceptualize\npoint cloud generation as an autoregressive up-sampling process. This leads to\nour novel model, PointARU, which progressively refines 3D point clouds from\ncoarse to fine scales. PointARU follows a two-stage training paradigm: first,\nit learns multi-scale discrete representations of point clouds, and then it\ntrains an autoregressive transformer for next-scale prediction. To address the\ninherent unordered and irregular structure of point clouds, we incorporate\nspecialized point-based up-sampling network modules in both stages and\nintegrate 3D absolute positional encoding based on the decoded point cloud at\neach scale during the second stage. Our model surpasses state-of-the-art (SoTA)\ndiffusion-based approaches in both generation quality and parameter efficiency\nacross diverse experimental settings, marking a new milestone for\nautoregressive methods in 3D point cloud generation. Furthermore, PointARU\ndemonstrates exceptional performance in completing partial 3D shapes and\nup-sampling sparse point clouds, outperforming existing generative models in\nthese tasks.",
        "In this paper, we introduce rerailing automata for $\\omega$-regular\nlanguages. They generalize both deterministic parity (DPW) and minimized\nhistory-deterministic co-B\\\"uchi automata (with transition based acceptance,\nHdTbcBW) while combining their favorable properties. In particular, rerailing\nautomata can represent arbitrary $\\omega$-regular languages while allowing for\npolynomial-time minimization, just as HdTbcBW do. Since DPW are a special case\nof rerailing automata, a minimized rerailing automaton is never larger than the\nsmallest deterministic parity automaton for the same language. We also show\nthat rerailing automata can be used as a replacement for deterministic parity\nautomata for the realizability check of open systems.\n  The price to be paid to obtain the useful properties of rerailing automata is\nthat the acceptance condition in such automata refers to the dominating colors\nalong all runs for a given word, where just as in parity automata, the\ndominating color along a run is the lowest one occurring infinitely often along\nit. A rerailing automaton accepts those words for which the greatest of the\ndominating colors along the runs is even. Additionally, rerailing automata\nguarantee that every prefix of a run for a word can be extended to eventually\nreach a point from which all runs for the word extending the prefix have the\nsame dominating color, and it is even if and only if the word is in the\nlanguage of the automaton. We show that these properties together allow\ncharacterizing the role of each state in such an automaton in a way that\nrelates it to state combinations in a sequence of co-B\\\"uchi automata for the\nrepresented language. This characterization forms the basis of the\npolynomial-time minimization approach in this paper.",
        "We investigate group fairness regularizers in federated learning, aiming to\ntrain a globally fair model in a distributed setting. Ensuring global fairness\nin distributed training presents unique challenges, as fairness regularizers\ntypically involve probability metrics between distributions across all clients\nand are not naturally separable by client. To address this, we introduce a\nfunction-tracking scheme for the global fairness regularizer based on a Maximum\nMean Discrepancy (MMD), which incurs a small communication overhead. This\nscheme seamlessly integrates into most federated learning algorithms while\npreserving rigorous convergence guarantees, as demonstrated in the context of\nFedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD\nregularization enables straightforward analysis through a change of kernel,\nleveraging an intuitive interpretation of kernel convolution. Numerical\nexperiments confirm our theoretical insights.",
        "Gender biases in scholarly metrics remain a persistent concern, despite\nnumerous bibliometric studies exploring their presence and absence across\nproductivity, impact, acknowledgment, and self-citations. However,\nmethodological inconsistencies, particularly in author name disambiguation and\ngender identification, limit the reliability and comparability of these\nstudies, potentially perpetuating misperceptions and hindering effective\ninterventions. A review of 70 relevant publications over the past 12 years\nreveals a wide range of approaches, from name-based and manual searches to more\nalgorithmic and gold-standard methods, with no clear consensus on best\npractices. This variability, compounded by challenges such as accurately\ndisambiguating Asian names and managing unassigned gender labels, underscores\nthe urgent need for standardized and robust methodologies. To address this\ncritical gap, we propose the development and implementation of ``Scholarly Data\nAnalysis (SoDA) Cards.\" These cards will provide a structured framework for\ndocumenting and reporting key methodological choices in scholarly data\nanalysis, including author name disambiguation and gender identification\nprocedures. By promoting transparency and reproducibility, SoDA Cards will\nfacilitate more accurate comparisons and aggregations of research findings,\nultimately supporting evidence-informed policymaking and enabling the\nlongitudinal tracking of analytical approaches in the study of gender and other\nsocial biases in academia.",
        "User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.",
        "Determining consumer preferences and utility is a foundational challenge in\neconomics. They are central in determining consumer behaviour through the\nutility-maximising consumer decision-making process. However, preferences and\nutilities are not observable and may not even be known to the individual making\nthe choice; only the outcome is observed in the form of demand. Without the\nability to observe the decision-making mechanism, demand estimation becomes a\nchallenging task and current methods fall short due to lack of scalability or\nability to identify causal effects. Estimating these effects is critical when\nconsidering changes in policy, such as pricing, the impact of taxes and\nsubsidies, and the effect of a tariff. To address the shortcomings of existing\nmethods, we combine revealed preference theory and inverse reinforcement\nlearning to present a novel algorithm, Preference Extraction and Reward\nLearning (PEARL) which, to the best of our knowledge, is the only algorithm\nthat can uncover a representation of the utility function that best\nrationalises observed consumer choice data given a specified functional form.\nWe introduce a flexible utility function, the Input-Concave Neural Network\nwhich captures complex relationships across goods, including cross-price\nelasticities. Results show PEARL outperforms the benchmark on both noise-free\nand noisy synthetic data.",
        "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
        "In this paper, we establish Anderson localization for the CMV matrices with\nmulti-frequency analytic quasi-periodic Verblunsky coefficients in the regime\nof the positive Lyapunov exponent. As an application, we further derive the\nAnderson localization for the multi-frequency analytic quasi-periodic quantum\nwalks. We extend the results of Wang and Damanik (J. Funct. Anal. 276 (2019))\nfor one-frequency quasi-periodic CMV matrices to multi-frequency case.",
        "The supervaluationist approach to fixed-point semantics is, arguably, the\nmost celebrated and studied competitor to the Strong Kleene approach within\nKripkean truth. In this paper, we show how to obtain a supervaluationist\nfixed-point theory of truth for intuitionistic logic. In particular, we show\nhow to do supervaluations over Kripke structures for intuitionistic logic, and\nwe obtain the corresponding theories of truth, both semantic and axiomatic.\nFurthermore, we show how the theory of truth changes when the Kripke structures\nover which the supervaluations are defined change. Finally, we advance a\nsupervaluationist theory of truth that, unlike the classical case or any of the\nother intuitionistic, supervaluationist theories, is compositional in nature.",
        "In a mass partition problem, we are interested in finding equitable\npartitions of smooth measures in $\\mathbb{R}^d$. In this manuscript, we study\nthe problem of finding simultaneous bisections of measures using scaled copies\nof a prescribed set $K$. We distinguish the problem when we are allowed to use\nscaled and translated copies of $K$ and the problem when we are allowed to use\nscaled isometric copies of $K$. These problems have only previously been\nstudied if $K$ is a half-space or a Euclidean ball. We obtain positive results\nfor simultaneous bisection of any $d+1$ masses for star-shaped compact sets $K$\nwith non-empty interior, where the conditions on the problem depend on the\nsmoothness of the boundary of $K$. Additional proofs are included for\nparticular instances of $K$, such as hypercubes and cylinders, answering\npositively a conjecture of Sober\\'on and Takahashi. The proof methods are\ntopological and involve new Borsuk--Ulam-type theorems.",
        "This paper is concerned with a stochastic linear-quadratic leader-follower\ndifferential game with elephant memory. The model is general in that the state\nequation for both the leader and the follower includes the elephant memory of\nthe state and the control, which are part of the diffusion term. Under certain\nassumptions, the state feedback representation of the open-loop Stackelberg\nstrategy is derived by introducing two Riccati equations and a special\nmatrix-valued equation. Finally, theoretical results are illustrated by means\nof an example concerning a dynamic advertising problem with elephant memory.",
        "We introduce InfiFusion, an efficient training pipeline designed to integrate\nmultiple domain-specialized Large Language Models (LLMs) into a single pivot\nmodel, effectively harnessing the strengths of each source model. Traditional\nfusion methods either merge model parameters directly or rely on knowledge\ndistillation with rigid assumptions, limiting their flexibility and efficiency.\nInfiFusion overcomes these limitations by enhancing Universal Logit\nDistillation (ULD) with Top-K selection and Logits Standardization. We propose\ntwo fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source\nmodel knowledge is distilled individually into the pivot model followed by\nmerging and Unified Fusion (InfiFusion$_u$), where knowledge from all source\nmodels is distilled simultaneously into the pivot model. InfiFusion outperforms\nthe state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11\nwidely applied benchmarks covering reasoning, coding, mathematics, and\ninstruction-following tasks. Notably, InfiFusion achieves this superior\nperformance while significantly reduces computational costs, completing full\ntraining with only 160 H800 GPU hours compared to the millions typically\nrequired for traditional LLM training.",
        "It is a folklore belief in the theory of spin glasses and disordered systems\nthat out-of-equilibrium dynamics fail to find stable local optima exhibiting\ne.g. local strict convexity on physical time-scales. In the context of the\nSherrington--Kirkpatrick spin glass, Behrens-Arpino-Kivva-Zdeborov\\'a and\nMinzer-Sah-Sawhney have recently conjectured that this obstruction may be\ninherent to all efficient algorithms, despite the existence of exponentially\nmany such optima throughout the landscape. We prove this search problem\nexhibits strong low degree hardness for polynomial algorithms of degree $D\\leq\no(N)$: any such algorithm has probability $o(1)$ to output a stable local\noptimum. To the best of our knowledge, this is the first result to prove that\neven constant-degree polynomials have probability $o(1)$ to solve a random\nsearch problem without planted structure. To prove this, we develop a\ngeneral-purpose enhancement of the ensemble overlap gap property, and as a\nbyproduct improve previous results on spin glass optimization, maximum\nindependent set, random $k$-SAT, and the Ising perceptron to strong low degree\nhardness. Finally for spherical spin glasses with no external field, we prove\nthat Langevin dynamics does not find stable local optima within dimension-free\ntime.",
        "We introduce DuCos, a novel depth super-resolution framework grounded in\nLagrangian duality theory, offering a flexible integration of multiple\nconstraints and reconstruction objectives to enhance accuracy and robustness.\nOur DuCos is the first to significantly improve generalization across diverse\nscenarios with foundation models as prompts. The prompt design consists of two\nkey components: Correlative Fusion (CF) and Gradient Regulation (GR). CF\nfacilitates precise geometric alignment and effective fusion between prompt and\ndepth features, while GR refines depth predictions by enforcing consistency\nwith sharp-edged depth maps derived from foundation models. Crucially, these\nprompts are seamlessly embedded into the Lagrangian constraint term, forming a\nsynergistic and principled framework. Extensive experiments demonstrate that\nDuCos outperforms existing state-of-the-art methods, achieving superior\naccuracy, robustness, and generalization. The source codes and pre-trained\nmodels will be publicly available.",
        "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https:\/\/github.com\/liuyuqi123study\/RankSVM_for_SLR",
        "This document describes the development and implementation of a technological\nsolution based on IoT devices to modernize a machine known as the Cyclone. This\nequipment is used by a contractor collaborating with petrochemical companies in\nthe state of Texas, performing specialized work in mechanics, engineering,\ncatalytic material replacement, and rescue operations in refinery complexes.\nThe Cyclone machine, with outdated relay logic technology, poses challenges in\nterms of operational efficiency, critical condition monitoring, and safety. The\nproject was carried out with the collaboration of specialists in equipment\nhandling, focusing on demonstrating the feasibility of integrating advanced\nIndustry 4.0 technologies into legacy industrial equipment. The methodology\nincluded the incorporation of IoT sensors for real-time monitoring, an\nautomated control system, and the digitization of key processes. Preliminary\nresults indicate improvements in the precision of operational control and the\nability for remote supervision, highlighting the potential for modernization in\ncritical industrial applications. This work not only validates the use of IoT\ndevices in obsolete equipment but also sets a precedent for the transition\ntowards more sustainable and efficient technologies in the petrochemical\nsector."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Additive manufacturing and sustainability: an exploratory study of the advantages and challenges",
    "start_abstract":"The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Machine learning in additive manufacturing: State-of-the-art and perspectives"
      ],
      "abstract":[
        "Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "RF Desense significance and its impact on the EVM at Signal Near the\n  Noise Floor",
        "Concentration phenomena for a mixed local\/nonlocal Schr\\\"{o}dinger\n  equation with Dirichlet datum",
        "Toward a Flexible Framework for Linear Representation Hypothesis Using\n  Maximum Likelihood Estimation",
        "Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based\n  Perspective",
        "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and\n  Deployment",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
        "Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum",
        "Disharmony: Forensics using Reverse Lighting Harmonization",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge",
        "Large language models streamline automated systematic review: A\n  preliminary study",
        "Semi-Quantum Conference Key Agreement with GHZ-type states",
        "Relative knot probabilities in confined lattice polygons",
        "Improving Similar Case Retrieval Ranking Performance By Revisiting\n  RankSVM",
        "The slicing conjecture via small ball estimates",
        "Supercritical phase transition on the Toeplitz algebra of $\\mathbb\n  N^\\times \\ltimes \\mathbb Z$",
        "Homotopy types of complexes of hyperplanes in quasi-median graphs and\n  applications to right-angled Artin groups",
        "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
        "Improved Sublinear Algorithms for Classical and Quantum Graph Coloring",
        "InDeed: Interpretable image deep decomposition with guaranteed\n  generalizability",
        "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
        "Forecasting Drought Using Machine Learning in California",
        "User Agency and System Automation in Interactive Intelligent Systems",
        "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber\n  Attack Taxonomies",
        "Towards Data-Driven Multi-Stage OPF"
      ],
      "abstract":[
        "Hardware impairments and system non-linearities impacting communication\nsignal is one of key aspect for having harmonics and RF desense which overall\ncausing the lower quality and integrity of the modulated signal, resulting in\nI\/Q imbalance, further bit error and spectral efficiency degradation. This\npresentation outlines the RF Desense results, EVM Measurement and it impact at\nthe almost noise floor with step size by 1 dB in QPSK at LTE Bands, Note for\nmmW 3GPP 38.521-2 clause 6.4.2.1 indicates single polarization.",
        "We consider the mixed local\/nonlocal semilinear equation\n  \\begin{equation*}\n  -\\epsilon^{2}\\Delta u +\\epsilon^{2s}(-\\Delta)^s u +u=u^p\\qquad \\text{in }\n\\Omega\n  \\end{equation*} with zero Dirichlet datum, where $\\epsilon>0$ is a small\nparameter, $s\\in(0,1)$, $p\\in(1,\\frac{n+2}{n-2})$ and $\\Omega$ is a smooth,\nbounded domain. We construct a family of solutions that concentrate, as\n$\\epsilon\\rightarrow 0$, at an interior point of $\\Omega$ having uniform\ndistance to $\\partial\\Omega$ (this point can also be characterized as a local\nminimum of a nonlocal functional).\n  In spite of the presence of the Laplace operator, the leading order of the\nrelevant reduced energy functional in the Lyapunov-Schmidt procedure is\npolynomial rather than exponential in the distance to the boundary, in light of\nthe nonlocal effect at infinity. A delicate analysis is required to establish\nsome uniform estimates with respect to $\\epsilon$, due to the difficulty caused\nby the different scales coming from the mixed operator.",
        "Linear representation hypothesis posits that high-level concepts are encoded\nas linear directions in the representation spaces of LLMs. Park et al. (2024)\nformalize this notion by unifying multiple interpretations of linear\nrepresentation, such as 1-dimensional subspace representation and\ninterventions, using a causal inner product. However, their framework relies on\nsingle-token counterfactual pairs and cannot handle ambiguous contrasting\npairs, limiting its applicability to complex or context-dependent concepts. We\nintroduce a new notion of binary concepts as unit vectors in a canonical\nrepresentation space, and utilize LLMs' (neural) activation differences along\nwith maximum likelihood estimation (MLE) to compute concept directions (i.e.,\nsteering vectors). Our method, Sum of Activation-base Normalized Difference\n(SAND), formalizes the use of activation differences modeled as samples from a\nvon Mises-Fisher (vMF) distribution, providing a principled approach to derive\nconcept directions. We extend the applicability of Park et al. (2024) by\neliminating the dependency on unembedding representations and single-token\npairs. Through experiments with LLaMA models across diverse concepts and\nbenchmarks, we demonstrate that our lightweight approach offers greater\nflexibility, superior performance in activation engineering tasks like\nmonitoring and manipulation.",
        "Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as\nthe number of layers increases, node embeddings become increasingly similar,\nand model performance drops sharply. Traditionally, oversmoothing has been\nquantified using metrics that measure the similarity of neighbouring node\nfeatures, such as the Dirichlet energy. While these metrics are related to\noversmoothing, we argue they have critical limitations and fail to reliably\ncapture oversmoothing in realistic scenarios. For instance, they provide\nmeaningful insights only for very deep networks and under somewhat strict\nconditions on the norm of network weights and feature representations. As an\nalternative, we propose measuring oversmoothing by examining the numerical or\neffective rank of the feature representations. We provide theoretical support\nfor this approach, demonstrating that the numerical rank of feature\nrepresentations converges to one for a broad family of nonlinear activation\nfunctions under the assumption of nonnegative trained weights. To the best of\nour knowledge, this is the first result that proves the occurrence of\noversmoothing without assumptions on the boundedness of the weight matrices.\nAlong with the theoretical findings, we provide extensive numerical evaluation\nacross diverse graph architectures. Our results show that rank-based metrics\nconsistently capture oversmoothing, whereas energy-based metrics often fail.\nNotably, we reveal that a significant drop in the rank aligns closely with\nperformance degradation, even in scenarios where energy metrics remain\nunchanged.",
        "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "Adaptive teaming, the ability to collaborate with unseen teammates without\nprior coordination, remains an underexplored challenge in multi-robot\ncollaboration. This paper focuses on adaptive teaming in multi-drone\ncooperative pursuit, a critical task with real-world applications such as\nborder surveillance, search-and-rescue, and counter-terrorism. We first define\nand formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone\n\\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a\ncomprehensive framework that integrates simulation, algorithm training and\nreal-world deployment. AT-MDP framework provides a flexible experiment\nconfigurator and interface for simulation, a distributed training framework\nwith an extensive algorithm zoo (including two newly proposed baseline methods)\nand an unseen drone zoo for evaluating adaptive teaming, as well as a\nreal-world deployment system that utilizes edge computing and Crazyflie drones.\nTo the best of our knowledge, AT-MDP framework is the first adaptive framework\nfor continuous-action decision-making in complex real-world drone tasks,\nenabling multiple drones to coordinate effectively with unseen teammates.\nExtensive experiments in four multi-drone pursuit environments of increasing\ndifficulty confirm the effectiveness of AT-MDP framework, while real-world\ndeployments further validate its feasibility in physical systems. Videos and\ncode are available at https:\/\/sites.google.com\/view\/at-mdp.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
        "Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps:\/\/anonymous.4open.science\/r\/momentum-increasing-batch-size-888C\/.",
        "Content generation and manipulation approaches based on deep learning methods\nhave seen significant advancements, leading to an increased need for techniques\nto detect whether an image has been generated or edited. Another area of\nresearch focuses on the insertion and harmonization of objects within images.\nIn this study, we explore the potential of using harmonization data in\nconjunction with a segmentation model to enhance the detection of edited image\nregions. These edits can be either manually crafted or generated using deep\nlearning methods. Our findings demonstrate that this approach can effectively\nidentify such edits. Existing forensic models often overlook the detection of\nharmonized objects in relation to the background, but our proposed Disharmony\nNetwork addresses this gap. By utilizing an aggregated dataset of harmonization\ntechniques, our model outperforms existing forensic networks in identifying\nharmonized objects integrated into their backgrounds, and shows potential for\ndetecting various forms of edits, including virtual try-on tasks.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.",
        "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field.",
        "We propose a semi-quantum conference key agreement (SQCKA) protocol that\nleverages on GHZ states. We provide a comprehensive security analysis for our\nprotocol that does not rely on a trusted mediator party. We present\ninformation-theoretic security proof, addressing collective attacks within the\nasymptotic limit of infinitely many rounds. This assumption is practical, as\nparticipants can monitor and abort the protocol if deviations from expected\nnoise patterns occur. This advancement enhances the feasibility of SQCKA\nprotocols for real-world applications, ensuring strong security without complex\nnetwork topologies or third-party trust.",
        "In this paper we examine the relative knotting probabilities in a lattice\nmodel of ring polymers confined in a cavity. The model is of a lattice knot of\nsize $n$ in the cubic lattice, confined to a cube of side-length $L$ and with\nvolume $V=(L{+}1)^3$ sites. We use Monte Carlo algorithms to approximately\nenumerate the number of conformations of lattice knots in the confining cube.\nIf $p_{n,L}(K)$ is the number of conformations of a lattice polygon of length\n$n$ and knot type $K$ in a cube of volume $L^3$, then the relative knotting\nprobability of a lattice polygon to have knot type $K$, relative to the\nprobability that the polygon is the unknot (the trivial knot, denoted by\n$0_1$), is $\\rho_{n,L}(K\/0_1) = p_{n,L}(K)\/p_{n,L}(0_1)$. We determine\n$\\rho_{n,L}(K\/0_1)$ for various knot types $K$ up to six crossing knots. Our\ndata show that these relative knotting probabilities are small so that the\nmodel is dominated by lattice polygons of knot type the unknot. Moreover, if\nthe concentration of the monomers of the lattice knot is $\\varphi = n\/V$, then\nthe relative knot probability increases with $\\varphi$ along a curve that\nflattens as the Hamiltonian state is approached.",
        "Given the rapid development of Legal AI, a lot of attention has been paid to\none of the most important legal AI tasks--similar case retrieval, especially\nwith language models to use. In our paper, however, we try to improve the\nranking performance of current models from the perspective of learning to rank\ninstead of language models. Specifically, we conduct experiments using a\npairwise method--RankSVM as the classifier to substitute a fully connected\nlayer, combined with commonly used language models on similar case retrieval\ndatasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM\ncould generally help improve the retrieval performance on the LeCaRDv1 and\nLeCaRDv2 datasets compared with original classifiers by optimizing the precise\nranking. It could also help mitigate overfitting owing to class imbalance. Our\ncode is available in https:\/\/github.com\/liuyuqi123study\/RankSVM_for_SLR",
        "Bourgain's slicing conjecture was recently resolved by Joseph Lehec and Bo'az\nKlartag. We present an alternative proof by establishing small ball probability\nestimates for isotropic log-concave measures. Our approach relies on the\nstochastic localization process and Guan's bound, techniques also used by\nKlartag and Lehec. The link between small ball probabilities and the slicing\nconjecture was first observed by Dafnis and Paouris and is established through\nMilman's theory of M-ellipsoids.",
        "We study the high-temperature equilibrium for the C*-algebra $\\mathcal\nT(\\mathbb N^\\times \\ltimes \\mathbb Z)$ recently considered by an Huef, Laca and\nRaeburn. We show that the simplex of KMS$_\\beta$ states at each inverse\ntemperature $\\beta$ in the critical interval $(0,1]$ is a Bauer simplex whose\nspace of extreme points is homeomorphic to $\\mathbb N \\sqcup\\{\\infty\\}$. This\nis in contrast to the uniqueness of equilibrium at high temperature observed in\npreviously considered systems arising from number theory. We also establish a\nconnection between the phase transitions on quotients of our system and the\nBost-Connes phase transition.",
        "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
        "Graph learning plays a vital role in mining and analyzing complex\nrelationships involved in graph data, which is widely used in many real-world\napplications like transaction networks and communication networks. Foundation\nmodels in CV and NLP have shown powerful cross-domain capabilities that are\nalso significant in graph domains. However, existing graph learning approaches\nstruggle with cross-domain tasks. Inspired by successes in CV and NLP,\ncross-domain graph learning has once again become a focal point of attention to\nrealizing true graph foundation models. In this survey, we present a\ncomprehensive review and analysis of existing works on cross-domain graph\nlearning. Concretely, we first propose a new taxonomy, categorizing existing\napproaches based on the learned cross-domain information: structure, feature,\nand structure-feature mixture. Next, we systematically survey representative\nmethods in these categories. Finally, we discuss the remaining limitations of\nexisting studies and highlight promising avenues for future research. Relevant\npapers are summarized and will be consistently updated at:\nhttps:\/\/github.com\/cshhzhao\/Awesome-Cross-Domain-Graph-Learning.",
        "We present three sublinear randomized algorithms for vertex-coloring of\ngraphs with maximum degree $\\Delta$. The first is a simple algorithm that\nextends the idea of Morris and Song to color graphs with maximum degree\n$\\Delta$ using $\\Delta+1$ colors. Combined with the greedy algorithm, it\nachieves an expected runtime of $O(n^{3\/2}\\sqrt{\\log n})$ in the query model,\nimproving on Assadi, Chen, and Khanna's algorithm by a $\\sqrt{\\log n}$ factor\nin expectation. When we allow quantum queries to the graph, we can accelerate\nthe first algorithm using Grover's famous algorithm, resulting in a runtime of\n$\\tilde{O}(n^{4\/3})$ quantum queries. Finally, we introduce a quantum algorithm\nfor $(1+\\epsilon)\\Delta$-coloring, achieving\n$O(\\epsilon^{-1}n^{5\/4}\\log^{3\/2}n)$ quantum queries, offering a polynomial\nimprovement over the previous best bound by Morris and Song.",
        "Image decomposition aims to analyze an image into elementary components,\nwhich is essential for numerous downstream tasks and also by nature provides\ncertain interpretability to the analysis. Deep learning can be powerful for\nsuch tasks, but surprisingly their combination with a focus on interpretability\nand generalizability is rarely explored. In this work, we introduce a novel\nframework for interpretable deep image decomposition, combining hierarchical\nBayesian modeling and deep learning to create an architecture-modularized and\nmodel-generalizable deep neural network (DNN). The proposed framework includes\nthree steps: (1) hierarchical Bayesian modeling of image decomposition, (2)\ntransforming the inference problem into optimization tasks, and (3) deep\ninference via a modularized Bayesian DNN. We further establish a theoretical\nconnection between the loss function and the generalization error bound, which\ninspires a new test-time adaptation approach for out-of-distribution scenarios.\nWe instantiated the application using two downstream tasks, \\textit{i.e.},\nimage denoising, and unsupervised anomaly detection, and the results\ndemonstrated improved generalizability as well as interpretability of our\nmethods. The source code will be released upon the acceptance of this paper.",
        "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https:\/\/scalingintelligence.stanford.edu\/pubs\/codemonkeys.",
        "Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.",
        "Balancing user agency and system automation is essential for effective\nhuman-AI interactions. Fully automated systems can deliver efficiency but risk\nundermining usability and user autonomy, while purely manual tools are often\ninefficient and fail to enhance user capabilities. This dissertation addresses\nthe question: \"How can we balance user agency and system automation for\ninteractions with intelligent systems?\"\n  We present four main contributions. First, we develop a spherical\nelectromagnet that provides adjustable forces on an untethered tool, allowing\nhaptic feedback while preserving user agency. Second, we create an integrated\nsensing and actuation system that tracks a passive magnetic tool in 3D and\ndelivers haptic feedback without external tracking. Third, we propose an\noptimal control method for electromagnetic haptic guidance that balances user\ninput with system control, enabling users to adjust trajectories and speed.\nFinally, we introduce a model-free reinforcement learning approach for adaptive\ninterfaces that learns interface adaptations without heuristics or real user\ndata. Our simulations and user studies show that shared control significantly\noutperforms naive strategies. By incorporating explicit or implicit models of\nhuman behavior into control strategies, intelligent systems can better account\nfor user agency. We demonstrate that the trade-off between agency and\nautomation is both an algorithmic challenge and an engineering concern, shaped\nby the design of physical devices and user interfaces. We advocate an\nintegrated, end-to-end approach-combining algorithmic, engineering, and design\nperspectives-to enable more intuitive and effective interactions with\nintelligent systems.",
        "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains.",
        "The operation of large-scale power systems is usually scheduled ahead via\nnumerical optimization. However, this requires models of grid topology, line\nparameters, and bus specifications. Classic approaches first identify the\nnetwork topology, i.e., the graph of interconnections and the associated\nimpedances. The power generation schedules are then computed by solving a\nmulti-stage optimal power flow (OPF) problem built around the model. In this\npaper, we explore the prospect of data-driven approaches to multi-stage optimal\npower flow. Specifically, we leverage recent findings from systems and control\nto bypass the identification step and to construct the optimization problem\ndirectly from data. We illustrate the performance of our method on a 118-bus\nsystem and compare it with the classical identification-based approach."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Machine learning in additive manufacturing: State-of-the-art and perspectives",
    "start_abstract":"Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Additive manufacturing and sustainability: an exploratory study of the advantages and challenges"
      ],
      "abstract":[
        "The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Modified FOX Optimizer for Solving optimization problems",
        "On the minimum cut-sets of the power graph of a finite cyclic group, II",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Bridging statistical mechanics and thermodynamics away from equilibrium:\n  a data-driven approach for learning internal variables and their dynamics",
        "Scalar probability density function mixing models need not comply with\n  the linearity and independence hypothesis",
        "The entropy profiles of a definable set over finite fields",
        "Optimal Functional $2^{s-1}$-Batch Codes: Exploring New Sufficient\n  Conditions",
        "On the ascent of almost and quasi-atomicity to monoid semidomains",
        "Refined curve counting with descendants and quantum mirrors",
        "Dual Control for Interactive Autonomous Merging with Model Predictive\n  Diffusion",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "The influence of missing data mechanisms and simple missing data\n  handling techniques on fairness",
        "From Mutation to Degradation: Predicting Nonsense-Mediated Decay with\n  NMDEP",
        "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)",
        "Zubarev response approach to polarization phenomena in local equilibrium",
        "On the perfect $k$-divisibility of graphs",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "Magnetic Trampoline Resonators from (La,Sr)MnO3 Single-Crystal Thin\n  Films",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning",
        "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration",
        "On conductor submonoids of factorial monoids",
        "Vertex Partitioning and $p$-Energy of Graphs",
        "Nuclear magnetic resonance spectroscopy in pulsed magnetic fields",
        "Exploring a tentative link between FRBs and pulsars with broad energy\n  distributions and applications for nearby FRB survey strategies",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "A second-order accurate, positivity-preserving numerical scheme for the\n  Poisson-Nernst-Planck-Navier-Stokes system",
        "Spacetime Structure of Regular Accelerating Black Hole Pair in General\n  Relativity",
        "A Few Observations on Sample-Conditional Coverage in Conformal\n  Prediction",
        "Spatiotemporal clustering of GHGs emissions in Europe: exploring the\n  role of spatial component"
      ],
      "abstract":[
        "The FOX optimizer, inspired by red fox hunting behavior, is a powerful\nalgorithm for solving real-world and engineering problems. However, despite\nbalancing exploration and exploitation, it can prematurely converge to local\noptima, as agent positions are updated solely based on the current best-known\nposition, causing all agents to converge on one location. This study proposes\nthe modified FOX optimizer (mFOX) to enhance exploration and balance\nexploration and exploitation in three steps. First, the Oppositional-Based\nLearning (OBL) strategy is used to improve the initial population. Second,\ncontrol parameters are refined to achieve a better balance between exploration\nand exploitation. Third, a new update equation is introduced, allowing agents\nto adjust their positions relative to one another rather than relying solely on\nthe best-known position. This approach improves exploration efficiency without\nadding complexity. The mFOX algorithm's performance is evaluated against 12\nwell-known algorithms on 23 classical benchmark functions, 10 CEC2019\nfunctions, and 12 CEC2022 functions. It outperforms competitors in 74% of the\nclassical benchmarks, 60% of the CEC2019 benchmarks, and 58% of the CEC2022\nbenchmarks. Additionally, mFOX effectively addresses four engineering problems.\nThese results demonstrate mFOX's strong competitiveness in solving complex\noptimization tasks, including unimodal, constrained, and high-dimensional\nproblems.",
        "The power graph $\\mathcal{P}(G)$ of a finite group $G$ is the simple graph\nwith vertex set $G$ and two distinct vertices are adjacent if one of them is a\npower of the other. Let $n=p_1^{n_1}p_2^{n_2}\\cdots p_r^{n_r},$ where\n$p_1,p_2,\\ldots,p_r$ are primes with $p_1<p_2<\\cdots <p_r$ and $n_1,n_2,\\ldots,\nn_r$ are positive integers. For the cyclic group $C_n$ of order $n$, the\nminimum cut-sets of $\\mathcal{P}(C_n)$ are characterized in \\cite{cps} for\n$r\\leq 3$. Recently, in \\cite{MPS}, certain cut-sets of $\\mathcal{P}(C_n)$ are\nidentified such that any minimum cut-set of $\\mathcal{P}(C_n)$ must be one of\nthem. In this paper, for $r\\geq 4$, we explicitly determine the minimum\ncut-sets, in particular, the vertex connectivity of $\\mathcal{P}(C_n)$ when:\n(i) $n_r\\geq 2$, (ii) $r=4$ and $n_r=1$, and (iii) $r=5$, $n_r=1$, $p_1\\geq 3$.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Thermodynamics with internal variables is a common approach in continuum\nmechanics to model inelastic (i.e., non-equilibrium) material behavior. While\nthis approach is computationally and theoretically attractive, it currently\nlacks a well-established statistical mechanics foundation. As a result,\ninternal variables are typically chosen phenomenologically and lack a direct\nlink to the underlying physics which hinders the predictability of the theory.\nTo address these challenges, we propose a machine learning approach that is\nconsistent with the principles of statistical mechanics and thermodynamics. The\nproposed approach leverages the following techniques (i) the information\nbottleneck (IB) method to ensure that the learned internal variables are\nfunctions of the microstates and are capable of capturing the salient feature\nof the microscopic distribution; (ii) conditional normalizing flows to\nrepresent arbitrary probability distributions of the microscopic states as\nfunctions of the state variables; and (iii) Variational Onsager Neural Networks\n(VONNs) to guarantee thermodynamic consistency and Markovianity of the learned\nevolution equations. The resulting framework, called IB-VONNs, is tested on two\nproblems of colloidal systems, governed at the microscale by overdamped\nLangevin dynamics. The first one is a prototypical model for a colloidal\nparticle in an optical trap, which can be solved analytically, and thus ideal\nto verify the framework. The second problem is a one-dimensional\nphase-transforming system, whose macroscopic description still lacks a\nstatistical mechanics foundation under general conditions. The results in both\ncases indicate that the proposed machine learning strategy can indeed bridge\nstatistical mechanics and thermodynamics with internal variables away from\nequilibrium.",
        "In a mixture of scalar fields undergoing diffusive processes governed by\nFick's law, the concentration at each point evolves linearly in the\nconcentrations at all points and independently from the other concentrations,\nwhen one considers a finite differences integration of their evolution\nequations. However, these properties must not necessarily be enforced in\nprobability density function models, since they are relaxed when conditional\nexpected values are taken.",
        "A definable set $X$ in the first-order language of rings defines a family of\nrandom vectors: for each finite field $\\mathbb{F}_q$, let the distribution be\nsupported and uniform on the $\\mathbb{F}_q$-rational points of $X$. We employ\nresults from the model theory of finite fields to show that their entropy\nprofiles settle into one of finitely many stable asymptotic behaviors as $q$\ngrows. The attainable asymptotic entropy profiles and their dominant terms as\nfunctions of $q$ are computable. This generalizes a construction of Mat\\'u\\v{s}\nwhich gives an information-theoretic interpretation to algebraic matroids.",
        "A functional $k$-batch code of dimension $s$ consists of $n$ servers storing\nlinear combinations of $s$ linearly independent information bits. These codes\nare designed to recover any multiset of $k$ requests, each being a linear\ncombination of the information bits, by $k$ disjoint subsets of servers. A\nrecent conjecture suggests that for any set of $k = 2^{s-1}$ requests, the\noptimal solution requires $2^s-1$ servers. This paper shows that the problem of\nfunctional $k$-batch codes is equivalent to several other problems. Using these\nequivalences, we derive sufficient conditions that improve understanding of the\nproblem and enhance the ability to find the optimal solution.",
        "A commutative monoid is atomic if every non-invertible element factors into\nirreducibles (also called atoms), while an integral (semi)domain is atomic if\nits multiplicative monoid is atomic. Notions weaker than atomicity have been\nintroduced and studied during the past decade, including almost atomicity and\nquasi-atomicity, which were coined and first investigated by Boynton and\nCoykendall in their study of graphs of divisibility of integral domains. The\nascent of atomicity to polynomial extensions was settled by Roitman back in\n1993 while the ascent of atomicity to monoid domains was settled by Coykendall\nand the second author in 2019 (in both cases the answer was negative). The main\npurpose of this paper is to study the ascent of almost atomicity and\nquasi-atomicity to polynomial extensions and monoid domains. Under certain\nreasonable conditions, we establish the ascent of both properties to polynomial\nextensions (over semidomains). Then we construct an explicit example\nillustrating that, with no extra conditions, quasi-atomicity does not ascend to\npolynomial extensions. Finally, we show that, in general, neither almost\natomicity nor quasi-atomicity ascend to monoid domains, improving upon a\nconstruction first provided by Coykendall and the second author for the\nnon-ascent of atomicity.",
        "We establish a formula for structure constants of the quantum mirror to a log\nCalabi Yau surface $(Y,D)$ in terms of descendent logarithmic Gromov--Witten\ninvariants of $(Y,D)$. Our result generalises the weak Frobenius structure\nconjecture for surfaces to the $q$-refined setting, and is proved by relating\nthese invariants to counts of quantum broken lines in the associated quantum\nscattering diagram.",
        "Interactive decision-making is essential in applications such as autonomous\ndriving, where the agent must infer the behavior of nearby human drivers while\nplanning in real-time. Traditional predict-then-act frameworks are often\ninsufficient or inefficient because accurate inference of human behavior\nrequires a continuous interaction rather than isolated prediction. To address\nthis, we propose an active learning framework in which we rigorously derive\npredicted belief distributions. Additionally, we introduce a novel model-based\ndiffusion solver tailored for online receding horizon control problems,\ndemonstrated through a complex, non-convex highway merging scenario. Our\napproach extends previous high-fidelity dual control simulations to hardware\nexperiments, which may be viewed at https:\/\/youtu.be\/Q_JdZuopGL4, and verifies\nbehavior inference in human-driven traffic scenarios, moving beyond idealized\nmodels. The results show improvements in adaptive planning under uncertainty,\nadvancing the field of interactive decision-making for real-world applications.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "Fairness of machine learning algorithms is receiving increasing attention, as\nsuch algorithms permeate the day-to-day aspects of our lives. One way in which\nbias can manifest in a dataset is through missing values. If data are missing,\nthese data are often assumed to be missing completely randomly; in reality the\npropensity of data being missing is often tied to the demographic\ncharacteristics of individuals. There is limited research into how missing\nvalues and the handling thereof can impact the fairness of an algorithm. Most\nresearchers either apply listwise deletion or tend to use the simpler methods\nof imputation (e.g. mean or mode) compared to the more advanced ones (e.g.\nmultiple imputation); we therefore study the impact of the simpler methods on\nthe fairness of algorithms. The starting point of the study is the mechanism of\nmissingness, leading into how the missing data are processed and finally how\nthis impacts fairness. Three popular datasets in the field of fairness are\namputed in a simulation study. The results show that under certain scenarios\nthe impact on fairness can be pronounced when the missingness mechanism is\nmissing at random. Furthermore, elementary missing data handling techniques\nlike listwise deletion and mode imputation can lead to higher fairness compared\nto more complex imputation methods like k-nearest neighbour imputation, albeit\noften at the cost of lower accuracy.",
        "Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional\nsurveillance mechanism that degrades transcripts with premature termination\ncodons, safeguarding transcriptome integrity and shaping disease phenotypes.\nHowever, accurately predicting NMD efficiency remains challenging, as existing\nmodels often rely on simplistic rule-based heuristics or limited feature sets,\nconstraining their accuracy and generalizability. Using paired DNA and RNA data\nfrom The Cancer Genome Atlas, we benchmark embedding-only models and\ndemonstrate that they underperform compared to a simple rule-based approach. To\naddress this, we develop NMDEP (NMD Efficiency Predictor), an integrative\nframework that combines optimized rule-based methods, sequence embeddings, and\ncurated biological features, achieving state-of-the-art predictive performance.\nThrough explainable AI, we identify key NMD determinants, reaffirming\nestablished factors such as variant position while uncovering novel\ncontributors like ribosome loading. Applied to over 2.9 million simulated\nstop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments,\nadvancing variant interpretation and disease research.",
        "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps:\/\/github.com\/m4opt\/m4opt.",
        "Using the expansion of Zubarev's density operator, we develop a linear\nresponse approach to study various spin physics in a locally equilibrated\nmedium, particularly focusing on various polarization phenomena in heavy-ion\ncollisions. Specifically, we connect familiar correlation functions and\ndiagrammatic methods to the Zubarev formalism, enabling the use of established\ntechniques like the Matsubara\/imaginary time formalism to facilitate\ncalculations. For a spin-1\/2 particle, we re-derive its vector polarization\nusing this Zubarev response approach, which exactly reproduces with our\nprevious results based on Luttinger's method. For a spin-1 particle, we\ncalculate the vector polarization and find the expected contributions from\nvorticity, temperature gradients, and shear, which are identical to those for\nspin-1\/2 particles except for a factor of 4\/3 as expected. For the tensor\npolarization and spin alignment of a spin-1 boson, we explicitly prove that the\nnon-dissipative contribution is zero at leading order in gradients, and briefly\nreiterate our previous findings for the dissipative contribution with further\ndiscussions on several concerns. Additionally, we discuss several relevant\nsubtleties and questions, including an alternative derivation for Zubarev\nresponse approach, the covariance issues of different spin density matrix\ndefinitions, a further explanation of slow and fast modes, the mode selection\nscheme, etc. We also discuss skeleton expansions, higher-order contributions,\nand non-perturbative methods, particularly their potential connection to\nlattice field theory. In summary, this work discusses the foundations and\nsubtleties of Zubarev response approach, with specific examples from spin\nphysics in heavy-ion collisions.",
        "A graph $G$ is perfectly divisible if, for every induced subgraph $H$ of $G$,\neither $V(H)$ is a stable set or admits a partition into two sets $X_1$ and\n$X_2$ such that $\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is a perfect graph.\nIn this article, we propose the following generalisation of perfectly divisible\ngraphs. A graph $G$ is perfectly $1$-divisible if $G$ is perfect and perfectly\n$k$-divisible if, for every induced subgraph $H$ of $G$, either $V(H)$ is a\nstable set or admits a partition into two sets $X_1$ and $X_2$ such that\n$\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is perfectly $(k-1)$-divisible, $k\n\\in \\mathbb{N}_{> 1}$. Our main result establishes that every perfectly\n$k$-divisible graph $G$ satisfies $\\chi(G) \\leq \\binom{\\omega(G)+k-1}{k}$ which\ngeneralises the known bound for perfectly divisible graphs.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "Micro-electro-mechanical resonators employing a magnetic element have been\nproposed for magnetic field sensing applications, but the integration of\nmagnetic materials with standard semiconductor compounds is challenging and\nrequires complex fabrication protocols. We present a different approach relying\non (La0.7,Sr0.3)MnO3 (LSMO), an oxide compound that works both as structural\nelement for the resonator and functional magnetic layer. Suspended trampolines\nare realized in a single step process from LSMO thin films and show quality\nfactor up to 60k and fQ products reaching 10$^{10}$ Hz. Their magnetic\nproperties are probed by a SQUID magnetometer and magnetic force microscopy,\nshowing saturation magnetization of 240 kA\/m at room temperature and in-plane\nmagnetic domains with coercivity of 2.5 mT. Being entirely made from a magnetic\nmaterial, these resonators exhibit a larger magnetic interaction volume\ncompared to other solutions, making them ideal candidates as building blocks\nfor high-sensitivity magnetic field sensors.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases.",
        "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing.",
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "For a Hermitian matrix $A$ of order $n$ with eigenvalues $\\lambda_1(A)\\ge\n\\cdots\\ge \\lambda_n(A)$, define \\[ \\mathcal{E}_p^+(A)=\\sum_{\\lambda_i > 0}\n\\lambda_i^p(A), \\quad \\mathcal{E}_p^-(A)=\\sum_{\\lambda_i<0} |\\lambda_i(A)|^p,\\]\nto be the positive and the negative $p$-energy of $A$, respectively. In this\nnote, first we show that if $A=[A_{ij}]_{i,j=1}^k$, where $A_{ii}$ are square\nmatrices, then \\[ \\mathcal{E}_p^+(A)\\geq \\sum_{i=1}^{k}\n\\mathcal{E}_p^+(A_{ii}), \\quad \\mathcal{E}_p^-(A)\\geq \\sum_{i=1}^{k}\n\\mathcal{E}_p^-(A_{ii}),\\] for any real number $p\\geq 1$. We then apply the\nprevious inequality to establish lower bounds for $p$-energy of the adjacency\nmatrix of graphs.",
        "This article provides an introduction to nuclear magnetic resonance\nspectroscopy in pulsed magnetic fields (PFNMR), focusing on its capabilities,\napplications, and future developments in research involving high magnetic\nfields. It highlights the significance of PFNMR in enhancing the understanding\nof solid-state materials, with particular emphasis on those exhibiting complex\ninteractions and strong electronic correlations. Several technical aspects are\ndiscussed, including the challenges associated with high-frequency NMR\nexperiments. The power of PFNMR is showcased through several examples,\nincluding studies on the topical materials LiCuVO$_4$, SrCu$_2$(BO$_3$)$_2$,\nand CeIn$_3$, offering insights into their magnetic and electronic properties\nat high magnetic fields. The article also discusses possible future directions\nfor the technique, including improvements in PFNMR instrumentation and the\nexploration of materials under extreme conditions. This exposition underscores\nthe role of PFNMR in advancing the frontiers of materials-science research.",
        "Fast radio bursts (FRBs) are energetic, short-duration radio pulses of\nunclear origins. In this study, we investigate the FRBs and pulsars with broad\nenergy distributions by fitting their high energy tails with a power-law model.\nTwo cosmological repeating FRBs (FRB 20201124A and FRB 20220912A), one nearby\nFRB (FRB 20200120E), and two pulsars (RRATs J1846-0257 and J1854+0306), exhibit\npower-law indices of $\\alpha \\gtrsim -1$, suggesting that their bright pulses\ncontribute significantly to the total radio pulse energy. The brightest bursts\nfrom these sources fit well with a simple power-law model ($\\alpha = -0.26 \\pm\n0.05$), indicating a tentative link between certain high-luminosity FRBs and\nlow-luminosity radio bursts. We also discuss detailed survey strategies for\nFAST, MeerKAT and Parkes cryoPAF in the search for FRBs in nearby globular\nclusters (GCs) using different power-law indices, recommending targets for\nobservation. We suggest that combining observations with FAST ($\\sim$ 3 hours)\nand Parkes cryoPAF (10-20 hours) are practicable for discovering new FRBs in\nthe nearby GCs.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "In this paper, we propose and analyze a second order accurate (in both time\nand space) numerical scheme for the Poisson-Nernst-Planck-Navier-Stokes system,\nwhich describes the ion electro-diffusion in fluids. In particular, the\nPoisson-Nernst-Planck equation is reformulated as a non-constant mobility\ngradient flow in the Energetic Variational Approach. The marker and cell finite\ndifference method is chosen as the spatial discretization, which facilitates\nthe analysis for the fluid part. In the temporal discretization, the mobility\nfunction is computed by a second order extrapolation formula for the sake of\nunique solvability analysis, while a modified Crank-Nicolson approximation is\napplied to the singular logarithmic nonlinear term. Nonlinear artificial\nregularization terms are added in the chemical potential part, so that the\npositivity-preserving property could be theoretically proved. Meanwhile, a\nsecond order accurate, semi-implicit approximation is applied to the convective\nterm in the PNP evolutionary equation, and the fluid momentum equation is\nsimilarly computed. In addition, an optimal rate convergence analysis is\nprovided, based on the higher order asymptotic expansion for the numerical\nsolution, the rough and refined error estimate techniques. The following\ncombined theoretical properties have been established for the second order\naccurate numerical method: (i) second order accuracy, (ii) unique solvability\nand positivity, (iii) total energy stability, and (iv) optimal rate\nconvergence. A few numerical results are displayed to validate the theoretical\nanalysis.",
        "We revisit the one-parameter generalization of the C-metric derived by Ernst,\nwhich solves the vacuum Einstein equations. Resolving conflicting claims in the\nliterature, we determine the correct value of the parameter that ensures the\nregularity of the metric on the axis. This \"regularized C-metric\" describes a\npair of accelerating black holes without the line source present in the\noriginal C-metric. Additionally, this generalization changes the Petrov type\nfrom D to I. We use the Gauss-Bonnet theorem to analyze the nodal\nsingularities, the line source, and their relation to the horizon topology.\nBoth the black hole and acceleration horizons are found to be embeddable in\n$\\mathrm{E}^3$. We examine various geometric and asymptotic properties in\ndetail using several coordinate systems and construct the corresponding 2D and\n3D conformal diagrams. This process is more involved than for the original\nC-metric due to the presence of the exponential factors. These exponential\nfactors also introduce curvature singularities at infinity, which obstructs\nasymptotic flatness. Contrary to Bonnor's expectation, we demonstrate why\nBondi's algorithm for obtaining the standard Bondi form fails for the C-metric,\ndespite its asymptotic flatness. We also show that Ernst's solution-generating\nprescription in boost-rotation symmetric coordinates is a symmetry of the wave\nequation.",
        "We revisit the problem of constructing predictive confidence sets for which\nwe wish to obtain some type of conditional validity. We provide new arguments\nshowing how ``split conformal'' methods achieve near desired coverage levels\nwith high probability, a guarantee conditional on the validation data rather\nthan marginal over it. In addition, we directly consider (approximate)\nconditional coverage, where, e.g., conditional on a covariate $X$ belonging to\nsome group of interest, we would like a guarantee that a predictive set covers\nthe true outcome $Y$. We show that the natural method of performing quantile\nregression on a held-out (validation) dataset yields minimax optimal guarantees\nof coverage here. Complementing these positive results, we also provide\nexperimental evidence that interesting work remains to be done to develop\ncomputationally efficient but valid predictive inference methods.",
        "In this study, we propose a novel application of spatiotemporal clustering in\nthe environmental sciences, with a particular focus on regionalised time series\nof greenhouse gases (GHGs) emissions from a range of economic sectors.\nUtilising a hierarchical spatiotemporal clustering methodology, we analyse\nyearly time series of emissions by gases and sectors from 1990 to 2022 for\nEuropean regions at the NUTS-2 level. While the clustering algorithm inherently\nincorporates spatial information based on geographical distance, the extent to\nwhich space contributes to the definition of groups still requires further\nexploration. To address this gap in the literature, we propose a novel\nindicator, namely the Joint Inertia, which quantifies the contribution of\nspatial distances when integrated with other features. Through a simulation\nexperiment, we explore the relationship between the Joint Inertia and the\nrelevance of geography in exploiting the groups structure under several\nconfigurations of spatial and features patterns, providing insights into the\nbehaviour and potential of the proposed indicator. The empirical findings\ndemonstrate the relevance of the spatial component in identifying emission\npatterns and dynamics, and the results reveal significant heterogeneity across\nclusters in trends and dynamics by gases and sectors. This reflects the\nheterogeneous economic and industrial characteristics of European regions. The\nstudy highlights the importance of the spatial and temporal dimensions in\nunderstanding GHGs emissions, offering baseline insights for future\nspatiotemporal modelling and supporting more targeted and regionally informed\nenvironmental policies."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
    "start_abstract":"This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals"
      ],
      "abstract":[
        "Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli)."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Anomalous Dynamics of a Liquid Corner Film",
        "HST Observations within the Sphere of Influence of the Powerful\n  Supermassive Black Hole in PKS0745-191",
        "Global bifurcations of nodal solutions for coupled elliptic equations",
        "A new convection scheme for GCMs of temperate sub-Neptunes",
        "Singularity of compound stationary measures",
        "The Stability and Accuracy of The Adams-Bashforth-type Integrator",
        "Characterizing Continuous Gravitational Waves from Supermassive Black\n  Hole Binaries in Realistic Pulsar Timing Array Data",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Theory of quantum-geometric charge and spin Josephson diode effects in\n  strongly spin-polarized hybrid structures with noncoplanar spin textures",
        "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Indirect reciprocity as a dynamics for weak balance",
        "Simple games with minimum",
        "Fully viable DHOST bounce with extra scalar",
        "Thermodynamic properties and Joule-Thomson expansion of AdS black hole\n  with Gaussian distribution in non-commutative geometry",
        "Goldstone bosons at non-zero temperature",
        "Dirac-type condition for Hamilton-generated graphs",
        "Prescribed-Time Newton Extremum Seeking using Delays and Time-Periodic\n  Gains",
        "Metrizability and Dynamics of Weil Bundles",
        "A formula of Perrin-Riou and characteristic power series of signed\n  Selmer groups",
        "A physical model approach to order lot sizing",
        "Intermediate band analysis in Green's functions calculations of\n  quasiparticle interference",
        "Forecasting Constraints on SIGW with Future Pulsar Timing Array\n  Observations",
        "Evolutions of in-medium baryon-baryon scattering cross sections and\n  stiffness of dense nuclear matter from Bayesian analyses of FOPI proton flow\n  excitation functions",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Identification of Genetic Factors Associated with Corpus Callosum\n  Morphology: Conditional Strong Independence Screening for Non-Euclidean\n  Responses",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Characterizing Gaussian quantum processes with Gaussian resources",
        "Behavior of Ising spins and ecological oscillators on dynamically\n  rewired small world networks"
      ],
      "abstract":[
        "Measuring the rheology of liquids typically requires precise control over\nshear rates and stresses. However, we demonstrate that the features of a\npower-law fluid can be predicted by simply observing the capillary spreading\ndynamics of viscous droplets within a wedge-shaped geometry. By considering the\ninfluence of capillary and viscous forces within this geometry, we show that\nthe spreading dynamics can be described by a nonlinear diffusion equation.\nAnalytical predictions indicate subdiffusive behavior, establishing a direct\nrelationship between the diffusion exponent and the rheological exponent, which\nis also corroborated by experimental results. Since this relationship is\nindependent of flow details, it provides robust predictions for the rheological\nproperties of power-law fluids.",
        "We present Space Telescope Imaging Spectrograph observations from the Hubble\nSpace Telescope of the supermassive black hole (SMBH) at the center of\nPKS0745-191, a brightest cluster galaxy (BCG) undergoing powerful radio-mode\nAGN feedback ($P_{\\rm cav}\\sim5\\times10^{45}$ erg s$^{-1}$). These\nhigh-resolution data offer the first spatially resolved map of gas dynamics\nwithin a SMBHs sphere of influence under such powerful feedback. Our results\nreveal the presence of highly chaotic, non-rotational ionized gas flows on\nsub-kpc scales, in contrast to the more coherent flows observed on larger\nscales. While radio-mode feedback effectively thermalizes hot gas in galaxy\nclusters on kiloparsec scales, within the core, the hot gas flow may decouple,\nleading to a reduction in angular momentum and supplying ionized gas through\ncooling, which could enhance accretion onto the SMBH. This process could, in\nturn, lead to a self-regulating feedback loop. Compared to other BCGs with\nweaker radio-mode feedback, where rotation is more stable, intense feedback may\nlead to more chaotic flows, indicating a stronger coupling between jet activity\nand gas dynamics. Additionally, we observe a sharp increase in velocity\ndispersion near the nucleus, consistent with a very massive $M_{\\rm\nBH}\\sim1.5\\times10^{10} M_\\odot$ SMBH. The density profile of the ionized gas\nis also notably flat, paralleling the profiles observed in X-ray gas around\ngalaxies where the Bondi radius is resolved. These results provide valuable\ninsights into the complex mechanisms driving galaxy evolution, highlighting the\nintricate relationship between SMBH fueling and AGN feedback within the host\ngalaxy.",
        "We investigate the global bifurcation structure of the radial nodal solutions\nto the coupled elliptic equations \\begin{equation}\n  \\left\\{\n  \\begin{array}{lr}\n  -{\\Delta}u+u=u^3+\\beta uv^2\\mbox{ in }B_1 ,\\nonumber\n  -{\\Delta}v+v=v^3+\\beta u^2v\\mbox{ in }B_1 ,\\nonumber\n  u,v\\in H_{0,r}^1(B_1).\\nonumber\n  \\end{array}\n  \\right. \\end{equation} Here $B_1$ is a unit ball in $\\mathbb{R}^3$ and\n$\\beta\\in\\mathbb{R}$ the coupling constant is used as bifurcation parameter.\nFor each $k$, the unique pair of nodal solutions $\\pm w_k$ with exactly $k-1$\nzeroes to the scalar field equation $-\\Delta w + w=w^3$ generate exactly four\nsynchronized solution curves and exactly four semi-trivial solution curves to\nthe above system. We obtain a fairly complete global bifurcation structure of\nall bifurcating branches emanating from these eight solution curves of the\nsystem, and show that for different $k$ these bifurcation structures are\ndisjoint. We obtain exact and distinct nodal information for each of the\nbifurcating branches, thus providing a fairly complete characterization of\nnodal solutions of the system in terms of the coupling.",
        "Atmospheric characterisation of temperate sub-Neptunes is the new frontier of\nexoplanetary science with recent JWST observations of possible Hycean world\nK2-18b. Accurate modelling of atmospheric processes is essential to\ninterpreting high-precision spectroscopic data given the wide range of possible\nconditions in the sub-Neptune regime, including on potentially habitable\nplanets. Notably, convection is an important process which can operate in\ndifferent modes across sub-Neptune conditions. Convection can act very\ndifferently in atmospheres with a high condensible mass fraction (non-dilute\natmospheres) or with a lighter background gas, e.g. water convection in a\nH$_2$-rich atmosphere, and can be much weaker or even shut down entirely in the\nlatter case. We present a new mass-flux scheme which can capture these\nvariations and simulate convection over a wide range of parameter space for use\nin 3D general circulation models (GCMs). We validate our scheme for two\nrepresentative cases, a terrestrial-like atmosphere and a mini-Neptune\natmosphere. In the terrestrial case, considering TRAPPIST-1e with an Earth-like\natmosphere, the model performs near-identically to Earth-tuned models in an\nEarth-like convection case. In the mini-Neptune case, considering the bulk\nproperties of K2-18b and assuming a deep H$_2$-rich atmosphere, we demonstrate\nthe capability of the scheme to reproduce non-condensing convection. We find\nconvection occurring at pressures greater than 0.3 bar and the dynamical\nstructure shows high-latitude prograde jets. Our convection scheme will aid in\nthe 3D climate modelling of a wide range of exoplanet atmospheres, and enable\nfurther exploration of temperate sub-Neptune atmospheres.",
        "We show that the product or convex combination of two Markov operators with\nequivalent stationary measures need not have a stationary measure from the same\nmeasure class. More specifically, we exhibit examples of a hitherto undescribed\nphenomenon: maximal entropy random walks for which the resulting compound\nrandom walks no longer have maximal entropy. The underlying group in these\nexamples is $PSL(2,\\mathbb Z)\\cong{{\\mathbb Z}_2}*{{\\mathbb Z}_3}$, and the\nassociated harmonic measures belong to the canonical Minkowski and Denjoy\nmeasure classes on the boundary. These examples also demonstrate that a number\nof other natural families of random walks are not closed under convolutions or\nconvex combinations of step distributions.",
        "This paper presents stability and accuracy analysis of a high-order explicit\ntime stepping scheme introduced by \\cite[Section 2.2]{Buvoli2019}, which\nexhibits superior stability compared to classical Adams-Bashforth. A conjecture\nthat is supported by several numerical phenomena in \\cite[Figure\n2.5]{Buvoli2018}, the method appears to remain stable when the accuracy\napproaches infinity, although it is not yet proven. It is regrettable that this\nhypothesis has been refuted from a fundamental perspective in harmonic\nanalysis. Notwithstanding the aforementioned, this method displays considerably\nenhanced stability in comparison to conventional explicit schemes. Furthermore,\nwe present a criterion for ascertaining the maximum permissible accuracy for a\ngiven specific parabolic stability radius. Conversely, the original method will\nlose one order associated with the expected accuracy, which can be recovered\nwith a slight modification. Consequently, a unified analysis strategy for the\n\\( L^2 \\)-stability will be presented for extensional PDEs under the CFL\ncondition. Finally, a selection of representative numerical examples will be\nshown in order to substantiate the theoretical analysis.",
        "Pulsar timing arrays recently found evidence for a gravitational wave\nbackground (GWB), likely the stochastic overlap of GWs from many supermassive\nblack hole binaries. Anticipating a continuous gravitational wave (CW)\ndetection from a single binary soon to follow, we examine how well current\nBayesian methods can detect CWs and characterize their binary properties by\nmodeling the response of the NANOGrav 15-year pulsar timing array to simulated\nbinary populations. We run Markov Chain Monte Carlo searches for CWs in these\ndatasets and compare them to quicker detection statistics including the optimal\nsignal-to-noise ratio, matched filter detection statistic, and reduced\nlog-likelihood ratio between the signal and noise models calculated at the\ninjected parameters. The latter is the best proxy for Bayesian detection\nfractions, corresponding to a 50% detection fraction (by Bayes factors >10\nfavoring a CW detection over noise-only model) at a signal-to-noise ratio of\n4.6. Source confusion between the GWB and a CW, or between multiple CWs, can\ncause false detections and unexpected dismissals. 53% of realistic binary\npopulations consistent with the recently observed GWB have successful CW\ndetections. 82% of these CWs are in the 4th or 5th frequency bin of the 16.03\nyr dataset (6.9 nHz and 10.8 nHz), with 95 percentile regions spanning\n4nHz-12nHz frequencies, $7-20\\times10^9 M_\\odot$ chirp masses, 60Mpc-8Gpc\nluminosity distances, and 18-13,000 sq. deg 68% confidence localization areas.\nThese successful detections often poorly recover the chirp mass, with only 29%\nidentifying the chirp mass accurately to within 1 dex with a 68% posterior\nwidth also narrower than 1 dex.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We present a systematic study of the spin-resolved Josephson diode effect\n(JDE) in strongly spin-polarized ferromagnets (sFM) coupled to singlet\nsuperconductors (SC) via ferromagnetic insulating interfaces (FI). All metallic\nparts are described in the framework of the quasiclassical Usadel Green's\nfunction theory applicable to diffusive systems. The interfaces are\ncharacterized by an S-matrix obtained for a model potential with exchange\nvectors pointing in an arbitrary direction with respect to the magnetization in\nthe sFM. Our theory predicts a large charge Josephson diode effect with an\nefficiency exceeding $33\\%$ and a perfect spin diode effect with $100\\%$\nefficiency. To achieve these the following conditions are necessary: (i) a\nnoncoplanar profile of the three magnetization vectors in the system and (ii)\ndifferent densities of states of spin-$\\uparrow$ and spin-$\\downarrow$ bands in\nthe sFM achieved by a strong spin polarization. The former gives rise to the\nquantum-geometric phase, $\\Delta\\varphi$, that enters the theory in a very\nsimilar manner as the superconducting phase difference across the junction,\n$\\Delta\\chi$. We perform a harmonic analysis of the Josephson current in both\nvariables and find symmetries between Fourier coefficients allowing an\ninterpretation in terms of transfer processes of multiple equal-spin Cooper\npairs across the two ferromagnetic spin bands. We point out the importance of\ncrossed pair transmission processes. Finally, we study a spin-switching effect\nof an equal-spin supercurrent by reversing the magnetic flux in a SQUID device\nincorporating the mentioned junction and propose a way for measuring it.",
        "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} \/ 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z\/(1 +\nz) = 0.14$.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "A social network is often divided into many factions. People are friends\nwithin each faction, while they are enemies of the other factions, and even my\nenemy's enemy is not necessarily my friend. This configuration can be described\nin terms of a weak form of structural balance. Although weak balance explains a\nnumber of real social networks, which dynamical rule achieves it has remained\nunexplored. In this work, we show that the answer can be found in the field of\nindirect reciprocity, which assumes that people assess each other's behavior\nand choose how to behave to others based on the assessment according to a\nsocial norm. We begin by showing that weak structural balance is equivalent to\nstationarity when the rule is given by a norm called `judging'. By analyzing\nits cluster dynamics of merging, division, and migration induced by assessment\nerror in complete graphs, we obtain the cluster size distribution in a steady\nstate, which shows the coexistence of a giant cluster and smaller ones. We\ncompare this shape with the distributions of seats among the parties in the\nparliaments of Germany, the United Kingdom, and Spain. This study suggests that\nindirect reciprocity can provide insight into the interplay between a norm that\nindividuals abide by and the macroscopic group structure in society.",
        "Every simple game is a monotone Boolean function. For the other direction we\njust have to exclude the two constant functions. The enumeration of monotone\nBoolean functions with distinguishable variables is also known as the\nDedekind's problem. The corresponding number for nine variables was determined\njust recently by two disjoint research groups. Considering permutations of the\nvariables as symmetries we can also speak about non-equivalent monotone Boolean\nfunctions (or simple games). Here we consider simple games with minimum, i.e.,\nsimple games with a unique minimal winning vector. A closed formula for the\nnumber of such games is found as well as its dimension in terms of the number\nof players and equivalence classes of players.",
        "In this paper we construct a class of Degenerate Higher-Order Scalar-Tensor\n(DHOST) theories with an extra scalar field, which admits viable solutions of\nbouncing universe satisfying the following requirements: (i) absence of\nBelinski-Khalatnikov-Lifshitz (BKL) instability, ghost and gradient\ninstability, (ii) absence of superluminality, (iii) generation of nearly\nscale-invariant curvature perturbations and very small tensor-to-scalar ratio,\nand (iv) conventional asymptotics in the distant past and future, where gravity\nsector is described by General Relativity and the DHOST scalar has a canonical\nform of Lagrangian. We also expect our models to have sufficiently small\nnon-Gaussianities of primordial curvature perturbations to be compatible with\nobservations. As such, this work exemplifies for the first time the fully\nviable two-field DHOST bouncing cosmology, which is free of instability and\nsuperluminality problems as well as compatible with observations.",
        "The thermodynamics and Joule-Thomson expansion of anti-de Sitter black hole\n(AdS BH) with Gaussian distribution in non-commutative geometry is\nsystematically studied. The metric of Gaussian-distributed BH is obtained,\nshowing a dS geometry at the core of BH. The research indicates that the BH\ncharacterized by a Gaussian distribution exhibit thermodynamic properties that\nare remarkably similar to those of BH with a Lorentzian distribution in\nnon-commutative geometry. This similarity is specifically manifested in the\nsmall BH-large BH phase transition, the corrected first law of thermodynamics,\nthe criticality, the heat capacity, the zeroth-order phase transition and the\nJoule-Thomson process. Notably, the critical ratio of Gaussian-distributed BH\n(0.46531) is significantly larger than those observed in Van der Waals fluids\n(0.375), and indeed, it is also substantially exceed those of\nLorentzian-distributed BH (0.36671). Moreover, compared to the case of\nLorentzian source, the zeroth-order phase transition effect in\nGaussian-distributed BH is exceedingly subtle (accompanied by a relative\nincrease in the Gibbs free energy on the order of $10^{-3}\\!\\sim\\!\\!10^{-2}$)\nand is difficult to detect distinctly.",
        "Spontaneous symmetry breaking in quantum field theories at non-zero\ntemperature still holds fundamental open questions, in particular what happens\nto vacuum Goldstone bosons when the temperature is increased. By investigating\na complex scalar field theory on the lattice we demonstrate that Goldstone\nbosons at non-zero temperature behave like screened massless particle-like\nexcitations, so-called thermoparticles, which continue to exist even in the\nsymmetry-restored phase of the theory. We provide non-perturbative evidence for\nthe functional form of the Goldstone mode's dissipative behaviour, which is\ndistinct from standard perturbative expectations, and determine its\ncorresponding spectral properties.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "We study prescribed-time extremum seeking (ES) for scalar maps in the\npresence of time delay. The problem has been solved by Yilmaz and Krstic using\nchirpy probing and time-varying singular gains. To alleviate the gain\nsingularity, we present an alternative approach, employing delays with bounded\ntime-periodic gains, for achieving prescribed-time convergence to the extremum.\nOur results are not extensions or refinements but a new methodological\ndirection, even in the absence of the delay on the map. The main result we\npresent compensates the map's delay and uses perturbation-based and the Newton\n(rather than gradient) approaches. The simultaneous presence of perturbation\nperiod, and two delays -- a map delay and a seeking feedback delay -- whose\nvalues are different (feedback delay must be longer than map delay), makes for\nan intricate situation in the design and analysis.\n  ES can settle arbitrarily soon after four times the map delay. In the absence\nof a map delay, the settling time is arbitrarily short, with feedback delay\nchosen as one quarter of the prescribed settling time, i.e., the search settles\nafter four times any positive feedback delay. In addition to removing the gain\nsingularity of the Yilmaz-Krstic singular-gain prescribed-time ES, we go beyond\nthat method's limitation to operating only up to the terminal time. With the\nhelp of averaging theorems in infinite dimension, we conduct a prescribed-time\nconvergence analysis on a suitable perturbation-averaged \\textit{target} ES\nsystem, which contains the time-periodic gains of the map and feedback delays.\nSince the notion of ``dead-beat'' Lyapunov stabilization by time-periodic\ndelayed feedback originates from Hale and Verduyn-Lunel (analysis, 1993) and\nKarafyllis (feedback design, 2006), we refer to our approach to prescribed-time\nES as the ``Karafyllis, Hale, Verduyn-Lunel\" (KHV) PT-ES approach.",
        "This paper bridges synthetic and classical differential geometry by\ninvestigating the metrizability and dynamics of Weil bundles. For a smooth,\ncompact manifold \\(M\\) and a Weil algebra \\(\\mathbf{A}\\), we prove that the\nmanifold \\(M^\\mathbf{A}\\) of \\(\\mathbf{A}\\)-points admits a canonical,\ncomplete, weighted metric \\(\\mathfrak{d}_w\\) that encodes both base-manifold\ngeometry and infinitesimal deformations. Key results include: (1) Metrization:\n\\(\\mathfrak{d}_w\\) induces a complete metric topology on \\(M^\\mathbf{A}\\). (2)\nPath Lifting: Curves lift from \\(M\\) to \\(M^\\mathbf{A}\\) while preserving\ntopological invariants. (3) Dynamics: Fixed-point theorems for diffeomorphisms\non \\(M^\\mathbf{A}\\) connected to stability analysis. (4) Topological\nEquivalence: \\(H^*(M^\\mathbf{A}) \\cong H^*(M)\\) and \\(\\pi_\\ast(M^\\mathbf{A})\n\\cong \\pi_\\ast(M)\\).",
        "We prove a conjecture of Kundu--Ray, following from the $p$-adic\nBirch--Swinnerton-Dyer conjecture for supersingular primes by\nBernardi--Perrin-Riou and Kato's Main Conjecture, predicting an expression for\nthe leading term (up to a $p$-adic unit) of a characteristic power series of\nKobayashi's signed Selmer groups attached to elliptic curves $E\/\\mathbb{Q}$\nwith supersingular reduction at a prime $p>2$ with $a_p=0$. The proof is\ndeduced from a similar formula due to Perrin-Riou for a generator of her module\nof arithmetic $p$-adic $L$-functions with values in the Dieudonn\\'{e} module of\n$E$.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies.",
        "The measurement of quasiparticle scattering patterns on material surfaces\nusing scanning tunneling microscopy (STM) is now an established technique for\naccessing the momentum-resolved electronic band structure of solids. However,\nsince these quasiparticle interference (QPI) patterns reflect spatial\nvariations related to differences in the band momenta rather than the momenta\nthemselves, their interpretation often relies on comparisons with simple\ngeometrical models such as the joint density of states (JDOS) or with the\nconvolution of Green's functions. In this paper, we highlight non-intuitive\ndifferences between Green's function and JDOS results. To understand the origin\nof these discrepancies, we analyze the convolution of Green's functions using\nthe Feynman parametrization technique and introduce a framework that we call\nthe intermediate band analysis. This approach allows us to derive simple\nselection rules for interband QPI, based on electron group velocities.\nConnecting the intermediate band analysis with the experiment, we consider\nexperimental Bogoliubov QPI patterns measured for FeSe1-xSx, which were\nrecently used to demonstrate a highly anisotropic superconducting gap,\nindicating superconductivity mediated by nematic fluctuations [1]. The\ncalculated Green's functions convolutions reproduce the particle-hole asymmetry\nin the intensity of QPI patterns across the Fermi level observed in\nexperiments. Finally, we demonstrate the utility of intermediate band analysis\nin tracing the origin of this asymmetry to a coherence factor effect of the\nsuperconducting state.",
        "Pulsar Timing Arrays are playing a crucial role in the ongoing gravitational\nwave astronomy revolution. The current evidence for a stochastic gravitational\nwave background (SGWB) at nHz frequencies offers an opportunity to discover\ncosmological signals and threatens the observability of other subdominant GWs.\nWe explore prospects to constrain second-order scalar-induced GWs (SIGWs)\nassociated with enhanced curvature perturbations in the primordial universe,\nforecasting realistic future PTA datasets. We assess how the currently observed\nsignal could eventually limit future capabilities to search for GW relics of\nprimordial phenomena and associated phenomenological consequences such as\nprimordial black hole (PBH) formation. Given the sensitivity of PBH abundance\nto spectral parameters, measuring it remains a challenge for realistic signals.\nHowever, future observation could still rule out nearly subsolar mass PBHs\nformed through standard formation scenarios in some cases. Future progress in\nconstraining PBH models is expected to stem from theoretical advancements in\nPBH computations, which should help resolve the tension between different\ncomputational methods. The analysis is based on and extends the Python code\n$\\texttt{fastPTA}$.",
        "Within a Bayesian statistical framework using a Gaussian Process (GP)\nemulator for an isospin-dependent Boltzmann-Uehling-Uhlenbeck (IBUU) transport\nmodel simulator of heavy-ion reactions, we infer from the proton directed and\nelliptical flow in mid-central Au+Au reactions at beam energies from 150 to\n1200 MeV\/nucleon taken by the FOPI Collaboration the posterior probability\ndistribution functions (PDFs) of the in-medium baryon-baryon scattering cross\nsection modification factor $X$ (with respect to their free-space values) and\nthe stiffness parameter $K$ of dense nuclear matter. We find that the most\nprobable value of $X$ evolves from around 0.7 to 1.0 as the beam energy\n$E_{beam}\/A$ increases. On the other hand, the posterior PDF($K$) may have dual\npeaks having roughly the same height or extended shoulders at high $K$ values.\nMore quantitatively, the posterior PDF($K$) changes from having a major peak\naround 220 MeV characterizing a soft EOS in the reaction at $E_{beam}\/A$=150\nMeV to one that peaks around 320 MeV indicating a stiff EOS in the reactions at\n$E_{beam}\/A$ higher than about 600 MeV. The transition from soft to stiff\nhappens in mid-central Au+Au reactions at beam energies around 250 MeV\/nucleon\nin which $K=220$ MeV and $K=320$ MeV are approximately equally probable.\nAltogether, the FOPI proton flow excitation function data indicate a gradual\nhardening of hot and dense nuclear matter as its density and temperature\nincrease in reactions with higher beam energies.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "The corpus callosum, the largest white matter structure in the brain, plays a\ncritical role in interhemispheric communication. Variations in its morphology\nare associated with various neurological and psychological conditions, making\nit a key focus in neurogenetics. Age is known to influence the structure and\nmorphology of the corpus callosum significantly, complicating the\nidentification of specific genetic factors that contribute to its shape and\nsize. We propose a conditional strong independence screening method to address\nthese challenges for ultrahigh-dimensional predictors and non-Euclidean\nresponses. Our approach incorporates prior knowledge, such as age. It\nintroduces a novel concept of conditional metric dependence, quantifying\nnon-linear conditional dependencies among random objects in metric spaces\nwithout relying on predefined models. We apply this framework to identify\ngenetic factors associated with the morphology of the corpus callosum.\nSimulation results demonstrate the efficacy of this method across various\nnon-Euclidean data types, highlighting its potential to drive genetic discovery\nin neuroscience.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "Characterizing quantum processes is indispensable for the implementation of\nany task in quantum information processing. In this paper, we develop an\nefficient method to fully characterize arbitrary Gaussian processes in\ncontinuous-variable quantum systems. This is done by directly obtaining all\nelements of the symplectic matrix that describes the process. Only Gaussian\nresources such as coherent probes and quadrature measurements are needed for\nthis task. The method is efficient, involving only $O(N^2)$ steps to\ncharacterize an $N$-mode system. Further, the method is resilient to uniform\nloss. We simulate this procedure using the Python package Strawberry Fields. We\nobserve that heterodyne measurements outperform homodyne measurements for\nreconstructing Gaussian processes.",
        "Many ecological populations are known to display a cyclic behavior with\nperiod 2. Previous work has shown that when a metapopulation (group of coupled\npopulations) with such dynamics is allowed to interact via nearest neighbor\ndispersal in two dimensions, it undergoes a phase transition from disordered\n(spatially asynchronous) to ordered (spatially synchronous) that falls under\nthe 2-D Ising universality class. While nearest neighbor dispersal may\nsatisfactorily describe how most individuals migrate between habitats, we\nshould expect a small fraction of individuals to venture on a journey to\nfurther locations. We model this behavior by considering ecological oscillators\non dynamically rewired small-world networks, in which at each time step a\nfraction $p$ of the nearest neighbor interactions is replaced by a new\ninteraction with a random node on the network. We measure how this connectivity\nchange affects the critical point for synchronizing ecological oscillators. Our\nresults indicate that increasing the amount of long-range interaction\n(increasing $p$) favors the ordered regime, but the presence of memory in\necological oscillators leads to quantitative differences in how much long-range\ndispersal is needed to order the network, relative to an analogous network of\nIsing spins. We also show that, even for very small values of $p$, the phase\ntransition falls into the mean-field universality class, and argue that\necosystems where dispersal can occasionally happen across the system's length\nscale will display a phase transition in the mean-field universality class."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
    "start_abstract":"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli).",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation"
      ],
      "abstract":[
        "This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Bridging Classical and Modern Approaches to Thales' Theorem",
        "S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time\n  Series Forecasting",
        "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery",
        "Family-wise Error Rate Control with E-values",
        "GRNFormer: A Biologically-Guided Framework for Integrating Gene\n  Regulatory Networks into RNA Foundation Models",
        "Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and\n  Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS",
        "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments",
        "The H\\\"{o}lder regularity of div-curl system with anisotropic\n  coefficients",
        "A Semi-Orthogonal Decomposition Theorem for Weighted Blowups",
        "Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions",
        "Algorithmic Data Minimization for Machine Learning over\n  Internet-of-Things Data Streams",
        "Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference",
        "Medial Axis in Pseudo-Euclidean Spaces",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
        "QLIO: Quantized LiDAR-Inertial Odometry",
        "From an odd arity signature to a Holant dichotomy",
        "What are Models Thinking about? Understanding Large Language Model\n  Hallucinations \"Psychology\" through Model Inner State Analysis",
        "Bootstrapped Reward Shaping",
        "Factor Modelling for Biclustering Large-dimensional Matrix-valued Time\n  Series",
        "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
        "Vulnerability Detection: From Formal Verification to Large Language\n  Models and Hybrid Approaches: A Comprehensive Overview",
        "Local geometry of high-dimensional mixture models: Effective spectral\n  theory and dynamical transitions",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space",
        "Optimal Transport-based Conformal Prediction",
        "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous\n  Sensors via Language Grounding",
        "Inference Computation Scaling for Feature Augmentation in Recommendation\n  Systems",
        "Non-invertible symmetry breaking in a frustration-free spin chain"
      ],
      "abstract":[
        "In this paper, we reconstruct Euclid's theory of similar triangles, as\ndeveloped in Book VI of the \\textit{Elements}, along with its 20th-century\ncounterparts, formulated within the systems of Hilbert, Birkhoff, Borsuk and\nSzmielew, Millman and Parker, as well as Hartshorne. In the final sections, we\npresent recent developments concerning non-Archimedean fields and mechanized\nproofs. Thales' theorem (VI.2) serves as the reference point in our\ncomparisons. It forms the basis of Euclid's system and follows from VI.1 the\nonly proposition within the theory of similar triangles that explicitly applies\nthe definition of proportion. Instead of the ancient proportion, modern systems\nadopt the arithmetic of line segments or real numbers. Accordingly, they adopt\nother propositions from Euclid's Book VI, such as VI.4, VI.6, or VI.9, as a\nbasis. In {\\S}\\,10, we present a system that, while meeting modern criteria of\nrigor, reconstructs Euclid's theory and mimics its deductive structure,\nbeginning with VI.1. This system extends to automated proofs of Euclid's\npropositions from Book VI. Systems relying on real numbers provide the\nfoundation for trigonometry as applied in modern mathematics. In {\\S}\\,9, we\nprove Thales' theorem in geometry over the hyperreal numbers. Just as Hilbert\nmanaged to prove Thales' theorem without referencing the Archimedean axiom, so\ndo we by applying the arithmetic of the non-Archimedean field of hyperreal\nnumbers.",
        "Time series forecasting has recently achieved significant progress with\nmulti-scale models to address the heterogeneity between long and short range\npatterns. Despite their state-of-the-art performance, we identify two potential\nareas for improvement. First, the variates of the multivariate time series are\nprocessed independently. Moreover, the multi-scale (long and short range)\nrepresentations are learned separately by two independent models without\ncommunication. In light of these concerns, we propose State Space Transformer\nwith cross-attention (S2TX). S2TX employs a cross-attention mechanism to\nintegrate a Mamba model for extracting long-range cross-variate context and a\nTransformer model with local window attention to capture short-range\nrepresentations. By cross-attending to the global context, the Transformer\nmodel further facilitates variate-level interactions as well as local\/global\ncommunications. Comprehensive experiments on seven classic long-short range\ntime-series forecasting benchmark datasets demonstrate that S2TX can achieve\nhighly robust SOTA results while maintaining a low memory footprint.",
        "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.",
        "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
        "Foundation models for single-cell RNA sequencing (scRNA-seq) have shown\npromising capabilities in capturing gene expression patterns. However, current\napproaches face critical limitations: they ignore biological prior knowledge\nencoded in gene regulatory relationships and fail to leverage multi-omics\nsignals that could provide complementary regulatory insights. In this paper, we\npropose GRNFormer, a new framework that systematically integrates multi-scale\nGene Regulatory Networks (GRNs) inferred from multi-omics data into RNA\nfoundation model training. Our framework introduces two key innovations. First,\nwe introduce a pipeline for constructing hierarchical GRNs that capture\nregulatory relationships at both cell-type-specific and cell-specific\nresolutions. Second, we design a structure-aware integration framework that\naddresses the information asymmetry in GRNs through two technical advances: (1)\nA graph topological adapter using multi-head cross-attention to weight\nregulatory relationships dynamically, and (2) a novel edge perturbation\nstrategy that perturb GRNs with biologically-informed co-expression links to\naugment graph neural network training. Comprehensive experiments have been\nconducted on three representative downstream tasks across multiple model\narchitectures to demonstrate the effectiveness of GRNFormer. It achieves\nconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$\nincrease in drug response prediction correlation, $9.6\\%$ improvement in\nsingle-cell drug classification AUC, and $1.1\\%$ average gain in gene\nperturbation prediction accuracy.",
        "This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary\nsearch (BS) locking, designed to cover a broad frequency range from 533 MHz to\n4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a\nlinear to a logarithmic function, completing in B+1 cycles, where B represents\nthe digital-to-analog (DAC) resolution controlling the voltage-controlled delay\nline (VCDL). At the start of the BS process, large step sizes can cause\nsignificant bias overshoots, potentially leading to clock failure conditions\n(i.e., clocks fail to propagate through the VCDL). To address this issue, a\ntoggle detector is introduced to monitor clock activity and adjust the binary\nsearch controller. Upon detecting a stalled clock, the controller reverts the\nDAC code to the previous working code and resumes the BS with a reduced step\nsize. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a\nlocking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at\n4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with\na static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter\nof 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit\n(FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.",
        "Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.",
        "This research examines the regularity of weak solutions to the Div-Curl\nsystem with low regularity anisotropic coefficients. The H\\\"older regularity of\nthe Div-Curl system with one anisotropic coefficient was an unresolved problem\nraised by Yin in 2016. We have addressed the open problem, and the findings\nextend to the scenario involving two anisotropic coefficients. We establish the\nH\\\"{o}lder regularity of the solution when the coefficients is H\\\"{o}lder\ncontinuous. Moreover, the degree of H\\\"{o}lder regularity of the solution can\nbe improved if the coefficient has a greater degree of H\\\"{o}lder regularity.",
        "We establish a semi-orthogonal decomposition for the weighted blowup of an\nalgebraic stack along a Koszul-regular weighted centre, generalising the\nclassic result of Orlov. Our approach is based on the work of Bergh-Schn\\\"urer.",
        "Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment\/emotion detection of a given input as a sentence\/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions.",
        "Machine learning can analyze vast amounts of data generated by IoT devices to\nidentify patterns, make predictions, and enable real-time decision-making. By\nprocessing sensor data, machine learning models can optimize processes, improve\nefficiency, and enhance personalized user experiences in smart systems.\nHowever, IoT systems are often deployed in sensitive environments such as\nhouseholds and offices, where they may inadvertently expose identifiable\ninformation, including location, habits, and personal identifiers. This raises\nsignificant privacy concerns, necessitating the application of data\nminimization -- a foundational principle in emerging data regulations, which\nmandates that service providers only collect data that is directly relevant and\nnecessary for a specified purpose. Despite its importance, data minimization\nlacks a precise technical definition in the context of sensor data, where\ncollections of weak signals make it challenging to apply a binary \"relevant and\nnecessary\" rule. This paper provides a technical interpretation of data\nminimization in the context of sensor streams, explores practical methods for\nimplementation, and addresses the challenges involved. Through our approach, we\ndemonstrate that our framework can reduce user identifiability by up to 16.7%\nwhile maintaining accuracy loss below 1%, offering a viable path toward\nprivacy-preserving IoT data processing.",
        "Multi-armed bandits (MABs) are frequently used for online sequential\ndecision-making in applications ranging from recommending personalized content\nto assigning treatments to patients. A recurring challenge in the applicability\nof the classic MAB framework to real-world settings is ignoring\n\\textit{interference}, where a unit's outcome depends on treatment assigned to\nothers. This leads to an exponentially growing action space, rendering standard\napproaches computationally impractical. We study the MAB problem under network\ninterference, where each unit's reward depends on its own treatment and those\nof its neighbors in a given interference graph. We propose a novel algorithm\nthat uses the local structure of the interference graph to minimize regret. We\nderive a graph-dependent upper bound on cumulative regret showing that it\nimproves over prior work. Additionally, we provide the first lower bounds for\nbandits with arbitrary network interference, where each bound involves a\ndistinct structural property of the interference graph. These bounds\ndemonstrate that when the graph is either dense or sparse, our algorithm is\nnearly optimal, with upper and lower bounds that match up to logarithmic\nfactors. We complement our theoretical results with numerical experiments,\nwhich show that our approach outperforms baseline methods.",
        "We investigate the notion of the medial axis for pseudo-Euclidean spaces. For\nmost of the article, we follow the path of Birbrair and Denkowski's article\n\"Medial Axis and Singularities\", checking its feasibility in the new context.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE",
        "LiDAR-Inertial Odometry (LIO) is widely used for autonomous navigation, but\nits deployment on Size, Weight, and Power (SWaP)-constrained platforms remains\nchallenging due to the computational cost of processing dense point clouds.\nConventional LIO frameworks rely on a single onboard processor, leading to\ncomputational bottlenecks and high memory demands, making real-time execution\ndifficult on embedded systems. To address this, we propose QLIO, a\nmulti-processor distributed quantized LIO framework that reduces computational\nload and bandwidth consumption while maintaining localization accuracy. QLIO\nintroduces a quantized state estimation pipeline, where a co-processor\npre-processes LiDAR measurements, compressing point-to-plane residuals before\ntransmitting only essential features to the host processor. Additionally, an\nrQ-vector-based adaptive resampling strategy intelligently selects and\ncompresses key observations, further reducing computational redundancy.\nReal-world evaluations demonstrate that QLIO achieves a 14.1% reduction in\nper-observation residual data while preserving localization accuracy.\nFurthermore, we release an open-source implementation to facilitate further\nresearch and real-world deployment. These results establish QLIO as an\nefficient and scalable solution for real-time autonomous systems operating\nunder computational and bandwidth constraints.",
        "\\textsf{Holant} is an essential framework in the field of counting\ncomplexity. For over fifteen years, researchers have been clarifying the\ncomplexity classification for complex-valued \\textsf{Holant} on the Boolean\ndomain, a challenge that remains unresolved. In this article, we prove a\ncomplexity dichotomy for complex-valued \\textsf{Holant} on Boolean domain when\na non-trivial signature of odd arity exists. This dichotomy is based on the\ndichotomy for \\textsf{\\#EO}, and consequently is an $\\text{FP}^\\text{NP}$ vs.\n\\#P dichotomy as well, stating that each problem is either in\n$\\text{FP}^\\text{NP}$ or \\#P-hard.\n  Furthermore, we establish a generalized version of the decomposition lemma\nfor complex-valued \\textsf{Holant} on Boolean domain. It asserts that each\nsignature can be derived from its tensor product with other signatures, or\nconversely, the problem itself is in $\\text{FP}^\\text{NP}$. We believe that\nthis result is a powerful method for building reductions in complex-valued\n\\textsf{Holant}, as it is also employed as a pivotal technique in the proof of\nthe aforementioned dichotomy in this article.",
        "Large language model (LLM) systems suffer from the models' unstable ability\nto generate valid and factual content, resulting in hallucination generation.\nCurrent hallucination detection methods heavily rely on out-of-model\ninformation sources, such as RAG to assist the detection, thus bringing heavy\nadditional latency. Recently, internal states of LLMs' inference have been\nwidely used in numerous research works, such as prompt injection detection,\netc. Considering the interpretability of LLM internal states and the fact that\nthey do not require external information sources, we introduce such states into\nLLM hallucination detection. In this paper, we systematically analyze different\ninternal states' revealing features during inference forward and\ncomprehensively evaluate their ability in hallucination detection.\nSpecifically, we cut the forward process of a large language model into three\nstages: understanding, query, generation, and extracting the internal state\nfrom these stages. By analyzing these states, we provide a deep understanding\nof why the hallucinated content is generated and what happened in the internal\nstate of the models. Then, we introduce these internal states into\nhallucination detection and conduct comprehensive experiments to discuss the\nadvantages and limitations.",
        "In reinforcement learning, especially in sparse-reward domains, many\nenvironment steps are required to observe reward information. In order to\nincrease the frequency of such observations, \"potential-based reward shaping\"\n(PBRS) has been proposed as a method of providing a more dense reward signal\nwhile leaving the optimal policy invariant. However, the required \"potential\nfunction\" must be carefully designed with task-dependent knowledge to not deter\ntraining performance. In this work, we propose a \"bootstrapped\" method of\nreward shaping, termed BSRS, in which the agent's current estimate of the\nstate-value function acts as the potential function for PBRS. We provide\nconvergence proofs for the tabular setting, give insights into training\ndynamics for deep RL, and show that the proposed method improves training speed\nin the Atari suite.",
        "A novel unsupervised learning method is proposed in this paper for\nbiclustering large-dimensional matrix-valued time series based on an entirely\nnew latent two-way factor structure. Each block cluster is characterized by its\nown row and column cluster-specific factors in addition to some common matrix\nfactors which impact on all the matrix time series. We first estimate the\nglobal loading spaces by projecting the observation matrices onto the row or\ncolumn loading space corresponding to common factors. The loading spaces for\ncluster-specific factors are then further recovered by projecting the\nobservation matrices onto the orthogonal complement space of the estimated\nglobal loading spaces. To identify the latent row\/column clusters\nsimultaneously for matrix-valued time series, we provide a $K$-means algorithm\nbased on the estimated row\/column factor loadings of the cluster-specific weak\nfactors. Theoretically, we derive faster convergence rates for global loading\nmatrices than those of the state-of-the-art methods available in the literature\nunder mild conditions. We also propose an one-pass eigenvalue-ratio method to\nestimate the numbers of global and cluster-specific factors. The consistency\nwith explicit convergence rates is also established for the estimators of the\nlocal loading matrices, the factor numbers and the latent cluster memberships.\nNumerical experiments with both simulated data as well as a real data example\nare also reported to illustrate the usefulness of our proposed method.",
        "Despite recent advances in artificial intelligence (AI), it poses challenges\nto ensure personalized decision-making in tasks that are not considered in\ntraining datasets. To address this issue, we propose ValuePilot, a two-phase\nvalue-driven decision-making framework comprising a dataset generation toolkit\nDGT and a decision-making module DMM trained on the generated data. DGT is\ncapable of generating scenarios based on value dimensions and closely mirroring\nreal-world tasks, with automated filtering techniques and human curation to\nensure the validity of the dataset. In the generated dataset, DMM learns to\nrecognize the inherent values of scenarios, computes action feasibility and\nnavigates the trade-offs between multiple value dimensions to make personalized\ndecisions. Extensive experiments demonstrate that, given human value\npreferences, our DMM most closely aligns with human decisions, outperforming\nClaude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is\na preliminary exploration of value-driven decision-making. We hope it will\nstimulate interest in value-driven decision-making and personalized\ndecision-making within the community.",
        "Software testing and verification are critical for ensuring the reliability\nand security of modern software systems. Traditionally, formal verification\ntechniques, such as model checking and theorem proving, have provided rigorous\nframeworks for detecting bugs and vulnerabilities. However, these methods often\nface scalability challenges when applied to complex, real-world programs.\nRecently, the advent of Large Language Models (LLMs) has introduced a new\nparadigm for software analysis, leveraging their ability to understand insecure\ncoding practices. Although LLMs demonstrate promising capabilities in tasks\nsuch as bug prediction and invariant generation, they lack the formal\nguarantees of classical methods. This paper presents a comprehensive study of\nstate-of-the-art software testing and verification, focusing on three key\napproaches: classical formal methods, LLM-based analysis, and emerging hybrid\ntechniques, which combine their strengths. We explore each approach's\nstrengths, limitations, and practical applications, highlighting the potential\nof hybrid systems to address the weaknesses of standalone methods. We analyze\nwhether integrating formal rigor with LLM-driven insights can enhance the\neffectiveness and scalability of software verification, exploring their\nviability as a pathway toward more robust and adaptive testing frameworks.",
        "We study the local geometry of empirical risks in high dimensions via the\nspectral theory of their Hessian and information matrices. We focus on settings\nwhere the data, $(Y_\\ell)_{\\ell =1}^n\\in \\mathbb R^d$, are i.i.d. draws of a\n$k$-component Gaussian mixture model, and the loss depends on the projection of\nthe data into a fixed number of vectors, namely $\\mathbf{x}^\\top Y$, where\n$\\mathbf{x}\\in \\mathbb{R}^{d\\times C}$ are the parameters, and $C$ need not\nequal $k$. This setting captures a broad class of problems such as\nclassification by one and two-layer networks and regression on multi-index\nmodels. We prove exact formulas for the limits of the empirical spectral\ndistribution and outlier eigenvalues and eigenvectors of such matrices in the\nproportional asymptotics limit, where the number of samples and dimension\n$n,d\\to\\infty$ and $n\/d=\\phi \\in (0,\\infty)$. These limits depend on the\nparameters $\\mathbf{x}$ only through the summary statistic of the $(C+k)\\times\n(C+k)$ Gram matrix of the parameters and class means, $\\mathbf{G} =\n(\\mathbf{x},\\mathbf{\\mu})^\\top(\\mathbf{x},\\mathbf{\\mu})$. It is known that\nunder general conditions, when $\\mathbf{x}$ is trained by stochastic gradient\ndescent, the evolution of these same summary statistics along training\nconverges to the solution of an autonomous system of ODEs, called the effective\ndynamics. This enables us to connect the spectral theory to the training\ndynamics. We demonstrate our general results by analyzing the effective\nspectrum along the effective dynamics in the case of multi-class logistic\nregression. In this setting, the empirical Hessian and information matrices\nhave substantially different spectra, each with their own static and even\ndynamical spectral transitions.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Spatial reasoning is a fundamental capability of embodied agents and has\ngarnered widespread attention in the field of multimodal large language models\n(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to\ncomprehensively evaluate the spatial reasoning capacities of current\nstate-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists\nof 9k VQA samples, collected using an efficient semi-automated tool in a\nhigh-fidelity urban simulator. We evaluate several SOTA MLLMs across various\naspects of spatial reasoning, such as relative and absolute spatial\nrelationships, situational reasoning, and object-centric spatial attributes.\nOur results reveal that: 1) MLLMs perform better at answering questions\nregarding relative spatial relationships than absolute spatial relationships,\n2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric\nand allocentric perspectives, and 3) Fine-tuning large models significantly\nimproves their performance across different spatial reasoning tasks. We believe\nthat our open-source data collection tools and in-depth analyses will inspire\nfurther research on MLLM spatial reasoning capabilities. The benchmark is\navailable at https:\/\/github.com\/WeichenZh\/Open3DVQA.",
        "Conformal Prediction (CP) is a principled framework for quantifying\nuncertainty in blackbox learning models, by constructing prediction sets with\nfinite-sample coverage guarantees. Traditional approaches rely on scalar\nnonconformity scores, which fail to fully exploit the geometric structure of\nmultivariate outputs, such as in multi-output regression or multiclass\nclassification. Recent methods addressing this limitation impose predefined\nconvex shapes for the prediction sets, potentially misaligning with the\nintrinsic data geometry. We introduce a novel CP procedure handling\nmultivariate score functions through the lens of optimal transport.\nSpecifically, we leverage Monge-Kantorovich vector ranks and quantiles to\nconstruct prediction region with flexible, potentially non-convex shapes,\nbetter suited to the complex uncertainty patterns encountered in multivariate\nlearning tasks. We prove that our approach ensures finite-sample,\ndistribution-free coverage properties, similar to typical CP methods. We then\nadapt our method for multi-output regression and multiclass classification, and\nalso propose simple adjustments to generate adaptive prediction regions with\nasymptotic conditional coverage guarantees. Finally, we evaluate our method on\npractical regression and classification problems, illustrating its advantages\nin terms of (conditional) coverage and efficiency.",
        "Interacting with the world is a multi-sensory experience: achieving effective\ngeneral-purpose interaction requires making use of all available modalities --\nincluding vision, touch, and audio -- to fill in gaps from partial observation.\nFor example, when vision is occluded reaching into a bag, a robot should rely\non its senses of touch and sound. However, state-of-the-art generalist robot\npolicies are typically trained on large datasets to predict robot actions\nsolely from visual and proprioceptive observations. In this work, we propose\nFuSe, a novel approach that enables finetuning visuomotor generalist policies\non heterogeneous sensor modalities for which large datasets are not readily\navailable by leveraging natural language as a common cross-modal grounding. We\ncombine a multimodal contrastive loss with a sensory-grounded language\ngeneration loss to encode high-level semantics. In the context of robot\nmanipulation, we show that FuSe enables performing challenging tasks that\nrequire reasoning jointly over modalities such as vision, touch, and sound in a\nzero-shot setting, such as multimodal prompting, compositional cross-modal\nprompting, and descriptions of objects it interacts with. We show that the same\nrecipe is applicable to widely different generalist policies, including both\ndiffusion-based generalist policies and large vision-language-action (VLA)\nmodels. Extensive experiments in the real world show that FuSeis able to\nincrease success rates by over 20% compared to all considered baselines.",
        "Large language models have become a powerful method for feature augmentation\nin recommendation systems. However, existing approaches relying on quick\ninference often suffer from incomplete feature coverage and insufficient\nspecificity in feature descriptions, limiting their ability to capture\nfine-grained user preferences and undermining overall performance. Motivated by\nthe recent success of inference scaling in math and coding tasks, we explore\nwhether scaling inference can address these limitations and enhance feature\nquality.\n  Our experiments show that scaling inference leads to significant improvements\nin recommendation performance, with a 12% increase in NDCG@10. The gains can be\nattributed to two key factors: feature quantity and specificity. In particular,\nmodels using extended Chain-of-Thought (CoT) reasoning generate a greater\nnumber of detailed and precise features, offering deeper insights into user\npreferences and overcoming the limitations of quick inference. We further\ninvestigate the factors influencing feature quantity, revealing that model\nchoice and search strategy play critical roles in generating a richer and more\ndiverse feature set. This is the first work to apply inference scaling to\nfeature augmentation in recommendation systems, bridging advances in reasoning\ntasks to enhance personalized recommendation.",
        "A nearest-neighbor, frustration-free spin $\\frac{1}{2}$ chain can be\nconstructed {\\it via} projectors of various ranks \\'{a} la Bravyi-Gosset. We\nshow that in the rank 1 case this system is gapped and has two ground states\nresembling ferromagnetic states. These states spontaneously break the\nnon-invertible symmetry connecting them. The latter is proved using the\nmachinery of algebraic quantum theory. The non-invertible symmetries of this\nsystem do not come from a duality."
      ]
    }
  },
  {
    "id":2412.2062,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Artificial intelligence in elderly healthcare: A scoping review",
    "start_abstract":"The ageing population has led to a surge in the adoption of artificial intelligence (AI) technologies in elderly healthcare worldwide. However, in the advancement of AI technologies, there is currently a lack of clarity about the types and roles of AI technologies in elderly healthcare. This scoping review aimed to provide a comprehensive overview of AI technologies in elderly healthcare by exploring the types of AI technologies employed, and identifying their roles in elderly healthcare based on existing studies. A total of 10 databases were searched for this review, from January 1 2000 to July 31 2022. Based on the inclusion criteria, 105 studies were included. The AI devices utilized in elderly healthcare were summarised as robots, exoskeleton devices, intelligent homes, AI-enabled health smart applications and wearables, voice-activated devices, and virtual reality. Five roles of AI technologies were identified: rehabilitation therapists, emotional supporters, social facilitators, supervisors, and cognitive promoters. Results showed that the impact of AI technologies on elderly healthcare is promising and that AI technologies are capable of satisfying the unmet care needs of older adults and demonstrating great potential in its further development in this area. More well-designed randomised controlled trials are needed in the future to validate the roles of AI technologies in elderly healthcare.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Digital health platforms for the elderly? Key adoption and usage barriers and ways to address them"
      ],
      "abstract":[
        "Digital healthcare platforms (DHPs) represent a relatively new phenomenon that could provide valuable complement to physical primary care \u2013 for example, by reducing costs, improving access healthcare, and allowing patient monitoring. However, such are mainly used today the younger generations, which creates \"digital divide\" between elderly. This article aims identify: i) perceived key barriers inhibit adoption usage of DHPs elderly, ii) what DHP providers can do facilitate increased The draws on qualitative interviews with elderly complementary process data from major Swedish DHP. We find perceives two initial DHPs: negative attitudes technology anxiety one barrier affecting both lack trust. analysis also identifies multiple development suggestions improvement better accommodate needs including application tailored education activities. an integrated framework outlining ways address them. In so doing, we contribute literature mHealth in healthcare."
      ],
      "categories":[
        "Healthcare"
      ]
    },
    "list":{
      "title":[
        "Complete heteroclinic networks derived from graphs consisting of two\n  cycles",
        "Continual Release Moment Estimation with Differential Privacy",
        "Exploratory study on the masses of odd-$Z$ nuclei and $r$-process\n  simulation based on the deformed relativistic Hartree-Bogoliubov theory in\n  continuum",
        "A Zero-Knowledge Proof for the Syndrome Decoding Problem in the Lee\n  Metric",
        "Estimating Network Models using Neural Networks",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "GLIMPSE: An ultra-faint $\\simeq$ 10$^{5}$ $M_{\\odot}$ Pop III Galaxy\n  Candidate and First Constraints on the Pop III UV Luminosity Function at\n  $z\\simeq6-7$",
        "Spin separation and filtering assisted by topological corner states in\n  the Kekul\\'{e} lattice",
        "Wavelet-based density sketching with functional hierarchical tensor",
        "Half a Million M Dwarf Stars Characterized Using Domain-Adapted Spectral\n  Analysis",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Tangent Currents, King's Residue Formula and Intersection Theory",
        "On the flow characteristics in the shock formation region due to the\n  diaphragm opening process in a shock tube",
        "Connections between the minimal neighborhood and the activity value of\n  cellular automata",
        "Qoala: an Application Execution Environment for Quantum Internet Nodes",
        "The probabilistic combinatorial attacks on atmospheric\n  continuous-variable quantum secret sharing",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "The Faber-Krahn inequality for partial sums of eigenvalues of Toeplitz\n  operators",
        "Inferring the density and membership of stellar streams with flexible\n  models: The GD-1 stream in Gaia Data Release 3",
        "Miniaturized optical system for a chip based cold atom inertial sensor",
        "A sturdy spin-momentum locking in a chiral organic superconductor",
        "Exploring experimental limit of deep quantum signal processing using a\n  trapped-ion simulator",
        "H0 tension in Tsallis and Renyi statistics",
        "Dual-Source SPIR over a noiseless MAC without Data Replication or Shared\n  Randomness",
        "Real-time Bus Travel Time Prediction and Reliability Quantification: A\n  Hybrid Markov Model",
        "Bulk-edge correspondence at the spin-to-integer quantum Hall effect\n  crossover in topological superconductors",
        "Aggregation of dipolar molecules in SiO$_2$ hybrid organic inorganic\n  films: use of silver nanoparticles as inhibitors of molecular aggregation",
        "Applications of Random Matrix Theory in Machine Learning and Brain\n  Mapping",
        "Crystal field splittings and magnetic ground state of the square-lattice\n  antiferromagnets YbBi2ClO4 and YbBi2IO4 with Jeff = 1\/2"
      ],
      "abstract":[
        "We address the question how a given connection structure (directed graph) can\nbe realised as a heteroclinic network that is complete in the sense that it\ncontains all unstable manifolds of its equilibria. For a directed graph\nconsisting of two cycles we provide a constructive method to achieve this: (i)\nenlarge the graph by adding some edges and (ii) apply the simplex method to\nobtain a network in phase space. Depending on the length of the cycles we\nderive the minimal number of required new edges. In the resulting network each\nadded edge leads to a positive transverse eigenvalue at the respective\nequilibrium. We discuss the total number of such positive eigenvalues in an\nindividual cycle and some implications for the stability of this cycle.",
        "We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.",
        "\\textbf{Background:} Nuclear masses of exotic nuclei are important for both\nnuclear physics and astrophysics. The deformed relativistic Hartree-Bogoliubov\ntheory in continuum (DRHBc) is capable of providing proper descriptions for\nexotic nuclei by simultaneously including deformation and continuum effects.\nThe mass table of even-$Z$ nuclei with $8\\le Z\\le 120$ has been established\nbased on the DRHBc theory [ADNDT 158, 101661 (2024)]. \\textbf{Purpose:} This\nwork aims to systematically estimate the masses of odd-$Z$ nuclei based on the\navailable DRHBc results of even-$Z$ nuclei, thereby providing a pseudo DRHBc\nmass table for all nuclei with $8\\le Z\\le 120$. This mass table can then be\nemployed in the $r$-process studies to investigate the influence of deformation\non $r$-process. \\textbf{Method:} The mass of an odd nucleus is expressed as a\nfunction of the masses and odd-even mass differences of its neighboring even\nnuclei, with the odd-even mass difference approximated by the average pairing\ngap. The $r$-process simulations are carried out using the site-independent\nclassical $r$-process model based on the waiting-point approximation.\n\\textbf{Results and Conclusions:} The approximation of the odd-even mass\ndifference with the average pairing gap is validated to be effective, by\nreproducing the masses of even-$Z$ odd-$N$ nuclei calculated by DRHBc.\nCombining the DRHBc masses of even-$Z$ nuclei and the estimated masses of\nodd-$Z$, a pseudo DRHBc mass table is established, with the root-mean-square\n(rms) deviation from available mass data $\\sigma=1.50$ MeV. This pseudo DRHBc\nmass table is applied to the $r$-process simulation, and the impact of nuclear\ndeformation effects is analyzed. The deformation effects can influence the\n$r$-process path and thus affect the $r$-process abundance. In particular, the\nnuclear shape transitions can even lead to the discontinuity of the $r$-process\npath.",
        "The syndrome decoding problem is one of the NP-complete problems lying at the\nfoundation of code-based cryptography. The variant thereof where the distance\nbetween vectors is measured with respect to the Lee metric, rather than the\nmore commonly used Hamming metric, has been analyzed recently in several works\ndue to its potential relevance for building more efficient code-based\ncryptosystems. The purpose of this article is to describe a zero-knowledge\nproof for this variant of the problem.",
        "Exponential random graph models (ERGMs) are very flexible for modeling\nnetwork formation but pose difficult estimation challenges due to their\nintractable normalizing constant. Existing methods, such as MCMC-MLE, rely on\nsequential simulation at every optimization step. We propose a neural network\napproach that trains on a single, large set of parameter-simulation pairs to\nlearn the mapping from parameters to average network statistics. Once trained,\nthis map can be inverted, yielding a fast and parallelizable estimation method.\nThe procedure also accommodates extra network statistics to mitigate model\nmisspecification. Some simple illustrative examples show that the method\nperforms well in practice.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "Detecting the first generation of stars, Population III (PopIII), has been a\nlong-standing goal in astrophysics, yet they remain elusive even in the JWST\nera. Here we present a novel NIRCam-based selection method for PopIII galaxies,\nand carefully validate it through completeness and contamination simulations.\nWe systematically search ~500 arcmin$^{2}$ across JWST legacy fields for PopIII\ncandidates, including GLIMPSE which, assisted by gravitational lensing, has\nproduced JWST's deepest NIRCam imaging thus far. We discover one promising\nPopIII galaxy candidate (GLIMPSE-16043) at $z=6.50^{+0.03}_{-0.24}$, a\nmoderately lensed galaxy (mu=2.9) with an intrinsic UV magnitude of\n$M_{UV}$=-15.89. It exhibits key PopIII features: strong H$\\alpha$ emission\n(rest-frame EW $2810\\pm550$\\AA); a Balmer jump; no dust (UV slope\n$\\beta=-2.34\\pm0.36$); and undetectable metal lines (e.g., [OIII];\n[OIII]\/H$\\beta$<0.44) implying a gas-phase metallicity of Zgas\/Zsun<0.5%. These\nproperties indicate the presence of a nascent, metal-deficient young stellar\npopulation (<5Myr) with a stellar mass of $\\simeq10^{5}M_{\\odot}$.\nIntriguingly, this source deviates significantly from the extrapolated\nUV-metallicity relation derived from recent JWST observations at $z=4-10$,\nconsistent with UV enhancement by a top-heavy PopIII initial mass function or\nthe presence of an extremely metal-poor AGN. We also derive the first\nobservational constraints on the PopIII UV luminosity function at z~6-7. The\nvolume density of GLIMPSE-16043 ($\\approx10^{-4}$ cMpc$^{-3}$) is in excellent\nagreement with theoretical predictions, independently reinforcing its\nplausibility. This study demonstrates the power of our novel NIRCam method to\nfinally reveal distant galaxies even more pristine than the Milky Way's most\nmetal-poor satellites, thereby promising to bring us closer to the first\ngeneration of stars than we have ever been before.",
        "Higher-order topological corner states have been realized in two-dimensional\nKekul\\'{e} lattice, which can be further coupled with spin polarization through\nthe implementation of local magnetization. In this work, we numerically\ninvestigate the spin-dependent transport properties assisted by topological\ncorner states in the Kekul\\'{e} lattice. By applying local magnetization and\nelectric potential, the topological corner states are spin polarized with\nopposite spins localized at different corners, thereby demonstrating a\nspin-corner state locking mechanism. Transport characteristics, including\ntransmission, local density of states, and local current density, are\ncalculated for a two-terminal setup consisting of a diamond-shaped Kekul\\'{e}\nlattice connected to two leads. When opposite local magnetization is applied to\nthe corners, spin-up and spin-down electrons are perfectly separated, forming\ntwo spin-polarized conducting channels and leading to spin spatial separation.\nIn the presence of identical local magnetization on both corners and an\nelectric potential at one corner, the spin-polarized corner states can\nfacilitate selective filtering of different spins and generate spin-polarized\ncurrents by tuning the energy. Furthermore, spin-resolved transmission diagrams\nas functions of both the Fermi energy and electric potential are presented,\nillustrating the global distribution of spin filtering through topological\ncorner states.",
        "We introduce the functional hierarchical tensor under a wavelet basis (FHT-W)\nansatz for high-dimensional density estimation in lattice models. Recently, the\nfunctional tensor network has emerged as a suitable candidate for density\nestimation due to its ability to calculate the normalization constant exactly,\na defining feature not enjoyed by neural network alternatives such as\nenergy-based models or diffusion models. While current functional tensor\nnetwork models show good performance for lattice models with weak or moderate\ncouplings, we show that they face significant model capacity constraints when\napplied to lattice models with strong coupling. To address this issue, this\nwork proposes to perform density estimation on the lattice model under a\nwavelet transformation. Motivated by the literature on scale separation, we\nperform iterative wavelet coarsening to separate the lattice model into\ndifferent scales. Based on this multiscale structure, we design a new\nfunctional hierarchical tensor ansatz using a hierarchical tree topology,\nwhereby information on the finer scale is further away from the root node of\nthe tree. Our experiments show that the numerical rank of typical lattice\nmodels is significantly lower under appropriate wavelet transformation.\nFurthermore, we show that our proposed model allows one to model challenging\nGaussian field models and Ginzburg-Landau models.",
        "We present fundamental atmospheric parameters (Teff and log g) and\nmetallicities ([M\/H]) for 507,513 M dwarf stars using low-resolution spectra\n(R~1800) from LAMOST DR10. By employing Cycle-StarNet, an innovative domain\nadaptation approach, we successfully bridge the gap between theoretical PHOENIX\nsynthetic spectra and observed LAMOST spectra, enabling parameter measurements\neven for lower signal-to-noise data (S\/N>5). The fitting residual analysis\nshows a reduction from 2.0 times to 1.68 times the flux uncertainty. Comparing\nwith available literature values, we find systematic offsets and precisions of\n12$\\pm$70 K in Teff, -0.04$\\pm$0.17 dex in log g, and -0.06$\\pm$0.20 dex in\n[M\/H]. The precision improves for higher quality spectra (S\/N>50) to 47 K, 0.12\ndex, and 0.14 dex respectively. The metallicity consistency between wide\nbinaries shows a scatter of 0.24 dex, improving to 0.15 dex at S\/N>50. We\nprovide a comprehensive catalog including stellar parameters, spectral\nclassifications, activity indicators, and binary\/variability flags,\nestablishing a resource for studies of the most numerous stellar population.\nThe complete catalog is available at https:\/\/doi.org\/10.5281\/zenodo.14030249.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "In this work, we study the intersection of positive closed currents on\ndomains. We use the theory of tangent currents in connection with King's\nresidue formula. We find a sufficient condition for the local existence of\ntangent currents, and express the shadow of tangent currents and the\n$h$-dimension of tangent currents in terms of the complex Monge-Amp\\`ere type\ncurrent. Further, a reasonable integrability condition for the existence of the\nunique tangent current with minimal $h$-dimension is introduced. We apply it to\nthe study of the intersection of positive closed currents, find a sufficient\ncondition for the intersection of positive closed currents on domains and\ndescribe the intersection in terms of the complex Monge-Amp\\`ere type current.\nAt the same time, we obtain regularizations of positive closed currents that\nwork well with the suggested intersection of positive closed currents. In\nparticular, the standard regularization of currents by convolution actually\nproduces the convergence towards the intersection of positive closed currents.\nIn this sense, our approach generalizes King's work on currents defined by\nanalytic varieties, which is obtained from Federer's slicing theory. Some\nclassical examples are computed. Our work is applicable to general complex\nmanifolds not necessarily compact or K\\\"ahler.",
        "The shock formation process in shock tubes has been extensively studied;\nhowever, significant gaps remain in understanding the effects of the diaphragm\nrupture process on the resulting flow non-uniformities. Existing models\npredicting the shock attenuation and propagation dynamics overlook critical\ndiaphragm mechanics and their impact on shock behavior. Addressing this gap is\nvital for improving predictive capabilities and optimizing shock tube designs\nfor applications in combustion kinetics, aerodynamics, and high-speed\ndiagnostics. This study investigates the shock wave formation and propagation\nthrough combined experimental and numerical approaches over a range of\ndriver-to-driven pressure ratios (Driver pressure: 9.4 - 24.2 bar of helium;\nDriven pressure: 100 Torr of argon). High-speed imaging captures the diaphragm\nopening dynamics, while pressure and shock velocity measurements along the\nentire driven section of the shock tube provide key validation data for CFD.\nTwo-dimensional numerical simulations incorporate experimentally measured\ndiaphragm opening profiles, offering detailed insights into flow features and\nthermodynamic gradients behind the moving shock front. Key parameters,\nincluding deceleration and acceleration phases within the shock formation\nregion, shock formation distances, and times, have been quantified. A novel\ntheoretical framework is introduced to correlate these parameters, enabling\naccurate predictions of shock Mach number evolution under varying conditions.\nThis unified methodology bridges theoretical and experimental gaps, providing a\nrobust foundation for advancing shock tube research and design.",
        "For a group $G$ and a finite set $A$, a cellular automaton is a\ntransformation of the configuration space $A^G$ defined via a finite\nneighborhood and a local map. Although neighborhoods are not unique, every CA\nadmits a unique minimal neighborhood, which consists on all the essential cells\nin $G$ that affect the behavior of the local map. An active transition of a\ncellular automaton is a pattern that produces a change on the current state of\na cell when the local map is applied. In this paper, we study the links between\nthe minimal neighborhood and the number of active transitions, known as the\nactivity value, of cellular automata. Our main results state that the activity\nvalue usually imposes several restrictions on the size of the minimal\nneighborhood of local maps.",
        "Recently, a first-of-its-kind operating system for programmable quantum\nnetwork nodes was developed, called QNodeOS. Here, we present an extension of\nQNodeOS called Qoala, which introduces (1) a unified program format for hybrid\ninteractive classical-quantum programs, providing a well-defined target for\ncompilers, and (2) a runtime representation of a program that allows joint\nscheduling of the hybrid classical-quantum program, multitasking, and\nasynchronous program execution. Based on concrete design considerations, we put\nforward the architecture of Qoala, including the program structure and\nexecution mechanism. We implement Qoala in the form of a modular and extendible\nsimulator that is validated against real-world quantum network hardware\n(available online). However, Qoala is not meant to be purely a simulator, and\nimplementation is planned on real hardware. We evaluate Qoala's effectiveness\nand performance sensitivity to latencies and network schedules using an\nextensive simulation study. Qoala provides a framework that opens the door for\nfuture computer science research into quantum network applications, including\nscheduling algorithms and compilation strategies that can now readily be\nexplored using the framework and tools provided.",
        "The combination of quantum secret sharing (QSS) and continuous-variable\nquantum key distribution (CV-QKD) has demonstrated clear advantages and has\nundergone significant development in recent years. However, research on the\npractical security of CV-QSS remains limited, particularly in the context of\nfree-space channels, which exhibit considerable flexibility. In this paper, we\nstudy the practical security of free-space CV-QSS, innovatively propose an\nattack strategy that probabilistically combines two-point distribution attack\n(TDA) and uniform distribution attack (UDA). We also establish channel\nparameter models, especially a channel noise model based on local local\noscillators (LLO), to further evaluate the key rate. In principle, the analysis\ncan be extended to any number of probabilistic combinations of channel\nmanipulation attacks. The numerical results demonstrate that the probabilistic\ncombination attacks reduce the real key rate of CV-QSS under moderate intensity\nturbulence, but still enable secure QSS at a distance of 8 km on a scale of\nhundreds. However, it should be noted that the probabilistic combination\nattacks will make the deviation between the estimated key rate and the real key\nrate, i.e., the key rate is overestimated, which may pose a security risk.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "We prove that, among all radial subsets $\\Omega\\subset \\mathbb{C}$ of\nprescribed measure, the ball is the only maximizer of the sum of the first $K$\neigenvalues ($K\\geq 1$) of the corresponding Toeplitz operator $T_\\Omega$ on\nthe Fock space $\\mathcal{F}^2(\\mathbb{C})$. As a byproduct, we prove that balls\nmaximize any Schatten $p$-norm of $T_\\Omega$ for $p>1$ (and minimize the\ncorresponding quasinorm for $p<1$), and that the second eigenvalue is maximized\nby a particular annulus. Moreover, we extend some of these results to general\nradial symbols in $L^p(\\mathbb{C})$, with $p > 1$, characterizing those that\nmaximize the sum of the first $K$ eigenvalues.",
        "As bound stellar systems orbit within a galaxy, stars may be tidally stripped\nto form streams of stars that nearly follow the orbit of their progenitor\nsystem. Stellar streams provide one of the most promising avenues for\nconstraining the global mass distribution of the Milky Way and the nature of\ndark matter (DM). The stream stars' kinematic \"track\" enables inferring\nlarge-scale properties of the DM distribution, while density variations and\nanomalies provide information about local DM clumps (e.g., from DM subhalos).\nUsing precise astrometric data from the Gaia Mission, which enables clean\nselections of Milky Way stream stars, we now know of a few streams with\nperturbations and density anomalies. A full accounting of the density tracks\nand substructures within all >100 Milky Way stellar streams will therefore\nenable powerful new constraints on DM. However, methods for discovering and\ncharacterizing membership of streams are heterogeneous and often highly\ncustomized to individual streams. Here we present a new, flexible framework for\nmodeling stellar stream density and membership. Our framework allows us to\ninclude off-track or non-Gaussian components to the stream density, meaning we\ncan capture anomalous features (such as the GD-1 steam's spur). We test our\nmodel on GD-1, where we characterize previously-known features and provide the\nlargest catalog of probable member stars to date (1689 stars). Our framework\n(built on JAX and numpyro) provides a path toward uniform analysis of all Milky\nWay streams, enabling tight constraints on the Galactic mass distribution and\nits dark matter.",
        "We miniaturized the complex optical system responsible for the cooling,\npumping and imaging of an on-chip based cold atom inertial sensor. This optical\nbench uses bonded miniature optics and includes all the necessary optical\nfunctions. The bench has a volume of 35x25x5~cm$^3$. We developed a laser\nfrequency lock adapted to the optical bench using saturated absorption in a\nrubidium cell. The entire laser source based on frequency doubling of\n1.56~$\\mu$m fiber lasers, including the control system and the saturated\nabsorption module, fits in a $5U$-rack. Using the miniaturized bench, we\nrealized two and three dimensional magneto optical traps for Rubidium 87 atoms.",
        "Among noncentrosymmetric structures, chirality has recently been recognized\nas a novel source of asymmetrical charge\/spin transports as exemplified by\nelectrical magnetochiral anisotropy (EMChA) and chirality-induced spin\nselectivity. Although similar bulk-charge rectification and Rashba-Edelstein\neffect in polar systems are quantitively reproducible by theory based on the\nelectronic band structures, the relevance of band parameters in chiral effects\nremains elusive. Here, by working with a chiral organic superconductor, we\nexperimentally demonstrate a gigantic EMChA and large superconducting diode\neffect, both of which are difficult to be explained solely by its band\nparameters. A two-critical-current signature and an enhanced critical field\nsuggested triplet-mixed Cooper pairs with anomalously enhanced spin-orbit\ncoupling above atomic limit. Our results clearly highlight a unique\nspin-momentum locking with large stiffness beyond the expectation, suggesting\nan unknown driving force for spin polarization inherent to chirality.",
        "Quantum signal processing (QSP), which enables systematic polynomial\ntransformations on quantum data through sequences of qubit rotations, has\nemerged as a fundamental building block for quantum algorithms and data\nre-uploading quantum neural networks. While recent experiments have\ndemonstrated the feasibility of shallow QSP circuits, the inherent limitations\nin scaling QSP to achieve complex transformations on quantum hardware remain an\nopen and critical question. Here we report the first experimental realization\nof deep QSP circuits in a trapped-ion quantum simulator. By manipulating the\nqubit encoded in a trapped $^{43}\\textrm{Ca}^{+}$ ion, we demonstrate\nhigh-precision simulation of some prominent functions used in quantum\nalgorithms and machine learning, with circuit depths ranging from 15 to 360\nlayers and implementation time significantly longer than coherence time of the\nqubit. Our results reveal a crucial trade-off between the precision of function\nsimulation and the concomitant accumulation of hardware noise, highlighting the\nimportance of striking a balance between circuit depth and accuracy in\npractical QSP implementation. This work addresses a key gap in understanding\nthe scalability and limitations of QSP-based algorithms on quantum hardware,\nproviding valuable insights for developing quantum algorithms as well as\npractically realizing quantum singular value transformation and data\nre-uploading quantum machine learning models.",
        "Motivated by recent attempts to study the implications of Tsallis and Renyi\nstatistics in gravitational, cosmological, and astrophysical systems, the\npossible relationships between H0 tension and generalized statistics are\nexplored. It is obtained that, in the light of H0 tension, the energytime\nuncertainty relations in Tsallis and Renyi statistics constrain the values of\nTsallis and Renyi parameters. Hence, the way to find the footprints of\nnon-extensivity in the universe is paved.",
        "Information-theoretically secure Symmetric Private Information Retrieval\n(SPIR) is known to be infeasible over noiseless channels with a single server.\nKnown solutions to overcome this infeasibility involve additional resources\nsuch as database replication, shared randomness, or noisy channels. In this\npaper, we propose an alternative approach for achieving SPIR with\ninformation-theoretic security guarantees, without relying on shared\nrandomness, noisy channels, or data replication. Specifically, we demonstrate\nthat it is sufficient to use a noiseless binary adder multiple-access channel,\nwhere inputs are controlled by two non-colluding servers and the output is\nobserved by the client, alongside a public noiseless communication channel\nbetween the client and the servers. Furthermore, in this setting, we\ncharacterize the optimal file rates, i.e., the file lengths normalized by the\nnumber of channel uses, that can be transferred.",
        "Accurate and reliable bus travel time prediction in real-time is essential\nfor improving the operational efficiency of public transportation systems.\nHowever, this remains a challenging task due to the limitations of existing\nmodels and data sources. This study proposed a hybrid Markovian framework for\nreal-time bus travel time prediction, incorporating uncertainty quantification.\nFirstly, the bus link travel time distributions were modeled by integrating\nvarious influential factors while explicitly accounting for heteroscedasticity.\nParticularly, the parameters of the distributions were estimated using Maximum\nLikelihood Estimation, and the Fisher Information Matrix was then employed to\ncalculate the 95\\% uncertainty bounds for the estimated parameters, ensuring a\nrobust and reliable quantification of prediction uncertainty of bus link travel\ntimes. Secondly, a Markovian framework with transition probabilities based on\npreviously predicted bus link travel times was developed to predict travel\ntimes and their uncertainties from a current location to any future stop along\nthe route. The framework was evaluated using the General Transit Feed\nSpecification (GTFS) Static and Realtime data collected in 2023 from\nGainesville, Florida. The results showed that the proposed model consistently\nachieved better prediction performance compared to the selected baseline\napproaches (including historical mean, statistical and AI-based models) while\nproviding narrower uncertainty bounds. The model also demonstrated high\ninterpretability, as the estimated coefficients provided insights into how\ndifferent factors influencing bus travel times across links with varying\ncharacteristics. These findings suggest that the model could serve as a\nvaluable tool for transit system performance evaluation and real-time trip\nplanning.",
        "The spin and integer quantum Hall effects are two cousins of topological\nphase transitions in two-dimensional electronic systems. Their close\nrelationship makes it possible to transform spin to integer quantum Hall effect\nin two-dimensional topological superconductors by continuous increase in a\nsymmetry breaking Zeeman magnetic field. We study peculiarities of bulk-edge\ncorrespondence and a fate of massless edge and bulk topological (instantons)\nexcitations at such the crossover.",
        "The technological implementation of hybrid organic-inorganic materials in\nsecond order nonlinear optical photonic devices depends strongly on the ability\nof the host matrixes to contain high loads of dipolar molecules without\naggregation. Some organic molecules are often used to diminish the attracting\ninteractions between dipolar molecules in such kind of materials, but their\nefficiency as inhibitors of molecular aggregation is limited by their\npolarizability. In this work, we report the use of silver nanoparticles as\ninhibitors of molecular aggregation in hybrid organic-inorganic films doped\nwith dipolar molecules. The large polarizability of the silver nanoparticles\nmakes them ideal moieties for the inhibition of the electrostatic interactions\nbetween dipolar nonlinear optical molecules. The average size of the silver\nnanoparticles in this work was 70.5 nm in diameter, they were synthesized using\nsilver nitrate (AgNO$_3$) as precursor and\naminoethylaminopropyltrimethoxysilane as reducing agent. These nanoparticles\nwere immersed in SiO$_2$ hybrid organic-inorganic sol-gel films doped with\ndipolar chromophores to study their effect as inhibitors of dipolar\nchromophores aggregation. The presence of the silver nanoparticles in the solid\nfilms was confirmed by transmission electronic microscopy and UV-Visible\nspectroscopy. UV-Visible spectroscopy was also used to monitor the dipolar\nchromophores aggregation in the SiO$_2$ films. We found that, at room\ntemperature, silver nanoparticles are good inhibiting chromophores aggregation\nin comparison with the performance of organic inhibitors.",
        "Brain mapping analyzes the wavelengths of brain signals and outputs them in a\nmap, which is then analyzed by a radiologist. Introducing Machine Learning (ML)\ninto the brain mapping process reduces the variable of human error in reading\nsuch maps and increases efficiency. A key area of interest is determining the\ncorrelation between the functional areas of the brain on a voxel (3-dimensional\npixel) wise basis. This leads to determining how a brain is functioning and can\nbe used to detect diseases, disabilities, and sicknesses. As such, random noise\npresents a challenge in consistently determining the actual signals from the\nscan. This paper discusses how an algorithm created by Random Matrix Theory\n(RMT) can be used as a tool for ML, as it detects the correlation of the\nfunctional areas of the brain. Random matrices are simulated to represent the\nvoxel signal intensity strength for each time interval where a stimulus is\npresented in an fMRI scan. Using the Marchenko-Pastur law for Wishart Matrices,\na result of RMT, it was found that no matter what type of noise was added to\nthe random matrices, the observed eigenvalue distribution of the Wishart\nMatrices would converge to the theoretical distribution. This means that RMT is\nrobust and has a high test-re-test reliability. These results further indicate\nthat a strong correlation exists between the eigenvalues, and hence the\nfunctional regions of the brain. Any eigenvalue that differs significantly from\nthose predicted from RMT may indicate the discovery of a new discrete brain\nnetwork.",
        "We report on the crystal field level splitting and magnetic ground state of\nthe Jeff = 1\/2 square lattice antiferromagnets YbBi2ClO4 and YbBi2IO4 using\npowder inelastic neutron scattering (INS) and neutron diffraction measurements.\nBoth compounds exhibit a well-isolated $\\Gamma_{7}$ doublet ground state under\na tetragonal crystal field environment, confirming a robust Jeff = 1\/2 picture\nwith slight XY-type anisotropic character in the g-tensor. Notably, the ground\nstate wave functions closely resemble the $\\Gamma_{7}$ doublet expected in the\nperfect cubic limit, consistent with the nearly cubic ligand configuration of\neight O2- ions surrounding Yb3+. Below TN =0.21 K, YbBi2IO4 exhibits a stripe\nlong-range magnetic order characterized by an ordering wave vector qm = (1\/2,\n0, 0) or its symmetry-equivalent (0, 1\/2, 0), with magnetic moments aligned\nalong qm. The ordered moment is approximately 79 % of the classical prediction,\nsignificantly larger than expected from the isotropic J1-J2 model, suggesting\nthe possible involvement of exchange anisotropy in explaining this observation.\nWe show that symmetry-allowed XXZ and bond-dependent anisotropic exchange terms\nin a square lattice can play a critical role in stabilizing the stripe order\nand suppressing the moment reduction as observed. These findings establish\nYbBi2ClO4 and YbBi2IO4 as unique platforms for exploring rich Jeff = 1\/2\nmagnetism from two less investigated perspectives: (i) on a square lattice and\n(ii) within a (nearly) cubic ligand environment."
      ]
    }
  },
  {
    "id":2412.2062,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Digital health platforms for the elderly? Key adoption and usage barriers and ways to address them",
    "start_abstract":"Digital healthcare platforms (DHPs) represent a relatively new phenomenon that could provide valuable complement to physical primary care \u2013 for example, by reducing costs, improving access healthcare, and allowing patient monitoring. However, such are mainly used today the younger generations, which creates \"digital divide\" between elderly. This article aims identify: i) perceived key barriers inhibit adoption usage of DHPs elderly, ii) what DHP providers can do facilitate increased The draws on qualitative interviews with elderly complementary process data from major Swedish DHP. We find perceives two initial DHPs: negative attitudes technology anxiety one barrier affecting both lack trust. analysis also identifies multiple development suggestions improvement better accommodate needs including application tailored education activities. an integrated framework outlining ways address them. In so doing, we contribute literature mHealth in healthcare.",
    "start_categories":[
      "Healthcare"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Artificial intelligence in elderly healthcare: A scoping review"
      ],
      "abstract":[
        "The ageing population has led to a surge in the adoption of artificial intelligence (AI) technologies in elderly healthcare worldwide. However, in the advancement of AI technologies, there is currently a lack of clarity about the types and roles of AI technologies in elderly healthcare. This scoping review aimed to provide a comprehensive overview of AI technologies in elderly healthcare by exploring the types of AI technologies employed, and identifying their roles in elderly healthcare based on existing studies. A total of 10 databases were searched for this review, from January 1 2000 to July 31 2022. Based on the inclusion criteria, 105 studies were included. The AI devices utilized in elderly healthcare were summarised as robots, exoskeleton devices, intelligent homes, AI-enabled health smart applications and wearables, voice-activated devices, and virtual reality. Five roles of AI technologies were identified: rehabilitation therapists, emotional supporters, social facilitators, supervisors, and cognitive promoters. Results showed that the impact of AI technologies on elderly healthcare is promising and that AI technologies are capable of satisfying the unmet care needs of older adults and demonstrating great potential in its further development in this area. More well-designed randomised controlled trials are needed in the future to validate the roles of AI technologies in elderly healthcare."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Beamforming with Oversampled Time-Modulated Arrays",
        "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired\n  Training",
        "The Impact of $^{12}$C($\\alpha, \\gamma$)$^{16}$O Reaction on the\n  Presupernova Evolution and Supernova Explodability of Massive Stars",
        "No Evidence of Asymmetrically Enhanced Star Formation in Infalling\n  Galaxies in UNIONS",
        "Hermite numbers and new families of polynomials",
        "Advancing Differentiable Economics: A Neural Network Framework for\n  Revenue-Maximizing Combinatorial Auction Mechanisms",
        "Efficient and inefficient hydrodynamic escape of exo-satellite\n  atmospheres driven by irradiation from their young giant planets",
        "Anisotropic Hybridization Dynamics in the Quasi-One-Dimensional Kondo\n  Lattice CeCo$_2$Ga$_8$ Revealed by Ultrafast Optical Spectroscopy",
        "Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal\n  Control",
        "Three topological phases of the elliptic Ginibre ensembles with a point\n  charge",
        "Bit-depth color recovery via off-the-shelf super-resolution models",
        "Gradient-descent methods for fast quantum state tomography",
        "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D\n  Brain Images",
        "Evaluating open-source Large Language Models for automated fact-checking",
        "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression\n  Prediction",
        "MutaGReP: Execution-Free Repository-Grounded Plan Search for Code-Use",
        "Cross-Model Validation of Coronagraphic Exposure Time Calculators for\n  the Habitable Worlds Observatory: A Report from the Exoplanet Science Yield\n  sub-Working Group",
        "The \"double\" square-root law: Evidence for the mechanical origin of\n  market impact using Tokyo Stock Exchange data",
        "Optical stabilization for laser communication satellite systems through\n  proportional-integral-derivative (PID) control and reinforcement learning\n  approach",
        "Quantized crystalline-electromagnetic responses in insulators",
        "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
        "A primer on optimal transport for causal inference with observational\n  data",
        "Group Decision-Making System with Sentiment Analysis of Discussion Chat\n  and Fuzzy Consensus Modeling",
        "Evaluating Visual Explanations of Attention Maps for Transformer-based\n  Medical Imaging",
        "The four-gluon vertex from lattice QCD",
        "Prompt-Aware Controllable Shadow Removal",
        "Quasiparticle Fermi surfaces of niobium and niobium-titanium alloys at\n  high pressure",
        "Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor\n  Graph",
        "Intrinsic momentum transport driven by almost-rational surfaces in\n  tokamak plasmas"
      ],
      "abstract":[
        "The time-modulated array (TMA) is a simple array architecture in which each\nantenna is connected via a multi-throw switch. The switch acts as a modulator\nswitching state faster than the symbol rate. The phase shifting and beamforming\nis achieved by a cyclic shift of the periodical modulating signal across\nantennas. In this paper, the TMA mode of operation is proposed to improve the\nresolution of a conventional phase shifter. The TMAs are analyzed under\nconstrained switching frequency being a small multiple of the symbol rate. The\npresented generic signal model gives insight into the magnitude, phase and\nspacing of the harmonic components generated by the quantized modulating\nsequence. It is shown that the effective phase-shifting resolution can be\nimproved multiplicatively by the oversampling factor ($O$) at the cost of\nintroducing harmonics. Finally, the array tapering with an oversampled\nmodulating signal is proposed. The oversampling provides $O+1$ uniformly\ndistributed tapering amplitudes.",
        "Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https:\/\/github.com\/ywxjm\/Diff-Dehazer.",
        "Among the uncertainties of stellar evolution theory, we investigate how the\n$^{12}$C($\\alpha, \\gamma$)$^{16}$O reaction rate affects the evolution of\nmassive stars for the initial masses of $M ({\\rm ZAMS})=$ 13 - 40 M$_\\odot$ and\nthe solar metallicity. We show that the {\\sl explodability} of these stars,\ni.e., which of a neutron star (NS) or a black hole (BH) is formed, is sensitive\nto the strength of convective shell burning of C and O, and thus the mass\nfractions of C ($X$(C)) and O in the shell. For the small $^{12}$C($\\alpha,\n\\gamma$)$^{16}$O reaction rate that yields larger $X$(C), $X$(C) is further\nenhanced by mixing of C from the overlying layer and then C shell burning is\nstrengthened. The extra heating by C shell burning tends to prevent the\ncontraction of outer layers and decrease the {\\sl compactness parameter} at\n$M_r$ = 2.5 M$_\\odot$. This effect leads to the formation of smaller mass cores\nof Si and Fe and steeper density and pressure gradients at the O burning shell\nin the presupernova models. If the pressure gradient there is steeper, the\nmodel is more likely to explode to form a NS rather than a BH. We describe the\npressure gradient against $M_r$ with $V\/U$ and the density drop with $1\/U$,\nwhere $U$ and $V$ are non-dimensional variables to describe the stellar\nstructure. We estimate the critical values of $V\/U$ and $1\/U$ at the O-burning\nshell above which the model is more likely to explode. We conclude that the\nsmaller $^{12}$C($\\alpha, \\gamma$)$^{16}$O reaction rate makes the mass range\nof $M ({\\rm ZAMS})$ that forms a NS larger.",
        "Ram pressure stripping is a well-known environmental quenching mechanism that\nremoves gas from galaxies infalling into groups and clusters. In some extreme\nexamples of ram pressure stripping, galaxies with extended gas tails show\nevidence of enhanced star formation prior to quenching. In this work we use a\nsample of 5277 local satellite galaxies in which a stripped tail of gas has not\nnecessarily been observed, to quantify the strength of ram pressure-enhanced\nstar formation and compare these results to a control sample of 8360 field\ngalaxies. We use u-band imaging from the Ultraviolet-Near Infrared Northern\nSurvey (UNIONS) as a star formation tracer and several metrics to quantify star\nformation asymmetry. We compare these results to environmental properties of\nthe galaxy, such as their time since infall and host halo mass, to constrain\nthe degree of ram pressure enhanced star formation as a function of\nenvironment. We find no significant differences between the satellite and the\nfield samples. We further restrict our sample to galaxies which we most expect\nto be experiencing significant ram pressure but find no strong evidence of\nthese galaxies having systematically enhanced star formation. Finally, we\ninvestigate the properties of the most asymmetric galaxies in our sample and\nagain find no strong evidence of ram pressure-induced star formation\nenhancement. We conclude that any star formation enhancement must be small for\ninfalling galaxies, suggesting that this effect is either uncommon or\nshort-lived.",
        "The operational calculus associated with Hermite numbers has been shown to be\nan effective tool for simplifying the study of special functions. Within this\ncontext, Hermite polynomials have been viewed as Newton binomials, with the\nconsequent possibility of establishing previously unknown properties. In this\narticle, this method is extended to study the lacunary Hermite polynomials and\nobtain novel results concerning their generating functions, recurrence\nrelations, differential equations and certain integral transforms. Furthermore,\nwe extend the idea to combinatorial interpretation of these polynomials,\nbroadening their applicability in mathematical analysis and discrete\nstructures.",
        "Differentiable economics, which uses neural networks as function\napproximators and gradient-based optimization in automated mechanism design\n(AMD), marked a significant breakthrough with the introduction of RegretNet\n\\citep{regretnet_paper}. It combines the flexibility of deep learning with a\nregret-based approach to relax incentive compatibility, allowing for\napproximations of revenue-maximizing auctions. However, applying these\ntechniques to combinatorial auctions (CAs) - where bidders value bundles rather\nthan individual items, capturing item interdependencies - remains a challenge,\nprimarily due to the lack of methodologies that can effectively deal with\ncombinatorial constraints. To tackle this, we propose two architectures: CANet,\na fully connected neural network, and CAFormer, a transformer-based model\ndesigned to learn optimal randomized mechanisms. Unlike existing methods in\ntraditional AMD, our approach is more scalable and free of assumptions about\nthe structures of allowable bundles or bidder valuations. We demonstrate that\nour models match current methods in non-combinatorial settings and set new\nbenchmarks for CAs. Specifically, our models consistently outperform benchmark\nmechanisms derived from heuristic approaches and provide empirical solutions\nwhere analytical results are unavailable. This work bridges the gap in applying\ndifferentiable economics to combinatorial auctions, offering a scalable and\nflexible framework for designing revenue-maximizing mechanisms.",
        "The bolometric radiation from a central body is potentially a powerful driver\nof atmospheric escape from planets or satellites. When heated above their\nequilibrium temperatures those satellites, due to their low surface gravity,\nare be prone to significant atmospheric erosion. Such high temperatures can be\nreached through a known mechanism: a large ratio of the irradiation to\nre-radiation opacities of the atmospheric species. We investigate this\nmechanism for irradiating black-bodies of sub-stellar temperatures and find\nthat specific molecules exist, such as $\\rm NH_3$ and $\\rm CH_4$, which develop\ntemperature inversions under the irradiation of young post-formation giant\nplanets. These non-isothermal temperature profiles lead to escape rates that\ncan significantly exceed isothermal Parker-model escape rates evaluated at the\nsatellite's equilibrium temperature. Our results indicate that exo-satellites\ncan lose most of their atmospheric mass through this mechanism if the cooling\nof the exo-satellite's interior is not too rapid. In all scenarios, we find a\nhierarchical ordering of escape rates of atmospheric species due to thermal\ndecoupling in the upper atmosphere. This thermal decoupling leads to a natural\ndepletion of $\\rm CH_4$ and retention of $\\rm NH_3$ in our models. We find that\ngiant planets with masses above 2$m_{\\rm Jup}$, for cold starts and above\n1$m_{\\rm Jup}$ in hot start scenarios are able to remove the majority of a\nTitan analogue's atmosphere. Hence, finding and characterizing exomoon\natmospheres in hypothetical future surveys can constrain the post-formation\ncooling behaviour of giant planets.",
        "We investigate the ultrafast dynamics of the quasi-one-dimensional Kondo\nlattice CeCo$_2$Ga$_8$ using optical pump-probe spectroscopy. Time-resolved\npump-probe reflectivity measurements reveal a strong anisotropy in the\nphotoinduced response, which is a direct consequence of the material's unique\nelectronic structure. The temperature dependence of the relaxation dynamics\nprovides evidence for the formation of two distinct hybridization gaps that\nappear at different temperatures in the heavy fermion state. A direct gap of\n2$\\Delta_{dir}$ $\\approx$ 50 meV that persists up to $T^\\dag$ $\\approx$ 90 K,\nwell above the coherence temperature $T^*$ $\\approx$ 20 K. We attribute this\nhigher-temperature gap to the hybridization fluctuations. An indirect gap of\n2$\\Delta_{ind}$ $\\approx$ 10 meV opens closer to $T^*$, signifying the\ndevelopment of long-range coherence in the heavy fermion state. Furthermore, we\nfind that the hybridization gap can be suppressed with increasing pump fluence,\nindicating a delicate interplay between photoexcitation and the coherent heavy\nfermion state. Our results provide insights into the interplay of Kondo physics\nand low dimensionality in CeCo$_2$Ga$_8$, and establish ultrafast optical\nspectroscopy as a sensitive probe of anisotropic hybridization in heavy fermion\nmaterials.",
        "Control policies that can achieve high task performance and satisfy safety\nconstraints are desirable for any system, including multi-agent systems (MAS).\nOne promising technique for ensuring the safety of MAS is distributed control\nbarrier functions (CBF). However, it is difficult to design distributed\nCBF-based policies for MAS that can tackle unknown discrete-time dynamics,\npartial observability, changing neighborhoods, and input constraints,\nespecially when a distributed high-performance nominal policy that can achieve\nthe task is unavailable. To tackle these challenges, we propose DGPPO, a new\nframework that simultaneously learns both a discrete graph CBF which handles\nneighborhood changes and input constraints, and a distributed high-performance\nsafe policy for MAS with unknown discrete-time dynamics. We empirically\nvalidate our claims on a suite of multi-agent tasks spanning three different\nsimulation engines. The results suggest that, compared with existing methods,\nour DGPPO framework obtains policies that achieve high task performance\n(matching baselines that ignore the safety constraints), and high safety rates\n(matching the most conservative baselines), with a constant set of\nhyperparameters across all environments.",
        "We consider the complex and symplectic elliptic Ginibre matrices of size\n$(c+1)N \\times (c+1)N$, conditioned to have a deterministic eigenvalue at $ p\n\\in \\mathbb{R} $ with multiplicity $ c N $. We show that their limiting\nspectrum is either simply connected, doubly connected, or composed of two\ndisjoint simply connected components. Moreover, denoting by $\\tau \\in [0,1]$\nthe non-Hermiticity parameter, we explicitly characterise the regions in the\nparameter space $ (p, c, \\tau) $ where each topological type emerges. For cases\nwhere the droplet is either simply or doubly connected, we provide an explicit\ndescription of the limiting spectrum and the corresponding electrostatic\nenergies. As an application, we derive the asymptotic behaviour of the moments\nof the characteristic polynomial for elliptic Ginibre matrices in the\nexponentially varying regime.",
        "Advancements in imaging technology have enabled hardware to support 10 to 16\nbits per channel, facilitating precise manipulation in applications like image\nediting and video processing. While deep neural networks promise to recover\nhigh bit-depth representations, existing methods often rely on scale-invariant\nimage information, limiting performance in certain scenarios. In this paper, we\nintroduce a novel approach that integrates a super-resolution architecture to\nextract detailed a priori information from images. By leveraging interpolated\ndata generated during the super-resolution process, our method achieves\npixel-level recovery of fine-grained color details. Additionally, we\ndemonstrate that spatial features learned through the super-resolution process\nsignificantly contribute to the recovery of detailed color depth information.\nExperiments on benchmark datasets demonstrate that our approach outperforms\nstate-of-the-art methods, highlighting the potential of super-resolution for\nhigh-fidelity color restoration.",
        "Quantum state tomography (QST) is a widely employed technique for\ncharacterizing the state of a quantum system. However, it is plagued by two\nfundamental challenges: computational and experimental complexity grows\nexponentially with the number of qubits, rendering experimental implementation\nand data post-processing arduous even for moderately sized systems. Here, we\nintroduce gradient-descent (GD) algorithms for the post-processing step of QST\nin discrete- and continuous-variable systems. To ensure physically valid state\nreconstruction at each iteration step of the algorithm, we use various\ndensity-matrix parameterizations: Cholesky decomposition, Stiefel manifold, and\nprojective normalization. These parameterizations have the added benefit of\nenabling a rank-controlled ansatz, which simplifies reconstruction when there\nis prior information about the system. We benchmark the performance of our\nGD-QST techniques against state-of-the-art methods, including constrained\nconvex optimization, conditional generative adversarial networks, and iterative\nmaximum likelihood estimation. Our comparison focuses on time complexity,\niteration counts, data requirements, state rank, and robustness against noise.\nWe find that rank-controlled ansatzes in our stochastic mini-batch GD-QST\nalgorithms effectively handle noisy and incomplete data sets, yielding\nsignificantly higher reconstruction fidelity than other methods. Simulations\nachieving full-rank seven-qubit QST in under three minutes on a standard\nlaptop, with 18 GB of RAM and no dedicated GPU, highlight that GD-QST is\ncomputationally more efficient and outperforms other techniques in most\nscenarios, offering a promising avenue for characterizing noisy\nintermediate-scale quantum devices. Our Python code for GD-QST algorithms is\npublicly available at https:\/\/github.com\/mstorresh\/GD-QST.",
        "We developed a generative patch based 3D image registration model that can\nregister very high resolution images obtained from a biochemical process name\ntissue clearing. Tissue clearing process removes lipids and fats from the\ntissue and make the tissue transparent. When cleared tissues are imaged with\nLight-sheet fluorescent microscopy, the resulting images give a clear window to\nthe cellular activities and dynamics inside the tissue.Thus the images obtained\nare very rich with cellular information and hence their resolution is extremely\nhigh (eg .2560x2160x676). Analyzing images with such high resolution is a\ndifficult task for any image analysis pipeline.Image registration is a common\nstep in image analysis pipeline when comparison between images are required.\nTraditional image registration methods fail to register images with such\nextant. In this paper we addressed this very high resolution image registration\nissue by proposing a patch-based generative network named InvGAN. Our proposed\nnetwork can register very high resolution tissue cleared images. The tissue\ncleared dataset used in this paper are obtained from a tissue clearing protocol\nnamed CUBIC. We compared our method both with traditional and deep-learning\nbased registration methods.Two different versions of CUBIC dataset are used,\nrepresenting two different resolutions 25% and 100% respectively. Experiments\non two different resolutions clearly show the impact of resolution on the\nregistration quality. At 25% resolution, our method achieves comparable\nregistration accuracy with very short time (7 minutes approximately). At 100%\nresolution, most of the traditional registration methods fail except Elastix\nregistration tool.Elastix takes 28 hours to register where proposed InvGAN\ntakes only 10 minutes.",
        "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
        "Spatial transcriptomics (ST) measures gene expression at fine-grained spatial\nresolution, offering insights into tissue molecular landscapes. Previous\nmethods for spatial gene expression prediction usually crop spots of interest\nfrom pathology tissue slide images, and learn a model that maps each spot to a\nsingle gene expression profile. However, it fundamentally loses spatial\nresolution of gene expression: 1) each spot often contains multiple cells with\ndistinct gene expression; 2) spots are cropped at fixed resolutions, limiting\nthe ability to predict gene expression at varying spatial scales. To address\nthese limitations, this paper presents PixNet, a dense prediction network\ncapable of predicting spatially resolved gene expression across spots of\nvarying sizes and scales directly from pathology images. Different from\nprevious methods that map individual spots to gene expression values, we\ngenerate a dense continuous gene expression map from the pathology image, and\naggregate values within spots of interest to predict the gene expression. Our\nPixNet outperforms state-of-the-art methods on 3 common ST datasets, while\nshowing superior performance in predicting gene expression across multiple\nspatial scales. The source code will be publicly available.",
        "When a human requests an LLM to complete a coding task using functionality\nfrom a large code repository, how do we provide context from the repo to the\nLLM? One approach is to add the entire repo to the LLM's context window.\nHowever, most tasks involve only fraction of symbols from a repo, longer\ncontexts are detrimental to the LLM's reasoning abilities, and context windows\nare not unlimited. Alternatively, we could emulate the human ability to\nnavigate a large repo, pick out the right functionality, and form a plan to\nsolve the task. We propose MutaGReP (Mutation-guided Grounded Repository Plan\nSearch), an approach to search for plans that decompose a user request into\nnatural language steps grounded in the codebase. MutaGReP performs neural tree\nsearch in plan space, exploring by mutating plans and using a symbol retriever\nfor grounding. On the challenging LongCodeArena benchmark, our plans use less\nthan 5% of the 128K context window for GPT-4o but rival the coding performance\nof GPT-4o with a context window filled with the repo. Plans produced by\nMutaGReP allow Qwen 2.5 Coder 32B and 72B to match the performance of GPT-4o\nwith full repo context and enable progress on the hardest LongCodeArena tasks.\nProject page: zaidkhan.me\/MutaGReP",
        "Estimating the exoplanet scientific productivity of the Habitable Worlds\nObservatory requires estimating science exposure times. From exoplanet yields\nto spectral retrievals, exposure times are at the heart of our understanding of\nthe capabilities of this future mission. As such, ensuring accuracy and\nconsistency between different exposure time calculators (ETCs) is critical. We\nsummarize the efforts of the Exoplanet Science Yield sub-Working Group's ETC\nCalibration Task Group, which conducted a calibration study from March 4 to\nJune 30 of 2024. We compare three commonly-used coronagraphic exposure time\ncalculators. We find that the ETCs use a broad variety of differing methods,\nassumptions, and inputs that produce variation in the final exposure times at\nthe ~60% level. The causes for the disagreement have largely been identified,\nflagged for further development efforts, and in some cases retired since the\nconclusion of this effort. We expect that addressing the flagged efforts will\nbring the ETCs to within better than ~30% agreement.",
        "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.",
        "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https:\/\/github.com\/ccccai239\/PixelRIST.",
        "The theory of optimal transportation has developed into a powerful and\nelegant framework for comparing probability distributions, with wide-ranging\napplications in all areas of science. The fundamental idea of analyzing\nprobabilities by comparing their underlying state space naturally aligns with\nthe core idea of causal inference, where understanding and quantifying\ncounterfactual states is paramount. Despite this intuitive connection, explicit\nresearch at the intersection of optimal transport and causal inference is only\nbeginning to develop. Yet, many foundational models in causal inference have\nimplicitly relied on optimal transport principles for decades, without\nrecognizing the underlying connection. Therefore, the goal of this review is to\noffer an introduction to the surprisingly deep existing connections between\noptimal transport and the identification of causal effects with observational\ndata -- where optimal transport is not just a set of potential tools, but\nactually builds the foundation of model assumptions. As a result, this review\nis intended to unify the language and notation between different areas of\nstatistics, mathematics, and econometrics, by pointing out these existing\nconnections, and to explore novel problems and directions for future work in\nboth areas derived from this realization.",
        "Group Decision-Making (GDM) plays a crucial role in various real-life\nscenarios where individuals express their opinions in natural language rather\nthan structured numerical values. Traditional GDM approaches often overlook the\nsubjectivity and ambiguity present in human discussions, making it challenging\nto achieve a fair and consensus-driven decision. This paper proposes a fuzzy\nconsensus-based group decision-making system that integrates sentiment and\nemotion analysis to extract preference values from textual inputs. The proposed\nframework combines explicit voting preferences with sentiment scores derived\nfrom chat discussions, which are then processed using a Fuzzy Inference System\n(FIS) to compute a total preference score for each alternative and determine\nthe top-ranked option. To ensure fairness in group decision-making, we\nintroduce a fuzzy logic-based consensus measurement model that evaluates\nparticipants' agreement and confidence levels to assess overall feedback. To\nillustrate the effectiveness of our approach, we apply the methodology to a\nrestaurant selection scenario, where a group of individuals must decide on a\ndining option based on brief chat discussions. The results demonstrate that the\nfuzzy consensus mechanism successfully aggregates individual preferences and\nensures a balanced outcome that accurately reflects group sentiment.",
        "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
        "The four-gluon one-particle irreducible Green function contributes to various\nquantities with phenomenological relevance. An example where the four-gluon\nplays a role is the determination of the gluon propagator, a basic building\nblock for QCD, using continuum methods. This four leg Green function is poorly\nknown and we are only starting to grasp its non-perturbative structure. Here,\nwe report on the computation of the one-particle irreducible four-gluon Green\nfunction, in the Landau gauge, with lattice simulations. Besides stating the\nproblems associated with the computation, several form factors that\ncharacterise this Green function are measured.",
        "Shadow removal aims to restore the image content in shadowed regions. While\ndeep learning-based methods have shown promising results, they still face key\nchallenges: 1) uncontrolled removal of all shadows, or 2) controllable removal\nbut heavily relies on precise shadow region masks. To address these issues, we\nintroduce a novel paradigm: prompt-aware controllable shadow removal. Unlike\nexisting approaches, our paradigm allows for targeted shadow removal from\nspecific subjects based on user prompts (e.g., dots, lines, or subject masks).\nThis approach eliminates the need for shadow annotations and offers flexible,\nuser-controlled shadow removal. Specifically, we propose an end-to-end\nlearnable model, the Prompt-Aware Controllable Shadow Removal Network\n(PACSRNet). PACSRNet consists of two key modules: a prompt-aware module that\ngenerates shadow masks for the specified subject based on the user prompt, and\na shadow removal module that uses the shadow prior from the first module to\nrestore the content in the shadowed regions. Additionally, we enhance the\nshadow removal module by incorporating feature information from the\nprompt-aware module through a linear operation, providing prompt-guided support\nfor shadow removal. Recognizing that existing shadow removal datasets lack\ndiverse user prompts, we contribute a new dataset specifically designed for\nprompt-based controllable shadow removal. Extensive experimental results\ndemonstrate the effectiveness and superiority of PACSRNet.",
        "The electronic structure of pure niobium and the niobium-titanium alloy\nNb$_{0.44}$Ti$_{0.56}$ in the bcc-phase at pressures up to $250$ GPa is\ninvestigated, to reveal possible factors conducing toward the robust\nsuperconductivity reported for Ti-doped niobium upon a considerable volume\nreduction. We model the structural disorder using the coherent potential\napproximation, and the electronic correlations are taken into account using\ndynamical mean-field theory. At high pressure, a significant change in the\ntopology of the Fermi surface is observed, while electronic correlations weaken\nwith increasing pressure. Thus, the normal state of Nb$_{0.44}$Ti$_{0.56}$ is\nfound to be a Fermi liquid with a well-defined Fermi surface, and well-defined\nquasiparticles near it. The systematic study of the impact of disorder upon the\nFermi surface at such ultra high pressures allows notable insights into the\nnature of the electronic states near the Fermi level, i.e., within the energy\nscale relevant for superconducting pairing. Furthermore, our results clearly\nindicate the necessity of further experimental Fermi surface explorations.",
        "Event prediction tasks often handle spatio-temporal data distributed in a\nlarge spatial area. Different regions in the area exhibit different\ncharacteristics while having latent correlations. This spatial heterogeneity\nand correlations greatly affect the spatio-temporal distributions of event\noccurrences, which has not been addressed by state-of-the-art models. Learning\nspatial dependencies of events in a continuous space is challenging due to its\nfine granularity and a lack of prior knowledge. In this work, we propose a\nnovel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event\nprediction. It adopts an encoder-decoder architecture that jointly models the\nstate dynamics of spatially localized regions using neural Ordinary\nDifferential Equations (ODEs). The state evolution is built on the foundation\nof a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial\ndependencies. By adaptively localizing the anchor nodes in the space and\njointly constructing the correlation edges between them, the SAAG enhances the\nmodel's ability of learning complex spatial event patterns. The proposed GSTPP\nmodel greatly improves the accuracy of fine-grained event prediction. Extensive\nexperimental results show that our method greatly improves the prediction\naccuracy over existing spatio-temporal event prediction approaches.",
        "We demonstrate that a symmetry of the local gyrokinetic model is broken when\nthe safety factor q is almost (but not exactly) a rational number and magnetic\nshear is $\\hat{s} \\approx 0$. Tokamaks with such a q profile will spontaneously\nrotate due to turbulent momentum transport. Nonlinear gyrokinetic simulations\nindicate this mechanism is significantly stronger than all other drives of\nintrinsic rotation. It also generates intrinsic electric current that pulls q\ntowards rational values, potentially aiding non-inductive current drive. This\nis likely important in the triggering of internal transport barriers."
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window",
    "start_abstract":"SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Fast fit-free analysis of fluorescence lifetime imaging via deep learning"
      ],
      "abstract":[
        "Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Testing the limits of ITkPixV2: the ATLAS inner tracker pixel detector\n  readout chip",
        "Analysis of pitchfork bifurcations and symmetry breaking in the elliptic\n  restricted three-body problem",
        "Range-Only Localization System for Small-Scale Flapping-Wing Robots",
        "Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding",
        "A Novel Interpretation of the Radon Transform's Ray- and Pixel-Driven\n  Discretizations under Balanced Resolutions",
        "ADAPT: An Autonomous Forklift for Construction Site Operation",
        "Perimeter length of the convex hull of Brownian motion in the hyperbolic\n  plane",
        "Post-disaster building indoor damage and survivor detection using\n  autonomous path planning and deep learning with unmanned aerial vehicles",
        "On families of strongly divisible modules of rank 2",
        "On-demand storage and retrieval of single photons from a semiconductor\n  quantum dot in a room-temperature atomic vapor memory",
        "High Energy Jet Emission from GRS 1758-258 & 1E 1740.7-2942 with\n  INTEGRAL?",
        "Emotional Multifaceted Feedback on AI Tool Use in EFL Learning\n  Initiation: Chain-Mediated Effects of Motivation and Metacognitive Strategies\n  in an Optimized TAM Model",
        "Derivation of the Planck Units Based in a Membranes Model",
        "Nonparametric Smoothing of Directional and Axial Data",
        "Carbonic anhydrase II simulated with a universal neural network\n  potential",
        "MADS: Multi-Attribute Document Supervision for Zero-Shot Image\n  Classification",
        "Enhanced Tuberculosis Bacilli Detection using Attention-Residual U-Net\n  and Ensemble Classification",
        "Iterative Counterfactual Data Augmentation",
        "Investigating the Effect of Relaxation Time on Richtmyer-Meshkov\n  Instability under Reshock Impact: A Two-Component Discrete Boltzmann Method\n  Study",
        "Planar tropical caustics: trivalency and convexity",
        "Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X\n  Collaboration",
        "Axial current as the origin of quantum intrinsic orbital angular\n  momentum",
        "Expected Return Symmetries",
        "Counterexamples for T\\\"urkelli's Modification on Malle's Conjecture",
        "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation",
        "Global well-posedness of Vlasov-Poisson-Boltzmann equations with neutral\n  initial data and small relative entropy",
        "Improving LLM-as-a-Judge Inference with the Judgment Distribution",
        "Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation",
        "Ion-Trap Chip Architecture Optimized for Implementation of Quantum\n  Error-Correcting Code"
      ],
      "abstract":[
        "The ITkPixV2 chip is the final production readout chip for the ATLAS Phase 2\nInner Tracker (ITk) upgrade at the upcoming High-Luminosity LHC (HL-LHC). Due\nto the extraordinarily high peak luminosity at the HL-LHC of $5 \\times 10^{34}$\ncm$^{-1}$s$^{-1}$, ITkPixV2 must meet significant increases in nearly all\ndesign requirements, including a 10x increase in trigger rate, a 7.5x increase\nin hit rate, a 3x increase in radiation tolerance, and a 12.5x decrease in\npixel current draw per unit area, all while maintaining a similar power per\nunit area as present pixel detectors. Here we present the first measurements of\nthe ITkPixV2 chip operated at the limits of the full chip design requirements,\nincluding in particular a measurement of the activity-induced current of the\nchip as a function of increasing hit rate.",
        "A unified framework is proposed to quantitatively characterize pitchfork\nbifurcations and associated symmetry breaking in the elliptic restricted\nthree-body problem (ERTBP). It is known that planar\/vertical Lyapunov orbits\nand Lissajous orbits near the collinear libration points undergo pitchfork\nbifurcations with varying orbital energy. These bifurcations induce symmetry\nbreaking, generating bifurcated families including halo\/quasi-halo orbits,\naxial\/quasi-axial orbits, and their corresponding invariant manifolds.\nTraditional semi-analytical methods for constructing halo orbits, based on\nresonant bifurcation mechanisms, have obstacles in fully exploiting the\nintrinsic symmetry breaking characteristics in pitchfork bifurcations. In this\npaper, we propose a unified trigonometric series-based framework to analyze\nthese bifurcated families from the perspective of coupling-induced bifurcation\nmechanisms. By introducing a coupling coefficient and various bifurcation\nequations into the ERTBP, different symmetry breaking is achieved when the\ncoupling coefficient is non-zero. This unified semi-analytical framework\ncaptures bifurcations of both periodic\/quasi-periodic and transit\/non-transit\norbits. Furthermore, it reveals that pitchfork bifurcation solutions in the\nERTBP fundamentally depend solely on the orbital eccentricity and three\namplitude parameters of the system's degrees of freedom, governing both the\nelliptic direction and the hyperbolic one.",
        "The design of localization systems for small-scale flapping-wing aerial\nrobots faces relevant challenges caused by the limited payload and onboard\ncomputational resources. This paper presents an ultra-wideband localization\nsystem particularly designed for small-scale flapping-wing robots. The solution\nrelies on custom 5 grams ultra-wideband sensors and provides robust, very\nefficient (in terms of both computation and energy consumption), and accurate\n(mean error of 0.28 meters) 3D position estimation. We validate our system\nusing a Flapper Nimble+ flapping-wing robot.",
        "We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.",
        "Tomographic investigations are a central tool in medical applications,\nallowing doctors to image the interior of patients. The corresponding\nmeasurement process is commonly modeled by the Radon transform. In practice,\nthe solution of the tomographic problem requires discretization of the Radon\ntransform and its adjoint (called the backprojection). There are various\ndiscretization schemes; often structured around three discretization\nparameters: spatial-, detector-, and angular resolutions. The most widespread\napproach uses the ray-driven Radon transform and the pixel-driven\nbackprojection in a balanced resolution setting, i.e., the spatial resolution\nroughly equals the detector resolution. The use of these particular\ndiscretization approaches is based on anecdotal reports of their approximation\nperformance, but there is little rigorous analysis of these methods'\napproximation errors. This paper presents a novel interpretation of ray-driven\nand pixel-driven methods as convolutional discretizations, illustrating that\nfrom an abstract perspective these methods are similar. Moreover, we announce\nstatements concerning the convergence of the ray-driven Radon transform and the\npixel-driven backprojection under balanced resolutions. Our considerations are\nsupported by numerical experiments highlighting aspects of the discussed\nmethods.",
        "Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics.",
        "We relate the expected hyperbolic length of the perimeter of the convex hull\nof the trajectory of Brownian motion in the hyperbolic plane to an expectation\nof a certain exponential functional of a one-dimensional real-valued Brownian\nmotion, and hence derive small- and large-time asymptotics for the expected\nhyperbolic perimeter. In contrast to the case of Euclidean Brownian motion with\nnon-zero drift, the large-time asymptotics are a factor of two greater than the\nlower bound implied by the fact that the convex hull includes the hyperbolic\nline segment from the origin to the endpoint of the hyperbolic Brownian motion.\nWe also obtain an exact expression for the expected perimeter length after an\nindependent exponential random time.",
        "Rapid response to natural disasters such as earthquakes is a crucial element\nin ensuring the safety of civil infrastructures and minimizing casualties.\nTraditional manual inspection is labour-intensive, time-consuming, and can be\ndangerous for inspectors and rescue workers. This paper proposed an autonomous\ninspection approach for structural damage inspection and survivor detection in\nthe post-disaster building indoor scenario, which incorporates an autonomous\nnavigation method, deep learning-based damage and survivor detection method,\nand a customized low-cost micro aerial vehicle (MAV) with onboard sensors.\nExperimental studies in a pseudo-post-disaster office building have shown the\nproposed methodology can achieve high accuracy in structural damage inspection\nand survivor detection. Overall, the proposed inspection approach shows great\npotential to improve the efficiency of existing manual post-disaster building\ninspection.",
        "Let $p$ be an odd prime, and $\\mathbf{Q}_{p^f}$ the unramified extension of\n$\\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of\nconstructing strongly divisible modules for $2$-dimensional semi-stable\nnon-crystalline representations of\n$\\mathrm{Gal}(\\overline{\\mathbf{Q}}_p\/\\mathbf{Q}_{p^f})$ with Hodge--Tate\nweights in the Fontaine--Laffaille range to solving systems of linear equations\nand inequalities. We also determine the Breuil modules corresponding to the\nmod-$p$ reduction of the strongly divisible modules. We expect our method to\nproduce at least one Galois-stable lattice in each such representation for\ngeneral $f$. Moreover, when the mod-$p$ reduction is an extension of distinct\ncharacters, we further expect our method to provide the two non-homothetic\nlattices. As applications, we show that our approach recovers previously known\nresults for $f=1$ and determine the mod-$p$ reduction of the semi-stable\nrepresentations with some small Hodge--Tate weights when $f=2$.",
        "Interfacing light from solid-state single-photon sources with scalable and\nrobust room-temperature quantum memories has been a long-standing challenge in\nphotonic quantum information technologies due to inherent noise processes and\ntime-scale mismatches between the operating conditions of solid-state and\natomic systems. Here, we demonstrate on-demand storage and retrieval of single\nphotons from a semiconductor quantum dot device in a room-temperature atomic\nvapor memory. A deterministically fabricated InGaAs quantum dot light source\nemits single photons at the wavelength of the cesium D1 line at 895\\,nm which\nexhibit an inhomogeneously broadened linewidth of 5.1(7)\\,GHz and are\nsubsequently stored in a low-noise ladder-type cesium vapor memory. We show\ncontrol over the interaction between the single photons and the atomic vapor,\nallowing for variable retrieval times of up to 19.8(3)\\,ns at an internal\nefficiency of $\\eta_\\mathrm{int}=0.6(1)\\%$. Our results significantly expand\nthe application space of both room-temperature vapor memories and semiconductor\nquantum dots in future quantum network architectures.",
        "GRS 1758-258 and 1E 1740.7-2942 are two long-known persistent black hole\nbinaries in the Galactic Center region. Using INTEGRAL's extensive monitoring\nof the Galactic Center and Bulge, we studied their temporal and spectral\nevolutions in the 30-610 keV energy range from March 2003 through April 2022\nwith the IBIS\/ISGRI gamma-ray telescope. Our analyses found that the sources\ntypically had Comptonized spectra, though not always with the same parameters.\nThe spectral states with more than 8 Ms of observation time show deviations\nfrom a Comptonized spectrum above ~200 keV or a \"hard tail\" that extends up to\nat least 600 keV. The origin of this component remains debated with the most\npopular scenarios being synchrotron emission from the jet or Comptonization in\na hybrid thermal\/non-thermal plasma. Anyway, the GRS 1758-258 and 1E\n1740.7-2942 spectra are acceptably described by CompTT+po (jet) and Eqpair\n(hybrid Comptonization) scenarios. To differentiate between the two scenarios,\nwe calculated the Spearman correlation coefficient comparing 30-50 keV count\nrates with those in higher energy bands (50-100, 100-300, and 300-600 keV). The\ncount rates below 300 keV are strongly correlated, indicating those photons\narise from the same physical process. Above 300 keV the count rates are either\nanti-correlated or not correlated with the 30-50 keV count rates for GRS\n1758-258, which suggests that the photons originate from a different physical\nprocess. For 1E 1740.7-2942, the level of correlation is unclear due to scatter\nin the data points. However, the 300-600 keV count rates are consistent with a\nconstant value. This disfavors the hybrid Comptonization scenario for both\nsources.",
        "This study specifically investigates the initiation phase of EFL learners'\nengagement with AI tools, focusing on how technology acceptance constructs\nperceived usefulness (PU), perceived ease of use (PEOU), and perceived\nself-efficacy (PSE) influence learning resilience. Drawing on an optimized\nTechnology Acceptance Model (TAM) and integrating constructs from positive\npsychology, the study examines the chain-mediated effects of learning\nmotivation (LM) and metacognitive strategies (MS) on resilience outcomes,\noperationalized through optimism (OP), psychological resilience (PR), and\ngrowth mindset (GM). A survey of first-year English majors (N = 730) was\nconducted, and structural equation modeling was employed to analyze the data.\nThe findings indicate that favorable perceptions of AI tools are significantly\nassociated with enhanced LM and MS, which in turn positively impact resilience\nmeasures. These results suggest that the interplay between technology\nacceptance and internal regulatory processes is vital in shaping EFL learners'\nearly experiences with AI-assisted learning. Practical implications for\neducators and researchers are discussed, with an emphasis on promoting\nuser-friendly and effective AI environments to support the development of\nadaptive learning behaviors.",
        "In this study, the Planck units (mass, time and length) have only been\nderived, explained and attributed a physical meaning when they were deduced\nbased on the concept of interacting membranes (membranes instead of strings of\nstring theory). For this purpose, a set of five assumptions were proposed: (a)\nthe existence of the interacting membranes; (b) the curvatures of the membranes\noscillate according to the classical wave equation; (c) the spatial period of\nthe wave that arise when the membranes oscillate is given by $\\lambda =\n{\\xi}{\\pi}\/k$; (d) the membranes oscillate with wavelength given by de Broglie\nrelation and (e) $x=ct$ holds. The parameter $\\xi$ determines the period of\noscillation of the given membranes. In deriving the Planck units in this work,\n$\\xi$ must take the value 2 and determines a period 2$\\pi$, closely to minimum\nvalue 1 or to fundamental period $\\pi$, respectively. In this context, Planck\nunits must be fundamental. Moreover, the parameter $\\xi$ was reported as a\nunification parameter between the formulas for the Coulomb$^{\\prime}$s law and\nNewton$^{\\prime}$s law of universal gravitation linking the forces of\nmicroworld and macroworld. Depending on the value $\\xi$ takes, one force or\nanother will be had. It is also shown that the potential $V = hc\/{\\xi}{\\pi}x$\ndeduced from the above assumptions and which contributes to deduce the Planck\nunits, can be derived from Yukawa$^{\\prime}$s equation. Hence, the present work\nwould be contributing to theoretical physics, since at the Planck scale\npredictions of some theories like Standard Model, quantum field theory and\ngeneral relativity are not expected to be valid.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "The carbonic anhydrase II enzyme (CA II) is one of the most significant\nenzymes in nature, reversibly converting CO$_2$ to bicarbonate at a remarkable\nrate. The precise mechanism it uses to achieve this rapid turnover remains\nunclear due to our inability to directly observe or simulate the full process\ndynamically. Here, we use a recently developed universal neural network\npotential (Orb) to simulate the active site of CA II. We reproduce several\nknown features of the reaction mechanism, including the proton relay that\nconducts protons out of the active site to the His64 residue. Additionally, we\nobserve a new reaction pathway where CO$_2$ reacts with a water molecule in the\nactive site, which donates a proton to the zinc-bound hydroxide. This differs\nfrom the established mechanism where CO$_2$ directly reacts with hydroxide.\nExisting experimental data and independent quantum chemistry calculations are\nused to support the plausibility of this new mechanism. This demonstrates the\npotential of Orb to efficiently generate novel insights into important\nmolecular scale processes that can potentially be harnessed to improve CO$_2$\ncapture technologies and drug design.",
        "Zero-shot learning (ZSL) aims to train a model on seen classes and recognize\nunseen classes by knowledge transfer through shared auxiliary information.\nRecent studies reveal that documents from encyclopedias provide helpful\nauxiliary information. However, existing methods align noisy documents,\nentangled in visual and non-visual descriptions, with image regions, yet solely\ndepend on implicit learning. These models fail to filter non-visual noise\nreliably and incorrectly align non-visual words to image regions, which is\nharmful to knowledge transfer. In this work, we propose a novel multi-attribute\ndocument supervision framework to remove noises at both document collection and\nmodel learning stages. With the help of large language models, we introduce a\nnovel prompt algorithm that automatically removes non-visual descriptions and\nenriches less-described documents in multiple attribute views. Our proposed\nmodel, MADS, extracts multi-view transferable knowledge with information\ndecoupling and semantic interactions for semantic alignment at local and global\nlevels. Besides, we introduce a model-agnostic focus loss to explicitly enhance\nattention to visually discriminative information during training, also\nimproving existing methods without additional parameters. With comparable\ncomputation costs, MADS consistently outperforms the SOTA by 7.2% and 8.2% on\naverage in three benchmarks for document-based ZSL and GZSL settings,\nrespectively. Moreover, we qualitatively offer interpretable predictions from\nmultiple attribute views.",
        "Tuberculosis (TB), caused by Mycobacterium tuberculosis, remains a critical\nglobal health issue, necessitating timely diagnosis and treatment. Current\nmethods for detecting tuberculosis bacilli from bright field microscopic sputum\nsmear images suffer from low automation, inadequate segmentation performance,\nand limited classification accuracy. This paper proposes an efficient hybrid\napproach that combines deep learning for segmentation and an ensemble model for\nclassification. An enhanced U-Net model incorporating attention blocks and\nresidual connections is introduced to precisely segment microscopic sputum\nsmear images, facilitating the extraction of Regions of Interest (ROIs). These\nROIs are subsequently classified using an ensemble classifier comprising\nSupport Vector Machine (SVM), Random Forest, and Extreme Gradient Boost\n(XGBoost), resulting in an accurate identification of bacilli within the\nimages. Experiments conducted on a newly created dataset, along with public\ndatasets, demonstrate that the proposed model achieves superior segmentation\nperformance, higher classification accuracy, and enhanced automation compared\nto existing methods.",
        "Counterfactual data augmentation (CDA) is a method for controlling\ninformation or biases in training datasets by generating a complementary\ndataset with typically opposing biases. Prior work often either relies on\nhand-crafted rules or algorithmic CDA methods which can leave unwanted\ninformation in the augmented dataset. In this work, we show iterative CDA\n(ICDA) with initial, high-noise interventions can converge to a state with\nsignificantly lower noise. Our ICDA procedure produces a dataset where one\ntarget signal in the training dataset maintains high mutual information with a\ncorresponding label and the information of spurious signals are reduced. We\nshow training on the augmented datasets produces rationales on documents that\nbetter align with human annotation. Our experiments include six human produced\ndatasets and two large-language model generated datasets.",
        "The Richtmyer-Meshkov (RM) instability plays an important role in various\nnatural and engineering fields, such as inertial confinement fusion. In this\nwork, the effect of relaxation time on the RM instability under reshock impact\nis investigated by using a two-component discrete Boltzmann method. The\nhydrodynamic and thermodynamic characteristics of the fluid system are\ncomprehensively analyzed from the perspectives of the density gradient,\nvorticity, kinetic energy, mixing degree, mixing width, and non-equilibrium\nintensity. Simulation results indicate that for larger relaxation time, the\ndiffusion and dissipation are enhanced, the physical gradients decrease, and\nthe growth of the interface is suppressed. Furthermore, the non-equilibrium\nmanifestations show complex patterns, driven by the competitive physical\nmechanisms of the diffusion, dissipation, shock wave, rarefaction wave,\ntransverse wave, and fluid instabilities. These findings provide valuable\ninsights into the fundamental mechanism of compressible fluid flows.",
        "Tropical caustic of a convex domain on the plane is a canonically associated\ntropical analytic curve inside the domain. In this note we give a graphical\nproof for the classification of its intermediate vertices, implying in\nparticular that they are always trivalent. Apart from that we explain how\nvarious known examples of tropical caustics are constructed and discuss the\npossibility of relaxing the convexity condition for the domain.",
        "Vehicle-to-everything (V2X) collaborative perception has emerged as a\npromising solution to address the limitations of single-vehicle perception\nsystems. However, existing V2X datasets are limited in scope, diversity, and\nquality. To address these gaps, we present Mixed Signals, a comprehensive V2X\ndataset featuring 45.1k point clouds and 240.6k bounding boxes collected from\nthree connected autonomous vehicles (CAVs) equipped with two different types of\nLiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides\nprecisely aligned point clouds and bounding box annotations across 10 classes,\nensuring reliable data for perception training. We provide detailed statistical\nanalysis on the quality of our dataset and extensively benchmark existing V2X\nmethods on it. Mixed Signals V2X Dataset is one of the highest quality,\nlarge-scale datasets publicly available for V2X perception research. Details on\nthe website https:\/\/mixedsignalsdataset.cs.cornell.edu\/.",
        "We show that it is impossible to experimentally observe the quantum intrinsic\norbital angular momentum (IOAM) effect without its axial current. Broadly\nspeaking, we argue that the spiral or interference characteristics of the axial\ncurrent density determine the occurrence of nonlinear or tunneling effects in\nany spacetimedependent quantum systems. Our findings offer a comprehensive\ntheoretical framework that addresses the limitations of Keldysh theory and\nprovides new insights into the angular momentum properties of quantum systems,\nparticularly in tunneling-dominated regimes. Using Wigner function methods,\nfermionic generalized two-level model, and Berry phase simulations, we predict\nthat IOAM effect can persist even in pure quantum tunneling processes. These\nresults open the door for experimental verification of IOAM effects in future\nhigh-intensity QED experiments, such as those using X-ray free electron lasers.",
        "Symmetry is an important inductive bias that can improve model robustness and\ngeneralization across many deep learning domains. In multi-agent settings, a\npriori known symmetries have been shown to address a fundamental coordination\nfailure mode known as mutually incompatible symmetry breaking; e.g. in a game\nwhere two independent agents can choose to move \"left'' or \"right'', and where\na reward of +1 or -1 is received when the agents choose the same action or\ndifferent actions, respectively. However, the efficient and automatic discovery\nof environment symmetries, in particular for decentralized partially observable\nMarkov decision processes, remains an open problem. Furthermore, environmental\nsymmetry breaking constitutes only one type of coordination failure, which\nmotivates the search for a more accessible and broader symmetry class. In this\npaper, we introduce such a broader group of previously unexplored symmetries,\nwhich we call expected return symmetries, which contains environment symmetries\nas a subgroup. We show that agents trained to be compatible under the group of\nexpected return symmetries achieve better zero-shot coordination results than\nthose using environment symmetries. As an additional benefit, our method makes\nminimal a priori assumptions about the structure of their environment and does\nnot require access to ground truth symmetries.",
        "We give counterexamples for the modification on Malle's Conjecture given by\nT\\\"urkelli. T\\\"urkelli's modification on Malle's conjecture is inspired by an\nanalogue of Malle's conjecture over a function field. As a result, our\ncounterexamples demonstrate that the $b$ constant can differ between function\nfields and number fields. We also show that Kl\\\"uners' counterexamples give\ncounterexamples for a natural extension of Malle's conjecture to counting\nnumber fields by product of ramified primes. We then propose a refined version\nof Malle's conjecture which implies a new conjectural value for the constant\n$b$ for number fields.",
        "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub.",
        "The dynamics of dilute plasma particles such as electrons and ions can be\nmodeled by the fundamental two species Vlasov-Poisson-Boltzmann equations,\nwhich describes mutual interactions of plasma particles through collisions in\nthe self-induced electric field. In this paper, we are concerned with global\nwell-posedness of mild solutions of the equations. We establish the global\nexistence and uniqueness of mild solutions to the two species\nVlasov-Poisson-Boltzmann equations in the torus for a class of initial data\nwith bounded time-velocity weighted $L^{\\infty}$ norm under nearly neutral\ncondition and some smallness condition on $L^1_xL^\\infty_v$ norm as well as\ndefect mass, energy and entropy so that the initial data allow large amplitude\noscillations. Due to the nonlinear effect of electric field, we consider the\nproblem in $W^{1, \\infty}_{x,v}$ with large amplitude data, new difficulty\narises when establishing globally uniform $W^{1, \\infty}_{x,v}$ bound, which\nhas been overcome based on nearly neutral condition, time-velocity weight\nfunction and a logarithmic estimate. Moreover, the large time behavior of\nsolutions in $W^{1, \\infty}_{x,v}$ norm with exponential decay rates of\nconvergence is also obtained.",
        "Using language models to scalably approximate human preferences on text\nquality (LLM-as-a-judge) has become a standard practice applicable to many\ntasks. A judgment is often extracted from the judge's textual output alone,\ntypically with greedy decoding. However, LLM judges naturally provide\ndistributions over judgment tokens, inviting a breadth of inference methods for\nextracting fine-grained preferences. We find that taking the mean of the\njudgment distribution consistently outperforms taking the mode (i.e. greedy\ndecoding) in all evaluation settings (i.e. pointwise, pairwise, and listwise).\nWe further explore novel methods of deriving preferences from judgment\ndistributions, and find that methods incorporating risk aversion often improve\nperformance. Lastly, we analyze LLM-as-a-judge paired with chain-of-thought\n(CoT) prompting, showing that CoT can collapse the spread of the judgment\ndistribution, often harming performance. Our findings suggest leveraging\ndistributional output can improve LLM-as-a-judge, as opposed to using the text\ninterface alone.",
        "First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https:\/\/github.com\/opendatalab\/ProverGen",
        "We propose a new ion-trap architecture optimized for the efficient execution\nof both transversal and non-transversal gate operations in a two-dimensional\ncolor code. By differentiating the regions for transversal gates from those for\nnon-transversal gates and syndrome extraction, which require distinct qubit\nconnectivities, our chip layout minimizes ion shuttling and simplifies physical\nimplementations. We also provide a dedicated transpiler and scheduler for this\narchitecture, wherein the scheduler coordinates the sequence of operations and\ninserts the necessary swap and shuttling operations. Finally, we developed an\nerror analyzer to evaluate the chip's performance across a variety of quantum\nalgorithms. Simulation results confirm that our architecture can significantly\nincrease success rates and reduce gate error probabilities, particularly\nlowering the effective two-qubit gate error probability to about 10^{-8} when a\nquantum error-correcting code of 31 physical qubits is employed. Our findings\nclearly show that the improvement in success rates clearly outweighs the\nruntime overhead, demonstrating that strategic hardware-scheduler co-design can\nadvance quantum systems towards reliable, large-scale computing, potentially\nsurpassing classical capabilities."
      ]
    }
  }
]