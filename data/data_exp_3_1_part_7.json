[
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction",
    "start_abstract":"Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer"
      ],
      "abstract":[
        "Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "FOCUS: First Order Concentrated Updating Scheme",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Equivalence between exponential concentration in quantum machine\n  learning kernels and barren plateaus in variational algorithms",
        "The Marginal Importance of Distortions and Alignment in CASSI systems",
        "Multi-scale physics of cryogenic liquid helium-4: Inverse\n  coarse-graining properties of smoothed particle hydrodynamics",
        "Stability of difference equations with interspecific density dependence,\n  competition, and maturation delays",
        "Design of a quantum diamond microscope with efficient scanning confocal\n  readout",
        "Timing and spectral studies of the Be\/X-ray binary EXO 2030+375 using\n  Insight-HXMT observations",
        "Numerical Study On Temperature Variations Of Superheated Steam Flowing\n  Through A Regulation Valve",
        "Visualizing quantum entanglement in Bose-Einstein condensates without\n  state vectors",
        "Runout of liquefaction-induced tailings dam failure: Influence of\n  earthquake motions and residual strength",
        "Multifunctional Altermagnet with Large Out-of-Plane Piezoelectric\n  Response in Janus V$_{2}$AsBrO Monolayer",
        "On finitary power monoids of linearly orderable monoids",
        "On the role of 5-wave resonances in the nonlinear dynamics of the\n  Fermi-Pasta-Ulam-Tsingou lattice",
        "Influences of accretion flow and dilaton charge on the images of\n  Einstein-Maxwell-dilation black hole",
        "Preservation of log-concavity on gamma polynomials",
        "An unstructured block-based adaptive mesh refinement approach for\n  explicit discontinuous Galerkin method",
        "Reinforcement Learning Based Goodput Maximization with Quantized\n  Feedback in URLLC",
        "Privacy Preservation in MIMO-OFDM Localization Systems: A Beamforming\n  Approach",
        "Tropical Poincar\\'e bundle, Fourier-Mukai transform, and a generalized\n  Poincar\\'e formula",
        "Note on surface defects in multiscalar critical models",
        "Advances in multiparameter quantum sensing and metrology",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm",
        "Coalitional control: a bottom-up approach",
        "Threshold Quantum Secret Sharing",
        "Array oscillator in coupled waveguides with nonlinear gain and radiation\n  resistances saturating at exceptional point",
        "A note on the differential spectrum of a class of locally APN functions",
        "The Simultaneous Operation of a Controllable Segmented Primary Mirror\n  and Single Conjugate Adaptive Optics System part 1 -- Design Concept and\n  Sensitivity Analysis"
      ],
      "abstract":[
        "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "We formalize a rigorous connection between barren plateaus (BP) in\nvariational quantum algorithms and exponential concentration of quantum kernels\nfor machine learning. Our results imply that recently proposed strategies to\nbuild BP-free quantum circuits can be utilized to construct useful quantum\nkernels for machine learning. This is illustrated by a numerical example\nemploying a provably BP-free quantum neural network to construct kernel\nmatrices for classification datasets of increasing dimensionality without\nexponential concentration.",
        "This paper introduces a differentiable ray-tracing based model that\nincorporates aberrations and distortions to render realistic coded\nhyperspectral acquisitions using Coded-Aperture Spectral Snapshot Imagers\n(CASSI). CASSI systems can now be optimized in order to fulfill simultaneously\nseveral optical design constraints as well as processing constraints. Four\ncomparable CASSI systems with varying degree of optical aberrations have been\ndesigned and modeled. The resulting rendered hyperspectral acquisitions from\neach of these systems are combined with five state-of-the-art hyperspectral\ncube reconstruction processes. These reconstruction processes encompass a\nmapping function created from each system's propagation model to account for\ndistortions and aberrations during the reconstruction process. Our analyses\nshow that if properly modeled, the effects of geometric distortions of the\nsystem and misalignments of the dispersive elements have a marginal impact on\nthe overall quality of the reconstructed hyperspectral data cubes. Therefore,\nrelaxing traditional constraints on measurement conformity and fidelity to the\nscene enables the development of novel imaging instruments, guided by\nperformance metrics applied to the design or the processing of acquisitions. By\nproviding a complete framework for design, simulation and evaluation, this work\ncontributes to the optimization and exploration of new CASSI systems, and more\ngenerally to the computational imaging community.",
        "Our recent numerical studies on cryogenic liquid helium-4 strongly indicate\nthe features of multiscale physics that can be identified using the Landau's\ntwo-fluid model. This study presents the possibility that two-fluid models\nbased on classical and quantum hydrodynamics have a relationship between the\nscale transformation using filtering in large eddy simulations (LES) and the\ninverse scale transformation using smoothed particle hydrodynamics (SPH). We\nshow that the spin angular momentum conservation term, which we previously\nintroduced into the two-fluid model as a quantum mechanical correction,\nformally corresponds to the subgrid-scale (SGS) model, which can be derived\nfrom the scale transformation of the two-fluid model from quantum to classical\nhydrodynamics. Our theoretical analysis shows that solving the two-fluid model\nbased on classical hydrodynamics using SPH can reproduce the microscopic\nfluctuations even at the macroscopic scale because the truncation errors owing\nto the smoothing kernel approximation can substitute the microscopic\nfluctuations. In particular, the fluctuations can be amplified according to the\nsize of the kernel radius at the macroscopic scale. Our further theoretical\nanalysis shows that the Condiff viscosity model can serve as an SGS model and\nincorporate the quantum vortex interactions into the two-fluid model. Our\nresults and discussion provide new insights into the microscopic composition of\nthe cryogenic liquid helium-4 within a multiscale framework. First, a normal\nfluid can be a mixture of inviscid and viscous fluid particles. Second, a flow\nidentified as a normal fluid on the microscopic scale because of the presence\nof molecular viscosity is still classified as an inviscid fluid on the\nhydrodynamic scale because its viscosity is insufficient to produce eddy\nviscosity.",
        "A general system of difference equations is presented for multispecies\ncommunities with density dependent population growth and delayed maturity.\nInterspecific competition, mutualism, predation, commensalism, and amensalism\nare accommodated. A sufficient condition for the local asymptotic stability of\na coexistence equilibrium in this system is then proven. Using this system, the\ngeneralisation of the Beverton-Holt and Leslie-Gower models of competition to\nmultispecies systems with possible maturation delays is presented and shown to\nyield interesting stability properties. The stability of coexistence depends on\nthe relative abundances of the species at the unique interior equilibrium. A\nsufficient condition for local stability is derived that only requires\nintraspecific competition to outweigh interspecific competition. The condition\ndoes not depend on maturation delays. The derived stability properties are used\nto develop a novel estimation approach for the coefficients of interspecific\ncompetition. This approach finds an optimal configuration given the conjecture\nthat coexisting species strive to outcompete competitors and are most likely\nobserved near the stable interior equilibrium with the greatest dampening of\nperturbations. The optimal solution is compared to estimates of niche overlap\nusing an empirical example of malaria mosquito vectors with delayed maturity in\nthe Anopheles gambiae sensu lato species complex.",
        "We introduce the light-sheet confocal quantum diamond microscope (LC-QDM) for\nwidefield 3D quantum sensing with efficient confocal readout. The LC-QDM\nleverages light-sheet illumination and laser scanning confocal methods to\nenable high-resolution, high-speed 3D measurements with nitrogen-vacancy (NV)\ndefects in diamond, combining the best of widefield and confocal modalities in\na single device and eliminating the need for thin-NV-layer diamond chips. We\nperform simulations and measurements of NV initialization and readout times to\nmodel the anticipated performance of the LC-QDM compared to existing QDM\ndesigns. Our findings show that the LC-QDM will provide significant advantages\nfor applications requiring limited laser power.",
        "We report the X-ray spectral and timing analysis of the high mass X-ray\nbinary EXO 2030+375 during the 2021 type-II outburst based on the Insight-HXMT\nobservations. Pulsations can be detected in the energy band of 1-150 keV. The\npulse profile shows energy and luminosity dependence and variability. We\nobserved transitions in the pulse profile shape during the rising and the\ndecaying phase of the outburst. The pulse fraction exhibits an anti-correlation\nwith luminosity and a non-monotonic energy dependence, with a possible dip near\n30 keV during the outburst peak. The hardness-intensity diagrams (7-10 keV\/4-7\nkeV) suggest state transitions during the early and late phases of the\noutburst. These transitions are consistent with the luminosity at which the\npulse profile shape changes occur, revealing the source reaching the critical\nluminosity and transitioning between super-critical and sub-critical accretion\nregimes. We performed the average and phase-resolved spectral analysis, where\nthe flux-resolved average spectra show a stable spectral evolution with\nluminosity. The phase-resolved spectral analysis reveals that the dependence of\nspectral parameters on the pulse phase varies with different luminosities.",
        "Superheated steam is widely employed in various energy systems, particularly\nin power plants, chemical industries, and other applications where\nhigh-temperature and high-pressure steam is essential for efficient energy\nconversion and process control. In these systems, regulation valves are crucial\ncomponents that control the flow of steam, adjusting its pressure and\ntemperature to ensure safe and efficient operation. Accurate understanding and\nprediction of temperature variations within regulation valves are essential for\noptimizing their performance and improving the overall system efficiency. This\nstudy investigates the temperature variations of superheated steam flowing\nthrough a regulation valve using computational fluid dynamics (CFD) simulations\ncombined with Proper Orthogonal Decomposition (POD) techniques. The analysis\nbegins with an examination of the internal flow field parameters, including\ntemperature and pressure, to understand the overall fluid dynamics within the\nvalve. POD is applied to reduce the dimensionality of the CFD results. Singular\nValue Decomposition (SVD) is employed to extract the dominant modes that\ncapture the key flow structures responsible for heat transfer and temperature\nfluctuations. The POD analysis reveals that the most influential modes are\nassociated with regions of high turbulence intensity and significant\ntemperature gradients, which are critical to the thermal performance of the\nsteam flow through the regulation valve. The application of POD to 3D CFD\nresults represents a novel approach, particularly for complex fluid flow models\nsuch as steam flow through regulation valves. The insights gained from this\nstudy have practical implications for the design and optimization of\ntemperature and pressure regulation valves in energy systems, providing a\ntheoretical foundation for enhancing the efficiency and reliability of these\nsystems.",
        "Ring polymer self-consistent field theory is used to calculate the critical\ntemperatures and heat capacities of an ideal Bose gas for an order of magnitude\nmore particles than previously reported. A lambda-transition indicative of\nBose-Einstein condensation is observed as expected. Using a known proof of\nspatial mode entanglement in Bose-Einstein condensates, a relationship between\nboson exchange and quantum entanglement is established. This is done without\nthe use of state vectors, since ring polymer quantum theory uses instead a\nthermal degree of freedom, sometimes called the \"imaginary time\", to map\nclassical statistical mechanics onto non-relativistic quantum mechanics through\nthe theorems of density functional theory. It is shown that quantum phenomena,\nsuch as Bose-Einstein condensation, boson exchange, entanglement and\ncontextuality, can be visualized in terms of merging and separating ring\npolymer threads in thermal-space. A possible extension to fermions is\nmentioned.",
        "This study utilizes a hybrid Finite Element Method (FEM) and Material Point\nMethod (MPM) to investigate the runout of liquefaction-induced flow slide\nfailures. The key inputs to this analysis are the earthquake ground motion,\nwhich induces liquefaction, and the post-liquefaction residual strength. The\ninfluence of these factors on runout is evaluated by subjecting a model of a\ntailings dam to thirty different earthquake motions and by assigning different\nvalues of post-liquefaction residual strength. Ground motions with larger peak\nground accelerations (PGA) generate liquefaction to larger depths, thus\nmobilizing a greater mass of material and resulting in a flow slide with\ngreater runout. However, different ground motions with the same PGA yield\nsignificant variations in the depth of liquefaction, indicating that other\nground motion characteristics (e.g., frequency content) also exert significant\ninfluence over the initiation of liquefaction. Ground motion characteristics of\npeak ground velocity (PGV) and Modified Acceleration Spectrum Intensity (MASI)\nshow a strong correlation to the induced depth of liquefaction because they\ncapture both the intensity and frequency content of the earthquake motion. The\ncomputed runout is directly related to the depth of liquefaction induced by the\nearthquake motion. For dam geometry analyzed, measurable runout occurs when\nliquefaction extends to 10 m depth and the runout is maximized when\nliquefaction extends to about 18 m. Strain-softening of the residual strength\nof the liquefied tailings during runout is shown to substantially increase the\nrunout distance of the flow slide, highlighting the need for additional\nresearch to better characterize the appropriate strength of liquefied materials\nduring flow failures.",
        "Altermagnetism has emerged as a third fundamental category of collinear\nmagnetism, characterized by spin-splitting in symmetry-compensated collinear\nantiferromagnets, opening new frontiers in spintronics and condensed matter\nphysics. Here, based on first-principles calculations, we propose a novel\naltermagnetic semiconductor, the asymmetric Janus V$_2$AsBrO monolayer, which\nexhibits a magnetic easy axis favoring the out-of-plane direction and a\nN\\'{e}el temperature ($T_N$) exceeding room temperature. The system exhibits a\nstrain-tunable piezovalley effect, generating valley polarization under\nuniaxial strain. Notably, hole doping under uniaxial strain generates a net\nmagnetization ($M$) through a piezomagnetic mechanism. Additionally, the broken\ninversion symmetry endows the monolayer with a substantial out-of-plane\npiezoelectric coefficient $d_{31}$ (2.19 pm\/V), presenting broad prospects for\nthe development and design of novel piezoelectric devices. Our findings provide\na promising candidate material for the advancement of 2D multifunctional\ndevices in nanoelectronics, spintronics, valleytronics, and piezoelectrics.",
        "A commutative monoid $M$ is called a linearly orderable monoid if there\nexists a total order on $M$ that is compatible with the monoid operation. The\nfinitary power monoid of a commutative monoid $M$ is the monoid consisting of\nall nonempty finite subsets of $M$ under the so-called sumset. In this paper,\nwe investigate whether certain atomic and divisibility properties ascend from\nlinearly orderable monoids to their corresponding finitary power monoids.",
        "We study the dynamics of the $(\\alpha+\\beta)$ Fermi-Pasta-Ulam-Tsingou\nlattice (FPUT lattice, for short) for an arbitrary number $N$ of interacting\nparticles, in regimes of small enough nonlinearity so that a Birkhoff-Gustavson\ntype of normal form can be found using tools from wave-turbulence theory.\nSpecifically, we obtain the so-called Zakharov equation for $4$-wave resonant\ninteractions and its extension to $5$-wave resonant interactions by Krasitskii,\nbut we introduce an important new feature: even the generic terms in these\nnormal forms contain $resonant$ $interactions$ $only$, via a $unique$ canonical\ntransformation. The resulting normal forms provide an approximation to the\noriginal FPUT lattice that possesses a significant number of exact quadratic\nconservation laws, beyond the quadratic part of the Hamiltonian. We call the\nnew equations \"exact-resonance evolution equations\" and examine their\nproperties: (i) Heisenberg representation's slow evolution allows us to\nimplement numerical methods with large time steps to obtain relevant dynamical\ninformation, such as Lyapunov exponents. (ii) We introduce tests, such as\nconvergence of the normal form transformation and truncation error\nverification, to successfully validate our exact-resonance evolution equations.\n(iii) The systematic construction of new quadratic invariants (via the resonant\ncluster matrix) allows us to use finite-time Lyapunov exponent calculations to\nquantify the level of nonlinearity at which the original FPUT lattice is well\napproximated by the exact-resonance evolution equations. We show numerical\nexperiments in the case $N=9$, but the theory and numerical methods are valid\nfor arbitrary values of $N$. We conclude that, when $3$ divides $N$, at small\nenough nonlinearity the FPUT lattice's dynamics and nontrivial hyperchaos are\ngoverned by $5$-wave resonant interactions.",
        "The characteristics and images of Einstein-Maxwell-Dilaton (EMD) black holes\nare examined in this paper, focusing on their effective potential, photon\ntrajectories, and images with thin and thick accretion disks. We found that the\nshadow and photon sphere radii decrease with increasing dilaton charge. As the\nobservation inclination increases, direct and secondary images become separate,\nwith the direct image appearing hat-shaped. Simulations indicate that the\nbrightness of the shadow and photon ring is higher in static spherical\naccretion flows compared to infalling ones. The study also shows that in thin\ndisk accretion flows, the direct emission predominantly influences observed\nluminosity, with photon ring emission being less significant. Additionally, the\nappearance of black hole images varies with the observer's inclination angle.",
        "Every symmetric polynomial $h(x) = h_0 + h_1\\,x + \\cdots + h_n\\,x^n$, where\n$h_i = h_{n-i}$ for each $i$, can be expressed as a linear combination in the\nbasis $\\{x^i(1+x)^{n-2i}\\}_{i=0}^{\\lfloor n\/2\\rfloor}$. The polynomial\n$\\gamma_h(x) = \\gamma_0 + \\gamma_1 \\,x+ \\cdots + \\gamma_{\\lfloor n\/2\\rfloor}\\,\nx^{\\lfloor n\/2\\rfloor}$, commonly referred to as the $\\gamma$-polynomial of\n$h(x)$, records the coefficients of the aforementioned linear combination. Two\ndecades ago, Br\\\"and\\'en and Gal independently showed that if $\\gamma_h(x)$ has\nnonpositive real roots only, then so does $h(x)$. More recently, Br\\\"and\\'en,\nFerroni, and Jochemko proved using Lorentzian polynomials that if $\\gamma_h(x)$\nis ultra log-concave, then so is $h(x)$, and they raised the question of\nwhether a similar statement can be proved for the usual notion of\nlog-concavity. The purpose of this article is to show that the answer to the\nquestion of Br\\\"anden, Ferroni, and Jochemko is affirmative. One of the crucial\ningredients of the proof is an inequality involving binomial numbers that we\nestablish via a path-counting argument.",
        "In the present paper, we present an adaptive mesh refinement(AMR) approach\ndesigned for the discontinuous Galerkin method for conservation laws. The\nblock-based AMR is adopted to ensure the local data structure simplicity and\nthe efficiency, while the unstructured topology of the initial blocks is\nsupported by the forest concept such that the complex geometry of the\ncomputational domain can be easily treated. The inter-block communication\nthrough guardcells is introduced to avoid the direct treatment of flux\ncomputing between cells at different refinement levels. The sharp corners and\ncreases generated during direct refinement can be avoided by projecting the\nboundary nodes to either the user-defined boundary surface function or the\nauto-generated NURBs. High-level MPI parallelization is implemented with\ndynamic load balancing through a space curve filling procedure. Some test cases\nare presented. As a result, ideal accuracy order and versatility in tracing and\ncontrolling the dynamic refinement are observed. Also, good parallelization\nefficiency is demonstrated.",
        "This paper presents a comprehensive system model for goodput maximization\nwith quantized feedback in Ultra-Reliable Low-Latency Communication (URLLC),\nfocusing on dynamic channel conditions and feedback schemes. The study\ninvestigates a communication system, where the receiver provides quantized\nchannel state information to the transmitter. The system adapts its feedback\nscheme based on reinforcement learning, aiming to maximize goodput while\naccommodating varying channel statistics. We introduce a novel Rician-$K$\nfactor estimation technique to enable the communication system to optimize the\nfeedback scheme. This dynamic approach increases the overall performance,\nmaking it well-suited for practical URLLC applications where channel statistics\nvary over time.",
        "We investigate an uplink MIMO-OFDM localization scenario where a legitimate\nbase station (BS) aims to localize a user equipment (UE) using pilot signals\ntransmitted by the UE, while an unauthorized BS attempts to localize the UE by\neavesdropping on these pilots, posing a risk to the UE's location privacy. To\nenhance legitimate localization performance while protecting the UE's privacy,\nwe formulate an optimization problem regarding the beamformers at the UE,\naiming to minimize the Cram\\'er-Rao bound (CRB) for legitimate localization\nwhile constraining the CRB for unauthorized localization above a threshold. A\npenalty dual decomposition optimization framework is employed to solve the\nproblem, leading to a novel beamforming approach for location privacy\npreservation. Numerical results confirm the effectiveness of the proposed\napproach and demonstrate its superiority over existing benchmarks.",
        "We construct a tropical analogue of the Poincar\\'e bundle and prove a\n(cohomological) Fourier-Mukai transform for real tori with integral structures.\nWe then prove a tropical analogue of Beauville's generalized Poincar\\'e formula\nfor polarized abelian varieties. Some consequences include a geometric\nRiemann-Roch theorem for tropical abelian varieties, as well as a tropical\nPoincar\\'e-Prym formula which was recently conjectured by R\\\"ohrle and\nZakharov.",
        "This paper studies generic surface defects for multiscalar critical models\nusing a perturbative $\\epsilon$ expansion in $4-\\epsilon$ dimensions. The beta\nfunctions of the defect couplings for a generic multiscalar bulk with quartic\ninteractions are computed at first non-trivial order in $\\epsilon$. Specific\nbulks of interest are then considered: $O(N)$, hypercubic, hypertetrahdral, and\nbiconical $O(m)\\times O(n)$. In each case, we compute fixed points for the\ndefect couplings and determine the remaining bulk symmetry. Expanding beyond\nthe $O(N)$ model, we find a greater variety of patterns of symmetry breaking.",
        "Recent years have witnessed a growing interest in understating the\nlimitations imposed by quantum noise in precision measurements and devising\ntechniques to reduce it. The attention is currently turning to the\nsimultaneously estimation of several parameters of interest, driven by its\npromising potential across a wide range of sensing applications as well as\nfueled by experimental progress in various optical and atomic platforms. Here,\nwe provide a comprehensive overview of key research directions in\nmultiparameter quantum sensing and metrology, highlighting both opportunities\nand challenges. We introduce the basic framework, discuss ultimate sensitivity\nbounds, optimal measurement strategies, and the role of quantum\nincompatibility, showing important differences with respect to single-parameter\nestimation. Additionally, we discuss emerging experimental implementations in\ndistributed quantum sensing, including cutting-edge optimization techniques.\nThis review aims to bridge the gap between theory and experiments, paving the\nway for the next-generation of quantum sensors and their integration with other\nquantum technologies.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis.",
        "The recent major developments in information technologies have opened\ninteresting possibilities for the effective management of multi-agent systems.\nIn many cases, the important role of central control nodes can now be\nundertaken by several controllers in a distributed topology that suits better\nthe structure of the system. This opens as well the possibility to promote\ncooperation between control agents in competitive environments, establishing\nlinks between controllers in order to adapt the exchange of critical\ninformation to the degree of subsystems' interactions. In this paper a\nbottom-up approach to coalitional control is presented, where the structure of\neach agent's model predictive controller is adapted to the time-variant\ncoupling conditions, promoting the formation of coalitions - clusters of\ncontrol agents where communication is essential to ensure the cooperation -\nwhenever it can bring benefit to the overall system performance.",
        "One crucial and basic method for disclosing a secret to every participant in\nquantum cryptography is quantum secret sharing. Numerous intricate protocols,\nincluding secure multiparty summation, multiplication, sorting, voting, and\nmore, can be designed with it. A quantum secret sharing protocol with a $(t,n)$\nthreshold approach and modulo d, where t and n represent the threshold number\nof participants and the total number of participants, respectively was recently\ndiscussed by Song et al. Kao et al. notes that without the information of other\nparticipants, the secret in Song {\\em et al.'s}protocol cannot be\nreconstructed. We address a protocol that solves this issue in this paper.",
        "A periodically loaded waveguide composed of periodic discrete nonlinear gain\nand radiating elements supports a stable oscillation regime related to the\npresence of an exceptional point of degeneracy (EPD). After reaching\nsaturation, the EPD in the system establishes the oscillation frequency. We\ndemonstrate a synchronization regime at a stable oscillation frequency,\nresulting in uniform saturated gain across the array and uniform radiating\npower. Unlike conventional one-dimensional cavity resonances, the oscillation\nfrequency is independent of the array length. Our investigations further show\nthat when small-signal gain is non-uniformly distributed across the array, the\nsaturated gain results in having a uniform distribution at a gain value that\ngenerates an EPD. Experimental validation using the measured board confirmed\nthat the system saturates at an EPD, with a measured spectrum exhibiting very\nlow phase noise. This low noise allows for operation at a clean oscillation\nfrequency. Additionally, the measured uniform power across the array\ncorresponds to the simulation results. The proposed scheme can pave the way for\na new generation of high-power radiating arrays with distributed active\nelements.",
        "Let $\\gf_{p^n}$ denote the finite field containing $p^n$ elements, where $n$\nis a positive integer and $p$ is a prime. The function\n$f_u(x)=x^{\\frac{p^n+3}{2}}+ux^2$ over $\\gf_{p^n}[x]$ with\n$u\\in\\gf_{p^n}\\setminus\\{0,\\pm1\\}$ was recently studied by Budaghyan and Pal in\n\\cite{Budaghyan2024ArithmetizationorientedAP}, whose differential uniformity is\nat most $5$ when $p^n\\equiv3~(mod~4)$. In this paper, we study the differential\nuniformity and the differential spectrum of $f_u$ for $u=\\pm1$. We first give\nsome properties of the differential spectrum of any cryptographic function.\nMoreover, by solving some systems of equations over finite fields, we express\nthe differential spectrum of $f_{\\pm1}$ in terms of the quadratic character\nsums.",
        "The maintenance of primary mirror segment co-phasing is a critical aspect to\nthe operation of segmented telescopes. However, speckle-based measurements of\nthe phasing of the Keck primary have estimated semi-static surface aberrations\nof approximately 65 nm rms, which were not sensed by the current phasing\ncontrol system. We propose directly sensing and controlling the primary via the\nadaptive optics (AO) system, as a Controllable Segmented Primary (CSP), to\nactively correct its phasing. We develop a methodology for separating the\nindependent controllable signal of the CSP from the rest of the AO system and\nshow estimations of the achievable measurement precision from models of the\nKeck-II AO system."
      ]
    }
  },
  {
    "id":2411.18767,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Clinical integration of machine learning for curative-intent radiation treatment of patients with prostate cancer",
    "start_abstract":"Machine learning (ML) holds great promise for impacting healthcare delivery; however, to date most methods are tested in \u2018simulated\u2019 environments that cannot recapitulate factors influencing real-world clinical practice. We prospectively deployed and evaluated a random forest algorithm for therapeutic curative-intent radiation therapy (RT) treatment planning for prostate cancer in a blinded, head-to-head study with full integration into the clinical workflow. ML- and human-generated RT treatment plans were directly compared in a retrospective simulation with retesting (n\u2009=\u200950) and a prospective clinical deployment (n\u2009=\u200950) phase. Consistently throughout the study phases, treating physicians assessed ML- and human-generated RT treatment plans in a blinded manner following a priori defined standardized criteria and peer review processes, with the selected RT plan in the prospective phase delivered for patient treatment. Overall, 89% of ML-generated RT plans were considered clinically acceptable and 72% were selected over human-generated RT plans in head-to-head comparisons. RT planning using ML reduced the median time required for the entire RT planning process by 60.1% (118 to 47\u2009h). While ML RT plan acceptability remained stable between the simulation and deployment phases (92 versus 86%), the number of ML RT plans selected for treatment was significantly reduced (83 versus 61%, respectively). These findings highlight that retrospective or simulated evaluation of ML methods, even under expert blinded review, may not be representative of algorithm acceptance in a real-world clinical setting when patient care is at stake.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "A Transformer-Embedded Multi-Task Model for Dose Distribution Prediction"
      ],
      "abstract":[
        "Radiation therapy is a fundamental cancer treatment in the clinic. However, to satisfy clinical requirements, radiologists have iteratively adjust radiotherapy plan based on experience, causing it extremely subjective and time-consuming obtain clinically acceptable plan. To this end, we introduce transformer-embedded multi-task dose prediction (TransMTDP) network automatically predict distribution radiotherapy. Specifically, achieve more stable accurate predictions, three highly correlated tasks are included our TransMTDP network, i.e. main task provide each pixel with fine-grained value, an auxiliary isodose lines produce coarse-grained ranges, gradient learn subtle information such as radiation patterns edges maps. The integrated through shared encoder, following learning strategy. strengthen connection of output layers for different tasks, further use two additional constraints, consistency loss loss, reinforce match between features generated by task. Additionally, considering many organs human body symmetrical maps present abundant global features, embed transformer into framework capture long-range dependencies Evaluated in-house rectum dataset public head neck dataset, method gains superior performance compared state-of-the-art ones. Code available at https:\/\/github.com\/luuuwen\/TransMTDP."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World\n  Multimodal Mathematical Error Detection",
        "Improved Rates of Differentially Private Nonconvex-Strongly-Concave\n  Minimax Optimization",
        "Online Learning of Danger Avoidance for Complex Structures of\n  Musculoskeletal Humanoids and Its Applications",
        "Orientation-dependent transport in junctions formed by $d$-wave\n  altermagnets and $d$-wave superconductors",
        "Assessing the impacts of tradable credit schemes through agent-based\n  simulation",
        "Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting",
        "Quantum Entanglement and Measurement Noise: A Novel Approach to\n  Satellite Node Authentication",
        "CallNavi: A Study and Challenge on Function Calling Routing and\n  Invocation in Large Language Models",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "Decentralized Personalization for Federated Medical Image Segmentation\n  via Gossip Contrastive Mutual Learning",
        "Using cyclic $(f,\\sigma)$-codes over finite chain rings to construct\n  $\\mathbb{Z}_p$- and $\\mathbb{F}_q[\\![t]\\!]$-lattices",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Transparent Decompilation for Timing Side-Channel Analyses",
        "Utilizing API Response for Test Refinement",
        "ScNeuGM: Scalable Neural Graph Modeling for Coloring-Based Contention\n  and Interference Management in Wi-Fi 7",
        "Journalists Knowledge and Utilisation of Google Translate Application in\n  South East, Nigeria",
        "Cove-edged Chiral Graphene Nanoribbons with Chirality-Dependent Bandgap\n  and Carrier Mobility",
        "Interplay of $d$- and $p$-States in RbTi$_3$Bi$_5$ and CsTi$_3$Bi$_5$\n  Flat-Band Kagome Metals",
        "Stellar Ages: A Code to Infer Properties of Stellar Populations",
        "Federated Distributed Key Generation",
        "Zooming In On The Multi-Phase Structure of Magnetically-Dominated Quasar\n  Disks: Radiation From Torus to ISCO Across Accretion Rates",
        "Is there Kibble-Zurek scaling of topological defects in first-order\n  phase transitions?",
        "Symplectic Optimization for Cross Subcarrier Precoder Design with\n  Channel Smoothing in Massive MIMO-OFDM System",
        "Small noise limits of Markov chains and the PageRank",
        "De Sitter quantum gravity within the covariant Lorentzian approach to\n  asymptotic safety",
        "TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified\n  Flow Models",
        "NFTs as a Data-Rich Test Bed: Conspicuous Consumption and its\n  Determinants",
        "Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User\n  Association"
      ],
      "abstract":[
        "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.",
        "In this paper, we study the problem of (finite sum) minimax optimization in\nthe Differential Privacy (DP) model. Unlike most of the previous studies on the\n(strongly) convex-concave settings or loss functions satisfying the\nPolyak-Lojasiewicz condition, here we mainly focus on the\nnonconvex-strongly-concave one, which encapsulates many models in deep learning\nsuch as deep AUC maximization. Specifically, we first analyze a DP version of\nStochastic Gradient Descent Ascent (SGDA) and show that it is possible to get a\nDP estimator whose $l_2$-norm of the gradient for the empirical risk function\nis upper bounded by $\\tilde{O}(\\frac{d^{1\/4}}{({n\\epsilon})^{1\/2}})$, where $d$\nis the model dimension and $n$ is the sample size. We then propose a new method\nwith less gradient noise variance and improve the upper bound to\n$\\tilde{O}(\\frac{d^{1\/3}}{(n\\epsilon)^{2\/3}})$, which matches the best-known\nresult for DP Empirical Risk Minimization with non-convex loss. We also\ndiscussed several lower bounds of private minimax optimization. Finally,\nexperiments on AUC maximization, generative adversarial networks, and temporal\ndifference learning with real-world data support our theoretical analysis.",
        "The complex structure of musculoskeletal humanoids makes it difficult to\nmodel them, and the inter-body interference and high internal muscle force are\nunavoidable. Although various safety mechanisms have been developed to solve\nthis problem, it is important not only to deal with the dangers when they occur\nbut also to prevent them from happening. In this study, we propose a method to\nlearn a network outputting danger probability corresponding to the muscle\nlength online so that the robot can gradually prevent dangers from occurring.\nApplications of this network for control are also described. The method is\napplied to the musculoskeletal humanoid, Musashi, and its effectiveness is\nverified.",
        "We investigate de Gennes-Saint-James states and Josephson effect in hybrid\njunctions based on $d$-wave altermagnet and $d$-wave superconductor. Even\nthough these states are associated to long junctions, we find that the\n$d_{x^{2}-y^{2}}$-altermagnet in a normal metal\/altermagnet\/$d$-wave\nsuperconductor junction forms de Gennes-Saint-James states in a short junction\ndue to an enhanced mismatch between electron and hole wave vectors. As a\nresult, the zero-bias conductance peak vanishes and pronounced resonance spikes\nemerge in the subgap conductance spectra. By contrast, the $d_{xy}$-altermagnet\nonly features de Gennes-Saint-James states in the long junction. Moreover, the\nwell-known features such as V-shape conductance for $d_{x^2-y^2}$ pairings and\nzero-biased conductance peak for $d_{xy}$ pairings are not affected by the\nstrength of $d_{xy}$-altermagnetism in the short junction. We also study the\nJosephson current-phase relation $I(\\varphi)$ of $d$-wave\nsuperconductor\/altermagnet\/$d$-wave superconductor hybrids, where $\\varphi$ is\nthe macroscopic phase difference between two $d$-wave superconductors. In\nsymmetric junctions, we obtain anomalous current phase relation such as a\n$0$-$\\pi$ transition by changing either the orientation or the magnitude of the\naltermagnetic order parameter and dominant higher Josephson harmonics.\nInterestingly, we find the first-order Josephson coupling in an asymmetric\n$d_{x^{2}-y^{2}}$-superconductor\/altermagnet\/$d_{xy}$-superconductor junction\nwhen the symmetry of altermagnetic order parameter is neither\n$d_{x^{2}-y^{2}}$- nor $d_{xy}$-wave. We present the symmetry analysis and\nconclude that the anomalous orientation-dependent current-phase relations are\nascribed to the peculiar feature of the altermagnetic spin-splitting field.",
        "Tradable credit schemes (TCS) have been attracting interest from the\ntransportation research community as an appealing alternative to congestion\npricing, due to the advantages of revenue neutrality and equity. Nonetheless,\nexisting research has largely employed network and market equilibrium\napproaches with simplistic characterizations of transportation demand, supply,\ncredit market operations, and market behavior. Agent- and activity-based\nsimulation affords a natural means to comprehensively assess TCS by more\nrealistically modeling demand, supply, and individual market interactions. We\npropose an integrated simulation framework for modeling a TCS, and implements\nit within the state-of-the-art open-source urban simulation platform\nSimMobility, including: (a) a flexible TCS design that considers multiple trips\nand explicitly accounts for individual trading behaviors; (b) a simulation\nframework that captures the complex interactions between a TCS regulator, the\ntraveler, and the TCS market itself, with the flexibility to test future TCS\ndesigns and relevant mobility models; and (c) a set of simulation experiments\non a large mesoscopic multimodal network combined with a Bayesian Optimization\napproach for TCS optimal design. The experiment results indicate network and\nmarket performance to stabilize over the day-to-day process, showing the\nalignment of our agent-based simulation with the known theoretical properties\nof TCS. We confirm the efficiency of TCS in reducing congestion under the\nadopted market behavioral assumptions and open the door for simulating\ndifferent individual behaviors. We measure how TCS impacts differently the\nlocal network, heterogeneous users, the different travel behaviors, and how\ntesting different TCS designs can avoid negative market trading behaviors.",
        "Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.",
        "In this paper, we introduce a novel authentication scheme for satellite nodes\nbased on quantum entanglement and measurement noise profiles. Our approach\nleverages the unique noise characteristics exhibited by each satellite's\nquantum optical communication system to create a distinctive \"quantum noise\nfingerprint.\" This fingerprint is used for node authentication within a\nsatellite constellation, offering a quantum-safe alternative to traditional\ncryptographic methods. The proposed scheme consists of a training phase, where\neach satellite engages in a training exercise with its neighbors to compile\nnoise profiles, and an online authentication phase, where these profiles are\nused for real-time authentication. Our method addresses the inherent challenges\nof implementing cryptographic-based schemes in space, such as key management\nand distribution, by exploiting the fundamental properties of quantum mechanics\nand the unavoidable imperfections in quantum systems. This approach enhances\nthe security and reliability of satellite communication networks, providing a\nrobust solution to the authentication challenges in satellite constellations.\nWe validated and tested several hypotheses for this approach using IBM System\nOne quantum computers.",
        "Interacting with a software system via a chatbot can be challenging,\nespecially when the chatbot needs to generate API calls, in the right order and\nwith the right parameters, to communicate with the system. API calling in\nchatbot systems poses significant challenges, particularly in complex,\nmulti-step tasks requiring accurate API selection and execution. We contribute\nto this domain in three ways: first, by introducing a novel dataset designed to\nassess models on API function selection, parameter generation, and nested API\ncalls; second, by benchmarking state-of-the-art language models across varying\nlevels of complexity to evaluate their performance in API function generation\nand parameter accuracy; and third, by proposing an enhanced API routing method\nthat combines general-purpose large language models for API selection with\nfine-tuned models for parameter generation and some prompt engineering\napproach. These approaches lead to substantial improvements in handling complex\nAPI tasks, offering practical advancements for real-world API-driven chatbot\nsystems.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Federated Learning (FL) presents a promising avenue for collaborative model\ntraining among medical centers, facilitating knowledge exchange without\ncompromising data privacy. However, vanilla FL is prone to server failures and\nrarely achieves optimal performance on all participating sites due to\nheterogeneous data distributions among them. To overcome these challenges, we\npropose Gossip Contrastive Mutual Learning (GCML), a unified framework to\noptimize personalized models in a decentralized environment, where Gossip\nProtocol is employed for flexible and robust peer-to-peer communication. To\nmake efficient and reliable knowledge exchange in each communication without\nthe global knowledge across all the sites, we introduce deep contrast mutual\nlearning (DCML), a simple yet effective scheme to encourage knowledge transfer\nbetween the incoming and local models through collaborative training on local\ndata. By integrating DCML with other efforts to optimize site-specific models\nby leveraging useful information from peers, we evaluated the performance and\nefficiency of the proposed method on three publicly available datasets with\ndifferent segmentation tasks. Our extensive experimental results show that the\nproposed GCML framework outperformed both centralized and decentralized FL\nmethods with significantly reduced communication overhead, indicating its\npotential for real-world deployment.",
        "We construct $\\mathbb{Z}_p$-lattices and $\\mathbb{F}_q[\\![t]\\!]$-lattices\nfrom cyclic $(f,\\sigma)$-codes over finite chain rings, employing quotients of\nnatural nonassociative orders and principal left ideals in carefully chosen\nnonassociative algebras. This approach generalizes the classical Construction A\nthat obtains $\\mathbb{Z}$-lattices from linear codes over finite fields or\ncommutative rings to the nonassociative setting. We mostly use proper\nnonassociative cyclic algebras that are defined over field extensions of\n$p$-adic fields. This means we focus on $\\sigma$-constacyclic codes to obtain\n$\\mathbb{Z}_p$-lattices, hence $\\mathbb{Z}_p$-lattice codes. We construct\nlinear maximum rank distance (MRD) codes that are $\\mathbb{Z}_p$-lattice codes\nemploying the left multiplication of a nonassociative algebra over a finite\nchain ring.\n  Possible applications of our constructions include post-quantum cryptography\ninvolving $p$-adic lattices, e.g. learning with errors, building rank-metric\ncodes like MRD-codes, or $p$-adic coset coding, in particular wire-tap coding.",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "This paper considers the problem of analyzing the timing side-channel\nsecurity of binary programs through decompilation and source-level analysis. We\nfocus on two popular policies, namely constant-time and speculative\nconstant-time, (S)CT for short, used to protect cryptographic libraries.\n  First, we observe that popular decompilers remove (S)CT violations, i.e.,\ntransform non-(S)CT programs into (S)CT programs; it follows that analyzing\ndecompiled programs is not sound. Second, we develop techniques to prove that\ndecompilers are transparent, i.e., neither introduce nor remove (S)CT\nviolations. Third, we apply our techniques to \\refleCT{}, a core but\nnon-trivial decompiler. As a contribution of independent interest, we find that\nconstant-time verification tools may not be sound, due to their use of\npreprocessors (e.g.\\, binary lifters or IR converters) that eliminate CT\nviolations.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools.",
        "Carrier-sense multiple access with collision avoidance in Wi-Fi often leads\nto contention and interference, thereby increasing packet losses. These\nchallenges have traditionally been modeled as a graph, with stations (STAs)\nrepresented as vertices and contention or interference as edges. Graph coloring\nassigns orthogonal transmission slots to STAs, managing contention and\ninterference, e.g., using the restricted target wake time (RTWT) mechanism\nintroduced in Wi-Fi 7 standards. However, legacy graph models lack flexibility\nin optimizing these assignments, often failing to minimize slot usage while\nmaintaining reliable transmissions. To address this issue, we propose ScNeuGM,\na neural graph modeling (NGM) framework that flexibly trains a neural network\n(NN) to construct optimal graph models whose coloring corresponds to optimal\nslot assignments. ScNeuGM is highly scalable to large Wi-Fi networks with\nmassive STA pairs: 1) it utilizes an evolution strategy (ES) to directly\noptimize the NN parameters based on one network-wise reward signal, avoiding\nexhaustive edge-wise feedback estimations in all STA pairs; 2) ScNeuGM also\nleverages a deep hashing function (DHF) to group contending or interfering STA\npairs and restricts NGM NN training and inference to pairs within these groups,\nsignificantly reducing complexity. Simulations show that the ES-trained NN in\nScNeuGM returns near-optimal graphs 4-10 times more often than algorithms\nrequiring edge-wise feedback and reduces 25\\% slots than legacy graph\nconstructions. Furthermore, the DHF in ScNeuGM reduces the training and the\ninference time of NGM by 4 and 8 times, respectively, and the online slot\nassignment time by 3 times in large networks, and up to 30\\% fewer packet\nlosses in dynamic scenarios due to the timely assignments.",
        "This study was aimed at finding out if journalists in South East Nigeria have\nknowledge of Google Translate Application and also utilise it. It adopted a\nsurvey design with a sample size of 320 which was determined using Krejcie &\nMorgan (1970). Its objectives were to ascertain the extent journalists in South\nEast Nigeria know about Google Translate Application, assess the utilisation of\nGoogle Translate Application among journalists in South East Nigeria, and\nidentify the challenges affecting the journalists in South East Nigeria while\nusing Google Translate Application. The theoretical underpin was Knowledge\nAttitude and Practise Model (KAP). The findings showed that journalists in\nSouth East Nigeria have knowledge of Google Translate Application but apply it\nmostly outside the region. It concludes that journalists in South East Nigeria\nhave the knowledge of the App. but apply it outside the zone. The study\nrecommends increased usage of the App. within South East Nigeria.",
        "Graphene nanoribbons (GNRs) have garnered significant interest due to their\nhighly customizable physicochemical properties and potential utility in\nnanoelectronics. Besides controlling widths and edge structures, the inclusion\nof chirality in GNRs brings another dimension for fine-tuning their\noptoelectronic properties, but related studies remain elusive owing to the\nabsence of feasible synthetic strategies. Here, we demonstrate a novel class of\ncove-edged chiral GNRs (CcGNRs) with a tunable chiral vector (n,m). Notably,\nthe bandgap and effective mass of (n,2)- CcGNR show a distinct positive\ncorrelation with the increasing value of n, as indicated by theory. Within this\nGNR family, two representative members, namely, (4,2)- CcGNR and (6,2)-CcGNR,\nare successfully synthesized. Both CcGNRs exhibit prominently curved geometries\narising from the incorporated [4]helicene motifs along their peripheries, as\nalso evidenced by the single-crystal structures of the two respective model\ncompounds (1 and 2). The chemical identities and optoelectronic properties of\n(4,2)- and (6,2)-CcGNRs are comprehensively investigated via a combination of\nIR, Raman, solid-state NMR, UV-vis, and THz spectroscopies as well as\ntheoretical calculations. In line with theoretical expectation, the obtained\n(6,2)-CcGNR possesses a low optical bandgap of 1.37 eV along with charge\ncarrier mobility of 8 cm2\/Vs, whereas (4,2)-CcGNR exhibits a narrower bandgap\nof 1.26 eV with increased mobility of 14 cm2\/Vs. This work opens up a new\navenue to precisely engineer the bandgap and carrier mobility of GNRs by\nmanipulating their chiral vector.",
        "Shifting the Fermi level of the celebrated $AM_3X_5$ (135) compounds into\nproximity of flat bands strongly enhances electronic correlations and severely\naffects the formation of density waves and superconductivity. Our broadband\ninfrared spectroscopy measurements of RbTi$_3$Bi$_5$ and CsTi$_3$Bi$_5$\ncombined with density-functional band-structure calculations reveal that the\ncorrelated Ti $d$-states are intricately coupled with the Bi $p$-states that\nform a tilted Dirac crossing. Electron-phonon coupling manifests itself in the\nstrong damping of itinerant carriers and in the anomalous shape of the phonon\nline in RbTi$_3$Bi$_5$. An anomaly in these spectral features around 150 K can\nbe paralleled to the onset of nematicity detected by low-temperature probes.\nOur findings show that the materials with low band filling open unexplored\ndirections in the physics of kagome metals and involve electronic states of\ndifferent nature strongly coupled with lattice dynamics.",
        "We present a novel statistical algorithm, Stellar Ages, which currently\ninfers the age, metallicity, and extinction posterior distributions of stellar\npopulations from their magnitudes. While this paper focuses on these\nparameters, the framework is readily adaptable to include additional\nproperties, such as rotation, in future work. Historical age-dating techniques\neither model individual stars or populations of stars, often sacrificing\npopulation context or precision for individual estimates. Stellar Ages does\nboth, combining the strengths of these approaches to provide precise individual\nages for stars while leveraging population-level constraints. We verify the\nalgorithm's capabilities by determining the age of synthetic stellar\npopulations and actual stellar populations surrounding a nearby supernova, SN\n2004dj. In addition to inferring an age, we infer a progenitor mass consistent\nwith direct observations of the precursor star. The median age inferred from\nthe brightest nearby stars is $\\log_{10}$(Age\/yr) = $7.19^{+0.10}_{-0.13}$, and\nits corresponding progenitor mass is $13.95^{+3.33}_{-1.96}$\n$\\text{M}_{\\odot}$.",
        "Distributed Key Generation (DKG) is vital to threshold-based cryptographic\nprotocols such as threshold signatures, secure multiparty computation, and\ni-voting. Yet, standard $(n,t)$-DKG requires a known set of $n$ participants\nand a fixed threshold $t$, making it impractical for public or decentralized\nsettings where membership and availability can change.\n  We introduce Federated Distributed Key Generation (FDKG), which relaxes these\nconstraints by allowing each participant to select its own guardian set, with a\nlocal threshold to reconstruct that participant's partial key. FDKG generalizes\nDKG and draws inspiration from Federated Byzantine Agreement, enabling dynamic\ntrust delegation with minimal message complexity (two rounds). The protocol's\nliveness can tolerate adversary that controls up to $k - t + 1$ nodes in every\nguardian set. The paper presents a detailed protocol, a formal description of\nliveness, privacy, and integrity properties, and a simulation-based evaluation\nshowcasing the efficacy of FDKG in mitigating node unreliability.\n  In a setting of 100 parties, a 50% participation rate, 80% retention, and 40\nguardians, the distribution phase incurred a total message size of 332.7 kB\n($O(n\\,k)$), and reconstruction phase 416.56 kB ($O(n\\,k)$. Groth16 client-side\nproving took about 5 s in the distribution phase and ranged from 0.619 s up to\n29.619 s in the reconstruction phase.\n  Our work advances distributed cryptography by enabling flexible trust models\nfor dynamic networks, with applications ranging from ad-hoc collaboration to\nblockchain governance.",
        "Recent radiation-thermochemical-magnetohydrodynamic simulations resolved\nformation of quasar accretion disks from cosmological scales down to ~300\ngravitational radii $R_{g}$, arguing they were 'hyper-magnetized' (plasma\n$\\beta\\ll1$ supported by toroidal magnetic fields) and distinct from\ntraditional $\\alpha$-disks. We extend these, refining to $\\approx 3\\,R_{g}$\naround a $10^{7}\\,{\\rm M_{\\odot}}$ BH with multi-channel radiation and\nthermochemistry, and exploring a factor of 1000 range of accretion rates\n($\\dot{m}\\sim0.01-20$). At smaller scales, we see the disks maintain steady\naccretion, thermalize and self-ionize, and radiation pressure grows in\nimportance, but large deviations from local thermodynamic equilibrium and\nsingle-phase equations of state are always present. Trans-Alfvenic and\nhighly-supersonic turbulence persists in all cases, and leads to efficient\nvertical mixing, so radiation pressure saturates at levels comparable to\nfluctuating magnetic and turbulent pressures even for $\\dot{m}\\gg1$. The disks\nalso become radiatively inefficient in the inner regions at high $\\dot{m}$. The\nmidplane magnetic field remains primarily toroidal at large radii, but at\nsuper-Eddington $\\dot{m}$ we see occasional transitions to a poloidal-field\ndominated state associated with outflows and flares. Large-scale\nmagnetocentrifugal and continuum radiation-pressure-driven outflows are weak at\n$\\dot{m}<1$, but can be strong at $\\dot{m}\\gtrsim1$. In all cases there is a\nscattering photosphere above the disk extending to $\\gtrsim 1000\\,R_{g}$ at\nlarge $\\dot{m}$, and the disk is thick and flared owing to magnetic support\n(with $H\/R$ nearly independent of $\\dot{m}$), so the outer disk is strongly\nilluminated by the inner disk and most of the inner disk continuum scatters or\nis reprocessed at larger scales, giving apparent emission region sizes as large\nas $\\gtrsim 10^{16}\\,{\\rm cm}$.",
        "Kibble-Zurek scaling is the scaling of the density of the topological defects\nformed via the Kibble-Zurek mechanism with respect to the rate at which a\nsystem is cooled across a continuous phase transition. Recently, the density of\nthe topological defects formed via the Kibble-Zurek mechanism was computed for\na system cooled through a first-order phase transition instead of the usual\ncontinuous transitions. Here we address the problem of whether such defects\ngenerated across a first-order phase transition exhibit Kibble-Zurek scaling\nsimilar to the case in continuous phase transitions. We show that any possible\nKibble-Zurek scaling for the topological defects can only be a very rough\napproximation due to an intrinsic field for the scaling. However, complete\nuniversal scaling for other properties does exist.",
        "In this paper, we propose a cross subcarrier precoder design (CSPD) for\nmassive multiple-input multiple-output (MIMO) orthogonal frequency division\nmultiplexing (OFDM) systems. The aim is to maximize the weighted sum-rate (WSR)\nperformance while considering the smoothness of the frequency domain effective\nchannel. To quantify the smoothness of the effective channel, we introduce a\ndelay indicator function to measure the large delay components of the effective\nchannel. An optimization problem is then formulated to balance the WSR\nperformance and the delay indicator function. By appropriately selecting the\nweight factors in the objective function and the parameters in the delay\nindicator function, the delay spread of the effective channel can be reduced,\nthereby enhancing the smoothness of the effective channel. To solve the\noptimization problem, we apply the symplectic optimization, which achieves\nfaster convergence compared to the gradient descent methods. Simulation results\nindicate that the proposed algorithm achieves satisfying WSR performance while\nmaintaining the smoothness of the effective channel.",
        "We recall the classical formulation of PageRank as the stationary\ndistribution of a singularly perturbed irreducible Markov chain that is not\nirreducible when the perturbation parameter goes to zero. Specifically, we use\nthe Markov chain tree theorem to derive explicit expressions for the PageRank.\nThis analysis leads to some surprising results. These results are then extended\nto a much more general class of perturbations that subsume personalized\nPageRank. We also give examples where even simpler formulas for PageRank are\npossible.",
        "Recent technical and conceptual advancements in the asymptotic safety\napproach to quantum gravity have enabled studies of the UV completion of\nLorentzian Einstein gravity, emphasizing the role of the state dependence. We\npresent here the first complete investigation of the flow equations of the\nEinstein-Hilbert action within a cosmological spacetime, namely de Sitter\nspacetime. Using the newly derived graviton propagator for general gauges and\nmasses in de Sitter spacetime, we analyze the dependence on the gauge and on\nfinite renormalization parameters. Our results provide evidence of a UV fixed\npoint for the most commonly used gauges.",
        "Recent advancements in diffusion techniques have propelled image and video\ngeneration to unprecedented levels of quality, significantly accelerating the\ndeployment and application of generative AI. However, 3D shape generation\ntechnology has so far lagged behind, constrained by limitations in 3D data\nscale, complexity of 3D data processing, and insufficient exploration of\nadvanced techniques in the 3D domain. Current approaches to 3D shape generation\nface substantial challenges in terms of output quality, generalization\ncapability, and alignment with input conditions. We present TripoSG, a new\nstreamlined shape diffusion paradigm capable of generating high-fidelity 3D\nmeshes with precise correspondence to input images. Specifically, we propose:\n1) A large-scale rectified flow transformer for 3D shape generation, achieving\nstate-of-the-art fidelity through training on extensive, high-quality data. 2)\nA hybrid supervised training strategy combining SDF, normal, and eikonal losses\nfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A data\nprocessing pipeline to generate 2 million high-quality 3D samples, highlighting\nthe crucial rules for data quality and quantity in training 3D generative\nmodels. Through comprehensive experiments, we have validated the effectiveness\nof each component in our new framework. The seamless integration of these parts\nhas enabled TripoSG to achieve state-of-the-art performance in 3D shape\ngeneration. The resulting 3D shapes exhibit enhanced detail due to\nhigh-resolution capabilities and demonstrate exceptional fidelity to input\nimages. Moreover, TripoSG demonstrates improved versatility in generating 3D\nmodels from diverse image styles and contents, showcasing strong generalization\ncapabilities. To foster progress and innovation in the field of 3D generation,\nwe will make our model publicly available.",
        "Conspicuous consumption occurs when a consumer derives value from a good\nbased on its social meaning as a signal of wealth, taste, and\/or community\naffiliation. Common conspicuous goods include designer footwear, country club\nmemberships, and artwork; conspicuous goods also exist in the digital sphere,\nwith non-fungible tokens (NFTs) as a prominent example. The NFT market merits\ndeeper study for two key reasons: first, it is poorly understood relative to\nits economic scale; and second, it is unusually amenable to analysis because\nNFT transactions are publicly available on the blockchain, making them useful\nas a test bed for conspicuous consumption dynamics. This paper introduces a\nmodel that incorporates two previously identified elements of conspicuous\nconsumption: the \\emph{bandwagon effect} (goods increase in value as they\nbecome more popular) and the \\emph{snob effect} (goods increase in value as\nthey become rarer). Our model resolves the apparent tension between these two\neffects, exhibiting net complementarity between others' and one's own\nconspicuous consumption. We also introduce a novel dataset combining NFT\ntransactions with embeddings of the corresponding NFT images computed using an\noff-the-shelf vision transformer architecture. We use our dataset to validate\nthe model, showing that the bandwagon effect raises an NFT collection's value\nas more consumers join, while the snob effect drives consumers to seek rarer\nNFTs within a given collection.",
        "Secure aggregation is motivated by federated learning (FL) where a cloud\nserver aims to compute an averaged model (i.e., weights of deep neural\nnetworks) of the locally-trained models of numerous clients, while adhering to\ndata security requirements. Hierarchical secure aggregation (HSA) extends this\nconcept to a three-layer network, where clustered users communicate with the\nserver through an intermediate layer of relays. In HSA, beyond conventional\nserver security, relay security is also enforced to ensure that the relays\nremain oblivious to the users' inputs (an abstraction of the local models in\nFL). Existing study on HSA assumes that each user is associated with only one\nrelay, limiting opportunities for coding across inter-cluster users to achieve\nefficient communication and key generation. In this paper, we consider HSA with\na cyclic association pattern where each user is connected to $B$ consecutive\nrelays in a wrap-around manner. We propose an efficient aggregation scheme\nwhich includes a message design for the inputs inspired by gradient coding-a\nwell-known technique for efficient communication in distributed computing-along\nwith a highly nontrivial security key design. We also derive novel converse\nbounds on the minimum achievable communication and key rates using\ninformation-theoretic arguments."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial Intelligence for Pediatric Ophthalmology",
    "start_abstract":"PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm"
      ],
      "abstract":[
        "In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Solutions of first passage times problems: a biscaling approach",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Incident beam optics optimization for the single crystal neutron\n  diffractometer Pioneer with a polarized beam option",
        "Enhanced Field-Free Perpendicular Magnetization Switching via spin\n  splitting torque in Altermagnetic RuO2-based Heterostructures",
        "Electromagnetic Radiation from High-Energy Nuclear Collisions",
        "First X-ray polarimetric view of a Low-Luminosity Active Galactic\n  Nucleus: the case of NGC 2110",
        "Classifier Weighted Mixture models",
        "Formation of condensations for non-radial solutions to 3-wave kinetic\n  equations",
        "Shifting Attention to You: Personalized Brain-Inspired AI Models",
        "Can wormholes mirror the quasi-normal mode spectrum of Schwarzschild\n  black holes?",
        "The influence of the Hardy potential and a Convection Term on a\n  Nonlinear Degenerate Elliptic Equations",
        "Transit Timing Variations of the Sub-Saturn Exoplanet HAT-P-12b",
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Bayesian Computation in Deep Learning",
        "Anomalies in the electronic, magnetic and thermal behavior near the\n  Invar compositions of Fe-Ni alloys",
        "Copula methods for modeling pair densities in density functional theory",
        "Visualizing beat phenomenon between two amplitude-modulated (AM) light\n  beams on a solar cell using smartphones",
        "Proportion of Nilpotent Subgroups in Finite Groups and Their Properties",
        "Striped Spin Density Wave in a Graphene\/Black Phosphorous\n  Heterostructure",
        "Breast Lump Detection and Localization with a Tactile Glove Using Deep\n  Learning",
        "Sampling Theory for Function Approximation with Numerical Redundancy",
        "The role of finite value of strange quark mass $(m_{s}\\neq0)$ and baryon\n  number density $(n)$ on the stability and maximum mass of strange stars",
        "A classification of van der Waerden complexes with linear resolution",
        "An assessment of observational coverage and gaps for robust Sun to\n  heliosphere integrated science",
        "Intermediate band analysis in Green's functions calculations of\n  quasiparticle interference",
        "Kinematically-enhanced interpolating operators for boosted hadrons",
        "Secondary ionisation in hot atmospheres and interactions between\n  planetary and stellar winds",
        "Systematic calculation on alpha decay and cluster radioactivity of\n  superheavy nuclei",
        "Processing the 2D and 3D Fresnel experimental databases via topological\n  derivative methods"
      ],
      "abstract":[
        "We study the first-passage time (FPT) problem for widespread recurrent\nprocesses in confined though large systems and present a comprehensive\nframework for characterizing the FPT distribution over many time scales. We\nfind that the FPT statistics can be described by two scaling functions: one\ncorresponds to the solution for an infinite system, and the other describes a\nscaling that depends on system size. We find a universal scaling relationship\nfor the FPT moments $\\langle t^q \\rangle$ with respect to the domain size and\nthe source-target distance. This scaling exhibits a transition at $q_c=\\theta$,\nwhere $\\theta$ is the persistence exponent. For low-order moments with $q<q_c$,\nconvergence occurs towards the moments of an infinite system. In contrast, the\nhigh-order moments, $q>q_c$, can be derived from an infinite density function.\nThe presented uniform approximation, connecting the two scaling functions,\nprovides a description of the first-passage time statistics across all time\nscales. We extend the results to include diffusion in a confining potential in\nthe high-temperature limit, where the potential strength takes the place of the\nsystem's size as the relevant scale. This study has been applied to various\nmediums, including a particle in a box, two-dimensional wedge, fractal\ngeometries, non-Markovian processes and the non-equilibrium process of\nresetting.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Pioneer, a next-generation single-crystal neutron diffractometer, is under\ndevelopment for Oak Ridge National Laboratory's Second Target Station (STS).\nDesigned to address a wide range of scientific questions, Pioneer will deliver\nhomogeneous neutron beams with customizable size and divergence, and provide a\npolarized beam option. This article introduces its incident beam optics,\nhighlighting the optimization methodology and the simulated performance.\nPioneer will utilize a modified elliptical-straight guide for neutron transport\nand deploy slit packages and insertable apertures to control beam size and\ndivergence. The optimized guide geometry matches the\noptimal-and-full-sample-illumination condition, and the beam control system\neffectively filters out unwanted neutrons while preserving the desired ones.\nAdditionally, we have found that polygon-approximated guides provide\nsatisfactory transport efficiency and beam homogeneity, eliminating the need of\ntruly curved guides. To enhance neutronics performance and reduce cost, the\ncoatings of supermirror elements are individually optimized to the lowest\nhalf-integer $m$-values that are sufficient to deliver the desired neutrons.\nAfter evaluating polarizing V-cavities and $^3$He spin filters over the default\npolarized wavelength band of 1.2-5.5~\\AA, we selected a translatable\nmultichannel polarizing V-cavity as the incident beam polarizer. Strategically\nplaced at a location where the beam divergence is low and a large in-guide gap\nhas negligible impact on transport efficiency, the optimized V-cavity achieves\nan average $P^2T$ of approximately 35\\%.",
        "Current-induced spin-orbit torque (SOT) has emerged as a promising method for\nachieving energy-efficient magnetization switching in advanced spintronic\ndevices. However, technological advancement has been inadequate because an\nexternal in-plane magnetic field is required to attain deterministic switching.\nSeveral approaches have been explored to address these challenges. In this\nwork, we explored the potential of a newly emerged altermagnetic material RuO2\nin combination with a Pt layer to achieve both field-free and low-power\nswitching concurrently. We leveraged out-of-plane (OOP) spin polarization via\nthe spin-splitter effect (SSE) in RuO2 for field-free switching (FFS) and\nin-plane spin polarization combined with spin Hall effect (SHE) in Pt for\nenhanced SOT efficiency. We revealed that the effective OOP magnetic field and\nFFS can be maximized by tuning the nominal thickness of the Pt underlayer and\nthe direction of the applied current. We observed a significant enhancement in\nFFS at an optimized Pt thickness of 1.5 nm for an applied current density as\nlow as 2.56e11 A\/m2 at a crystal angle of 90 deg. Our study paves the way for\nenergy-efficient spintronics devices for non-volatile memory, logic circuits,\nand neuromorphic computing.",
        "We highlight some of the developments in the theory and the observation of\nthe electromagnetic radiation, thermal and otherwise, emitted in relativistic\nheavy-ion collisions.",
        "Low-Luminosity Active Galactic Nuclei (LLAGN) provides a unique view of\nComptonization and non-thermal emission from accreting black holes in the\nlow-accretion rate regime. However, to decipher the exact nature of the\nComptonizing corona in LLAGN, its geometry and emission mechanism must be\nunderstood beyond the limits of spectro-timing techniques. Spectro-polarimetry\noffers the potential to break the degeneracies between different coronal\nemission models. Compton-thin LLAGN provide an opportunity for such\nspectro-polarimetric exploration in the 2-8 keV energy range using IXPE. In\nthis work, we carry out a spectro-polarimetric analysis of the first IXPE\nobservation, in synergy with a contemporaneous NuSTAR observation, of an LLAGN:\nNGC 2110. Using 554.4 ks of IXPE data from October 2024, we constrain the 99%\nupper limit on the Polarization Degree (PD) to be less than 8.3% assuming the\ncorresponding Polarization Angle (PA) to be aligned with the radio jet, and\nless than 3.6% if in the perpendicular direction. In the absence of a\nsignificant PD detection, the PA remains formally unconstrained, yet the\npolarization significance contours appear to be aligned with the radio jet,\ntentatively supporting models in which the corona is radially extended in the\nplane of the disk. We also carry out detailed Monte Carlo simulations using\nMONK and STOKES codes to test different coronal models against our results and\ncompare the polarization properties between NGC 2110 and brighter Seyferts.",
        "This paper proposes an extension of standard mixture stochastic models, by\nreplacing the constant mixture weights with functional weights defined using a\nclassifier. Classifier Weighted Mixtures enable straightforward density\nevaluation, explicit sampling, and enhanced expressivity in variational\nestimation problems, without increasing the number of components nor the\ncomplexity of the mixture components.",
        "We consider in this work a $2$-dimensional $3$-wave kinetic equation\ndescribing the dynamics of the thermal cloud outside a Bose-Einstein\nCondensate. We construct global non-radial mild solutions for the equation.\nThose mild solutions are the summation of Dirac masses on circles. We prove\nthat in each spatial direction, either Dirac masses at the origin, which are\nthe so-called Bose-Einstein condensates, can be formed in finite time or the\nsolutions converge to Bose-Einstein condensates as time evolves to infinity. We\nalso describe a dynamics of the formation of the Bose-Einstein condensates\nlatter case. In this case, on each direction, the solutions accumulate around\ncircles close to the origin at growth rates at least linearly in time.",
        "The integration of human and artificial intelligence represents a scientific\nopportunity to advance our understanding of information processing, as each\nsystem offers unique computational insights that can enhance and inform the\nother. The synthesis of human cognitive principles with artificial intelligence\nhas the potential to produce more interpretable and functionally aligned\ncomputational models, while simultaneously providing a formal framework for\ninvestigating the neural mechanisms underlying perception, learning, and\ndecision-making through systematic model comparisons and representational\nanalyses. In this study, we introduce personalized brain-inspired modeling that\nintegrates human behavioral embeddings and neural data to align with cognitive\nprocesses. We took a stepwise approach, fine-tuning the Contrastive\nLanguage-Image Pre-training (CLIP) model with large-scale behavioral decisions,\ngroup-level neural data, and finally, participant-level neural data within a\nbroader framework that we have named CLIP-Human-Based Analysis (CLIP-HBA). We\nfound that fine-tuning on behavioral data enhances its ability to predict human\nsimilarity judgments while indirectly aligning it with dynamic representations\ncaptured via MEG. To further gain mechanistic insights into the temporal\nevolution of cognitive processes, we introduced a model specifically fine-tuned\non millisecond-level MEG neural dynamics (CLIP-HBA-MEG). This model resulted in\nenhanced temporal alignment with human neural processing while still showing\nimprovement on behavioral alignment. Finally, we trained individualized models\non participant-specific neural data, effectively capturing individualized\nneural dynamics and highlighting the potential for personalized AI systems.\nThese personalized systems have far-reaching implications for the fields of\nmedicine, cognitive research, human-computer interfaces, and AI development.",
        "Wormholes are exotic compact objects characterized by the absence of\nessential singularities and horizons, acting as slender bridges linking two\ndistinct regions of spacetime. Despite their theoretical significance, they\nremain however undetected, possibly due to their ability to closely mimic the\nobservational properties of black holes. This study explores whether a static\nand spherically symmetric wormhole within General Relativity can reproduce the\nquasi-normal mode spectrum of a Schwarzschild black hole under scalar,\nelectromagnetic, and axial gravitational perturbations, both individually and\nin combination. To address this, we reformulate the wormhole metric components\nusing a near-throat parametrization. Our analysis concentrates on the\nfundamental mode and first overtone, estimated via the\nWentzel-Kramers-Brillouin method. By employing a customized minimization\nstrategy, we demonstrate that within a specific region of the parameter space,\na wormhole can successfully replicate a subset of the black hole quasi-normal\nmode spectrum.",
        "This paper is devoted to prove existence of renormalized solutions for a\nclass of non--linear degenerate elliptic equations involving a non--linear\nconvection term, which satisfies a growth properties, and a Hardy potential.\nAdditionally, we assume that the right-hand side is an $L^m$ function, with\n$m\\geq 1$.",
        "We present Transit Timing Variations (TTVs) of HAT-P-12b, a low-density\nsub-Saturn mass planet orbiting a metal-poor K4 dwarf star. Using 14 years of\nobservational data (2009-2022), our study incorporates 7 new ground-based\nphotometric transit observations, three sectors of Transiting Exoplanet Survey\nSatellite (TESS) data, and 23 previously published light curves. A total of 46\nlight curves were analyzed using various analytical models, such as linear,\norbital decay, apsidal precession, and sinusoidal models to investigate the\npresence of additional planets. The stellar tidal quality factor ($Q_\\star'\n\\sim$ 28.4) is lower than the theoretical predictions, making the orbital decay\nmodel an unlikely explanation. The apsidal precession model with a $\\chi_r^2$\nof 4.2 revealed a slight orbital eccentricity (e = 0.0013) and a precession\nrate of 0.0045 rad\/epoch. Frequency analysis using the Generalized Lomb-Scargle\n(GLS) periodogram identified a significant periodic signal at 0.00415\ncycles\/day (FAP = 5.1$\\times$10$^{-6}$ %), suggesting the influence of an\nadditional planetary companion. The sinusoidal model provides the lowest\nreduced chi-squared value ($\\chi_r^2$) of 3.2. Sinusoidal fitting of the timing\nresiduals estimated this companion to have a mass of approximately 0.02 $M_J$ ,\nassuming it is in a 2:1 Mean-Motion Resonance (MMR) with HAT-P-12b.\nAdditionally, the Applegate mechanism, with an amplitude much smaller than the\nobserved TTV amplitude of 156 s, confirms that stellar activity is not\nresponsible for the observed variations.",
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "This review paper is intended for the 2nd edition of the Handbook of Markov\nchain Monte Carlo. We provide an introduction to approximate inference\ntechniques as Bayesian computation methods applied to deep learning models. We\norganize the chapter by presenting popular computational methods for Bayesian\nneural networks and deep generative models, explaining their unique challenges\nin posterior inference as well as the solutions.",
        "The structural and magnetic properties of Fe$_{1-x}$Ni$_x$~($x$ = 0.32, 0.36,\n0.40, 0.50) alloys have been investigated using synchrotron based x-ray\ndiffraction (XRD) technique with x-rays of wavelength 0.63658 \\AA\\ down to 50 K\ntemperature, magnetic measurement using superconducting quantum interference\ndevice (SQUID) magnetometer and high resolution x-ray photoelectron\nspectroscopy (XPS) with monochromatic AlK$_\\alpha$ radiation. The XRD studies\nsuggest a single phase with fcc structure for $x$ = 0.36, 0.40, and 0.50\n~alloys and a mixed phase for $x$ = 0.32 alloy containing both bcc and fcc\nstructures. The lattice parameter of the alloys exhibits a linear dependence on\ntemperature giving rise to a temperature independent coefficient of thermal\nexpansion (CTE). The lowest CTE is observed for $x$ = 0.36 Invar alloy as\nexpected while $x$ = 0.50 alloy exhibits the highest CTE among the alloys\nstudied. The CTE of the fcc component of mixed phase alloy is close to that of\nInvar alloy. The temperature dependence of magnetization of the alloys down to\n2 K reveals an overall antiferromagnetic interactions within the ferromagnetic\nphase causing the magnetization decreasing with cooling. The field cooled and\nzero field cooled data show larger differences for the Invar compositions; this\nis also manifested in the magnetic hysteresis data at 2 K and 300 K.",
        "We propose a new approach towards approximating the density-to-pair-density\nmap based on copula theory from statistics. We extend the copula theory to\nmulti-dimensional marginals, and deduce that one can describe any (exact or\napproximate) pair density by the single-particle density and a copula. We\npresent analytical formulas for the exact copula in scaling limits, numerically\ncompute the copula for dissociating systems with two to four particles in one\ndimension, and propose accurate approximations of the copula between\nequilibrium and dissociation for two-particle systems.",
        "We present a new simple experimental setup for demonstrating beat phenomenon.\nWe have combined two amplitude-modulated light beams on a solar cell using two\nsmartphones as signal generators and a third smartphone as an oscilloscope to\nvisualize the resulting wave beats. A very good agreement is obtained between\nthe theoretical model and the experimental result. It is an innovative approach\nto bring physics experimentation to the students and discover the potential\npossibilities of smartphones in basic physics courses.",
        "This work introduces and investigates the function $J(G) =\n\\frac{\\text{Nil}(G)}{L(G)}$, where $\\text{Nil}(G)$ denotes the number of\nnilpotent subgroups and $L(G)$ the total number of subgroups of a finite group\n$G$. The function $J(G)$, defined over the interval $(0,1]$, serves as a tool\nto analyze structural patterns in finite groups, particularly within\nnon-nilpotent families such as supersolvable and dihedral groups. Analytical\nresults demonstrate the product density of $J(G)$ values in $(0,1]$,\nhighlighting its distribution across products of dihedral groups. Additionally,\na probabilistic analysis was conducted, and based on extensive computational\nsimulations, it was conjectured that the sample mean of $J(G)$ values converges\nin distribution to the standard normal distribution, in accordance with the\nCentral Limit Theorem, as the sample size increases. These findings expand the\nunderstanding of multiplicative functions in group theory, offering novel\ninsights into the structural and probabilistic behavior of finite groups.",
        "A bilayer formed by stacking two distinct materials creates a moir\\'e\nlattice, which can serve as a platform for novel electronic phases. In this\nwork we study a unique example of such a system: the graphene-black phosphorus\nheterostructure (G\/BP), which has been suggested to have an intricate band\nstructure. Most notably, the valence band hosts a quasi-one-dimensional region\nin the Brillouin zone of high density of states, suggesting that various\nmany-body electronic phases are likely to emerge. We derive an effective\ntight-binding model that reproduces this band structure, and explore the\nemergent broken-symmetry phases when interactions are introduced. Employing a\nmean-field analysis, we find that the favored ground-state exhibits a striped\nspin density wave (SDW) order, characterized by either one of two-fold\ndegenerate wave-vectors that are tunable by gating. Further exploring the\nphase-diagram controlled by gate voltage and the interaction strength, we find\nthat the SDW-ordered state undergoes a metal to insulator transition via an\nintermediate metallic phase which supports striped SDW correlations. Possible\nexperimental signatures are discussed, in particular a highly anisotropic\ndispersion of the collective excitations which should be manifested in electric\nand thermal transport.",
        "Breast cancer is the leading cause of mortality among women. Inspection of\nbreasts by palpation is the key to early detection. We aim to create a wearable\ntactile glove that could localize the lump in breasts using deep learning (DL).\nIn this work, we present our flexible fabric-based and soft wearable tactile\nglove for detecting the lumps within custom-made silicone breast prototypes\n(SBPs). SBPs are made of soft silicone that imitates the human skin and the\ninner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cm\ndiameters are embedded inside to create another set with lumps. Our approach is\nbased on the InceptionTime DL architecture with transfer learning between\nexperienced and non-experienced users. We collected a dataset from 10 naive\nparticipants and one oncologist-mammologist palpating SBPs. We demonstrated\nthat the DL model can classify lump presence, size and location with an\naccuracy of 82.22%, 67.08% and 62.63%, respectively. In addition, we showed\nthat the model adapted to unseen experienced users with an accuracy of 95.01%,\n88.54% and 82.98% for lump presence, size and location classification,\nrespectively. This technology can assist inexperienced users or healthcare\nproviders, thus facilitating more frequent routine checks.",
        "The study of numerical rounding errors is often greatly simplified in the\nanalytical treatment of mathematical problems, or even entirely separated from\nit. In sampling theory, for instance, it is standard to assume the availability\nof an orthonormal basis for computations, ensuring that numerical errors are\nnegligible. In reality, however, this assumption is often unmet. In this paper,\nwe discard it and demonstrate the advantages of integrating numerical insights\nmore deeply into sampling theory. To clearly pinpoint when the numerical\nphenomena play a significant role, we introduce the concept of numerical\nredundancy. A set of functions is numerically redundant if it spans a\nlower-dimensional space when analysed numerically rather than analytically.\nThis property makes it generally impossible to compute the best approximation\nof a function in its span using finite precision. In contrast,\n$\\ell^2$-regularized approximations are computable and, therefore, form the\nfoundation of many practical methods. Regularization generally reduces accuracy\ncompared to the best approximation, but our analysis shows that there is a\nbenefit: it also significantly reduces the amount of data needed for accurate\napproximation. Furthermore, we present a constructive method for optimally\nselecting data points for $L^2$-approximations, explicitly accounting for the\neffects of regularization. The results are illustrated for two common scenarios\nthat lead to numerical redundancy: (1) approximations on irregular domains and\n(2) approximations that incorporate specific features of the function to be\napproximated. In doing so, we obtain new results on random sampling for Fourier\nextension frames. Finally, we establish that regularization is implicit in\nnumerical orthogonalization of a numerically redundant set, indicating that its\nanalysis cannot be bypassed in a much broader range of methods.",
        "This study describes the impact of non-zero value of strange quark mass\n$(m_{s})$ and number density of baryons $(n)$ on the structure, stability and\nmaximum mass of strange stars. We derive an exact relativistic solution of the\nEinstein field equation using the Tolman-IV metric potential and modified MIT\nbag model EoS, $p_{r}=\\frac{1}{3}(\\rho-4B')$, where $B'$ is a function of bag\nconstant $B$, $m_{s}$ and baryon number density $(n)$. Following CERN's\nfindings, transition of phase from hadronic matter to Quark-Gluon Plasma (QGP)\nmay occur at high densities in presence of favourable conditions. The standard\nMIT bag model, with a constant $B$, fails to explain such transition properly.\nIntroducing a finite $m_{s}$ and Wood-Saxon parametrisation for $B$, dependent\non baryon number density $(n)$, provides a more realistic EoS to address such\nphase transition. Both $m_{s}$ and $n$ constrain the EoS, making it softer as\n$m_{s}$ increases. Solutions to the TOV equations reveal that for massless\nstrange quarks, maximum mass is 2.01 $M_{\\odot}$ and corresponding radius is\n10.96 Km when $n=0.66~fm^{-3}$. These values decrease to 1.99 $M_{\\odot}$ and\n1.96 $M_{\\odot}$, with corresponding radii of 10.88 Km and 10.69 Km for\n$m_{s}=50$ and $100~MeV$ respectively having same $n$ value. It is interesting\nto note that a corelation exists between $n$ and $m_{s}$. The hadronic to quark\nmatter transition occurs at higher values of $n$, when $m_{s}$ increases such\nas $n\\geq0.484,~0.489$ and $0.51~fm^{-3}$ for $m_{s}=50$ and $100~MeV$\nrespectively. Beyond these values, the energy per baryon $(\\mathcal{E_{B}})$\ndrops below $930.4~MeV$, indicating a complete transition to quark matter. For\nphysical analysis, we have considered $n~(=0.578~fm^{-3})$ which lies in the\nstable region with $B(n)=70~MeV\/fm^{3}$. The model provides a viable\ndescription of strange stars, satisfying all necessary physical requirements.",
        "In 2017, Ehrenborg, Govindaiah, Park, and Readdy defined the van der Waerden\ncomplex ${\\tt vdW}(n,k)$ to be the simplicial complex whose facets correspond\nto all the arithmetic sequences on the set $\\{1,\\ldots,n\\}$ of a fixed length\n$k$. To complement a classification of the Cohen--Macaulay van der Waerden\ncomplexes obtained by Hooper and Van Tuyl in 2019, a classification of van der\nWaerden complexes with linear resolution is presented. Furthermore, we show\nthat the Stanley--Reisner ring of a Cohen--Macaulay van der Waerden complex is\nlevel.",
        "Understanding the generation and development of the continuous outflow from\nthe Sun requires tracing the physical conditions from deep in the corona to the\nheliosphere. Detailed global observations of plasma state variables and the\nmagnetic field are needed to provide critical constraints to the underlying\nphysics driving models of the corona and solar wind. Key diagnostics of the\nsolar wind require measurements at its formation site and during its outflow to\ncontinuously track it across rapidly changing regions of space. A unified view\nof the solar wind is only possible through coordinated remote and in situ\nobservations that probe these different regions. Here, we discuss current\nobservational coverage and gaps of different plasma properties and review\nrecent coordinated studies. We highlight how these efforts may become more\nroutine with the launch of upcoming and planned missions.",
        "The measurement of quasiparticle scattering patterns on material surfaces\nusing scanning tunneling microscopy (STM) is now an established technique for\naccessing the momentum-resolved electronic band structure of solids. However,\nsince these quasiparticle interference (QPI) patterns reflect spatial\nvariations related to differences in the band momenta rather than the momenta\nthemselves, their interpretation often relies on comparisons with simple\ngeometrical models such as the joint density of states (JDOS) or with the\nconvolution of Green's functions. In this paper, we highlight non-intuitive\ndifferences between Green's function and JDOS results. To understand the origin\nof these discrepancies, we analyze the convolution of Green's functions using\nthe Feynman parametrization technique and introduce a framework that we call\nthe intermediate band analysis. This approach allows us to derive simple\nselection rules for interband QPI, based on electron group velocities.\nConnecting the intermediate band analysis with the experiment, we consider\nexperimental Bogoliubov QPI patterns measured for FeSe1-xSx, which were\nrecently used to demonstrate a highly anisotropic superconducting gap,\nindicating superconductivity mediated by nematic fluctuations [1]. The\ncalculated Green's functions convolutions reproduce the particle-hole asymmetry\nin the intensity of QPI patterns across the Fermi level observed in\nexperiments. Finally, we demonstrate the utility of intermediate band analysis\nin tracing the origin of this asymmetry to a coherence factor effect of the\nsuperconducting state.",
        "We propose to use interpolating operators for lattice quantum chromodyanmics\n(QCD) calculations of highly-boosted pions and nucleons with\nkinematically-enhanced ground-state overlap factors at large momentum. Because\nthis kinematic enhancement applies to the signal but not the variance of the\ncorrelation function, these interpolating operators can achieve better\nsignal-to-noise ratios at large momentum. We perform proof-of-principle\ncalculations with boosted pions and nucleons using close-to-physical and larger\nquark masses to explore the utility of our proposal. Results for effective\nenergies and matrix elements, as well as Lanczos ground-state energy\nestimators, are consistent with theoretical expectations for signal-to-noise\nimprovement at large momenta.",
        "The loss of close-in planetary atmospheres is influenced by various physical\nprocesses, such as photoionisation, which could potentially affect the\natmosphere survivability on a secular timescale. The amount of stellar\nradiation converted into heat depends on the energy of the primary electrons\nproduced by photoionisation and the local ionisation fraction. The Lyman-alpha\nline is an excellent probe for atmospheric escape. We study the interaction\nbetween the planetary and the stellar wind, the difference of the predicted\nmass-loss rates between 1D and 2D models, the signal of Ly-a and the impact of\nstellar flares. Using the PLUTO code, we perform 2D hydrodynamics simulations\nfor four different planets. We consider planets in the size range from Neptune\nto Jupiter. We produce synthetic Ly-a profiles to comprehend the origin of the\nsignal, and in particular its high velocity Doppler shift. Our results indicate\na trend similar to the 1D models, with a decrease in the planetary mass-loss\nrate for all systems when secondary ionisation is taken into account. The\nmass-loss rates are found to decrease by 48% for the least massive planet when\nsecondary ionisation is accounted for. We find nevertheless a decrease that is\nless pronounced in 2D than in 1D. We observe differences in the Ly-a profile\nbetween the different cases and significant asymmetries in all of them,\nespecially for the lower mass planets. Finally, we observe that stellar flares\ndo not affect the mass-loss rate because they act, in general, on a timescale\nthat is too short. We find velocities in the escaping atmosphere up to 100\nkm\/s, with the gas moving away from the star, which could be the result of the\ninteraction with the stellar wind. Furthermore, we find that stellar flares\ngenerally occur on a timescale that is too short to have a visible impact on\nthe mass-loss rate of the atmosphere.",
        "In the Coulomb and Proximity Potential Model (CPPM) framework, we have\ninvestigated the cluster radioactivity and alpha decay half-lives of superheavy\nnuclei. We study 22 different versions of proximity potential forms that have\nbeen proposed to describe proton radioactivity, two-proton radioactivity,\nheavy-ion radioactivity, quasi-elastic scattering, fusion reactions, and other\napplications. The half-lives of cluster radioactivity and alpha decay of 41\natomic nuclei ranging from 221Fr to 244Cm were calculated, and the results\nindicate that the refined nuclear potential named BW91 is the most suitable\nproximity potential form for the cluster radioactivity and alpha decay of\nsuperheavy nuclei since the root-mean-square (RMS) deviation between the\nexperimental data and the relevant theoretical calculation results is the\nsmallest ({\\sigma}= 0.841). By using CPPM, we predicted the half-lives of 20\npotential cluster radioactivity and alpha decay candidates. These cluster\nradioactivities and alpha decays are energetically allowed or observable but\nnot yet quantified in NUBASE2020.",
        "This paper presents reconstructions of homogeneous targets from the 2D and 3D\nFresnel databases by one-step imaging methods based on the computation of\ntopological derivative and topological energy fields. The electromagnetic\ninverse scattering problem is recast as a constrained optimization problem, in\nwhich we seek to minimize the error when comparing experimental microwave\nmeasurements with computer-generated synthetic data for arbitrary targets by\napproximating a Maxwell forward model. The true targets are then characterized\nby combining the topological derivatives or energies of such shape functionals\nfor all available receivers and emitters at different frequencies. Our\napproximations are comparable to the best approximations already obtained by\nother methods. However, these topological fields admit easy to evaluate\nclosed-form expressions, which speeds up the process."
      ]
    }
  },
  {
    "id":2411.10004,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Classification of Retinal Diseases in Optical Coherence Tomography Images Using Artificial Intelligence and Firefly Algorithm",
    "start_abstract":"In recent years, the number of studies for automatic diagnosis biomedical diseases has increased. Many these have used Deep Learning, which gives extremely good results but requires a vast amount data and computing load. If processor is insufficient quality, this takes time places an excessive load on processor. On other hand, Machine Learning faster than does not much-needed load, it provide as high accuracy value Learning. Therefore, our goal to develop hybrid system that provides value, while requiring smaller less diagnose such retinal we chose study. For purpose, first, layer extraction was conducted through image preprocessing. Then, traditional feature extractors were combined with pre-trained extractors. To select best features, Firefly algorithm. end, multiple binary classifications instead multiclass classification classifiers. Two public datasets in The first dataset had mean 0.957, second 0.954.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial Intelligence for Pediatric Ophthalmology"
      ],
      "abstract":[
        "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\nintelligence (AI) applications to general ophthalmology, comparatively less\nprogress has been made toward solving problems in pediatric ophthalmology using\nsimilar techniques. This article discusses the unique needs of pediatric\nophthalmology patients and how AI techniques can address these challenges,\nsurveys recent applications of AI to pediatric ophthalmology, and discusses\nfuture directions in the field.\n  RECENT FINDINGS: The most significant advances involve the automated\ndetection of retinopathy of prematurity (ROP), yielding results that rival\nexperts. Machine learning (ML) has also been successfully applied to the\nclassification of pediatric cataracts, prediction of post-operative\ncomplications following cataract surgery, detection of strabismus and\nrefractive error, prediction of future high myopia, and diagnosis of reading\ndisability via eye tracking. In addition, ML techniques have been used for the\nstudy of visual development, vessel segmentation in pediatric fundus images,\nand ophthalmic image synthesis.\n  SUMMARY: AI applications could significantly benefit clinical care for\npediatric ophthalmology patients by optimizing disease detection and grading,\nbroadening access to care, furthering scientific discovery, and improving\nclinical efficiency. These methods need to match or surpass physician\nperformance in clinical trials before deployment with patients. Due to\nwidespread use of closed-access data sets and software implementations, it is\ndifficult to directly compare the performance of these approaches, and\nreproducibility is poor. Open-access data sets and software implementations\ncould alleviate these issues, and encourage further AI applications to\npediatric ophthalmology.\n  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\ndeep learning"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Compressed Image Generation with Denoising Diffusion Codebook Models",
        "Juggling with Tensor Bases in Functional Approaches",
        "A Comprehensive Search for Leptoquarks Decaying into Top-$\\tau$ Final\n  States at the Future LHC",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "Measuring Diversity in Synthetic Datasets",
        "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to\n  Adversarial Attacks",
        "A Parareal in time numerical method for the collisional Vlasov equation\n  in the hyperbolic scaling",
        "POSMAC: Powering Up In-Network AR\/CG Traffic Classification with Online\n  Learning",
        "Evaluating Stenosis Detection with Grounding DINO, YOLO, and DINO-DETR",
        "Quantum superposition of boundary condition in $\\mathrm{PAdS}_2$",
        "Towards Automated Fact-Checking of Real-World Claims: Exploring Task\n  Formulation and Assessment with LLMs",
        "AUTOFRAME -- A Software-driven Integration Framework for Automotive\n  Systems",
        "Multi-Fidelity Policy Gradient Algorithms",
        "Structural Damping Identification Sensitivity in Flutter Speed\n  Estimation",
        "Comment on \"QCD factorization with multihadron fragmentation functions\"",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "Elliptic Relaxation Strategies to Support Numerical Stability of\n  Segregated Continuous Adjoint Flow Solvers",
        "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting",
        "Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for\n  Furniture Assembly Using Vision-Language Models",
        "On the Underlying Nonrelativistic Nature of Relativistic Holography",
        "A model for dynamical systems with strange attractors",
        "BiRating -- Iterative averaging on a bipartite graph of Beat Saber\n  scores, player skills, and map difficulties",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Anisotropic Raman scattering and lattice orientation identification of\n  2M-WS2",
        "Self-Taught Agentic Long Context Understanding",
        "Advancing Language Model Reasoning through Reinforcement Learning and\n  Inference Scaling",
        "A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large\n  Language Models Using Cross-Attention Signals",
        "The disrupting and growing open cluster spiral arm patterns of the Milky\n  Way",
        "Unveiling Biases while Embracing Sustainability: Assessing the Dual\n  Challenges of Automatic Speech Recognition Systems"
      ],
      "abstract":[
        "We present a novel generative approach based on Denoising Diffusion Models\n(DDMs), which produces high-quality image samples along with their losslessly\ncompressed bit-stream representations. This is obtained by replacing the\nstandard Gaussian noise sampling in the reverse diffusion with a selection of\nnoise samples from pre-defined codebooks of fixed iid Gaussian vectors.\nSurprisingly, we find that our method, termed Denoising Diffusion Codebook\nModel (DDCM), retains sample quality and diversity of standard DDMs, even for\nextremely small codebooks. We leverage DDCM and pick the noises from the\ncodebooks that best match a given image, converting our generative model into a\nhighly effective lossy image codec achieving state-of-the-art perceptual image\ncompression results. More generally, by setting other noise selections rules,\nwe extend our compression method to any conditional image generation task\n(e.g., image restoration), where the generated images are produced jointly with\ntheir condensed bit-stream representations. Our work is accompanied by a\nmathematical interpretation of the proposed compressed conditional generation\nschemes, establishing a connection with score-based approximations of posterior\nsamplers for the tasks considered.",
        "Systematic expansion schemes in functional approaches require the inclusion\nof higher order vertices. These vertices are expanded in independent tensor\nbases with a rapidly increasing number of basis elements. Amongst the related\ntasks are the construction of bases and projection operators, the importance\nordering of their elements, and the optimisation of such tensor bases, as well\nas an analysis of their regularity in momentum space. We present progress in\nall these directions and introduce the Mathematica package TensorBases designed\nfor the aforementioned tasks.",
        "We studied the collider phenomenology of third-generation scalar leptoquarks\nat the Large Hadron Collider (LHC) with a 14 TeV center-of-mass energy. The\nanalysis focuses on leptoquarks decaying exclusively into top quarks and tau\nleptons, employing machine learning-based tagging techniques for identifying\nhadronically decaying boosted top quarks, W\/Z, and Higgs bosons, as well as a\nmultivariate classifier to distinguish signal events from Standard Model (SM)\nbackgrounds. The expected 95% confidence level (CL) upper limits on the\nleptoquark production cross-section are computed assuming integrated\nluminosities of 200 and 500 inverse femtobarns at the 14 TeV LHC. The results\ndemonstrate significant sensitivity improvements for detecting leptoquarks at\nmasses beyond the current experimental limits.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons\/TinyML (MLC\/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
        "We present the design of a multiscale parareal method for kinetic equations\nin the fluid dynamic regime. The goal is to reduce the cost of a fully kinetic\nsimulation using a parallel in time procedure. Using the multiscale property of\nkinetic models, the cheap, coarse propagator consists in a fluid solver and the\nfine (expensive) propagation is achieved through a kinetic solver for a\ncollisional Vlasov equation. To validate our approach, we present simulations\nin the 1D in space, 3D in velocity settings over a wide range of initial data\nand kinetic regimes, showcasing the accuracy, efficiency, and the speedup\ncapabilities of our method.",
        "In this demonstration, we showcase POSMAC1, a platform designed to deploy\nDecision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU,\nequipped with an ARM processor, for real-time network traffic classification.\nDeveloped specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic\nclassification, POSMAC streamlines model evaluation, and generalization while\noptimizing throughput to closely match line rates.",
        "Detecting stenosis in coronary angiography is vital for diagnosing and\nmanaging cardiovascular diseases. This study evaluates the performance of\nstate-of-the-art object detection models on the ARCADE dataset using the\nMMDetection framework. The models are assessed using COCO evaluation metrics,\nincluding Intersection over Union (IoU), Average Precision (AP), and Average\nRecall (AR). Results indicate variations in detection accuracy across different\nmodels, attributed to differences in algorithmic design, transformer-based vs.\nconvolutional architectures. Additionally, several challenges were encountered\nduring implementation, such as compatibility issues between PyTorch, CUDA, and\nMMDetection, as well as dataset inconsistencies in ARCADE. The findings provide\ninsights into model selection for stenosis detection and highlight areas for\nfurther improvement in deep learning-based coronary artery disease diagnosis.",
        "We explore the quantum superposition of boundary conditions in the context of\nthe Poincar\\'e patch of the two-dimensional Anti-de Sitter space\n($\\mathrm{PAdS}_2$). Focusing on Robin (mixed) boundary conditions (RBC), we\ninvestigate the response function of the Unruh-DeWitt (UDW) detector\ninteracting with two or more scalar fields, each respecting a different\nboundary condition. The role of this quantum superposition is two-fold: i) it\nmay represent different fields propagating on the same spacetime and\ninteracting with an UDW detector or ii) it may describe an UDW detector on a\nsuperposition of spacetimes, each one with an inequivalent propagating field.",
        "Fact-checking is necessary to address the increasing volume of\nmisinformation. Traditional fact-checking relies on manual analysis to verify\nclaims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models\n(LLMs) across multiple labeling schemes (binary, three-class, five-class) and\nextends traditional claim verification by incorporating analysis, verdict\nclassification, and explanation in a structured setup to provide comprehensive\njustifications for real-world claims. We evaluate Llama-3 models of varying\nsizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024)\nusing evidence retrieved via restricted web searches. We utilize TIGERScore as\na reference-free evaluation metric to score the justifications. Our results\nshow that larger LLMs consistently outperform smaller LLMs in classification\naccuracy and justification quality without fine-tuning. We find that smaller\nLLMs in a one-shot scenario provide comparable task performance to fine-tuned\nSmall Language Models (SLMs) with large context sizes, while larger LLMs\nconsistently surpass them. Evidence integration improves performance across all\nmodels, with larger LLMs benefiting most. Distinguishing between nuanced labels\nremains challenging, emphasizing the need for further exploration of labeling\nschemes and alignment with evidences. Our findings demonstrate the potential of\nretrieval-augmented AFC with LLMs.",
        "The evolution of automotive technologies towards more integrated and\nsophisticated systems requires a shift from traditional distributed\narchitectures to centralized vehicle architectures. This work presents a novel\nframework that addresses the increasing complexity of Software Defined Vehicles\n(SDV) through a centralized approach that optimizes software and hardware\nintegration. Our approach introduces a scalable, modular, and secure automotive\ndeployment framework that leverages a hardware abstraction layer and dynamic\nsoftware deployment capabilities to meet the growing demands of the industry.\nThe framework supports centralized computing of vehicle functions, making\nsoftware development more dynamic and easier to update and upgrade. We\ndemonstrate the capabilities of our framework by implementing it in a simulated\nenvironment where it effectively handles several automotive operations such as\nlane detection, motion planning, and vehicle control. Our results highlight the\nframework's potential to facilitate the development and maintenance of future\nvehicles, emphasizing its adaptability to different hardware configurations and\nits readiness for real-world applications. This work lays the foundation for\nfurther exploration of robust, scalable, and secure SDV systems, setting a new\nstandard for future automotive architectures.",
        "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
        "Predicting flutter remains a key challenge in aeroelastic research, with\ncertain models relying on modal parameters, such as natural frequencies and\ndamping ratios. These models are particularly useful in early design stages or\nfor the development of small UAVs (maximum take-off mass below 7 kg). This\nstudy evaluates two frequency-domain system identification methods, Fast\nRelaxed Vector Fitting (FRVF) and the Loewner Framework (LF), for predicting\nthe flutter onset speed of a flexible wing model. Both methods are applied to\nextract modal parameters from Ground Vibration Testing data, which are\nsubsequently used to develop a reduced-order model with two degrees of freedom.\nResults indicate that FRVF and LFinformed models provide reliable flutter\nspeed, with predictions deviating by no more than 3% (FRVF) and 5% (LF) from\nthe N4SID-informed benchmark. The findings highlight the sensitivity of flutter\nspeed predictions to damping ratio identification accuracy and demonstrate the\npotential of these methods as computationally efficient alternatives for\npreliminary aeroelastic assessments.",
        "We make several comments on the recent work in Ref.~\\cite{Rogers:2024nhb}\nwhile also reaffirming and adding to the work in Ref.~\\cite{Pitonyak:2023gjx}.\nWe show that the factorization formula for $e^+e^-\\to (h_1\\cdots h_n)\\, X$ in\nRef.~\\cite{Rogers:2024nhb} is equivalent to a version one can derive using the\ndefinition of a $n$-hadron fragmentation function (FF) introduced in\nRef.~\\cite{Pitonyak:2023gjx}. In addition, we scrutinize how to generalize the\nnumber density definition of a single-hadron FF to a $n$-hadron FF, arguing\nthat the definition given in Ref.~\\cite{Pitonyak:2023gjx} should be considered\nthe standard one. We also emphasize that the evolution equations for dihadron\nFFs~(DiFFs) in Ref.~\\cite{Pitonyak:2023gjx} have the same splitting functions\nas those for single-hadron FFs. Therefore, the DiFF (and $n$-hadron FF)\ndefinitions in Ref.~\\cite{Pitonyak:2023gjx} have a natural number density\ninterpretation and are consistent with collinear factorization using the\nstandard hard factors and evolution kernels. Moreover, we make clear that the\noperator definition for the DiFF $D_1^{h_1h_2}(\\xi,M_h)$ written down in\nRef.~\\cite{Rogers:2024nhb} agrees exactly with the one in\nRef.~\\cite{Pitonyak:2023gjx}. Contrary to what is implied in\nRef.~\\cite{Rogers:2024nhb}, this definition did not appear in the literature\nprior to the work in Ref.~\\cite{Pitonyak:2023gjx}. There also seem to be\ninconsistencies in how $D_1^{h_1h_2}(\\xi,M_h)$ appears in previous unpolarized\ncross section formulas in the literature.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "This paper introduces a novel method for numerically stabilizing sequential\ncontinuous adjoint flow solvers utilizing an elliptic relaxation strategy. The\nproposed approach is formulated as a Partial Differential Equation (PDE)\ncontaining a single user-defined parameter, which analytical investigations\nreveal to represent the filter width of a probabilistic density function or\nGaussian kernel. Key properties of the approach include (a) smoothing features\nwith redistribution capabilities while (b) preserving integral properties. The\ntechnique targets explicit adjoint cross-coupling terms, such as the Adjoint\nTranspose Convection (ATC) term, which frequently causes numerical\ninstabilities, especially on unstructured grids common in industrial\napplications. A trade-off is made by sacrificing sensitivity consistency to\nachieve enhanced numerical robustness.\n  The method is validated on a two-phase, laminar, two-dimensional cylinder\nflow test case at Re=20 and Fn=0.75, focusing on minimizing resistance or\nmaximizing lift. A range of homogeneous and inhomogeneous filter widths is\nevaluated. Subsequently, the relaxation method is employed to stabilize adjoint\nsimulations during shape optimizations that aim at drag reduction of ship\nhulls. Two case studies are considered: A model-scale bulk carrier traveling at\nRe=7.246E+06 and Fn=0.142 as well as a harbor ferry cruising at Re=2.43E+08 and\nFn=0.4 in full-scale conditions. Both cases, characterized by unstructured\ngrids prone to adjoint divergence, demonstrate the effectiveness of the\nproposed method in overcoming stability challenges. The resulting optimizations\nachieve superior outcomes compared to approaches that omit problematic coupling\nterms.",
        "This paper investigates an open research challenge of reconstructing\nhigh-quality, large 3D open scenes from images. It is observed existing methods\nhave various limitations, such as requiring precise camera poses for input and\ndense viewpoints for supervision. To perform effective and efficient 3D scene\nreconstruction, we propose a novel graph-guided 3D scene reconstruction\nframework, GraphGS. Specifically, given a set of images captured by RGB cameras\non a scene, we first design a spatial prior-based scene structure estimation\nmethod. This is then used to create a camera graph that includes information\nabout the camera topology. Further, we propose to apply the graph-guided\nmulti-view consistency constraint and adaptive sampling strategy to the 3D\nGaussian Splatting optimization process. This greatly alleviates the issue of\nGaussian points overfitting to specific sparse viewpoints and expedites the 3D\nreconstruction process. We demonstrate GraphGS achieves high-fidelity 3D\nreconstruction from images, which presents state-of-the-art performance through\nquantitative and qualitative evaluation across multiple datasets. Project Page:\nhttps:\/\/3dagentworld.github.io\/graphgs.",
        "Humans possess an extraordinary ability to understand and execute complex\nmanipulation tasks by interpreting abstract instruction manuals. For robots,\nhowever, this capability remains a substantial challenge, as they cannot\ninterpret abstract instructions and translate them into executable actions. In\nthis paper, we present Manual2Skill, a novel framework that enables robots to\nperform complex assembly tasks guided by high-level manual instructions. Our\napproach leverages a Vision-Language Model (VLM) to extract structured\ninformation from instructional images and then uses this information to\nconstruct hierarchical assembly graphs. These graphs represent parts,\nsubassemblies, and the relationships between them. To facilitate task\nexecution, a pose estimation model predicts the relative 6D poses of components\nat each assembly step. At the same time, a motion planning module generates\nactionable sequences for real-world robotic implementation. We demonstrate the\neffectiveness of Manual2Skill by successfully assembling several real-world\nIKEA furniture items. This application highlights its ability to manage\nlong-horizon manipulation tasks with both efficiency and precision,\nsignificantly enhancing the practicality of robot learning from instruction\nmanuals. This work marks a step forward in advancing robotic systems capable of\nunderstanding and executing complex manipulation tasks in a manner akin to\nhuman capabilities.",
        "Over the past quarter century, considerable effort has been invested in the\nstudy of nonrelativistic (NR) string theory, its U-dual NR brane theories, and\ntheir geometric foundations in (generalized) Newton-Cartan geometry. Many\ninteresting results have been obtained, both for their intrinsic value and in\nthe hope that they hold useful lessons for relativistic string\/M theory. By\nsynthesizing two strands of recent developments (especially, arXiv:2312.13243\nand arXiv:2410.03591), we argue that this hope has already come to fruition,\nbecause standard, relativistic holography can now be recognized as a statement\nwithin a corresponding nonrelativistic brane theory. Our main conclusions are\ngeneral, but within the familiar example of D3-brane based holography, they\nread as follows: (i) N=4 SYM is exactly the worldvolume theory of D3-branes\nwithin `NR D3-brane theory'; (ii) AdS_5*S^5 is exactly the corresponding RR\nblack 3-brane, and includes an asymptotically-flat-Newton-Cartan region; (iii)\nAdS\/CFT duality is precisely synonymous with black-brane\/D-brane (i.e.,\nclosed-string\/open-string) duality within NR D3-brane theory; (iv)\nNewton-Cartan geometry is the underlying structure upon which entanglement of\nthe D3-brane degrees of freedom builds relativistic spacetime.",
        "We derive a system with one degree of freedom that models a class of\ndynamical systems with strange attractors in three dimensions. This system\nretains all the characteristics of chaotic attractors and is expressed by a\nsecond-order integro-differential equation which mimics a spring-like problem.\nWe determine the potential energy, the rate of change of the kinetic energy of\nthis system, and show that is self-oscillating.",
        "Difficulty estimation of Beat Saber maps is an interesting data analysis\nproblem and valuable to the Beat Saber competitive scene. We present a simple\nalgorithm that iteratively averages player skill and map difficulty estimations\nin a bipartite graph of players and maps, connected by scores, using scores\nonly as input. This approach simultaneously estimates player skills and map\ndifficulties, exploiting each of them to improve the estimation of the other,\nexploitng the relation of multiple scores by different players on the same map,\nor on different maps by the same player. While we have been unable to prove or\ncharacterize theoretical convergence, the implementation exhibits convergent\nbehaviour to low estimation error in all instances, producing accurate results.\nAn informal qualitative evaluation involving experienced Beat Saber community\nmembers was carried out, comparing the difficulty estimations output by our\nalgorithm with their personal perspectives on the difficulties of different\nmaps. There was a significant alignment with player perceived perceptions of\ndifficulty and with other existing methods for estimating difficulty. Our\napproach showed significant improvement over existing methods in certain known\nproblematic maps that are not typically accurately estimated, but also produces\nproblematic estimations for certain families of maps where the assumptions on\nthe meaning of scores were inadequate (e.g. not enough scores, or scores over\noptimized by players). The algorithm has important limitations, related to data\nquality and meaningfulness, assumptions on the domain problem, and theoretical\nconvergence of the algorithm. Future work would significantly benefit from a\nbetter understanding of adequate ways to quantify map difficulty in Beat Saber,\nincluding multidimensionality of skill and difficulty, and the systematic\nbiases present in score data.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Anisotropic materials with low symmetries hold significant promise for\nnext-generation electronic and quantum devices. 2M-WS2, a candidate for\ntopological superconductivity, has garnered considerable interest. However, a\ncomprehensive understanding of how its anisotropic features contribute to\nunconventional superconductivity, along with a simple, reliable method to\nidentify its crystal orientation, remains elusive. Here, we combine theoretical\nand experimental approaches to investigate angle- and polarization-dependent\nanisotropic Raman modes of 2M-WS2. Through first-principles calculations, we\npredict and analyze phonon dispersion and lattice vibrations of all Raman modes\nin 2M-WS2. We establish a direct correlation between their anisotropic Raman\nspectra and high-resolution transmission electron microscopy images. Finally,\nwe demonstrate that anisotropic Raman spectroscopy can accurately determine the\ncrystal orientation and twist angle between two stacked 2M-WS2 layers. Our\nfindings provide insights into the electron-phonon coupling and anisotropic\nproperties of 2M-WS2, paving the way for the use of anisotropic materials in\nadvanced electronic and quantum devices.",
        "Answering complex, long-context questions remains a major challenge for large\nlanguage models (LLMs) as it requires effective question clarifications and\ncontext retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a\nframework designed to enhance an LLM's understanding of such queries by\nintegrating targeted self-clarification with contextual grounding within an\nagentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),\nwhere models refine their understanding through self-generated clarification\nquestions and corresponding contextual groundings. By scaling inference as a\ntree search where each node represents a CoC step, we achieve 97.8% answer\nrecall on NarrativeQA with a search depth of up to three and a branching factor\nof eight. To amortize the high cost of this search process to training, we\nleverage the preference pairs for each step obtained by the CoC workflow and\nperform two-stage model finetuning: (1) supervised finetuning to learn\neffective decomposition strategies, and (2) direct preference optimization to\nenhance reasoning quality. This enables AgenticLU models to generate\nclarifications and retrieve relevant context effectively and efficiently in a\nsingle inference pass. Extensive experiments across seven long-context tasks\ndemonstrate that AgenticLU significantly outperforms state-of-the-art prompting\nmethods and specialized long-context LLMs, achieving robust multi-hop reasoning\nwhile sustaining consistent performance as context length grows.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https:\/\/github.com\/THUDM\/T1}.",
        "We propose a novel reinforcement learning framework for post training large\nlanguage models that does not rely on human in the loop feedback. Instead, our\napproach uses cross attention signals within the model itself to derive a self\nsupervised reward, thereby guiding iterative fine tuning of the model policy.\nBy analyzing how the model attends to the input prompt during generation, we\nconstruct measures of prompt coverage, focus, and coherence. We then use these\nmeasures to rank or score candidate responses, providing a reward signal that\nencourages the model to produce well aligned, on topic text. In empirical\ncomparisons against standard policy gradient methods and RL fine tuning with\nsynthetic preference models, our method shows significant gains in prompt\nrelevance and consistency over a non RL baseline. While it does not yet match\nthe performance of fully human supervised RLHF systems, it highlights an\nimportant direction for scaling alignment with minimal human labeling. We\nprovide a detailed analysis, discuss potential limitations, and outline future\nwork for combining cross-attention based signals with smaller amounts of human\nfeedback.",
        "Star clusters provide unique advantages for investigating Galactic spiral\narms, particularly due to their precise ages, positions, and kinematic\nproperties, which are further enhanced by ongoing updates from the astrometric\ndata. In this study, we employ the latest extensive catalogue of open clusters\nfrom Gaia DR3 to examine the positional deviations of clusters belonging to\ndifferent age groups. Additionally, we employ dynamical simulations to probe\nthe evolutionary behavior of spiral arm positions. Our analysis reveals an\nabsence of a theoretical age pattern in the spiral arms traced by open\nclusters, and the pattern speeds of the spiral arms are consistent with the\nrotation curve. Both of these results do not align with the predictions of\nquasi-stationary density wave theory, suggesting a more dynamic or transient\narm scenario for the Milky Way. From this perspective, combined with vertex\ndeviation estimates, it appears that the Local arm is in a state of growth. In\ncontrast, the Sagittarius-Carina arm and the Perseus arm exhibit opposing\ntrends. Consequently, we speculate that the Galactic stellar disk does not\nexhibit a grand-design spiral pattern with a fixed pattern speed, but rather\nmanifests as a multi-armed structure with arms that continuously emerge and\ndissipate.",
        "In this paper, we present a bias and sustainability focused investigation of\nAutomatic Speech Recognition (ASR) systems, namely Whisper and Massively\nMultilingual Speech (MMS), which have achieved state-of-the-art (SOTA)\nperformances. Despite their improved performance in controlled settings, there\nremains a critical gap in understanding their efficacy and equity in real-world\nscenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well\nas their effect on downstream tasks. In addition, we examine the environmental\nimpact of ASR systems, scrutinizing the use of large acoustic models on carbon\nemission and energy consumption. We also provide insights into our empirical\nanalyses, offering a valuable contribution to the claims surrounding bias and\nsustainability in ASR systems."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Learning Parities with Neural Networks",
    "start_abstract":"In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Testing conditional independence of discrete distributions"
      ],
      "abstract":[
        "We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary."
      ],
      "categories":[
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "CoHiRF: A Scalable and Interpretable Clustering Framework for\n  High-Dimensional Data",
        "A look-up table algorithm to model radiation damage effects in Monte\n  Carlo events for HL-LHC experiments",
        "Consistent crust-core interpolation and its effect on non-radial neutron\n  star oscillations",
        "Some Bohr-type inequalities with two parameters for bounded analytic\n  functions",
        "Spatially resolved dust properties over 50 kpc in a hyperluminous galaxy\n  merger at $z = 4.6$",
        "Inductive Construction of Variational Quantum Circuit for Constrained\n  Combinatorial Optimization",
        "Safety in safe Bayesian optimization and its ramifications for control",
        "Sample and Map from a Single Convex Potential: Generation using\n  Conjugate Moment Measures",
        "Improving Student Self-Efficacy in Quantum Computing with the Qubit\n  Touchdown Board Game",
        "A linearly-implicit energy preserving scheme for geometrically nonlinear\n  mechanics based on non-canonical Hamiltonian formulations",
        "Colimits of internal categories",
        "A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral\n  Microbiome FISH Image Data",
        "Chiral Magnetic Effect enhancement at lower collision energies",
        "Rotational beta expansions and Schmidt games",
        "Constructing the low-temperature phase diagram for the $2+p$-quantum\n  spin glass using the nonperturbative renormalization group",
        "Integrating Product Coefficients for Improved 3D LiDAR Data\n  Classification",
        "A note on rank $\\frac{3}{2}$ Liouville irregular block",
        "Rational points on the non-split Cartan modular curve of level 27 and\n  quadratic Chabauty over number fields",
        "Generalized Uncertainty Principle mimicking dynamical Dark Energy:\n  matter perturbations and gravitational wave data analysis",
        "Statistical analysis of Multipath Entanglement Purification in Quantum\n  Networks",
        "Singleshot Multispectral Imaging via a Chromatic Metalens Array",
        "Wrapped Floer homology and hyperbolic sets",
        "Momentum and Matter Matter for Axion Dark Matter Matters on Earth",
        "Magneto-$\\nu$: Heavy neutral lepton search using $^{241}$Pu $\\beta^-$\n  decays",
        "Charged rotating black holes in scalar multipolar universes",
        "Impact of Optic Nerve Tortuosity, Globe Proptosis, and Size on Retinal\n  Ganglion Cell Thickness Across General, Glaucoma, and Myopic Populations:\n  Insights from the UK Biobank",
        "Affine diffractive beam dividers",
        "Building Machine Learning Challenges for Anomaly Detection in Science",
        "Maxwell-Vlasov-Uehling-Uhlenbeck (VUU) Simulation for Coupled\n  Laser-Electron Dynamics in a Metal Irradiated by Ultrashort Intense Laser\n  Pulses"
      ],
      "abstract":[
        "Clustering high-dimensional data poses significant challenges due to the\ncurse of dimensionality, scalability issues, and the presence of noisy and\nirrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF),\na novel clustering method designed to address these challenges effectively.\nCoHiRF leverages random feature selection to mitigate noise and dimensionality\neffects, repeatedly applies K-Means clustering in reduced feature spaces, and\ncombines results through a unanimous consensus criterion. This iterative\napproach constructs a cluster assignment matrix, where each row records the\ncluster assignments of a sample across repetitions, enabling the identification\nof stable clusters by comparing identical rows. Clusters are organized\nhierarchically, enabling the interpretation of the hierarchy to gain insights\ninto the dataset. CoHiRF is computationally efficient with a running time\ncomparable to K-Means, scalable to massive datasets, and exhibits robust\nperformance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and\nOPTICS. Experimental results on synthetic and real-world datasets confirm the\nmethod's ability to reveal meaningful patterns while maintaining scalability,\nmaking it a powerful tool for high-dimensional data analysis.",
        "Radiation damage significantly impacts the performance of silicon tracking\ndetectors in Large Hadron Collider (LHC) experiments such as ATLAS and CMS,\nwith signal reduction being the most critical effect. Adjusting sensor bias\nvoltage and detection thresholds can help mitigate these effects, but\ngenerating simulated data that accurately mirror the performance evolution with\nthe accumulation of luminosity, hence fluence, is crucial. The ATLAS\ncollaboration has developed and implemented algorithms to correct simulated\nMonte Carlo (MC) events for radiation damage effects, achieving impressive\nagreement between collision data and simulated events. In preparation for the\nhigh-luminosity phase (HL-LHC), the demand for a faster ATLAS MC production\nalgorithm becomes imperative due to escalating collision, events, tracks, and\nparticle hit rates, imposing stringent constraints on available computing\nresources. This article outlines the philosophy behind the new algorithm, its\nimplementation strategy, and the essential components involved. The results\nfrom closure tests indicate that the events simulated using the new algorithm\nagree with fully simulated events at the level of few \\%. The first tests on\ncomputing performance show that the new algorithm is as fast as it is when no\nradiation damage corrections are applied.",
        "To model the structure of neutron stars (NSs) theoretically,it is common to\nconsider layers with different density regimes. Matching the equation of state\n(EoS) for the crust and core and obtaining a suitable description of these\nextreme conditions are crucial for understanding the properties of these\ncompact objects. In this work, we construct ten different NS EoSs incorporating\nthree distinct crust models, which are connected to the core using a\nthermodynamically and causally consistent formalism. For cold NSs, we propose a\nlinear relationship between pressure and energy density in a narrow region\nbetween the crust and core, effectively establishing an interpolation function\nin the pressure-baryonic chemical potential plane. We then compare this EoS\nmatching method with the classical approach, which neglects causal and\nthermodynamic consistency. We solve the Tolman-Oppenheimer-Volkoff equation to\nobtain the mass-radius relationship and compare our results with observational\nconstraints on NSs. Furthermore, we investigate the influence of the new\nmatching formalism on non-radial oscillation frequencies and damping times. Our\nfindings suggest that the method used to glue the crust and core EoS impacts NS\nobservables, such as the radius, oscillation frequencies, and damping times of\nnon-radial modes, which may be crucial for interpreting future gravitational\nwave observations from neutron star mergers or isolated pulsars. The effects\nare particularly noticeable for low-mass NSs, regardless of the specific EoS\nmodel chosen. In particular, we find that the $p_1$ oscillation mode exhibits\nsignificant differences in frequencies among alternative matching methods,\nwhereas the fundamental $f$-mode remains unaffected by changes in crust models\nor interpolation schemes.",
        "In this article, some Bohr inequalities for analytical functions on the unit\ndisk are generalized to the forms with two parameters. One of our results is\nsharp.",
        "We present spatially resolved dust-continuum ALMA observations from\nrest-frame $\\sim$60 to $\\sim$600 $\\mu$m (bands 3-10) of the hyperluminous hot\ndust-obscured galaxy (hot DOG) WISE J224607.6-052634.9 (W2246-0526), at\nredshift $z=4.6$. W2246-0526 is interacting with at least three companion\ngalaxies, forming a system connected by tidal streams. We model the\nmultiwavelength ALMA observations of the dust continuum using a modified\nblackbody, from which we derive the dust properties (mass, emissivity index,\narea of the emitting region, and temperature) in the hot DOG and resolved\nstructures across a region of nearly $\\sim$50 kpc. The peak temperature at the\nlocation of the hot DOG, $\\sim$110 K, is likely the consequence of heating by\nthe central quasar. The dust temperature drops to $\\sim$40 K at a radius of\n$\\sim$8 kpc, suggesting that heating by the quasar beyond that distance is\nnondominant. The dust in the connecting streams between the host and companion\ngalaxies is at temperatures between 30-40 K, typical of starburst galaxies,\nsuggesting it is most likely heated by recent, in-situ star formation. This is\nthe first time dust properties are spatially resolved over several tens of kpc\nin a galaxy system beyond Cosmic Noon --this is more than six times the scales\npreviously probed in galaxies at those redshifts.",
        "In this study, we propose a new method for constrained combinatorial\noptimization using variational quantum circuits. Quantum computers are\nconsidered to have the potential to solve large combinatorial optimization\nproblems faster than classical computers. Variational quantum algorithms, such\nas Variational Quantum Eigensolver (VQE), have been studied extensively because\nthey are expected to work on noisy intermediate scale devices. Unfortunately,\nmany optimization problems have constraints, which induces infeasible solutions\nduring VQE process. Recently, several methods for efficiently solving\nconstrained combinatorial optimization problems have been proposed by designing\na quantum circuit so as to output only the states that satisfy the constraints.\nHowever, the types of available constraints are still limited. Therefore, we\nhave started to develop variational quantum circuits that can handle a wider\nrange of constraints. The proposed method utilizes a forwarding operation that\nmaps from feasible states for subproblems to those for larger subproblems. As\nlong as appropriate forwarding operations can be defined, iteration of this\nprocess can inductively construct variational circuits outputting feasible\nstates even in the case of multiple and complex constraints. In this paper, the\nproposed method was applied to facility location problem and was found to\nincrease the probability for measuring feasible solutions or optimal solutions.\nIn addition, the cost of the obtained circuit was comparable to that of\nconventional variational circuits.",
        "A recurring and important task in control engineering is parameter tuning\nunder constraints, which conceptually amounts to optimization of a blackbox\nfunction accessible only through noisy evaluations. For example, in control\npractice parameters of a pre-designed controller are often tuned online in\nfeedback with a plant, and only safe parameter values should be tried, avoiding\nfor example instability. Recently, machine learning methods have been deployed\nfor this important problem, in particular, Bayesian optimization (BO). To\nhandle safety constraints, algorithms from safe BO have been utilized,\nespecially SafeOpt-type algorithms, which enjoy considerable popularity in\nlearning-based control, robotics, and adjacent fields. However, we identify two\nsignificant obstacles to practical safety. First, SafeOpt-type algorithms rely\non quantitative uncertainty bounds, and most implementations replace these by\ntheoretically unsupported heuristics. Second, the theoretically valid\nuncertainty bounds crucially depend on a quantity - the reproducing kernel\nHilbert space norm of the target function - that at present is impossible to\nreliably bound using established prior engineering knowledge. By careful\nnumerical experiments we show that these issues can indeed cause safety\nviolations. To overcome these problems, we propose Lipschitz-only Safe Bayesian\nOptimization (LoSBO), a safe BO algorithm that relies only on a known Lipschitz\nbound for its safety. Furthermore, we propose a variant (LoS-GP-UCB) that\navoids gridding of the search space and is therefore applicable even for\nmoderately high-dimensional problems.",
        "A common approach to generative modeling is to split model-fitting into two\nblocks: define first how to sample noise (e.g. Gaussian) and choose next what\nto do with it (e.g. using a single map or flows). We explore in this work an\nalternative route that ties sampling and mapping. We find inspiration in moment\nmeasures, a result that states that for any measure $\\rho$ supported on a\ncompact convex set of $\\mathbb{R}^d$, there exists a unique convex potential\n$u$ such that $\\rho=\\nabla u\\,\\sharp\\,e^{-u}$. While this does seem to tie\neffectively sampling (from log-concave distribution $e^{-u}$) and action\n(pushing particles through $\\nabla u$), we observe on simple examples (e.g.,\nGaussians or 1D distributions) that this choice is ill-suited for practical\ntasks. We study an alternative factorization, where $\\rho$ is factorized as\n$\\nabla w^*\\,\\sharp\\,e^{-w}$, where $w^*$ is the convex conjugate of $w$. We\ncall this approach conjugate moment measures, and show far more intuitive\nresults on these examples. Because $\\nabla w^*$ is the Monge map between the\nlog-concave distribution $e^{-w}$ and $\\rho$, we rely on optimal transport\nsolvers to propose an algorithm to recover $w$ from samples of $\\rho$, and\nparameterize $w$ as an input-convex neural network.",
        "Qubit Touchdown is a two-player, competitive board game that was developed to\nintroduce students to quantum computing. A quantum computer is a new kind of\ncomputer that is based on the laws of quantum physics, and it can solve certain\nproblems faster than normal computers because it follows a different set of\nrules. Qubit Touchdown's game play mirrors the rules of (American) football,\nwith players taking turns moving the football to score the most touchdowns, and\nno knowledge of quantum computing is needed to play the game. We evaluated the\ngame with 107 public high school students in Precalculus, Advanced Placement\n(AP) Statistics, and\/or AP Physics 1 courses, assessing whether their interest\nin and self-efficacy toward quantum computing changed as a result of playing\nthe game and learning about its connections to quantum computing. We also\nassessed whether the game was easy to learn and enjoyable. We found that\nstudents' self-efficacy was improved by 33.4%, and they widely considered the\ngame accessible and fun. Thus, Qubit Touchdown could be an effective resource\nto introduce students to Quantum Computing and boost their confidence in\nlearning about the field. Free printables of the game are available, and\nprofessionally produced copies can be purchased on demand.",
        "This work presents a novel formulation and numerical strategy for the\nsimulation of geometrically nonlinear structures. First, a non-canonical\nHamiltonian (Poisson) formulation is introduced by including the dynamics of\nthe stress tensor. This framework is developed for von-K\\'arm\\'an\nnonlinearities in beams and plates, as well as finite strain elasticity with\nSaint-Venant material behavior. In the case of plates, both negligible and\nnon-negligible membrane inertia are considered. For the former case the\ntwo-dimensional elasticity complex is leveraged to express the dynamics in\nterms of the Airy stress function. The finite element discretization employs a\nmixed approach, combining a conforming approximation for displacement and\nvelocity fields with a discontinuous stress tensor representation. A staggered,\nlinear implicit time integration scheme is proposed, establishing connections\nwith existing explicit-implicit energy-preserving methods. The stress degrees\nof freedom are statically condensed, reducing the computational complexity to\nsolving a system with a positive definite matrix. The methodology is validated\nthrough numerical experiments on the Duffing oscillator, a von-K\\'arm\\'an beam,\nand a column undergoing finite strain elasticity. Comparisons with fully\nimplicit energy-preserving method and the explicit Newmark scheme demonstrate\nthat the proposed approach achieves superior accuracy while maintaining energy\nstability. Additionally, it enables larger time steps compared to explicit\nschemes and exhibits computational efficiency comparable to the leapfrog\nmethod.",
        "We show that for a list-arithmetic pretopos $\\mathcal{E}$ with pullback\nstable coequalisers, the $2$-category $\\mathbf{Cat}(\\mathcal{E})$ of internal\ncategories, functors and natural transformations has finite $2$-colimits.",
        "Advances in cellular imaging technologies, especially those based on\nfluorescence in situ hybridization (FISH) now allow detailed visualization of\nthe spatial organization of human or bacterial cells. Quantifying this spatial\norganization is crucial for understanding the function of multicellular tissues\nor biofilms, with implications for human health and disease. To address the\nneed for better methods to achieve such quantification, we propose a flexible\nmultivariate point process model that characterizes and estimates complex\nspatial interactions among multiple cell types. The proposed Bayesian framework\nis appealing due to its unified estimation process and the ability to directly\nquantify uncertainty in key estimates of interest, such as those of inter-type\ncorrelation and the proportion of variance due to inter-type relationships. To\nensure stable and interpretable estimation, we consider shrinkage priors for\ncoefficients associated with latent processes. Model selection and comparison\nare conducted by using a deviance information criterion designed for models\nwith latent variables, effectively balancing the risk of overfitting with that\nof oversimplifying key quantities. Furthermore, we develop a hierarchical\nmodeling approach to integrate multiple image-specific estimates from a given\nsubject, allowing inference at both the global and subject-specific levels. We\napply the proposed method to microbial biofilm image data from the human tongue\ndorsum and find that specific taxon pairs, such as Streptococcus\nmitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit\nstrong positive spatial correlations, while others, such as Actinomyces-Rothia,\nshow slight negative correlations. For most of the taxa, a substantial portion\nof spatial variance can be attributed to inter-taxon relationships.",
        "We extend previous holographic studies of the Chiral Magnetic Effect (CME) by\nincorporating a time-dependent magnetic field. Various magnetic field profiles\nproposed in the literature are implemented, and their impact on the CME signal\nis analyzed in both static and expanding backgrounds. Interestingly, the\nintegrated chiral magnetic current can exhibit a non-monotonic dependence on\nthe collision energy. Our results suggest that the CME signal is enhanced at\ncollision energies below $\\sqrt{s}=200$ GeV. In addition, we derive a\nquasi-equilibrium formula for the chiral magnetic effect in the expanding\nbackground that is valid at late times.",
        "We consider rotational beta expansions in dimensions 1, 2 and 4 and view them\nas expansions on real numbers, complex numbers, and quaternions, respectively.\nWe give sufficient conditions on the parameters $\\alpha, \\beta \\in (0,1)$ so\nthat particular cylinder sets arising from the expansions are winning or losing\nSchmidt $(\\alpha,\\beta)$-game.",
        "In this paper, we use a nonperturbative renormalization group approach to\nconstruct the dynamical phase space of a quantum spin glass in the large $N$\nlimit. The disordered Hamiltonian is of ``$2 + p$\" type, and we perform a\ncoarse-graining procedure over the Wigner spectrum for the matrix-like\ndisorder. The phase space reconstruction relies on phase transitions derived\nfrom the Luttinger-Ward functional, which accounts for interactions that are\nforbidden by perturbation theory. Various phases are identified, characterized\nby large correlations between replicas and\/or the breaking of time translation\nsymmetry.",
        "In this paper, we address the enhancement of classification accuracy for 3D\npoint cloud Lidar data, an optical remote sensing technique that estimates the\nthree-dimensional coordinates of a given terrain. Our approach introduces\nproduct coefficients, theoretical quantities derived from measure theory, as\nadditional features in the classification process. We define and present the\nformulation of these product coefficients and conduct a comparative study,\nusing them alongside principal component analysis (PCA) as feature inputs.\nResults demonstrate that incorporating product coefficients into the feature\nset significantly improves classification accuracy within this new framework.",
        "This paper focuses on a conformal block with rank $\\frac{3}{2}$ irregular\nsingularity which corresponds to the prepotential of the ${\\cal H}_1$\nArgyres-Douglas theory in $\\Omega$ background. We derive this irregular\nconformal block using generalized holomorphic anomaly recursion relation. This\nresults is an expression which is a power series in $\\Omega$-background\nparameters $\\epsilon_{1,2}$ and exact in coupling. We have verified that in\nsmall coupling regime our result is consistent with previously known\nexpressions.\n  Furthermore we derive the Deformed Seiberg-Witten curve which provides an\nalternative tool to explore above mentioned theory in Nekrasov-Shatashvili\nlimit of $\\Omega$-background. We checked that the results are in complete\nagreement with the holomorphic anomaly approach.",
        "Thanks to work of Rouse, Sutherland, and Zureick-Brown, it is known exactly\nwhich subgroups of GL$_2(\\mathbf{Z}_3)$ can occur as the image of the $3$-adic\nGalois representation attached to a non-CM elliptic curve over $\\mathbf{Q}$,\nwith a single exception: the normaliser of the non-split Cartan subgroup of\nlevel 27. In this paper, we complete the classification of 3-adic Galois images\nby showing that the normaliser of the non-split Cartan subgroup of level 27\ncannot occur as a 3-adic Galois image of a non-CM elliptic curve.\n  Our proof proceeds via computing the $\\mathbf{Q}(\\zeta_3)$-rational points on\na certain smooth plane quartic curve $X'_H$ (arising as a quotient of the\nmodular curve $X_{ns}^+(27)$) defined over $\\mathbf{Q}(\\zeta_3)$ whose Jacobian\nhas Mordell--Weil rank 6. To this end, we describe how to carry out the\nquadratic Chabauty method for a modular curve $X$ defined over a number field\n$F$, which, when applicable, determines a finite subset of\n$X(F\\otimes\\mathbf{Q}_p)$ in certain situations of larger Mordell--Weil rank\nthan previously considered. Together with an analysis of local heights above 3,\nwe apply this quadratic Chabauty method to determine\n$X'_H(\\mathbf{Q}(\\zeta_3))$. This allows us to compute the set\n$X_{ns}^+(27)(\\mathbf{Q})$, finishing the classification of 3-adic images of\nGalois.",
        "The Generalized Uncertainty Principle (GUP) stands out as a nearly ubiquitous\nfeature in quantum gravity modeling, predicting the emergence of a minimum\nlength at the Planck scale. Recently, it has been shown to modify the area-law\nscaling of the Bekenstein-Hawking entropy, giving rise to deformed Friedmann\nequations within Jacobson's approach. The ensuing model incorporates the GUP\ncorrection as a quintessence-like dark energy that supplements the cosmological\nconstant, influencing the dynamics of the early Universe while aligning with\nthe $\\Lambda$CDM paradigm in the current epoch. In this extended scenario, we\nexamine the growth of matter perturbations and structure formation employing\nthe Top-Hat Spherical Collapse approach. Our analysis reveals that the profile\nof the density contrast is sensitive to the GUP parameter $\\beta$, resulting in\na slower gravitational evolution of primordial fluctuations in the matter\ndensity. We also discuss implications for the relic density of Primordial\nGravitational Waves (PGWs), identifying the parameter space that enhances the\nPGW spectrum. Using the sensitivity of the next-generation GW observatories in\nthe frequency range below $10^3\\,\\mathrm{Hz}$, we constrain\n$\\beta\\lesssim10^{39}$, which is more stringent than most other\ncosmological\/astrophysical limits. This finding highlights the potential role\nof GWs in the pursuit of understanding quantum gravity phenomenology.",
        "In quantum networks, a set of entangled states distributed over multiple,\nalternative, distinct paths between a pair of source-destination nodes can be\npurified to obtain a higher fidelity entangled state between the nodes. This\nmultipath entanglement purification (MP-EP) strategy can exploit the network's\ncomplex structure to strengthen the entanglement connection between node pairs\nseparated by appropriate graph distances. We investigate the network scenarios\nin which MP-EP outperforms entanglement distribution over single network paths\nutilising a statistical model of a quantum network and find that MP-EP can be\nan effective entanglement distribution strategy over a range of node\nseparations determined by the average edge fidelities and probabilities of the\nnetwork. We find that MP-EP can bost the entanglement connection between\nsuitably separated node pairs to reach fidelities sufficient for a given\nquantum task thereby increasing the functionality of a quantum network.\nFurther, we provide statistical criteria in terms of network parameters that\ncan determine the regions of the network where MP-EP can be a useful\nentanglement distribution strategy.",
        "Real time, singleshot multispectral imaging systems are crucial for\nenvironment monitoring and biomedical imaging. Most singleshot multispectral\nimagers rely on complex computational backends, which precludes real time\noperations. In this work, we leverage the spectral selectivity afforded by\nengineered photonic materials to perform bulk of the multispectral data\nextraction in the optical domain, thereby circumventing the need for heavy\nbackend computation. We use our imager to extract multispectral data for two\nreal world objects at 8 predefined spectral channels in the 400 to 900 nm\nwavelength range. For both objects, an RGB image constructed using extracted\nmultispectral data shows good agreement with an image taken using a phone\ncamera, thereby validating our imaging approach. We believe that the proposed\nsystem can provide new avenues for the development of highly compact and low\nlatency multispectral imaging technologies.",
        "In this paper, we continue the quest to understand the interplay between\nwrapped Floer homology barcode and topological entropy. Wrapped Floer homology\nbarcode entropy is defined as the exponential growth, with respect to the left\nendpoints, of the number of not-too-short bars in its barcode. We prove that,\nin the presence of a topologically transitive, locally maximal hyperbolic set\nfor the Reeb flow on the boundary of a Liouville domain, the barcode entropy is\nbounded from below by the topological entropy restricted to the hyperbolic set.",
        "We investigate the implications of matter effects to searches for axion Dark\nMatter on Earth. The finite momentum of axion Dark Matter is crucial to\nelucidating the effects of Earth on both the axion Dark Matter field value and\nits gradient. We find that experiments targeting axion couplings compatible\nwith canonical solutions of the strong CP puzzle are likely not affected by\nEarth's matter effects. However, experiments sensitive to lighter axions with\nstronger couplings can be significantly affected, with a significant part of\nthe parameter space suffering from a reduced axion field value, and therefore\ndecreased experimental sensitivity. In contrast, the spatial gradient of the\naxion field can be enhanced along Earth's radial direction, with important\nimplications for ongoing and planned experiments searching for axion Dark\nMatter.",
        "We present experimental beta spectra of $^{241}$Pu as part of the\nMagneto-$\\nu$ experiment aimed at searching for keV heavy neutral leptons\n(HNLs). Total 200 million beta decays are measured by metallic magnetic\ncalorimeters (MMCs), representing the highest statistical precision achieved\nfor $^{241}$Pu to date. The end-point energy of $^{241}$Pu beta decay is\nmeasured via in-situ alpha calibration, yielding a value of 21.52(2) keV. The\ndifferential decay rate of the first forbidden non-unique transition of\n$^{241}$Pu is described with a quadratic shape correction factor $C(w) = 1 -\n1.931w + 0.940w^2$. Using these high-statistics spectra, we set an upper limit\non the admixture of a 10.5\\,keV sterile neutrino to the electron neutrino at\n$|U_{e4}|^2 < 2 \\times 10^{-3}$ for HNL mass of 10.5\\,keV.",
        "Recently, Cardoso and Nat\\'ario [6] constructed an exact solution of\nEinstein-scalar field equations that describes a scalar counterpart of the\nSchwarzschild-Melvin Universe. In fact, this solution belongs to a more general\nclass of solutions described by Herdeiro in [7]. In this work we show how to\nfurther generalize these solutions in presence of rotation and various charges.\nMore specifically, we describe the general charged rotating black holes with\nNUT charge and present some of their properties.",
        "Purpose: To investigate the impact of optic nerve tortuosity (ONT), and the\ninteraction of globe proptosis and globe size on retinal ganglion cell (RGC)\nthickness, using Retinal Nerve Fiber Layer (RNFL) thickness, across general,\nglaucoma, and myopic populations. Methods: We analyzed 17,970 eyes from the\nUKBiobank cohort (ID 76442), including 371 glaucoma and 2481 myopic eyes. AI\nmodels segmented structures from 3D optical coherence tomography (OCT) scans\nand magnetic resonance images (MRI). RNFL thickness was derived from OCT scans\nand corrected for ocular magnification, was derived from OCT. From MRIs, we\nextracted: ONT, globe proptosis, axial length, and a novel interzygomatic\nline-to-posterior pole (ILPP) distance, a composite marker of globe proptosis\nand size. GEE models assessed associations between orbital and retinal features\nacross all populations. Results: Segmentation models achieved Dice coefficients\nover 0.94 for both MRI and OCT. RNFL thickness was positively correlated with\nboth ONT and ILPP distance (r = 0.065, p < 0.001, and r = 0.206, p < 0.001\nrespectively). The same was true for glaucoma (r = 0.140, p < 0.01, and r =\n0.256, p < 0.01), and for myopia (r = 0.071, p < 0.001, and r = 0.100, p <\n0.0001). GEE models revealed straighter optic nerves and shorter ILPP distance\nas predictive of thinner RNFL in all populations. Conclusions: This study\nemphasizes the impact of ONT, globe size, and proptosis on retinal health,\nsuggesting RNFL thinning may arise from biomechanical stress due to straighter\noptic nerves or reduced ILPP distance, particularly in glaucoma or myopia. The\nnovel ILPP metric, integrating globe size and position, shows potential as a\nbiomarker for axonal health. These findings highlight the role of orbit\nstructures in RGC axonal health and warrant further exploration of the\nbiomechanical relationship between the orbit and optic nerve.",
        "Diffractive optical elements that divide an input beam into a set of replicas\nare used in many optical applications ranging from image processing to\ncommunications. Their design requires time-consuming optimization processes,\nwhich, for a given number of generated beams, are to be separately treated for\none-dimensional and two-dimensional cases because the corresponding optimal\nefficiencies may be different. After generalizing their Fourier treatment, we\nprove that, once a particular divider has been designed, its transmission\nfunction can be used to generate numberless other dividers through affine\ntransforms that preserve the efficiency of the original element without\nrequiring any further optimization.",
        "Scientific discoveries are often made by finding a pattern or object that was\nnot predicted by the known rules of science. Oftentimes, these anomalous events\nor objects that do not conform to the norms are an indication that the rules of\nscience governing the data are incomplete, and something new needs to be\npresent to explain these unexpected outliers. The challenge of finding\nanomalies can be confounding since it requires codifying a complete knowledge\nof the known scientific behaviors and then projecting these known behaviors on\nthe data to look for deviations. When utilizing machine learning, this presents\na particular challenge since we require that the model not only understands\nscientific data perfectly but also recognizes when the data is inconsistent and\nout of the scope of its trained behavior. In this paper, we present three\ndatasets aimed at developing machine learning-based anomaly detection for\ndisparate scientific domains covering astrophysics, genomics, and polar\nscience. We present the different datasets along with a scheme to make machine\nlearning challenges around the three datasets findable, accessible,\ninteroperable, and reusable (FAIR). Furthermore, we present an approach that\ngeneralizes to future machine learning challenges, enabling the possibility of\nlarge, more compute-intensive challenges that can ultimately lead to scientific\ndiscovery.",
        "The description of electron-electron scattering presents challenges in the\nmicroscopic modeling of the interaction of ultrashort intense laser pulses with\nsolids. We extend the semiclassical approach based on the Vlasov equation\n[Phys. Rev. B 104, 075157(2021)] to account for dynamic electron-electron\nscattering by introducing the Vlasov-Uehling-Uhlenbeck (VUU) equation. We\nfurther couple the VUU equation with Maxwell's equations to describe the laser\npulse propagation. We apply the present approach to simulate laser-electron\ninteractions in bulk and thin-film aluminum, focusing on energy absorption and\ntransport. Our calculation results reveal that electron-electron scattering\naffects energy absorption more significantly under p-polarization than under\ns-polarization, highlighting the role of the non-uniform surface potential. Our\nsimulations also show that the energy transport extends beyond the optical\npenetration depth, which is consistent with observations in previous laser\nablation experiments. The developed Maxwell-VUU approach is expected to advance\nthe understanding of intense laser-material interactions not only as a\ncost-effective alternative to the time-dependent density functional theory\n(TDDFT), but also by incorporating fermionic two-body collisions whose\ndescription is limited in TDDFT."
      ]
    }
  },
  {
    "id":2411.01375,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"Testing conditional independence of discrete distributions",
    "start_abstract":"We study the problem of testing *conditional independence* for discrete distributions. Specifically, given samples from a random variable (X, Y, Z) on domain [\u21131]\u00d7[\u21132] \u00d7 [n], we want to distinguish, with probability at least 2\/3, between case that X and Y are conditionally independent Z is \u0454-far, in \u21131-distance, every distribution has this property. Conditional independence concept central importance statistics important applications various scientific domains. As such, statistical task conditional been extensively studied forms within econometrics community nearly century. Perhaps surprisingly, not previously considered framework property particular no tester *sublinear* sample complexity known, even special domains binary.",
    "start_categories":[
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Learning Parities with Neural Networks"
      ],
      "abstract":[
        "In recent years we see a rapidly growing line of research which shows learnability various models via common neural network algorithms. Yet, besides very few outliers, these results show that can be learned using linear methods. Namely, such learning neural-networks with gradient-descent is competitive classifier on top data-independent representation the examples. This leaves much to desired, as networks are far more successful than Furthermore, conceptual level, don't seem capture deepness deep networks. this paper make step towards showing leanability inherently non-linear. We under certain distributions, sparse parities learnable gradient decent depth-two network. On other hand, same cannot efficiently by"
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate\n  Tool for Ligand Strain Calculations",
        "Towards Fair and Robust Face Parsing for Generative AI: A\n  Multi-Objective Approach",
        "On the generalized eigenvalue problem in subspace-based excited state\n  methods for quantum computers",
        "Measuring Similarity in Causal Graphs: A Framework for Semantic and\n  Structural Analysis",
        "Hyperparameters in Score-Based Membership Inference Attacks",
        "Tensor renormalization group study of the two-dimensional lattice U(1)\n  gauge-Higgs model with a topological $\\theta$ term under L\\\"uscher's\n  admissibility condition",
        "Nonflat bands and chiral symmetry in magic-angle twisted bilayer\n  graphene",
        "Effect of imaginary gauge on wave transport in driven-dissipative\n  systems",
        "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
        "Explainable Reinforcement Learning via Temporal Policy Decomposition",
        "Towards Change Impact Analysis in Microservices-based System Evolution",
        "RIO EPICS device support application case study on an ion source control\n  system (ISHP)",
        "Migration of phthalate plasticisers in heritage objects made of\n  poly(vinyl chloride): mechanical and environmental aspects",
        "Near-extremal dumb holes and some aspects of the Hawking effect",
        "Can Safety Fine-Tuning Be More Principled? Lessons Learned from\n  Cybersecurity",
        "Fusion of Indirect Methods and Iterative Learning for Persistent\n  Velocity Trajectory Optimization of a Sustainably Powered Autonomous Surface\n  Vessel",
        "Removing Atmospheric Carbon Dioxide Using Large Land Or Ocean Areas Will\n  Change Earth Albedo And Force Climate",
        "Efficient Quantum Frequency Conversion of Ultra-Violet Single Photons\n  from a Trapped Ytterbium Ion",
        "Construction of Simultaneously Good Polar Codes and Polar Lattices",
        "Convexification With the Viscocity Term for Electrical Impedance\n  Tomography",
        "HALoGEN: Fantastic LLM Hallucinations and Where to Find Them",
        "Transfer Learning for EDFA Gain Modeling: A Semi-Supervised Approach\n  Using Internal Amplifier Features",
        "Rutherford Backscattering Spectrometry analysis of the formation of\n  superconducting V$_3$Si thin films",
        "Large Language Model-based Nonnegative Matrix Factorization For\n  Cardiorespiratory Sound Separation",
        "Foundation Models for CPS-IoT: Opportunities and Challenges",
        "Federated Digital Twin Construction via Distributed Sensing: A\n  Game-Theoretic Online Optimization with Overlapping Coalitions",
        "Enhanced Beampattern Synthesis Using Electromagnetically Reconfigurable\n  Antennas",
        "NAS-PINNv2: Improved neural architecture search framework for\n  physics-informed neural networks in low-temperature plasma simulation",
        "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
      ],
      "abstract":[
        "Ligand strain energy, the energy difference between the bound and unbound\nconformations of a ligand, is an important component of structure-based small\nmolecule drug design. A large majority of observed ligands in protein-small\nmolecule co-crystal structures bind in low-strain conformations, making strain\nenergy a useful filter for structure-based drug design. In this work we present\na tool for calculating ligand strain with a high accuracy. StrainRelief uses a\nMACE Neural Network Potential (NNP), trained on a large database of Density\nFunctional Theory (DFT) calculations to estimate ligand strain of neutral\nmolecules with quantum accuracy. We show that this tool estimates strain energy\ndifferences relative to DFT to within 1.4 kcal\/mol, more accurately than\nalternative NNPs. These results highlight the utility of NNPs in drug\ndiscovery, and provide a useful tool for drug discovery teams.",
        "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
        "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
        "Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.",
        "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA.",
        "We investigate the two-dimensional lattice U(1) gauge-Higgs model with a\ntopological term, employing L\\\"uscher's admissibility condition. The standard\nMonte Carlo simulation for this model is hindered not only by the complex\naction problem due to the topological term but also by the topological freezing\nproblem originating from the admissibility condition. Resolving both obstacles\nsimultaneously with the tensor renormalization group approach, we show the\nadvantage of the admissibility condition in dealing with the topological term\ndiscretized with the so-called field-theoretical definition.",
        "In this work, we study an interacting tight-binding model of magic-angle\ntwisted bilayer graphene (MATBG), with a twist angle of $1.05^\\circ$. We derive\neffective theories based on a mean-field normal state at charge neutrality,\nthereby including the renormalizations coming from integrating out high-energy\nmodes. In these theories, the flat bands display a sizable increase of the\nbandwidth, suggesting a renormalization of the magic angle. Additionally, the\ncorresponding wavefunctions flow towards the limit of perfect particle-hole\nsymmetry and sublattice polarization (the 'chiral' limit). We further represent\nthe flat bands in the 'vortex Chern' basis and discuss the implications on the\ndynamics regarding the 'flat' and 'chiral' symmetries of MATBG, as manifested\nin the symmetry-broken states at neutrality.",
        "Wave transport in disordered media is a fundamental problem with direct\nimplications in condensed matter, materials science, optics, atomic physics,\nand even biology. The majority of studies are focused on Hermitian systems to\nunderstand disorder-induced localization. However, recent studies of\nnon-Hermitian disordered media have revealed unique behaviors, with a universal\nprinciple emerging that links the eigenvalue spectrum of the disordered\nHamiltonian and its statistics with its transport properties. In this work we\nshow that the situation can be very different in driven-dissipative lattices of\ncavities, where a uniform gain applied equally to all the components of the\nsystem can act as a knob for controlling the wave transport properties without\naltering the eigenvalue statistics of the underlying Hamiltonian. Our results\nopen a new avenue for developing a deeper insight into the transport properties\nin disordered media and will aid in building new devices as well. Our work\nwhich is presented in the context of optics generalizes to any physical\nplatforms where gain can be implemented. These include acoustics, electronics,\nand coupled quantum oscillators such as atoms, diamond centers and\nsuperconducting qubits.",
        "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
        "We investigate the explainability of Reinforcement Learning (RL) policies\nfrom a temporal perspective, focusing on the sequence of future outcomes\nassociated with individual actions. In RL, value functions compress information\nabout rewards collected across multiple trajectories and over an infinite\nhorizon, allowing a compact form of knowledge representation. However, this\ncompression obscures the temporal details inherent in sequential\ndecision-making, presenting a key challenge for interpretability. We present\nTemporal Policy Decomposition (TPD), a novel explainability approach that\nexplains individual RL actions in terms of their Expected Future Outcome (EFO).\nThese explanations decompose generalized value functions into a sequence of\nEFOs, one for each time step up to a prediction horizon of interest, revealing\ninsights into when specific outcomes are expected to occur. We leverage\nfixed-horizon temporal difference learning to devise an off-policy method for\nlearning EFOs for both optimal and suboptimal actions, enabling contrastive\nexplanations consisting of EFOs for different state-action pairs. Our\nexperiments demonstrate that TPD generates accurate explanations that (i)\nclarify the policy's future strategy and anticipated trajectory for a given\naction and (ii) improve understanding of the reward composition, facilitating\nfine-tuning of the reward function to align with human expectations.",
        "Cloud-native systems are the mainstream for enterprise solutions, given their\nscalability, resilience, and other benefits. While the benefits of cloud-native\nsystems fueled by microservices are known, less guidance exists on their\nevolution. One could assume that since microservices encapsulate their code,\ncode changes remain encapsulated as well; however, the community is becoming\nmore aware of the possible consequences of code change propagation across\nmicroservices. Moreover, an active mitigation instrument for negative\nconsequences of change propagation across microservices (i.e., ripple effect)\nis yet missing, but the microservice community would greatly benefit from it.\nThis paper introduces what it could look like to have an infrastructure to\nassist with change impact analysis across the entire microservice system and\nintends to facilitate advancements in laying out the foundations and building\nguidelines on microservice system evolution. It shares a new direction for\nincremental software architecture reconstruction that could serve as the\ninfrastructure concept and demonstrates early results from prototyping to\nillustrate the potential impact.",
        "Experimental Physics and Industrial Control System (EPICS) is a software tool\nthat during last years has become relevant as a main framework to deploy\ndistributed control systems in large scientific environments. At the moment,\nESS Bilbao uses this middleware to perform the control of their Ion Source\nHydrogen Positive (ISHP) project. The implementation of the control system was\nbased on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS\ntools; and RIO devices based on Field-Programmable Gate Array (FPGA)\ntechnology. Intended to provide a full compliant EPICS IOCs for RIO devices and\nto avoid additional efforts on the system maintainability, a migration of the\ncurrent system to a derivative Red Hat Linux (CentOS) environment has been\nconducted. This paper presents a real application case study for using the\nNIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although\nRIO FPGA configurations are particular solutions for ISHP performance, the\nNIRIO-EDS has permitted the control and monitoring of devices by applying a\nwell-defined design methodology into the previous FPGA configuration for\nRIO\/FlexRIO devices. This methodology has permitted a fast and easy deployment\nfor the new robust, scalable and maintainable software to support RIO devices\ninto the ISHP control architecture.",
        "To clean or not to clean? The solution to this dilemma is related to\nunderstanding the plasticiser migration which has a few practical implications\nfor the state of museum artefacts made of plasticised poly(vinyl chloride) -\nPVC and objects stored in their vicinity. The consequences of this process\nencompass aesthetic changes due to the presence of exudates and dust\ndeposition, an increase in air pollution and the development of mechanical\nstresses. Therefore, this paper discusses the plasticiser migration in PVC to\nprovide evidence and support the development of recommendations and guidelines\nfor conservators, collection managers and heritage scientists. Particularly,\nthe investigation is focused on the migration of the ortho-phthalates\nrepresenting the group of the most abundant plasticisers in PVC collections.\nThe predominance of inner diffusion or surface emission (evaporation)\ndetermining the rate-limiting step of the overall migration process is\nconsidered a fundament for understanding the potential environmental and\nmechanical risk. According to this concept, general correlations for various\northo-phthalates are proposed depending on their molar mass with the support of\nmolecular dynamics simulations and NMR diffusometry. The study reveals that for\nthe majority of the PVC objects in collections, the risk of accelerated\nmigration upon mild removal of surface plasticiser exudate is low. Thus,\nsurface cleaning would allow for diminishing dust deposition and air pollution\nby phthalate-emitting objects in a museum environment. Bearing in mind\nsimplicity and the need for fast decision-supporting solutions, the\nstep-by-step protocol for non-destructive identification and quantification of\nplasticisers in objects made of or containing plasticised PVC, determination of\nthe physical state of investigated artefacts and rate-limiting process of\nplasticiser migration is proposed.",
        "We propose novel non-relativistic fluid analogue models, that is dumb hole\nmodels, for extremal and near-extremal black holes. Further we study the\nback-reaction effects of analogue Hawking radiation emitted from these dumb\nholes. We discuss and quantify the reduction in the background fluid velocity\ncaused by radiation of Hawking phonons. In doing so, we speculate on the\nexistence of an emergent Hawking force which leads to the reduction in the\nbackground fluid velocity and which is produced as a consequence of phonon\nemission. In addition to the analogue gravity literature, our results might be\nof relevance to black hole pedagogy.",
        "As LLMs develop increasingly advanced capabilities, there is an increased\nneed to minimize the harm that could be caused to society by certain model\noutputs; hence, most LLMs have safety guardrails added, for example via\nfine-tuning. In this paper, we argue the position that current safety\nfine-tuning is very similar to a traditional cat-and-mouse game (or arms race)\nbetween attackers and defenders in cybersecurity. Model jailbreaks and attacks\nare patched with bandaids to target the specific attack mechanism, but many\nsimilar attack vectors might remain. When defenders are not proactively coming\nup with principled mechanisms, it becomes very easy for attackers to sidestep\nany new defenses. We show how current defenses are insufficient to prevent new\nadversarial jailbreak attacks, reward hacking, and loss of control problems. In\norder to learn from past mistakes in cybersecurity, we draw analogies with\nhistorical examples and develop lessons learned that can be applied to LLM\nsafety. These arguments support the need for new and more principled approaches\nto designing safe models, which are architected for security from the\nbeginning. We describe several such approaches from the AI literature.",
        "In this paper, we present the methodology and results for a real-time\nvelocity trajectory optimization for a solar-powered autonomous surface vessel\n(ASV), where we combine indirect optimal control techniques with iterative\nlearning. The ASV exhibits cyclic operation due to the nature of the solar\nprofile, but weather patterns create inevitable disturbances in this profile.\nThe nature of the problem results in a formulation where the satisfaction of\npointwise-in-time state of charge constraints does not generally guarantee\npersistent feasibility, and the goal is to maximize information gathered over a\nvery long (ultimately persistent) time duration. To address these challenges,\nwe first use barrier functions to tighten pointwise-in-time state of charge\nconstraints by the minimal amount necessary to achieve persistent feasibility.\nWe then use indirect methods to derive a simple switching control law, where\nthe optimal velocity is shown to be an undetermined constant value during each\nconstraint-inactive time segment. To identify this optimal constant velocity\n(which can vary from one segment to the next), we employ an iterative learning\napproach. The result is a simple closed-form control law that does not require\na solar forecast. We present simulation-based validation results, based on a\nmodel of the SeaTrac SP-48 ASV and solar data from the North Carolina coast.\nThese simulation results show that the proposed methodology, which amounts to a\nclosed-form controller and simple iterative learning update law, performs\nnearly as well as a model predictive control approach that requires an accurate\nfuture solar forecast and significantly greater computational capability.",
        "When large surface areas of the Earth are altered, radiative forcing due to\nchanges in surface reflectance can drive climate change. Yet to achieve the\nnecessary scale to remove the substantial amounts of carbon dioxide from the\natmosphere relevant for ameliorating climate change, enhanced rock weathering\n(ERW) will need to be applied to very large land areas. Likewise, marine carbon\ndioxide removal (mCDR) must alter a large fraction of the ocean surface waters\nto have a significant impact upon climate. We show that surface albedo\nmodification (SAM) associated with ERW or mCDR can easily overwhelm the\nradiative forcing from the decrease of atmospheric CO2 over years or even\ndecades. A change in albedo as small as parts per thousand has a radiative\nimpact comparable to the removal of 10 tons of carbon per hectare. SAM via ERW\ncan be either cooling or warming. We identify some of the many questions raised\nby radiative forcing due to these forms of CDR.",
        "Ion trap system is a leading candidate for quantum network privileged by its\nlong coherence time, high-fidelity gate operations, and the ion-photon\nentanglement that generates an ideal pair of a stationary memory qubit and a\nflying communication qubit. Rapid developments in nonlinear quantum frequency\nconversion techniques have enhanced the potential for constructing a trapped\nion quantum network via optical fiber connections. The generation of\nlong-distance entanglement has been demonstrated with ions such as Ca$^{+}$ and\nBa$^{+}$, which emit photons in visible or near-infrared range naturally. On\nthe other hand, as the qubit-native photons reside in ultra-violet (UV)\nspectrum, the Yb$^{+}$ ion has not been considered as a strong competitor for\ntelecommunication qubits despite extensive research on it. Here, we demonstrate\nan efficient difference-frequency conversion of UV photons, emitted from a\ntrapped Yb$^{+}$ ion, into a visible range. We provide experimental evidence\nthat confirms the converted photons are radiated from the Yb$^{+}$ ion. Our\nresults provide a crucial step toward realizing a long-distance trapped ion\nquantum network based on Yb$^{+}$ ions through quantum frequency conversion.",
        "In this work, we investigate the simultaneous goodness of polar codes and\npolar lattices. The simultaneous goodness of a lattice or a code means that it\nis optimal for both channel coding and source coding simultaneously. The\nexistence of such kind of lattices was proven by using random lattice\nensembles. Our work provides an explicit construction based on the polarization\ntechnique.",
        "A version of the globally convergent convexification numerical method is\nconstructed for the problem of Electrical Impedance Tomography in the 2D case.\nAn important element of this version is the presence of the viscosity term.\nGlobal convergence analysis is carried out. Results of numerical experiments\nare presented.",
        "Despite their impressive ability to generate high-quality and fluent text,\ngenerative large language models (LLMs) also produce hallucinations: statements\nthat are misaligned with established world knowledge or provided input context.\nHowever, measuring hallucination can be challenging, as having humans verify\nmodel generations on-the-fly is both expensive and time-consuming. In this\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\nof: (1) 10,923 prompts for generative models spanning nine domains including\nprogramming, scientific attribution, and summarization, and (2) automatic\nhigh-precision verifiers for each use case that decompose LLM generations into\natomic units, and verify each unit against a high-quality knowledge source. We\nuse this framework to evaluate ~150,000 generations from 14 language models,\nfinding that even the best-performing models are riddled with hallucinations\n(sometimes up to 86% of generated atomic facts depending on the domain). We\nfurther define a novel error classification for LLM hallucinations based on\nwhether they likely stem from incorrect recollection of training data (Type A\nerrors), or incorrect knowledge in training data (Type B errors), or are\nfabrication (Type C errors). We hope our framework provides a foundation to\nenable the principled study of why generative models hallucinate, and advances\nthe development of trustworthy large language models.",
        "The gain spectrum of an Erbium-Doped Fiber Amplifier (EDFA) has a complex\ndependence on channel loading, pump power, and operating mode, making accurate\nmodeling difficult to achieve. Machine Learning (ML) based modeling methods can\nachieve high accuracy, but they require comprehensive data collection. We\npresent a novel ML-based Semi-Supervised, Self-Normalizing Neural Network\n(SS-NN) framework to model the wavelength dependent gain of EDFAs using minimal\ndata, which achieve a Mean Absolute Error (MAE) of 0.07\/0.08 dB for\nbooster\/pre-amplifier gain prediction. We further perform Transfer Learning\n(TL) using a single additional measurement per target-gain setting to transfer\nthis model among 22 EDFAs in Open Ireland and COSMOS testbeds, which achieves a\nMAE of less than 0.19 dB even when operated across different amplifier types.\nWe show that the SS-NN model achieves high accuracy for gain spectrum\nprediction with minimal data requirement when compared with current benchmark\nmethods.",
        "Vanadium silicide, V$_3$Si, is a promising superconductor for silicon-based\nsuperconducting (SC) devices due to its compatibility with silicon substrates\nand its potential for integration into existing semiconductor technologies.\nHowever, to date there have been only a limited number of studies of the\nformation of SC V$_3$Si thin films and the associated structural and\nsuperconducting properties. This work aims to explore the structural\ncharacteristics and SC properties of V$_3$Si films, paving the way for the\ndevelopment of functional SC devices for quantum technology applications. We\nhave investigated the formation of V$_3$Si films by directly depositing\nvanadium (V) onto thermally grown SiO$_2$ on Si, followed by high-vacuum\nannealing to induce the phase transformation into V$_3$Si. Rutherford\nBackscattering Spectrometry(RBS) was employed throughout the sample growth\nprocess to analyze the material composition as a function of depth using a\n$^4He^+$ ion beam. Analysis of the RBS data confirmed that the V layer fully\nreacted with the SiO$_2$ substrate to form V$_3$Si at the interface, in\naddition to a vanadium oxide (VO$_x$) layer forming atop the V$_3$Si film. The\nthickness of the V$_3$Si layer ranges from 63 to 130 nm, with annealing\ntemperatures between 750$^\\circ$C and 800$^\\circ$C. A sharp SC transition was\nobserved at T$_c$ = 13 K in the sample annealed at 750$^\\circ$C, with a narrow\ntransition width ($\\Delta T_c$) of 0.6 K. Initial reactive ion etching (RIE)\nstudies yielded promising results for local removal of the (VO$_x$) to\nfacilitate electrical contact formation to the SC layer.",
        "This study represents the first integration of large language models (LLMs)\nwith non-negative matrix factorization (NMF), marking a novel advancement in\nthe source separation field. The LLM is employed in two unique ways: enhancing\nthe separation results by providing detailed insights for disease prediction\nand operating in a feedback loop to optimize a fundamental frequency penalty\nadded to the NMF cost function. We tested the algorithm on two datasets: 100\nsynthesized mixtures of real measurements, and 210 recordings of heart and lung\nsounds from a clinical manikin including both individual and mixed sounds,\ncaptured using a digital stethoscope. The approach consistently outperformed\nexisting methods, demonstrating its potential to significantly enhance medical\nsound analysis for disease diagnostics.",
        "Methods from machine learning (ML) have transformed the implementation of\nPerception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS)\nand the Internet of Things (IoT), replacing mechanistic and basic statistical\nmodels with those derived from data. However, the first generation of ML\napproaches, which depend on supervised learning with annotated data to create\ntask-specific models, faces significant limitations in scaling to the diverse\nsensor modalities, deployment configurations, application tasks, and operating\ndynamics characterizing real-world CPS-IoT systems. The success of\ntask-agnostic foundation models (FMs), including multimodal large language\nmodels (LLMs), in addressing similar challenges across natural language,\ncomputer vision, and human speech has generated considerable enthusiasm for and\nexploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics\npipelines, promising to reduce the need for costly task-specific engineering.\n  Nonetheless, a significant gap persists between the current capabilities of\nFMs and LLMs in the CPS-IoT domain and the requirements they must meet to be\nviable for CPS-IoT applications. In this paper, we analyze and characterize\nthis gap through a thorough examination of the state of the art and our\nresearch, which extends beyond it in various dimensions. Based on the results\nof our analysis and research, we identify essential desiderata that CPS-IoT\ndomain-specific FMs and LLMs must satisfy to bridge this gap. We also propose\nactions by CPS-IoT researchers to collaborate in developing key community\nresources necessary for establishing FMs and LLMs as foundational tools for the\nnext generation of CPS-IoT systems.",
        "In this paper, we propose a novel federated framework for constructing the\ndigital twin (DT) model, referring to a living and self-evolving visualization\nmodel empowered by artificial intelligence, enabled by distributed sensing\nunder edge-cloud collaboration. In this framework, the DT model to be built at\nthe cloud is regarded as a global one being split into and integrating from\nmultiple functional components, i.e., partial-DTs, created at various edge\nservers (ESs) using feature data collected by associated sensors. Considering\ntime-varying DT evolutions and heterogeneities among partial-DTs, we formulate\nan online problem that jointly and dynamically optimizes partial-DT assignments\nfrom the cloud to ESs, ES-sensor associations for partial-DT creation, and as\nwell as computation and communication resource allocations for global-DT\nintegration. The problem aims to maximize the constructed DT's model quality\nwhile minimizing all induced costs, including energy consumption and\nconfiguration costs, in long runs. To this end, we first transform the original\nproblem into an equivalent hierarchical game with an upper-layer two-sided\nmatching game and a lower-layer overlapping coalition formation game. After\nanalyzing these games in detail, we apply the Gale-Shapley algorithm and\nparticularly develop a switch rules-based overlapping coalition formation\nalgorithm to obtain short-term equilibria of upper-layer and lower-layer\nsubgames, respectively. Then, we design a deep reinforcement learning-based\nsolution, called DMO, to extend the result into a long-term equilibrium of the\nhierarchical game, thereby producing the solution to the original problem.\nSimulations show the effectiveness of the introduced framework, and demonstrate\nthe superiority of the proposed solution over counterparts.",
        "Beampattern synthesis seeks to optimize array weights to shape radiation\npatterns, playing a critical role in various wireless applications. In addition\nto theoretical advancements, recent hardware innovations have facilitated new\navenues to enhance beampattern synthesis performance. This paper studies the\nbeampattern synthesis problem using newly proposed electromagnetically\nreconfigurable antennas (ERAs). By utilizing spherical harmonics decomposition,\nwe simultaneously optimize each antenna's radiation pattern and phase shift to\nmatch a desired beampattern of the entire array. The problem is formulated for\nboth far-field and near-field scenarios, with the optimization solved using\nRiemannian manifold techniques. The simulation results validate the\neffectiveness of the proposed solution and illustrate that ERAs exhibit\nsuperior beampattern synthesis capabilities compared to conventional fixed\nradiation pattern antennas. This advantage becomes increasingly significant as\nthe array size grows.",
        "Limited by the operation and measurement conditions, numerical simulation is\noften the only feasible approach for studying plasma behavior and mechanisms.\nAlthough artificial intelligence methods, especially physics-informed neural\nnetwork (PINN), have been widely applied in plasma simulation, the design of\nthe neural network structures still largely relies on the experience of\nresearchers. Meanwhile, existing neural architecture search methods tailored\nfor PINN have encountered failures when dealing with complex plasma governing\nequations characterized by variable coefficients and strong nonlinearity.\nTherefore, we propose an improved neural architecture search-guided method,\nnamely NAS-PINNv2, to address the limitations of existing methods. By analyzing\nthe causes of failure, the sigmoid function is applied to calculate the\narchitecture-related weights, and a new loss term is introduced. The\nperformance of NAS-PINNv2 is verified in several numerical experiments\nincluding the Elenbaas-Heller equation without and with radial velocity, the\ndrift-diffusion-Poisson equation and the Boltzmann equation. The results again\nemphasize that larger neural networks do not necessarily perform better, and\nthe discovered neural architecture with multiple neuron numbers in a single\nhidden layer imply a more flexible and sophisticated design rule for fully\nconnected networks.",
        "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https:\/\/github.com\/DD-DuDa\/BitDecoding."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cond-mat.dis-nn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning"
      ],
      "abstract":[
        "Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection",
        "The effect of minimum wages on employment in the presence of\n  productivity fluctuations",
        "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "An Analytical Study of the Min-Sum Approximation for Polar Codes",
        "Modulo arithmetic of function spaces: Subset hyperspaces as quotients of\n  function spaces",
        "Finding all solutions of qKZ equations in characteristic $p$",
        "A characterization of Generalized functions of Bounded Deformation",
        "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical\n  Deformable Scene Reconstruction",
        "Recommendations to OSCE\/ODIHR (on how to give better recommendations for\n  Internet voting)",
        "On the Khovanov homology of 3-braids",
        "Concerns and Values in Human-Robot Interactions: A Focus on Social\n  Robotics",
        "Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation",
        "The Reconstruction of Theaetetus' Theory of Ratios of Magnitudes",
        "How Can Video Generative AI Transform K-12 Education? Examining\n  Teachers' Perspectives through TPACK and TAM",
        "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment"
      ],
      "abstract":[
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies.",
        "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
        "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https:\/\/mm-rlhf.github.io.",
        "The min-sum approximation is widely used in the decoding of polar codes.\nAlthough it is a numerical approximation, hardly any penalties are incurred in\npractice. We give a theoretical justification for this. We consider the common\ncase of a binary-input, memoryless, and symmetric channel, decoded using\nsuccessive cancellation and the min-sum approximation. Under mild assumptions,\nwe show the following. For the finite length case, we show how to exactly\ncalculate the error probabilities of all synthetic (bit) channels in time\n$O(N^{1.585})$, where $N$ is the codeword length. This implies a code\nconstruction algorithm with the above complexity. For the asymptotic case, we\ndevelop two rate thresholds, denoted $R_{\\mathrm{L}} = R_{\\mathrm{L}}(\\lambda)$\nand $R_{\\mathrm{U}} =R_{\\mathrm{U}}(\\lambda)$, where $\\lambda(\\cdot)$ is the\nlabeler of the channel outputs (essentially, a quantizer). For any $0 < \\beta <\n\\frac{1}{2}$ and any code rate $R < R_{\\mathrm{L}}$, there exists a family of\npolar codes with growing lengths such that their rates are at least $R$ and\ntheir error probabilities are at most $2^{-N^\\beta}$. That is, strong\npolarization continues to hold under the min-sum approximation. Conversely, for\ncode rates exceeding $R_{\\mathrm{U}}$, the error probability approaches $1$ as\nthe code-length increases, irrespective of which bits are frozen. We show that\n$0 < R_{\\mathrm{L}} \\leq R_{\\mathrm{U}} \\leq C$, where $C$ is the channel\ncapacity. The last inequality is often strict, in which case the ramification\nof using the min-sum approximation is that we can no longer achieve capacity.",
        "Let $X$ be a (topological) space and $Cl(X)$ the collection of nonempty\nclosed subsets of $X$. Given a topology on $Cl(X)$, making $Cl(X)$ a space, a\n\\emph{(subset) hyperspace} of $X$ is a subspace $\\mathcal{J}\\subset Cl(X)$ with\nan embedding $X\\hookrightarrow\\mathcal{J}$, $x\\mapsto\\{x\\}$ (which thus\nrequires $X$ to be $T_1$). In this note, we characterize certain hyperspaces\n$\\mathcal{J}\\subset Cl(X)$ as explicit quotient spaces of function spaces\n$\\mathcal{F}\\subset X^Y$ and discuss metrization of associated compact-subset\nhyperspaces in this setting.",
        "In [MV] the difference qKZ equations were considered modulo a prime number\n$p$ and a family of polynomial solutions of the qKZ equations modulo $p$ was\nconstructed by an elementary procedure as suitable $p$-approximations of the\nhypergeometric integrals. In this paper, we study in detail the first family of\nnontrivial example of the qKZ equations in characteristic $p$. We describe all\nsolutions of these qKZ equations in characteristic $p$ by demonstrating that\nthey all stem from the $p$-hypergeometric solutions. We also prove a Lagrangian\nproperty (called the orthogonality property) of the subbundle of the qKZ bundle\nspanned by the $p$-hypergeometric sections. This paper extends the results of\n[VV1] on the differential KZ equations to the difference qKZ equations.",
        "We show that Dal Maso's GBD space, introduced for tackling crack growth in\nlinearized elasticity, can be defined by simple conditions in a finite number\nof directions of slicing.",
        "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
        "This paper takes a critical look at the recommendations OSCE\/ODIHR has given\nfor the Estonian Internet voting over the 20 years it has been running. We\npresent examples of recommendations that can not be fulfilled at all, but also\nexamples where fulfilling a recommendation requires a non-trivial trade-off,\npotentially weakening the system in some other respect. In such cases\nOSCE\/ODIHR should take an explicit position which trade-off it recommends. We\nalso look at the development of the recommendation to introduce end-to-end\nverifiability. In this case we expect OSCE\/ODIHR to define what it exactly\nmeans by this property, as well as to give explicit criteria to determine\nwhether and to which extent end-to-end verifiability has been achieved.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "Robots, as AI with physical instantiation, inhabit our social and physical\nworld, where their actions have both social and physical consequences, posing\nchallenges for researchers when designing social robots. This study starts with\na scoping review to identify discussions and potential concerns arising from\ninteractions with robotic systems. Two focus groups of technology ethics\nexperts then validated a comprehensive list of key topics and values in\nhuman-robot interaction (HRI) literature. These insights were integrated into\nthe HRI Value Compass web tool, to help HRI researchers identify ethical values\nin robot design. The tool was evaluated in a pilot study. This work benefits\nthe HRI community by highlighting key concerns in human-robot interactions and\nproviding an instrument to help researchers design robots that align with human\nvalues, ensuring future robotic systems adhere to these values in social\napplications.",
        "In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.",
        "In the present chapter, we obtain the reconstruction of Theaetetus' theory of\nratios of magnitudes based, according to Aristotle's Topics 158b, on the\ndefinition of proportion in terms of equal anthyphairesis. Our reconstruction\nis built on the anthyphairetic interpretation of the notoriously difficult\nTheaetetus 147d6-e1 passage on Theaetetus' mathematical discovery of quadratic\nincommensurabilities, itself based on the traces it has left on Plato's\nphilosophical definition of Knowledge in his dialogues Theaetetus, Sophist and\nMeno. Contrary to earlier reconstructions by Becker, van der Waerden and Knorr,\nour reconstruction reveals a theory that (a) applies only to the restricted\nclass of pairs of magnitudes whose anthyphairesis is finite or eventually\nperiodic, and (b) avoids the problematic use of Eudoxus' definition 4 of Book V\nof Euclid's Elements.\n  The final version of this paper will appear as a chapter in the book Essays\non Topology: Dedicated to Valentin Po\\'enaru, ed. L. Funar and A. Papadopoulos,\nSpringer, 2025.",
        "The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.",
        "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https:\/\/agav-rater.github.io."
      ]
    }
  },
  {
    "id":2412.00544,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Space objects classification via lightcurve measurements: deep convolutional neural networks and model-based transfer learning",
    "start_abstract":"Developing a detailed understanding of the Space Object (SO) population is a fundamental goal of Space Situational Awareness (SSA). The current SO catalog includes simplified characteristic for the observed space objects, mainly the solar radiation pressure and\/or drag ballistic coefficients. Such simplified description limits the dynamic propagation model used for predicting the state of motion of SO to models that assume cannon ball shapes and generic surface properties. The future SO catalog and SSA systems will have to be capable of building a detailed picture of SO characteristics. Traditional measurement sources for SO tracking, such as radar and optical, provide information on SO characteristics. These measurements have been shown to be sensitive to shape, attitude, angular velocity, and surface parameters. State-of-the-art in the literature has been advanced over the past decades and in recent years seen the development of multiple models, nonlinear state estimation, and full Bayesian inversion approaches for SO characterization. The key shortcoming of approaches in literature is their overall computational cost and the limited flexibility to deal with a larger and larger amount of data. In this paper, we present a data-driven method to classification of SO based on a deep learning approach that takes advantage of the representational power of deep neural networks. Here, we design, train and validate a Convolutional Neural Network (CNN) capable of learning to classify SOs from collected light-curve measurements. The proposed methodology relies a physically-based model capable of accurately representing SO reflected light as function of time, size shape and state of motion. The model generates thousands of light-curves per selected class of SO which are employ to train a deep CNN to learn the functional relationship between light curves and SO class. Additionally, a deep CNN is trained using real SO light curves to evaluate the performance on a real, but limited training set. CNNs are compared with more conventional machine learning techniques (bagged trees, support vector machines) and are shown to outperform such methods especially when trained on real data. The concept of model-based transfer learning is proposed as possible path forward to increase the accuracy and speedup the training process.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cond-mat.dis-nn"
      ]
    },
    "list":{
      "title":[
        "Optimizing Model Selection for Compound AI Systems",
        "Leveraging Intermediate Representations for Better Out-of-Distribution\n  Detection",
        "Generative Modeling for Mathematical Discovery",
        "Binary Diffusion Probabilistic Model",
        "Identification of non-causal systems with random switching modes\n  (Extended Version)",
        "Convergence rates for the vanishing viscosity approximation of\n  Hamilton-Jacobi equations: the convex case",
        "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views",
        "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an\n  Extreme Value Theory Approach",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "ATLaS: Agent Tuning via Learning Critical Steps",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Asymptotic lengths of permutahedra and associahedra",
        "Cross-Domain Continual Learning for Edge Intelligence in Wireless ISAC\n  Networks",
        "Slamming: Training a Speech Language Model on One GPU in a Day",
        "Deep Change Monitoring: A Hyperbolic Representative Learning Framework\n  and a Dataset for Long-term Fine-grained Tree Change Detection",
        "The effect of minimum wages on employment in the presence of\n  productivity fluctuations",
        "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "An Analytical Study of the Min-Sum Approximation for Polar Codes",
        "Modulo arithmetic of function spaces: Subset hyperspaces as quotients of\n  function spaces",
        "Finding all solutions of qKZ equations in characteristic $p$",
        "A characterization of Generalized functions of Bounded Deformation",
        "Feature-EndoGaussian: Feature Distilled Gaussian Splatting in Surgical\n  Deformable Scene Reconstruction",
        "Recommendations to OSCE\/ODIHR (on how to give better recommendations for\n  Internet voting)",
        "On the Khovanov homology of 3-braids",
        "Concerns and Values in Human-Robot Interactions: A Focus on Social\n  Robotics",
        "Improving Scientific Document Retrieval with Concept Coverage-based\n  Query Set Generation",
        "The Reconstruction of Theaetetus' Theory of Ratios of Magnitudes",
        "How Can Video Generative AI Transform K-12 Education? Examining\n  Teachers' Perspectives through TPACK and TAM",
        "AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment"
      ],
      "abstract":[
        "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "In real-world applications, machine learning models must reliably detect\nOut-of-Distribution (OoD) samples to prevent unsafe decisions. Current OoD\ndetection methods often rely on analyzing the logits or the embeddings of the\npenultimate layer of a neural network. However, little work has been conducted\non the exploitation of the rich information encoded in intermediate layers. To\naddress this, we analyze the discriminative power of intermediate layers and\nshow that they can positively be used for OoD detection. Therefore, we propose\nto regularize intermediate layers with an energy-based contrastive loss, and by\ngrouping multiple layers in a single aggregated response. We demonstrate that\nintermediate layer activations improves OoD detection performance by running a\ncomprehensive evaluation across multiple datasets.",
        "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
        "We introduce the Binary Diffusion Probabilistic Model (BDPM), a novel\ngenerative model optimized for binary data representations. While denoising\ndiffusion probabilistic models (DDPMs) have demonstrated notable success in\ntasks like image synthesis and restoration, traditional DDPMs rely on\ncontinuous data representations and mean squared error (MSE) loss for training,\napplying Gaussian noise models that may not be optimal for discrete or binary\ndata structures. BDPM addresses this by decomposing images into bitplanes and\nemploying XOR-based noise transformations, with a denoising model trained using\nbinary cross-entropy loss. This approach enables precise noise control and\ncomputationally efficient inference, significantly lowering computational costs\nand improving model convergence. When evaluated on image restoration tasks such\nas image super-resolution, inpainting, and blind image restoration, BDPM\noutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQ\ndatasets. Notably, BDPM requires fewer inference steps than traditional DDPM\nmodels to reach optimal results, showcasing enhanced inference efficiency.",
        "We consider the identification of non-causal systems with random switching\nmodes (NCSRSM), a class of models essential for describing typical power load\nmanagement and department store inventory dynamics. The simultaneous\nidentification of causal-andanticausal subsystems, along with the presence of\nrandom switching sequences, however, make the overall identification problem\nparticularly challenging. To this end, we develop an expectation-maximization\n(EM) based system identification technique, where the E-step proposes a\nmodified Kalman filter (KF) to estimate the states and switching sequences of\ncausal-and-anticausal subsystems, while the M-step consists in a switching\nleast-squares algorithm to estimate the parameters of individual subsystems. We\nestablish the main convergence features of the proposed identification\nprocedure, also providing bounds on the parameter estimation errors under mild\nconditions. Finally, the effectiveness of our identification method is\nvalidated through two numerical simulations.",
        "We study the speed of convergence in $L^\\infty$ norm of the vanishing\nviscosity process for Hamilton-Jacobi equations with uniformly or strictly\nconvex Hamiltonian terms with superquadratic behavior. Our analysis boosts\nprevious findings on the rate of convergence for this procedure in $L^p$ norms,\nshowing rates in sup-norm of order $\\mathcal{O}(\\epsilon^\\beta)$,\n$\\beta\\in(1\/2,1)$, or $\\mathcal{O}(\\epsilon|\\log\\epsilon|)$ with respect to the\nvanishing viscosity parameter $\\epsilon$, depending on the regularity of the\ninitial datum of the problem and convexity properties of the Hamiltonian. Our\nproofs are based on integral methods and avoid the use of techniques based on\nstochastic control or the maximum principle.",
        "We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https:\/\/zhanghe3z.github.io\/FLARE\/",
        "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "We define asymptotic lengths for families of oriented polytopes. We show that\npermutahedra with weak order orientations have asymptotic total length 1 and\nassociahedra with Tamari order orientations have asymptotic total length 1\/2.",
        "In wireless networks with integrated sensing and communications (ISAC), edge\nintelligence (EI) is expected to be developed at edge devices (ED) for sensing\nuser activities based on channel state information (CSI). However, due to the\nCSI being highly specific to users' characteristics, the CSI-activity\nrelationship is notoriously domain dependent, essentially demanding EI to learn\nsufficient datasets from various domains in order to gain cross-domain sensing\ncapability. This poses a crucial challenge owing to the EDs' limited resources,\nfor which storing datasets across all domains will be a significant burden. In\nthis paper, we propose the EdgeCL framework, enabling the EI to continually\nlearn-then-discard each incoming dataset, while remaining resilient to\ncatastrophic forgetting. We design a transformer-based discriminator for\nhandling sequences of noisy and nonequispaced CSI samples. Besides, we propose\na distilled core-set based knowledge retention method with robustness-enhanced\noptimization to train the discriminator, preserving its performance for\nprevious domains while preventing future forgetting. Experimental evaluations\nshow that EdgeCL achieves 89% of performance compared to cumulative training\nwhile consuming only 3% of its memory, mitigating forgetting by 79%.",
        "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps:\/\/pages.cs.huji.ac.il\/adiyoss-lab\/slamming .",
        "In environmental protection, tree monitoring plays an essential role in\nmaintaining and improving ecosystem health. However, precise monitoring is\nchallenging because existing datasets fail to capture continuous fine-grained\nchanges in trees due to low-resolution images and high acquisition costs. In\nthis paper, we introduce UAVTC, a large-scale, long-term, high-resolution\ndataset collected using UAVs equipped with cameras, specifically designed to\ndetect individual Tree Changes (TCs). UAVTC includes rich annotations and\nstatistics based on biological knowledge, offering a fine-grained view for tree\nmonitoring. To address environmental influences and effectively model the\nhierarchical diversity of physiological TCs, we propose a novel Hyperbolic\nSiamese Network (HSN) for TC detection, enabling compact and hierarchical\nrepresentations of dynamic tree changes.\n  Extensive experiments show that HSN can effectively capture complex\nhierarchical changes and provide a robust solution for fine-grained TC\ndetection. In addition, HSN generalizes well to cross-domain face anti-spoofing\ntask, highlighting its broader significance in AI. We believe our work,\ncombining ecological insights and interdisciplinary expertise, will benefit the\ncommunity by offering a new benchmark and innovative AI technologies.",
        "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
        "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https:\/\/mm-rlhf.github.io.",
        "The min-sum approximation is widely used in the decoding of polar codes.\nAlthough it is a numerical approximation, hardly any penalties are incurred in\npractice. We give a theoretical justification for this. We consider the common\ncase of a binary-input, memoryless, and symmetric channel, decoded using\nsuccessive cancellation and the min-sum approximation. Under mild assumptions,\nwe show the following. For the finite length case, we show how to exactly\ncalculate the error probabilities of all synthetic (bit) channels in time\n$O(N^{1.585})$, where $N$ is the codeword length. This implies a code\nconstruction algorithm with the above complexity. For the asymptotic case, we\ndevelop two rate thresholds, denoted $R_{\\mathrm{L}} = R_{\\mathrm{L}}(\\lambda)$\nand $R_{\\mathrm{U}} =R_{\\mathrm{U}}(\\lambda)$, where $\\lambda(\\cdot)$ is the\nlabeler of the channel outputs (essentially, a quantizer). For any $0 < \\beta <\n\\frac{1}{2}$ and any code rate $R < R_{\\mathrm{L}}$, there exists a family of\npolar codes with growing lengths such that their rates are at least $R$ and\ntheir error probabilities are at most $2^{-N^\\beta}$. That is, strong\npolarization continues to hold under the min-sum approximation. Conversely, for\ncode rates exceeding $R_{\\mathrm{U}}$, the error probability approaches $1$ as\nthe code-length increases, irrespective of which bits are frozen. We show that\n$0 < R_{\\mathrm{L}} \\leq R_{\\mathrm{U}} \\leq C$, where $C$ is the channel\ncapacity. The last inequality is often strict, in which case the ramification\nof using the min-sum approximation is that we can no longer achieve capacity.",
        "Let $X$ be a (topological) space and $Cl(X)$ the collection of nonempty\nclosed subsets of $X$. Given a topology on $Cl(X)$, making $Cl(X)$ a space, a\n\\emph{(subset) hyperspace} of $X$ is a subspace $\\mathcal{J}\\subset Cl(X)$ with\nan embedding $X\\hookrightarrow\\mathcal{J}$, $x\\mapsto\\{x\\}$ (which thus\nrequires $X$ to be $T_1$). In this note, we characterize certain hyperspaces\n$\\mathcal{J}\\subset Cl(X)$ as explicit quotient spaces of function spaces\n$\\mathcal{F}\\subset X^Y$ and discuss metrization of associated compact-subset\nhyperspaces in this setting.",
        "In [MV] the difference qKZ equations were considered modulo a prime number\n$p$ and a family of polynomial solutions of the qKZ equations modulo $p$ was\nconstructed by an elementary procedure as suitable $p$-approximations of the\nhypergeometric integrals. In this paper, we study in detail the first family of\nnontrivial example of the qKZ equations in characteristic $p$. We describe all\nsolutions of these qKZ equations in characteristic $p$ by demonstrating that\nthey all stem from the $p$-hypergeometric solutions. We also prove a Lagrangian\nproperty (called the orthogonality property) of the subbundle of the qKZ bundle\nspanned by the $p$-hypergeometric sections. This paper extends the results of\n[VV1] on the differential KZ equations to the difference qKZ equations.",
        "We show that Dal Maso's GBD space, introduced for tackling crack growth in\nlinearized elasticity, can be defined by simple conditions in a finite number\nof directions of slicing.",
        "Minimally invasive surgery (MIS) has transformed clinical practice by\nreducing recovery times, minimizing complications, and enhancing precision.\nNonetheless, MIS inherently relies on indirect visualization and precise\ninstrument control, posing unique challenges. Recent advances in artificial\nintelligence have enabled real-time surgical scene understanding through\ntechniques such as image classification, object detection, and segmentation,\nwith scene reconstruction emerging as a key element for enhanced intraoperative\nguidance. Although neural radiance fields (NeRFs) have been explored for this\npurpose, their substantial data requirements and slow rendering inhibit\nreal-time performance. In contrast, 3D Gaussian Splatting (3DGS) offers a more\nefficient alternative, achieving state-of-the-art performance in dynamic\nsurgical scene reconstruction. In this work, we introduce Feature-EndoGaussian\n(FEG), an extension of 3DGS that integrates 2D segmentation cues into 3D\nrendering to enable real-time semantic and scene reconstruction. By leveraging\npretrained segmentation foundation models, FEG incorporates semantic feature\ndistillation within the Gaussian deformation framework, thereby enhancing both\nreconstruction fidelity and segmentation accuracy. On the EndoNeRF dataset, FEG\nachieves superior performance (SSIM of 0.97, PSNR of 39.08, and LPIPS of 0.03)\ncompared to leading methods. Additionally, on the EndoVis18 dataset, FEG\ndemonstrates competitive class-wise segmentation metrics while balancing model\nsize and real-time performance.",
        "This paper takes a critical look at the recommendations OSCE\/ODIHR has given\nfor the Estonian Internet voting over the 20 years it has been running. We\npresent examples of recommendations that can not be fulfilled at all, but also\nexamples where fulfilling a recommendation requires a non-trivial trade-off,\npotentially weakening the system in some other respect. In such cases\nOSCE\/ODIHR should take an explicit position which trade-off it recommends. We\nalso look at the development of the recommendation to introduce end-to-end\nverifiability. In this case we expect OSCE\/ODIHR to define what it exactly\nmeans by this property, as well as to give explicit criteria to determine\nwhether and to which extent end-to-end verifiability has been achieved.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "Robots, as AI with physical instantiation, inhabit our social and physical\nworld, where their actions have both social and physical consequences, posing\nchallenges for researchers when designing social robots. This study starts with\na scoping review to identify discussions and potential concerns arising from\ninteractions with robotic systems. Two focus groups of technology ethics\nexperts then validated a comprehensive list of key topics and values in\nhuman-robot interaction (HRI) literature. These insights were integrated into\nthe HRI Value Compass web tool, to help HRI researchers identify ethical values\nin robot design. The tool was evaluated in a pilot study. This work benefits\nthe HRI community by highlighting key concerns in human-robot interactions and\nproviding an instrument to help researchers design robots that align with human\nvalues, ensuring future robotic systems adhere to these values in social\napplications.",
        "In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.",
        "In the present chapter, we obtain the reconstruction of Theaetetus' theory of\nratios of magnitudes based, according to Aristotle's Topics 158b, on the\ndefinition of proportion in terms of equal anthyphairesis. Our reconstruction\nis built on the anthyphairetic interpretation of the notoriously difficult\nTheaetetus 147d6-e1 passage on Theaetetus' mathematical discovery of quadratic\nincommensurabilities, itself based on the traces it has left on Plato's\nphilosophical definition of Knowledge in his dialogues Theaetetus, Sophist and\nMeno. Contrary to earlier reconstructions by Becker, van der Waerden and Knorr,\nour reconstruction reveals a theory that (a) applies only to the restricted\nclass of pairs of magnitudes whose anthyphairesis is finite or eventually\nperiodic, and (b) avoids the problematic use of Eudoxus' definition 4 of Book V\nof Euclid's Elements.\n  The final version of this paper will appear as a chapter in the book Essays\non Topology: Dedicated to Valentin Po\\'enaru, ed. L. Funar and A. Papadopoulos,\nSpringer, 2025.",
        "The rapid advancement of generative AI technology, particularly video\ngenerative AI (Video GenAI), has opened new possibilities for K-12 education by\nenabling the creation of dynamic, customized, and high-quality visual content.\nDespite its potential, there is limited research on how this emerging\ntechnology can be effectively integrated into educational practices. This study\nexplores the perspectives of leading K-12 teachers on the educational\napplications of Video GenAI, using the TPACK (Technological Pedagogical Content\nKnowledge) and TAM (Technology Acceptance Model) frameworks as analytical\nlenses. Through interviews and hands-on experimentation with video generation\ntools, the research identifies opportunities for enhancing teaching strategies,\nfostering student engagement, and supporting authentic task design. It also\nhighlights challenges such as technical limitations, ethical considerations,\nand the need for institutional support. The findings provide actionable\ninsights into how Video GenAI can transform teaching and learning, offering\npractical implications for policy, teacher training, and the future development\nof educational technology.",
        "Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https:\/\/agav-rater.github.io."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b9",
    "start_title":"Kinetic description and convergence analysis of genetic algorithms for global optimization",
    "start_abstract":"Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Genetic Algorithms + Data Structures = Evolution Programs"
      ],
      "abstract":[
        "Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation."
      ],
      "categories":[
        "cs.DS"
      ]
    },
    "list":{
      "title":[
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "Bargaining with transfers",
        "Constraints on extended axion structures from the lensing of fast radio\n  bursts",
        "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
        "Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM",
        "MAIA: A new detector concept for a 10 TeV muon collider",
        "High-pressure growth effect on the properties of high-Tc iron-based\n  superconductors: A short review",
        "The Dead Internet Theory: A Survey on Artificial Interactions and the\n  Future of Social Media",
        "Talking to GDELT Through Knowledge Graphs",
        "The First Chemical Census the Milky Way's Nuclear Star Cluster",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "Drop size distribution from laboratory experiments based on single-drop\n  fragmentation and comparison with aerial in-situ measurements",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Temperature-Distance Relations in Casimir Physics",
        "Formation of Singularity and Apparent Horizon for Dissipative Collapse\n  in $f(R,T)$ Theory of Gravity",
        "Adapting Video Diffusion Models for Time-Lapse Microscopy",
        "Generalized black-bounces solutions in f(R) gravity and their field\n  sources",
        "The angular momentum spiral of the Milky Way disc in Gaia",
        "Near-Term Fermionic Simulation with Subspace Noise Tailored Quantum\n  Error Mitigation",
        "Extremal eigenvectors of sparse random matrices",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Shedding Infrared Light on QCD Axion and ALP Dark Matter with JWST",
        "Ancilla-free Quantum Adder with Sublinear Depth",
        "What Does a Software Engineer Look Like? Exploring Societal Stereotypes\n  in LLMs",
        "Lifelong Evolution of Swarms",
        "Metastable Cosmic Strings and Gravitational Waves from Flavour Symmetry\n  Breaking",
        "Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect\n  Controllable Text Generation with Large Language Models",
        "FedSDP: Explainable Differential Privacy in Federated Learning via\n  Shapley Values",
        "Decision Making in Changing Environments: Robustness, Query-Based\n  Learning, and Differential Privacy"
      ],
      "abstract":[
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
        "Axions are hypothetical pseudoscalar particles that have been regarded as\npromising dark matter (DM) candidates. On the other hand, extended compact\nobjects such as axion stars, which are supported by gravity and axion self\ninteractions, may have also been formed in the early Universe and comprise part\nof DM. In this work, we consider the lensing of electromagnetic signals from\ndistant sources by axion stars, as a way to constrain the properties of axion\nstars and fundamental axion parameters. Accounting for the effect of the finite\nsize of the axion star, we study the lensing effect induced by gravity and the\naxion-photon coupling. The latter effect is frequency dependent, and is\nrelevant in the low frequency band, which motivates the use of fast radio burst\n(FRB) signals as a probe. We calculate the predicted number of lensed FRB\nevents by specifying the fundamental axion parameters, axion star radial\nprofile, fraction of DM residing in axion stars, and imposing lensing criteria\nbased on the flux ratio and time delay between the brightest images from\nlensing. Assuming an optimistic case of $10^4$ observed FRB events, and a\ntiming resolution of $1~\\mu{\\rm s}$, the lack of observed FRB lensing events in\nCHIME allows us to probe axion stars with mass $ \\gtrsim 2 \\times 10^{-2}\nM_\\odot$, corresponding to axion masses $\\lesssim 10^{-10}{\\rm eV}$. We obtain\nconstraints for even lighter axion stars up to $\\sim 10^{-3} M_\\odot$, when the\naxion-photon interactions are taken into account. Our results indicate that FRB\nlensing lead to constraints that are competitive with conventional microlensing\nsearches operating in the optical band.",
        "Images generated by text-to-image (T2I) models often exhibit visual biases\nand stereotypes of concepts such as culture and profession. Existing\nquantitative measures of stereotypes are based on statistical parity that does\nnot align with the sociological definition of stereotypes and, therefore,\nincorrectly categorizes biases as stereotypes. Instead of oversimplifying\nstereotypes as biases, we propose a quantitative measure of stereotypes that\naligns with its sociological definition. We then propose OASIS to measure the\nstereotypes in a generated dataset and understand their origins within the T2I\nmodel. OASIS includes two scores to measure stereotypes from a generated image\ndataset: (M1) Stereotype Score to measure the distributional violation of\nstereotypical attributes, and (M2) WALS to measure spectral variance in the\nimages along a stereotypical attribute. OASIS also includes two methods to\nunderstand the origins of stereotypes in T2I models: (U1) StOP to discover\nattributes that the T2I model internally associates with a given concept, and\n(U2) SPI to quantify the emergence of stereotypical attributes in the latent\nspace of the T2I model during image generation. Despite the considerable\nprogress in image fidelity, using OASIS, we conclude that newer T2I models such\nas FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts\nand still generate images with widespread stereotypical attributes.\nAdditionally, the quantity of stereotypes worsens for nationalities with lower\nInternet footprints.",
        "There have recently been many cases of unverified or misleading information\ncirculating quickly over bogus web networks and news portals. This false news\ncreates big damage to society and misleads people. For Example, in 2019, there\nwas a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for\nsacrifice. This rumor turns into a deadly position and this misleading\ninformation takes the lives of innocent people. There is a lot of work in\nEnglish but a few works in Bangla. In this study, we are going to identify the\nfake news from the unconsidered news source to provide the newsreader with\nnatural news or real news. The paper is based on the combination of\nconvolutional neural network (CNN) and long short-term memory (LSTM), where CNN\nis used for deep feature extraction and LSTM is used for detection using the\nextracted feature. The first thing we did to deploy this piece of work was data\ncollection. We compiled a data set from websites and attempted to deploy it\nusing the methodology of deep learning which contains about 50k of news. With\nthe proposed model of Multichannel combined CNN-LSTM architecture, our model\ngained an accuracy of 75.05%, which is a good sign for detecting fake news in\nBangla.",
        "Muon colliders offer a compelling opportunity to explore the TeV scale and\nconduct precision tests of the Standard Model, all within a relatively compact\ngeographical footprint. This paper introduces a new detector concept, MAIA\n(Muon Accelerator Instrumented Apparatus), optimized for $\\sqrt{s}=10$ TeV\n$\\mu\\mu$ collisions. The detector features an all-silicon tracker immersed in a\n5T solenoid field. High-granularity silicon-tungsten and iron-scintillator\ncalorimeters surrounding the solenoid capture high-energy electronic and\nhadronic showers, respectively, and support particle-flow reconstruction. The\noutermost subsystem comprises an air-gap muon spectrometer, which enables\nstandalone track reconstruction for high-momentum muons. The performance of the\nMAIA detector is evaluated in terms of differential particle reconstruction\nefficiencies and resolutions. Beam-induced background (BIB) simulations\ngenerated in FLUKA are overlaid with single particle gun samples to assess\ndetector reconstruction capabilities under realistic experimental conditions.\nEven with BIB, reconstruction efficiencies exceed 95% for energetic tracks,\nphotons, and neutrons in the central region of the detector. This paper\noutlines promising avenues of future work, including forward region\noptimization and opportunities for enhanced flavor\/boosted object tagging, and\naddresses the technological assumptions needed to achieve the desired detector\nperformance.",
        "The high-pressure growth technique is a vital approach that facilitates the\nstabilization of new phases and allows for meticulous control of structural\nparameters, which significantly impact electronic and magnetic properties. We\npresent a short review of our ongoing investigations into various families of\niron-based superconductors (IBS), employing the high-gas pressure and\nhigh-temperature synthesis (HP-HTS) method. This technique is capable of\nproducing the gas pressures up to 1.8 GPa and a heating temperature of up to\n1700 {\\deg}C through a three-zone furnace within a cylindrical chamber.\nDifferent kinds of IBS samples are prepared using HPHTS and characterized\nthrough various measurements to reach the final conclusions. The results\ndemonstrate that the high-pressure growth technique significantly enhances the\nproperties of IBS, including the transition temperature, critical current\ndensity, and pinning force. In addition, the quality of the samples and their\ndensity are improved through the intergrain connections. Furthermore, the\ncomprehensive evaluations and investigations prove that a growth pressure of\n0.5 GPa is sufficient for producing high-quality IBS bulks under the optimized\nsynthesis conditions.",
        "The Dead Internet Theory (DIT) suggests that much of today's internet,\nparticularly social media, is dominated by non-human activity, AI-generated\ncontent, and corporate agendas, leading to a decline in authentic human\ninteraction. This study explores the origins, core claims, and implications of\nDIT, emphasizing its relevance in the context of social media platforms. The\ntheory emerged as a response to the perceived homogenization of online spaces,\nhighlighting issues like the proliferation of bots, algorithmically generated\ncontent, and the prioritization of engagement metrics over genuine user\ninteraction. AI technologies play a central role in this phenomenon, as social\nmedia platforms increasingly use algorithms and machine learning to curate\ncontent, drive engagement, and maximize advertising revenue. While these tools\nenhance scalability and personalization, they also prioritize virality and\nconsumption over authentic communication, contributing to the erosion of trust,\nthe loss of content diversity, and a dehumanized internet experience. This\nstudy redefines DIT in the context of social media, proposing that the\ncommodification of content consumption for revenue has taken precedence over\nmeaningful human connectivity. By focusing on engagement metrics, platforms\nfoster a sense of artificiality and disconnection, underscoring the need for\nhuman-centric approaches to revive authentic online interaction and community\nbuilding.",
        "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
        "An important step in understanding the formation and evolution of the Nuclear\nStar Cluster (NSC) is to investigate its chemistry and chemical evolution.\nAdditionally, exploring the relationship of the NSC to the other structures in\nthe Galactic Center and the Milky Way disks is of great interest. Extreme\noptical extinction has previously prevented optical studies, but near-IR\nhigh-resolution spectroscopy is now possible. Here, we present a detailed\nchemical abundance analysis of 19 elements - more than four times as many as\npreviously published - for 9 stars in the NSC of the Milky Way, observed with\nthe IGRINS spectrometer on the Gemini South telescope. This study provides new,\ncrucial observational evidence to shed light on the origin of the NSC. We\ndemonstrate that it is possible to probe a variety of nucleosynthetic channels,\nreflecting different chemical evolution timescales. Our findings reveal that\nthe NSC trends for the elements F, Mg, Al, Si, S, K, Ca, Ti, Cr, Mn, Co, Ni,\nCu, and Zn, as well as the s-process elements Ba, Ce, Nd, and Yb, generally\nfollow the inner bulge trends within uncertainties. This suggests a likely\nshared evolutionary history and our results indicate that the NSC population is\nconsistent with the chemical sequence observed in the inner Galaxy (the\ninner-disk sequence). However, we identify a significant and unexplained\ndifference in the form of higher Na abundances in the NSC compared to the\ninner-bulge. This is also observed in few Galactic globular clusters, and may\nsuggest a common enrichment process at work in all these systems.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "Laboratory experiments and theoretical modelling are conducted to determine\nthe raindrop size distribution (DSD) resulting from distinct fragmentation\nprocesses under various upward airstreams. Since weather radar echoes are\nproportional to the sixth power of the average droplet diameter, understanding\nthe fragmentation mechanisms that lead to different breakup sizes is crucial\nfor accurate rainfall predictions. We utilize a two-parameter gamma\ndistribution for theoretical modelling and estimate the average droplet\ndiameter from the theoretically obtained characteristic sizes, often treated as\nassumed input parameters for different rain conditions in rainfall modelling.\nOur experimental and theoretical findings demonstrate a close agreement with\nthe DSD predicted by the Marshall and Palmer relationship for steady rain\nconditions. Additionally, in situ DSD measurements at different altitudes were\nobtained through research flights equipped with advanced sensors, further\nvalidating our rainfall model. This study underscores the effectiveness of\nlaboratory-scale experiments and the critical importance of accurately\ncharacterizing DSD to enhance rainfall predictions.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "The Casimir-Lifshitz force arises from thermal and quantum mechanical\nfluctuations between classical bodies and becomes significant below the micron\nscale. We explore temperature-distance relations based on the concepts of Wick\nand Bohr arising from energy-time uncertainty relations. We show that\ntemperature-distance relations similar to those arising from the uncertainty\nprinciple are found in various Casimir interactions, with an exact relation\noccurring in the low-temperature regime when the zero point energy contribution\ncancels the thermal radiation pressure contribution between two plates.",
        "In this paper, we consider the spherically symmetric gravitational collapse\nof isotropic matter undergoing dissipation in the form of heat flux, with a\ngeneralized Vaidya exterior, in the context of $f(R, T)$ gravity. Choosing\n$f(R, T)=R+2\\lambda T$, and applying the $f(R, T)$ junction conditions on the\nfield equations for the interior and exterior regions, we have obtained\nmatching conditions of the matter-Lagrangian and its derivatives across the\nboundary. The time of formation of singularity and the time of formation of\napparent horizon have been determined and constraints on the integration\nconstants are examined for which the final singularity is hidden behind the\nhorizon.",
        "We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation.",
        "In this work, following our recent findings in [1], we extend our analysis to\nexplore the generalization of spherically symmetric and static black-bounce\nsolutions, known from General Relativity, within the framework of the $f(R)$\ntheory in the metric formalism. We develop a general approach to determine the\nsources for any model where $f(R) = R + H(R)$, provided that the corresponding\nsource for the bounce metric in General Relativity is known. As a result, we\ndemonstrate that black-bounce solutions can emerge from this theory when\nconsidering the coupling of $f(R)$ gravity with nonlinear electrodynamics and a\npartially phantom scalar field. We also analyzed the energy conditions of these\nsolutions and found that, unlike in General Relativity, it is possible to\nsatisfy all energy conditions in certain regions of space-time.",
        "Data from the Gaia mission shows prominent phase-space spirals that are the\nsignatures of disequilibrium in the Milky Way (MW) disc. In this work, we\npresent a novel perspective on the phase-space spiral in angular momentum (AM)\nspace. Using Gaia DR3, we detect a prominent AM spiral in the solar\nneighbourhood. We demonstrate the relation of AM to the $z-v_z$ spiral and show\nthat we can map to this space from angular momentum through simplifying\nassumptions. By modelling the orbit of stars in AM, we develop a generative\nmodel for the spiral where the disc is perturbed by a bulk tilt at an earlier\ntime. Our model successfully describes the salient features of the AM spiral in\nthe data. Modelling the phase spiral in AM is a promising method to constrain\nboth perturbation and MW potential parameters. Our AM framework simplifies the\ninterpretation of the spiral and offers a robust approach to modelling\ndisequilibrium in the MW disc using all six dimensions of phase space\nsimultaneously.",
        "Quantum error mitigation (QEM) has emerged as a powerful tool for the\nextraction of useful quantum information from quantum devices. Here, we\nintroduce the Subspace Noise Tailoring (SNT) algorithm, which efficiently\ncombines the cheap cost of Symmetry Verification (SV) and low bias of\nProbabilistic Error Cancellation (PEC) QEM techniques. We study the performance\nof our method by simulating the Trotterized time evolution of the spin-1\/2\nFermi-Hubbard model (FHM) using a variety of local fermion-to-qubit encodings,\nwhich define a computational subspace through a set of stabilizers, the\nmeasurement of which can be used to post-select noisy quantum data. We study\ndifferent combinations of QEM and encodings and uncover a rich phase diagram of\noptimal combinations, depending on the hardware performance, system size and\navailable shot budget. We then demonstrate how SNT extends the reach of current\nnoisy quantum computers in terms of the number of fermionic lattice sites and\nthe number of Trotter steps, and quantify the required hardware performance\nbeyond which a noisy device may outperform classical computational methods.",
        "We consider a class of sparse random matrices, which includes the adjacency\nmatrix of Erd\\H{o}s-R\\'enyi graph ${\\bf G}(N,p)$. For $N^{-1+o(1)}\\leq p\\leq\n1\/2$, we show that the non-trivial edge eigenvectors are asymptotically jointly\nnormal. The main ingredient of the proof is an algorithm that directly computes\nthe joint eigenvector distributions, without comparisons with GOE. The method\nis applicable in general. As an illustration, we also use it to prove the\nnormal fluctuation in quantum ergodicity at the edge for Wigner matrices.\nAnother ingredient of the proof is the isotropic local law for sparse matrices,\nwhich at the same time improves several existing results.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "James Webb Space Telescope (JWST) has opened up a new chapter in infrared\nastronomy. Besides the discovery and a deeper understanding of various\nastrophysical sources, JWST can also uncover the non-gravitational nature of\ndark matter (DM). If DM is QCD axion or an eV-scale Axion-like particle (ALP),\nit can decay into two photons in the infrared band. This will produce a\ndistinct line signature in the spectroscopic observations made by JWST. Using\nthe latest NIRSpec IFU spectroscopic observations from JWST, we put the\nstrongest bound on the photon coupling for QCD axion\/ ALP DM in the mass range\nbetween 0.47 and 2.55 eV. In particular, we are able to probe a new mass range\nfor ALP DM between $\\sim$ 0.47 eV to 0.78 eV beyond what can be probed by\nglobular cluster observations. We constrain well-motivated and UV complete\nmodels of QCD axion and ALP DM, including predictions from some models derived\nfrom string theory and\/ or various Grand Unification scenarios. Future JWST\nobservations of DM-rich systems with a better understanding of the\nastrophysical and instrumental backgrounds can thus enable us to potentially\ndiscover QCD axion and ALP DM. The datasets used in this work are available at:\nhttps:\/\/dx.doi.org\/10.17909\/3e5f-nv69",
        "We present the first exact quantum adder with sublinear depth and no ancilla\nqubits. Our construction is based on classical reversible logic only and\nemploys low-depth implementations for the CNOT ladder operator and the Toffoli\nladder operator, two key components to perform ripple-carry addition. Namely,\nwe demonstrate that any ladder of $n$ CNOT gates can be replaced by a\nCNOT-circuit with $O(\\log n)$ depth, while maintaining a linear number of\ngates. We then generalize this construction to Toffoli gates and demonstrate\nthat any ladder of $n$ Toffoli gates can be substituted with a circuit with\n$O(\\log^2 n)$ depth while utilizing a linearithmic number of gates. This builds\non the recent works of Nie et al. and Khattar and Gidney on the technique of\nconditionally clean ancillae. By combining these two key elements, we present a\nnovel approach to design quantum adders that can perform the addition of two\n$n$-bit numbers in depth $O(\\log^2 n)$ without the use of any ancilla and using\nclassical reversible logic only (Toffoli, CNOT and X gates).",
        "Large language models (LLMs) have rapidly gained popularity and are being\nembedded into professional applications due to their capabilities in generating\nhuman-like content. However, unquestioned reliance on their outputs and\nrecommendations can be problematic as LLMs can reinforce societal biases and\nstereotypes. This study investigates how LLMs, specifically OpenAI's GPT-4 and\nMicrosoft Copilot, can reinforce gender and racial stereotypes within the\nsoftware engineering (SE) profession through both textual and graphical\noutputs. We used each LLM to generate 300 profiles, consisting of 100\ngender-based and 50 gender-neutral profiles, for a recruitment scenario in SE\nroles. Recommendations were generated for each profile and evaluated against\nthe job requirements for four distinct SE positions. Each LLM was asked to\nselect the top 5 candidates and subsequently the best candidate for each role.\nEach LLM was also asked to generate images for the top 5 candidates, providing\na dataset for analysing potential biases in both text-based selections and\nvisual representations. Our analysis reveals that both models preferred male\nand Caucasian profiles, particularly for senior roles, and favoured images\nfeaturing traits such as lighter skin tones, slimmer body types, and younger\nappearances. These findings highlight underlying societal biases influence the\noutputs of LLMs, contributing to narrow, exclusionary stereotypes that can\nfurther limit diversity and perpetuate inequities in the SE field. As LLMs are\nincreasingly adopted within SE research and professional practices, awareness\nof these biases is crucial to prevent the reinforcement of discriminatory norms\nand to ensure that AI tools are leveraged to promote an inclusive and equitable\nengineering culture rather than hinder it.",
        "Adapting to task changes without forgetting previous knowledge is a key skill\nfor intelligent systems, and a crucial aspect of lifelong learning. Swarm\ncontrollers, however, are typically designed for specific tasks, lacking the\nability to retain knowledge across changing tasks. Lifelong learning, on the\nother hand, focuses on individual agents with limited insights into the\nemergent abilities of a collective like a swarm. To address this gap, we\nintroduce a lifelong evolutionary framework for swarms, where a population of\nswarm controllers is evolved in a dynamic environment that incrementally\npresents novel tasks. This requires evolution to find controllers that quickly\nadapt to new tasks while retaining knowledge of previous ones, as they may\nreappear in the future. We discover that the population inherently preserves\ninformation about previous tasks, and it can reuse it to foster adaptation and\nmitigate forgetting. In contrast, the top-performing individual for a given\ntask catastrophically forgets previous tasks. To mitigate this phenomenon, we\ndesign a regularization process for the evolutionary algorithm, reducing\nforgetting in top-performing individuals. Evolving swarms in a lifelong fashion\nraises fundamental questions on the current state of deep lifelong learning and\non the robustness of swarm controllers in dynamic environments.",
        "Metastable cosmic strings (MSCSs) are among the best-fitting explanations of\nthe 2023 pulsar timing array (PTA) signal for gravitational waves at nanohertz\nfrequencies. We propose the novel possibility that a network of MSCSs\ngenerating this signal originates from the multi-step spontaneous breaking of a\ngauged flavour symmetry. As a specific example, we construct a model of $SU(2)$\nflavour symmetry in the context of $SU(5)$ grand unification, where the $SU(2)$\nacts exclusively on the first two generations of the matter 10-plet, such that\nit is ``right for leptons'' and allows for large lepton mixing. The model\nexplains the mass hierarchies of the Standard Model fermions, and predicts the\nstring scale of the MSCSs in a range compatible with the 2023 PTA signal.\nCosmic inflation is associated with the latter step of (two-step) family\nsymmetry breaking, and the phase transition ending inflation generates the\ncosmic string network.",
        "Multi-aspect controllable text generation aims to control text generation in\nattributes from multiple aspects, making it a complex but powerful task in\nnatural language processing. Supervised fine-tuning methods are often employed\nfor this task due to their simplicity and effectiveness. However, they still\nhave some limitations: low rank adaptation (LoRA) only fine-tunes a few\nparameters and has suboptimal control effects, while full fine-tuning (FFT)\nrequires significant computational resources and is susceptible to overfitting,\nparticularly when data is limited. Moreover, existing works typically train\nmulti-aspect controllable text generation models using only single-aspect\nannotated data, which results in discrepancies in data distribution; at the\nsame time, accurately generating text with specific attributes is a challenge\nthat requires strong attribute-aware capabilities. To address these\nlimitations, we propose a lightweight, adaptive and attribute-aware framework\nfor multi-aspect controllable text generation. Our framework can dynamically\nadjust model parameters according to different aspects of data to achieve\ncontrollable text generation, aiming to optimize performance across multiple\naspects. Experimental results show that our framework outperforms other strong\nbaselines, achieves state-of-the-art performance, adapts well to data\ndiscrepancies, and is more accurate in attribute perception.",
        "Federated learning (FL) enables participants to store data locally while\ncollaborating in training, yet it remains vulnerable to privacy attacks, such\nas data reconstruction. Existing differential privacy (DP) technologies inject\nnoise dynamically into the training process to mitigate the impact of excessive\nnoise. However, this dynamic scheduling is often grounded in factors indirectly\nrelated to privacy, making it difficult to clearly explain the intricate\nrelationship between dynamic noise adjustments and privacy requirements. To\naddress this issue, we propose FedSDP, a novel and explainable DP-based privacy\nprotection mechanism that guides noise injection based on privacy contribution.\nSpecifically, FedSDP leverages Shapley values to assess the contribution of\nprivate attributes to local model training and dynamically adjusts the amount\nof noise injected accordingly. By providing theoretical insights into the\ninjection of varying scales of noise into local training, FedSDP enhances\ninterpretability. Extensive experiments demonstrate that FedSDP can achieve a\nsuperior balance between privacy preservation and model performance, surpassing\nstate-of-the-art (SOTA) solutions.",
        "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint."
      ]
    }
  },
  {
    "id":2411.0484,
    "research_type":"basic",
    "start_id":"b25",
    "start_title":"Genetic Algorithms + Data Structures = Evolution Programs",
    "start_abstract":"Genetic algorithms are founded upon the principle of evolution, i.e., survival of the fittest. Hence evolution programming techniques, based on genetic algorithms, are applicable to many hard optimization problems, such as optimization of functions with linear and nonlinear constraints, the traveling salesman problem, and problems of scheduling, partitioning, and control. The importance of these techniques is still growing, since evolution programs are parallel in nature, and parallelism is one of the most promising directions in computer science. The book is self-contained and the only prerequisite is basic undergraduate mathematics. This third edition has been substantially revised and extended by three new chapters and by additional appendices containing working material to cover recent developments and a change in the perception of evolutionary computation.",
    "start_categories":[
      "cs.DS"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Kinetic description and convergence analysis of genetic algorithms for global optimization"
      ],
      "abstract":[
        "Genetic Algorithms (GA) are a class of metaheuristic global optimization methods inspired by the process natural selection among individuals in population. Despite their widespread use, comprehensive theoretical analysis these remains challenging due to complexity heuristic mechanisms involved. In this work, relying on tools statistical physics, we take first step towards mathematical understanding GA showing how behavior for large number can be approximated through time-discrete kinetic model. This allows us prove convergence algorithm minimum under mild assumptions objective function popular choice mechanism. Furthermore, derive time-continuous model GA, represented Boltzmann-like partial differential equation, and establish relations with other mean-field dynamics optimization. Numerical experiments support validity proposed approximation investigate asymptotic configurations particle system different benchmark problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Strongly nonlinear age structured equation,time-elapsed model and large\n  delays",
        "Benchmarking nuclear matrix elements of $0\\nu\\beta\\beta$ decay with\n  high-energy nuclear collisions",
        "Periodic orbits and their gravitational wave radiations around the\n  Schwarzschild-MOG black hole",
        "A positive product formula of integral kernels of $k$-Hankel transforms",
        "Floquet geometric squeezing in fast-rotating condensates",
        "Clarkson-McCarthy inequality on a locally compact group",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Low-rank variance reduction for uncertain radiative transfer with\n  control variates",
        "Lyapunov exponent for quantum graphs that are elements of a subshift of\n  finite type",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Empirical Thermophotovoltaic Performance Predictions and Limits",
        "The waves-in-space Purcell effect for superconducting qubits",
        "Anomalous nuclear spin coherence in superconducting Nb$_3$Sn",
        "Effects of Initial Nucleon-Nucleon Correlations on Light Nuclei\n  Production in Au+Au Collisions at $\\sqrt{s_\\mathrm{NN}} = 3\\ $ GeV",
        "Prospection and dispersal in metapopulations: a perspective from opinion\n  dynamics models",
        "An assessment of observational coverage and gaps for robust Sun to\n  heliosphere integrated science",
        "Crosstalk analysis in single hole-spin qubits within highly anisotropic\n  g-tensors",
        "Global and local approaches for the minimization of a sum of pointwise\n  minima of convex functions",
        "Mortality simulations for insured and general populations",
        "Catalytic Nanoparticles: An Introduction",
        "Temperature-redshift relation in energy-momentum-powered gravity models",
        "iLOCO: Distribution-Free Inference for Feature Interactions",
        "On the structure of modular lattices -- Unique gluing and dissection",
        "On solvmanifolds with complex commutator and constant holomorphic\n  sectional curvature",
        "Unravelling The potential of Hybrid Borocarbonitride Biphenylene 2D\n  Network for Thermoelectric Applications: A First Principles Study",
        "Formation of Anomalously Energetic Ions in Hollow Cathode Plume by\n  Charge Separation Instability",
        "Asset pre-selection for a cardinality constrained index tracking\n  portfolio with optional enhancement",
        "Strichartz's conjecture for the spinor bundle over the real hyperbolic\n  space"
      ],
      "abstract":[
        "The time-elapsed model for neural networks is a nonlinear age structured\nequationwhere the renewal term describes the network activity and influences\nthe dischargerate, possibly with a delay due to the length of connections.We\nsolve a long standing question, namely that an inhibitory network withoutdelay\nwill converge to a steady state and thus the network is desynchonised.\nOurapproach is based on the observation that a non-expansion property holds\ntrue.However a non-degeneracy condition is needed and, besides the standard\none, weintroduce a new condition based on strict nonlinearity.When a delay is\nincluded, and following previous works for Fokker-Planck models,we prove that\nthe network may generate periodic solutions. We introduce a newformalism to\nestablish rigorously this property for large delays.The fundamental contraction\nproperty also holds for some other age structuredequations and systems.",
        "Reducing uncertainties in the nuclear matrix element (NME) remains a critical\nchallenge in designing and interpreting experiments aimed at discovering\nneutrinoless double beta ($0\\nu\\beta\\beta$) decay. Here, we identify a class of\nobservables, distinct from those employed in low-energy nuclear structure\napplications, that are strongly correlated with the NME: momentum correlations\namong hadrons produced in high-energy nuclear collisions. Focusing on the\n$^{150}$Nd$\\rightarrow$$^{150}$Sm transition, we combine a Bayesian analysis of\nthe structure of $^{150}$Nd with simulations of high-energy\n$^{150}$Nd+$^{150}$Nd collisions. We reveal prominent correlations between the\nNME and features of the quark-gluon plasma (QGP) formed in these processes,\nsuch as spatial gradients and anisotropies, which are accessible via collective\nflow measurements. Our findings demonstrate collider experiments involving\n$0\\nu\\beta\\beta$ decay candidates as a platform for benchmarking theoretical\npredictions of the NME.",
        "This article explores the motion of massive particles in the gravitational\nfield of a modified gravity (MOG) black hole (BH), characterized by the\nparameter $\\alpha$. Using the Hamiltonian formalism, the geodesic equations and\nthe effective potential governing particle trajectories are derived. Key\nfeatures, including the innermost stable circular orbit (ISCO) and the\ninnermost bound circular orbit (IBCO), are analyzed, revealing their dependence\non the particle's energy, angular momentum, and the MOG parameter. In the\nextremal case, where $\\alpha=-1$, the event horizon merges with the Cauchy\nhorizon, forming a distinctive BH configuration. Numerical methods are employed\nto compute periodic orbits in this spacetime, with a comparison drawn to the\nSchwarzschild BH. The findings indicate that for $\\alpha>0$, periodic orbits\naround Schwarzschild-MOG BH exhibit lower energy requirements than those in\nSchwarzschild spacetime, whereas for $-1<\\alpha<0$, the energy requirements are\nhigher. Precessing orbits near periodic trajectories are also examined,\noffering insights into their complex dynamical behavior. Finally, the\ngravitational wave (GW) radiation from the periodic orbits of a test particle\naround the Schwarzschild-MOG BH is examined, generating intricate waveforms\nthat provide insights into the gravitational structure of the system.",
        "Let $R$ be a root system in $\\mathbb R^N$ and $G$ be the finite subgroup\ngenerated by the reflections associated to the root system. We establish a\npositive radial product formula for the integral kernels $B_{k,1}(x,y)$ of\n$(k,1)$-generalized Fourier transforms (or the $k$-Hankel transforms) $F_{k,1}$\n$$B_{k,1}(x,z)j_{2\\left\\langle\nk\\right\\rangle+N-2}\\left(2\\sqrt{t\\left|z\\right|}\\right)=\\int_{\\mathbb R^N}\nB_{k,1}(\\xi,z)\\,d\\sigma_{x,t}^{k,1}(\\xi),$$ where $j_{\\lambda}$ is the\nnormalized Bessel function, and $\\sigma_{x,t}^{k,1}(\\xi)$ is the unique\nprobability measure. Such a product formula is equivalent to the following\nrepresentation of the generalized spherical mean operator $f\\mapsto M_f,\\;f\\in\nC_b(\\mathbb{R}^N)$ in $(k,1)$-generalized Fourier analysis \\begin{align*}\nM_f(x,t)=\\int_{\\mathbb{R}^N}f\\,d\\sigma_{x,t}^{k,1},\\;(x,t)\\in\\mathbb{R}^N\\times{\\mathbb{R}}_+.\\end{align*}\nWe will then analyze the representing measure $\\sigma_{x,t}^{k,1}(\\xi)$ and\nshow that the support of the measure is contained in\n$$\\left\\{\\xi\\in\\mathbb{R}^N:\\sqrt{\\vert\\xi\\vert}\\geq\\vert\\sqrt{\\vert\nx\\vert}-\\sqrt t\\vert\\right\\}\\cap\\left(\\bigcup_{g\\in\nG}\\{\\xi\\in\\mathbb{R}^N:d(\\xi,gx)\\leq\\sqrt t\\}\\right),$$ where\n$d\\left(x,y\\right)=\\sqrt{\\left|x\\right|+\\left|y\\right|-\\sqrt{2\\left(\\left|x\\right|\\left|y\\right|+\\left\\langle\nx,y\\right\\rangle\\right)}}$. Based on the support of the representing measure\n$\\sigma_{x,t}^{k,1}$, we will get a weak Huygens's principle for the deformed\nwave equation in $(k,1)$-generalized Fourier analysis. Moreover, for $N\\geq 2$,\nif we assume that $F_{k,1}\\left(\\mathcal S(\\mathbb{R}^N)\\right)$ consists of\nrapidly decreasing functions at infinity, then we get two different results on\n$\\text{supp}\\sigma_{x,t}^{k,1}$, which indirectly denies such assumption.",
        "Constructing and manipulating quantum states in fast-rotating Bose-Einstein\ncondensates (BEC) has long stood as a significant challenge as the rotating\nspeed approaching the critical velocity. Although the recent experiment\n[Science, 372, 1318 (2021)] has realized the geometrically squeezed state of\nthe guiding-center mode, the remaining degree of freedom, the cyclotron mode,\nremains unsqueezed due to the large energy gap of Landau levels. To overcome\nthis limitation, in this paper, we propose a Floquet-based state-preparation\nprotocol by periodically driving an anisotropic potential. This protocol not\nonly facilitates the single cyclotron-mode squeezing, but also enables a\ntwo-mode squeezing. Such two-mode squeezing offers a richer set of dynamics\ncompared to single-mode squeezing and can achieve wavepacket width well below\nthe lowest Landau level limit. Our work provides a highly controllable knob for\nrealizing diverse geometrically squeezed states in ultracold quantum gases\nwithin the quantum Hall regime.",
        "Let $G$ be a locally compact group, $\\mu$ its Haar measure, $\\hat G$ its\nPontryagin dual and $\\nu$ the dual measure. For any $A_\\theta\\in L^1(G;\\mathcal\nC_p)\\cap L^2(G;\\mathcal C_p)$, ($\\mathcal C_p$ is Schatten ideal), and\n$1<p\\le2$ we prove $$\\int_{\\hat\nG}\\left\\|\\int_GA_\\theta\\overline{\\xi(\\theta)}\\,\\mathrm\nd\\mu(\\theta)\\right\\|_p^q\\,\\mathrm d\\nu(\\xi)\\le\n  \\left(\\int_G\\|A_\\theta\\|_p^p\\,\\mathrm d\\mu(\\theta)\\right)^{q\/p}, $$ where\n$q=p\/(p-1)$. This appears to be a generalization of some earlier obtained\ninequalities, including Clarkson-McCarthy inequalities (in the case $G=\\mathbf\nZ_2$), and Hausdorff-Young inequality. Some corollaries are also given.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The radiative transfer equation models various physical processes ranging\nfrom plasma simulations to radiation therapy. In practice, these phenomena are\noften subject to uncertainties. Modeling and propagating these uncertainties\nrequires accurate and efficient solvers for the radiative transfer equations.\nDue to the equation's high-dimensional phase space, fine-grid solutions of the\nradiative transfer equation are computationally expensive and memory-intensive.\nIn recent years, dynamical low-rank approximation has become a popular method\nfor solving kinetic equations due to the development of computationally\ninexpensive, memory-efficient and robust algorithms like the augmented basis\nupdate \\& Galerkin integrator. In this work, we propose a low-rank Monte Carlo\nestimator and combine it with a control variate strategy based on\nmulti-fidelity low-rank approximations for variance reduction. We investigate\nthe error analytically and numerically and find that a joint approach to\nbalance rank and grid size is necessary. Numerical experiments further show\nthat the efficiency of estimators can be improved using dynamical low-rank\napproximation, especially in the context of control variates.",
        "We consider the Schr\\\"odinger operator on the quantum graph whose edges\nconnect the points of ${\\Bbb Z}$. The numbers of the edges connecting two\nconsecutive points $n$ and $n+1$ are read along the orbits of a shift of finite\ntype. We prove that the Lyapunov exponent is potitive for energies $E$ that do\nnot belong to a discrete subset of $[0,\\infty)$. The number of points $E$ of\nthis subset in $[(\\pi (j-1))^2, (\\pi j)^2]$ is the same for all $j\\in {\\Bbb\nN}$.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "Significant progress has been made in the field of thermophotovoltaics, with\nefficiency recently rising to over 40% due to improvements in cell design and\nmaterial quality, higher emitter temperatures, and better spectral management.\nHowever, inconsistencies in trends for efficiency with semiconductor bandgap\nenergy across various temperatures pose challenges in predicting optimal\nbandgaps or expected performance for different applications. To address these\nissues, here we present realistic performance predictions for various types of\nsingle-junction cells over a broad range of emitter temperatures using an\nempirical model based on past cell measurements. Our model is validated using\ndata from different authors with various bandgaps and emitter temperatures, and\nan excellent agreement is seen between the model and the experimental data.\nUsing our model, we show that in addition to spectral losses, it is important\nto consider practical electrical losses associated with series resistance and\ncell quality to avoid overestimation of system efficiency. We also show the\neffect of modifying various system parameters such as bandgap, above and\nbelow-bandgap reflectance, saturation current, and series resistance on the\nefficiency and power density of thermophotovoltaics at different temperatures.\nFinally, we predict the bandgap energies for best performance over a range of\nemitter temperatures for different cell material qualities.",
        "Quantum information processing, especially with quantum error correction,\nrequires both long-lived qubits and fast, quantum non-demolition readout. In\nsuperconducting circuits this leads to the requirement to both strongly couple\nqubits, such as transmons, to readout modes while also protecting them from\nassociated Purcell decay through the readout port. So-called Purcell filters\ncan provide this protection, at the cost of significant increases in circuit\ncomponents and complexity. However, as we demonstrate in this work, visualizing\nthe qubit fields in space reveals locations where the qubit fields are strong\nand cavity fields weak; simply placing ports at these locations provides\nintrinsic Purcell protection. For a $\\lambda\/2$ readout mode in the\n`chip-in-tube' geometry, we show both millisecond level Purcell protection and,\nconversely, greatly enhanced Purcell decay (qubit lifetime of 1~$\\mu$s) simply\nby relocating the readout port. This method of integrating the Purcell\nprotection into the qubit-cavity geometry can be generalized to other 3D\nimplementations, such as post-cavities, as well as planar geometries. For qubit\nfrequencies below the readout mode this effect is quite distinct from the\nmulti-mode Purcell effect, which we demonstrate in a 3D-post geometry where we\nshow both Purcell protection of the qubit while spoiling the quality factor of\nhigher cavity harmonics to protect against dephasing due to stray photons in\nthese modes.",
        "We have investigated the normal and superconducting states of the\ntechnologically important compound Nb$_3$Sn using $^{93}$Nb nuclear magnetic\nresonance. From spin-lattice relaxation we find strong suppression of the\nzero-temperature superconducting order parameter by magnetic field. We have\nidentified an anomalously large electron-nuclear exchange interaction from\nspin-spin relaxation measurements, an order of magnitude beyond that of the\ndipole-dipole interaction, and thereby sensitive to vortex dynamics and vortex\npinning.",
        "Light nuclei production in heavy-ion collisions serves as a sensitive probe\nof the QCD phase structure. In coalescence models, triton ($N_t$) and deuteron\n($N_d$) yields depend on the spatial separation of nucleon pairs ($\\Delta r$)\nin Wigner functions, yet the impact of initial two-nucleon correlations\n$\\rho(\\Delta r)$ remains underexplored. We develop a method to sample nucleons\nin $^{197}$Au nuclei that simultaneously satisfies both the single-particle\ndistribution $f(r)$ and the two-nucleon correlation $\\rho(\\Delta r)$. Using\nthese nuclei, we simulate Au+Au collisions at $\\sqrt{s_\\mathrm{NN}}=3$ GeV via\nthe SMASH transport model (mean-field mode) to calculate proton, deuteron, and\ntriton yields. Simulations reveal a 36% enhancement in mid-rapidity deuteron\nyields across all centrality ranges and a 33% rise in mid-rapidity triton\nproduction for 0-10% central collisions. Calculated transverse momentum of\nlight nuclei aligns with STAR data. We further analyze impacts of baryon\nconservation, spectator exclusion, and centrality determination via charged\nmultiplicity. Notably, observed discrepancies in the double yield ratio suggest\nunaccounted physical mechanisms, such as critical fluctuations or inaccuracies\nin coalescence parameters or light nuclei cross-sections. This underscores the\ncritical role of initial nucleon-nucleon correlations, linking microscopic\nnuclear structure to intermediate-energy collision dynamics.",
        "Dispersal is often used by living beings to gather information from\nconspecifics, integrating it with personal experience to guide decision-making.\nThis mechanism has only recently been studied experimentally, facilitated by\nadvancements in tracking animal groups over extended periods. Such studies\nenable the analysis of the adaptive dynamics underlying sequential decisions\nand collective choices. Here, we present a theoretical framework based on the\nVoter Model to investigate these processes. The model, originally designed to\nstudy opinion or behavioral consensus within groups through imitation, is\nadapted to include the prospection of others' decisions as a mechanism for\nupdating personal criteria. We demonstrate that several properties of our model\n(such as average consensus times and polarization dynamic) can be analytically\nmapped onto those of the classical Voter Model under simplifying assumptions.\nFinally, we discuss the potential of this framework for studying more complex\nscenarios.",
        "Understanding the generation and development of the continuous outflow from\nthe Sun requires tracing the physical conditions from deep in the corona to the\nheliosphere. Detailed global observations of plasma state variables and the\nmagnetic field are needed to provide critical constraints to the underlying\nphysics driving models of the corona and solar wind. Key diagnostics of the\nsolar wind require measurements at its formation site and during its outflow to\ncontinuously track it across rapidly changing regions of space. A unified view\nof the solar wind is only possible through coordinated remote and in situ\nobservations that probe these different regions. Here, we discuss current\nobservational coverage and gaps of different plasma properties and review\nrecent coordinated studies. We highlight how these efforts may become more\nroutine with the launch of upcoming and planned missions.",
        "Spin qubits based on valence band hole states are highly promising for\nquantum information processing due to their strong spin-orbit coupling and\nultrafast operation speed. As these systems scale up, achieving high-fidelity\nsingle-qubit operations becomes essential. However, mitigating crosstalk\neffects from neighboring qubits in larger arrays, particularly for anisotropic\nqubits with strong spin-orbit coupling, presents a significant challenge. We\ninvestigate the impact of crosstalk on qubit fidelities during single-qubit\noperations and derive an analytical equation that serves as a synchronization\ncondition to eliminate crosstalk in anisotropic media. Our analysis proposes\noptimized driving field conditions that can robustly synchronize Rabi\noscillations and minimize crosstalk, showing a strong dependence on qubit\nanisotropy and the orientation of the external magnetic field. Taking\nexperimental data into our analysis, we identify a set of parameter values that\nenable nearly crosstalk-free single-qubit gates, thereby paving the way for\nscalable quantum computing architectures.",
        "Numerous machine learning and industrial problems can be modeled as the\nminimization of a sum of $N$ so-called clipped convex functions (SCC), i.e.\neach term of the sum stems as the pointwise minimum between a constant and a\nconvex function. In this work, we extend this framework to capture more\nproblems of interest. Specifically, we allow each term of the sum to be a\npointwise minimum of an arbitrary number of convex functions, called\ncomponents, turning the objective into a sum of pointwise minima of convex\nfunctions (SMC).\n  Problem (SCC) is NP-hard, highlighting an appeal for scalable local\nheuristics. In this spirit, one can express (SMC) objectives as the difference\nbetween two convex functions to leverage the possibility to apply (DC)\nalgorithms to compute critical points of the problem. Our approach relies on a\nbi-convex reformulation of the problem. From there, we derive a family of local\nmethods, dubbed as relaxed alternating minimization (r-AM) methods, that\ninclude classical alternating minimization (AM) as a special case. We prove\nthat every accumulation point of r-AM is critical. In addition, we show the\nempirical superiority of r-AM, compared to traditional AM and (DC) approaches,\non piecewise-linear regression and restricted facility location problems.\n  Under mild assumptions, (SCC) can be cast as a mixed-integer convex program\n(MICP) using perspective functions. This approach can be generalized to (SMC)\nbut introduces many copies of the primal variable. In contrast, we suggest a\ncompact big-M based (MICP) equivalent formulation of (SMC), free of these extra\nvariables. Finally, we showcase practical examples where solving our (MICP),\nrestricted to a neighbourhood of a given candidate (i.e. output iterate of a\nlocal method), will either certify the candidate's optimality on that\nneighbourhood or providing a new point, strictly better, to restart the local\nmethod.",
        "This study presents a framework for high-resolution mortality simulations\ntailored to insured and general populations. Due to the scarcity of detailed\ndemographic-specific mortality data, we leverage Iterative Proportional Fitting\n(IPF) and Monte Carlo simulations to generate refined mortality tables that\nincorporate age, gender, smoker status, and regional distributions. This\nmethodology enhances public health planning and actuarial analysis by providing\nenriched datasets for improved life expectancy projections and insurance\nproduct development.",
        "This study explores the transformative potential of nanocatalysts,\nemphasizing their pivotal role in catalysis and material science. Key synthesis\ntechniques, including chemical reduction and hybrid methods, are highlighted\nfor their ability to control particle size and enhance stability. Applications\nin environmental remediation, fuel quality improvement, and renewable energy\nshowcase the broad impact of nanocatalysts. Despite challenges in scalability\nand stabilization, advancements in bimetallic configurations and electro-steric\napproaches demonstrate significant progress. This research underscores\nnanocatalysts' promise for sustainable industrial processes and global\nchallenges.",
        "There has been recent interest in the cosmological consequences of\nenergy-momentum-powered gravity models, in which the matter side of Einstein's\nequations includes a term proportional to some power, $n$, of the\nenergy-momentum tensor, in addition to the canonical linear term. Previous\nworks have suggested that these models can lead to a recent accelerating\nuniverse without a cosmological constant, but they can also be seen as\nphenomenological extensions of the standard $\\Lambda$CDM, which are\nobservationally constrained to be close to the $\\Lambda$CDM limit. Here we show\nthat these models violate the temperature-redshift relation, and are therefore\nfurther constrained by astrophysical measurements of the cosmic microwave\nbackground temperature. We provide joint constraints on these models from the\ncombination of astrophysical and background cosmological data, showing that\nthis power is constrained to be about $|n|<0.01$ and $|n|<0.1$, respectively in\nmodels without and with a cosmological constant, and improving previous\nconstraints on this parameter by more than a factor of three. By breaking\ndegeneracies between this parameter and the matter density, constraints on the\nlatter are also improved by a factor of about two.",
        "Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions.",
        "This work proves that the process of gluing finite lattices to form a larger\nlattice is bijective, that is each lattice is the glued sum of a unique system\nof finite lattices, provided the class of lattices is constrained to modular,\nlocally-finite lattices with finite covers. The results of this work are not\nsurprising given the prior literature, but this seems to be the first proof\nthat the processes of gluing and dissection can be made inverses, and hence\nthat gluing is bijective.",
        "An old open question in non-K\\\"ahler geometry predicts that any compact\nHermitian manifold with constant holomorphic sectional curvature must be\nK\\\"ahler or Chern flat. The conjecture is known to be true in dimension $2$ due\nto the work by Balas-Gauduchon and Apostolov-Davidov-Muskarov in the 1980s and\n1990s, but is still open in dimensions $3$ or higher, except in several special\ncases. The difficulty in this quest for `Hermitian space forms' is largely due\nto the algebraic complicity or lack of symmetry for the curvature tensor of a\ngeneral Hermitian metric. In this article, we confirm the conjecture for all\nsolvmanifolds with complex commutator, extending earlier result on nilmanifolds\nby Li and the second named author.",
        "In this study, we investigate a novel hybrid borocarbonitrides (bpn-BCN) 2D\nmaterial inspired by recent advances in carbon biphenylene synthesis, using\nfirst-principles calculations and semi-classical Boltzmann transport theory.\nOur analysis confirms the structural stability of bpn-BCN through formation\nenergy, elastic coefficients, phonon dispersion, and molecular dynamics\nsimulations at 300 K and 800 K. The material exhibits an indirect band gap of\n0.19 eV (PBE) between the X and Y points and a direct band gap of 0.58 eV (HSE)\nat the X point. Thermoelectric properties reveal a high Seebeck coefficient,\npeaking at for n-type carriers at 200K along the x-axis, while n-type has a\nmaximum of The electrical conductivity is for hole carriers, surpassing that of\nconventional 2D materials. The consequences of the high Seebeck coefficient and\nconductivity reflect a high-power factor with a peak value of at 1000K for\np-type carriers along the y-axis whereas, for n-type. Moreover, the highest\nobserved values were 0.78 (0.72) along the x (y) direction at 750 K for p-type\nand 0.57 (0.53) at 750 K along the x (y) axis for n-type. Our findings suggest\nthat the bpn-BCN 2D network holds significant potential for thermoelectric\napplications due to its exceptional performance.",
        "Hollow cathodes are becoming the bottleneck of many electric propulsion\nsystems, because of the sputtering and erosion on both cathodes and thrusters\nfrom the generation of anomalously energetic ions. So far, it is believed that\nenergetic ions are formed by waves and instabilities always accompanied in\ncathode discharge, but there is no evidence yet that those proposed\ninstabilities can lead to such high ion energies measured in experiments. In\nthis work, a new mechanism of charge separation instability in hollow cathode\nplume is found via fully kinetic PIC simulations, which can easily produce\nenergetic ions to the same level as measured in experiments.",
        "An index tracker is a passive investment reproducing the return and risk of a\nmarket index, an enhanced index tracker offers a return greater than the index.\nWe consider the selection of a portfolio of given cardinality to track an\nindex, both without and with enhancement. We divide the problem into two steps\n- (1) pre-selection of assets; (2) estimation of weights on the assets chosen.\nThe eight pre-selection procedures considered use: forward selection (FS) or\nbackward elimination (BE); implemented using ordinary least squares (OLS) or\nleast absolute deviation (LAD) regression; with a regression constant (c) or\nwithout (n). The two-step approach avoids the NP-hard problem arising when\nasset selection and asset weight computation are combined, leading to the\nselection of a cardinality constrained index tracking portfolio by computer\nintensive heuristic procedures with many examples in the literature solving for\nportfolios of 10 or fewer assets. Avoiding these restrictions, we show that\nout-of-sample tracking errors are roughly proportional to 1\/sqrt(cardinality).\n  We find OLS more effective than LAD; BE marginally more effective than FS;\n(n) marginally more effective than (c). For index tracking, both without and\nwith enhancement, we use BE-OLS(n) in sensitivity analyses on the periods used\nfor selection and evaluation. For a S&P 500 index tracker, we find that\nout-of-sample tracking error, transaction volume and return-risk ratios all\nimprove as cardinality increases. By contrast for enhanced returns,\ncardinalities of the order 10 to 20 are most effective. The S&P 500 data used\nfrom 3\/1\/2005 to 29\/12\/2023 is available to researchers.",
        "Let $H^n(\\mathbb R)$ denote the real hyperbolic space realized as the\nsymmetric space $Spin_0(1,n)\/Spin(n)$. In this paper, we provide a\ncharacterization for the image of the Poisson transform for $L^2$-sections of\nthe spinor bundle over the boundary ${\\partial H}^n(\\mathbb R)$. As a\nconsequence, we obtain an $L^2$ uniform estimate for the generalized spectral\nprojections associated to the spinor bundle over $H^n(\\mathbb R)$, thereby\nextending Strichartz's conjecture from the scalar case to the spinor setting."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques",
    "start_abstract":"Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b35"
      ],
      "title":[
        "Deep learning-based framework for automatic cranial defect reconstruction and implant modeling"
      ],
      "abstract":[
        "This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "RL + Transformer = A General-Purpose Problem Solver",
        "Matter creation, adiabaticity and phantom behavior",
        "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone",
        "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference\n  in Serverless Computing",
        "Tempo: Helping Data Scientists and Domain Experts Collaboratively\n  Specify Predictive Modeling Tasks",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Upper limits on the gamma-ray emission from the microquasar V4641 Sgr",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "The bright, dusty aftermath of giant eruptions & H-rich supernovae. Late\n  interaction of supernova shocks & dusty circumstellar shells",
        "Evolution and Pathogenicity of SARS-CoVs: A Microcanonical Analysis of\n  Receptor-Binding Motifs",
        "PVTree: Realistic and Controllable Palm Vein Generation for Recognition\n  Tasks",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "Generative Learning of Densities on Manifolds",
        "scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation\n  Model Knowledge",
        "Learning Cascade Ranking as One Network",
        "Ball Lightning as a profound manifestation of the Dark Matter physics",
        "Covariate Adjusted Response Adaptive Design with Delayed Outcomes",
        "Core-radius approximation of singular minimizers in nonlinear elasticity",
        "Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees",
        "Tensor Product in the Category of Effect Algebras",
        "Performance-driven Constrained Optimal Auto-Tuner for MPC",
        "Developing a Network Discovery Protocol for the Constellation Control\n  and Data Acquisition Framework",
        "Prediction-Powered Inference with Imputed Covariates and Nonuniform\n  Sampling",
        "Computational Safety for Generative AI: A Signal Processing Perspective",
        "Uniqueness of Weak Solutions to One-Dimensional Doubly Degenerate\n  Cross-Diffusion System",
        "A Modal-Based Approach for System Frequency Response and Frequency Nadir\n  Prediction",
        "A Non-Relativistic Limit for Heterotic Supergravity and its Gauge\n  Lagrangian",
        "A Faster Algorithm for Constrained Correlation Clustering"
      ],
      "abstract":[
        "What if artificial intelligence could not only solve problems for which it\nwas trained but also learn to teach itself to solve new problems (i.e.,\nmeta-learn)? In this study, we demonstrate that a pre-trained transformer\nfine-tuned with reinforcement learning over multiple episodes develops the\nability to solve problems that it has never encountered before - an emergent\nability called In-Context Reinforcement Learning (ICRL). This powerful\nmeta-learner not only excels in solving unseen in-distribution environments\nwith remarkable sample efficiency, but also shows strong performance in\nout-of-distribution environments. In addition, we show that it exhibits\nrobustness to the quality of its training data, seamlessly stitches together\nbehaviors from its context, and adapts to non-stationary environments. These\nbehaviors demonstrate that an RL-trained transformer can iteratively improve\nupon its own solutions, making it an excellent general-purpose problem solver.",
        "We present a novel cosmological framework that unifies matter creation\ndynamics with thermodynamic principles. Starting with a single-component fluid\ncharacterized by a constant equation of state parameter, $\\omega$, we introduce\na generalized second law of thermodynamics by considering the entropy\nassociated with the cosmic horizon. Imposing an adiabatic expansion condition\nuniquely determines the particle creation rate, $\\Gamma$, a feature\nunprecedented in previous matter creation models. This mechanism yields a\ncosmology featuring phantom-like expansion while relying solely on a single\nconstituent, which can be either a quintessence-like fluid or a non-exotic,\nnon-relativistic dark matter component. Remarkably, this framework avoids the\nneed for exotic physics while providing a consistent explanation for the\naccelerated expansion of the universe. Our results open new pathways for\nunderstanding the interplay between horizon thermodynamics, particle creation,\nand cosmic evolution, offering fresh insights into the nature of dark energy\nand its potential thermodynamic origins.",
        "Although 85% of sub-Saharan Africa's population is covered by mobile\nbroadband signal, only 37% use the internet, and those who do seldom use the\nweb. The most frequently cited reason for low internet usage is the cost of\ndata. We investigate whether AI can bridge this gap by analyzing 40,350 queries\nsubmitted to an AI chatbot by 469 teachers in Sierra Leone over 17 months.\nTeachers use AI for teaching assistance more frequently than web search. We\ncompare the AI responses to the corresponding top search results for the same\nqueries from the most popular local web search engine, google.com.sl. Only 2%\nof results for corresponding web searches contain content from in country.\nAdditionally, the average web search result consumes 3,107 times more data than\nan AI response. Bandwidth alone costs \\$2.41 per thousand web search results\nloaded, while the total cost of AI is \\$0.30 per thousand responses. As a\nresult, AI is 87% less expensive than web search. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
        "With the advancement of serverless computing, running machine learning (ML)\ninference services over a serverless platform has been advocated, given its\nlabor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models\nhave been a dominant type of model architectures to enable large models\nnowadays, with parallel expert networks. Serving large MoE models on serverless\ncomputing is potentially beneficial, but has been underexplored due to\nsubstantial challenges in handling the skewed expert popularity and\nscatter-gather communication bottleneck in MoE model execution, for\ncost-efficient serverless MoE deployment and performance guarantee. We study\noptimized MoE model deployment and distributed inference serving on a\nserverless platform, that effectively predict expert selection, pipeline\ncommunication with model execution, and minimize the overall billed cost of\nserving MoE models. Especially, we propose a Bayesian optimization framework\nwith multi-dimensional epsilon-greedy search to learn expert selections and\noptimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian\ndecision-making method for predicting expert popularity; 2) flexibly pipelined\nscatter-gather communication; and 3) an optimal model deployment algorithm for\ndistributed MoE serving. Extensive experiments on AWS Lambda show that our\ndesigns reduce the billed cost of all MoE layers by at least 75.67% compared to\nCPU clusters while maintaining satisfactory inference throughput. As compared\nto LambdaML in serverless computing, our designs achieves 43.41% lower cost\nwith a throughput decrease of at most 18.76%.",
        "Temporal predictive models have the potential to improve decisions in health\ncare, public services, and other domains, yet they often fail to effectively\nsupport decision-makers. Prior literature shows that many misalignments between\nmodel behavior and decision-makers' expectations stem from issues of model\nspecification, namely how, when, and for whom predictions are made. However,\nmodel specifications for predictive tasks are highly technical and difficult\nfor non-data-scientist stakeholders to interpret and critique. To address this\nchallenge we developed Tempo, an interactive system that helps data scientists\nand domain experts collaboratively iterate on model specifications. Using\nTempo's simple yet precise temporal query language, data scientists can quickly\nprototype specifications with greater transparency about pre-processing\nchoices. Moreover, domain experts can assess performance within data subgroups\nto validate that models behave as expected. Through three case studies, we\ndemonstrate how Tempo helps multidisciplinary teams quickly prune infeasible\nspecifications and identify more promising directions to explore.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "Following a recent detection of TeV radiation by the Large High Altitude Air\nShower Observatory (LHAASO) and the High-Altitude Water Cherenkov Observatory\n(HAWC), coincident with the direction of the microquasar V4641 Sgr, we search\nfor possible GeV emission from this source. We explored the morphology and\ntemporal features of the source as well as two nearby unassociated point\nsources which could be a part of extended structure of V4641 Sgr, and compared\nresults with corresponding X-ray and TeV emissions. The 95% confidence level\nupper limits for the flux from the source, assuming both point and extended\nsource models were 5.38$\\times$ 10$^{-13}$ erg cm$^{-2}$ s$^{-1}$ and\n1.12$\\times$ 10$^{-12}$ erg cm$^{-2}$ s$^{-1}$, respectively. Additionally, no\ncorrelation between gamma-ray light curve and X-ray outbursts was observed.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "The late-stage evolution of massive stars is marked by intense instability as\nthey approach core-collapse. During these phases, giant stellar eruptions lead\nto exceptionally high mass-loss rates, forming significant amounts of dust.\nHowever, the survival of these dust grains is challenged by the powerful shock\nwaves generated when the progenitor explodes as a supernova (SN). We explore\nthe impact of hydrogen-rich SN explosions from 45, 50, and 60 M$_\\odot$\nprogenitors on dust formed after these eruptions, focusing on interactions with\ncircumstellar shells occurring from a few years to centuries after the event.\nUsing 3D hydrodynamical simulations, we track the evolution of dust particles\nin a scenario that includes the progenitor's stellar wind, a giant eruption,\nand the subsequent SN explosion, following the mass budgets predicted by\nstellar evolution models. For a standard SN ejecta mass of 10 M$_\\odot$ and\nkinetic energy of $10^{51}$ erg, only 25% of the dust mass survives 250 years\npost-explosion in a spherical circumstellar medium (CSM), while merely 2%\nremains a century after the explosion in a bipolar CSM. If the SN follows the\neruption within a dozen years, 75% of the dust survives for a standard\nexplosion, dropping to 20% for more massive ejecta (15-20 M$_\\odot$) with\nkinetic energy of $5 \\times 10^{51}$ erg. The geometry of the CSM and the early\ntransition of the SN remnant into a radiative phase significantly influence\ndust survival. As the shock wave weakens and efficiently converts kinetic\nenergy into thermal radiation (up to half of the injected kinetic energy) the\nlikelihood of dust survival increases, affecting not only pre-existing dust in\nthe CSM but also SN-condensed dust and ambient interstellar dust. Contrary to\nexpectations, a larger fraction of the dust mass can survive if the SN occurs\nonly a few years after the eruption.",
        "The rapid evolution and global impact of coronaviruses, notably SARS-CoV-1\nand SARS-CoV-2, underscore the importance of understanding their molecular\nmechanisms in detail. This study focuses on the receptor-binding motif (RBM)\nwithin the Spike protein of these viruses, a critical element for viral entry\nthrough interaction with the ACE2 receptor. We investigate the sequence\nvariations in the RBM across SARS-CoV-1, SARS-CoV-2 and its early variants of\nconcern (VOCs). Utilizing multicanonical simulations and microcanonical\nanalysis, we examine how these variations influence the folding dynamics,\nthermostability, and solubility of the RBMs. Our methodology includes\ncalculating the density of states (DoS) to identify structural phase\ntransitions and assess thermodynamic properties. Furthermore, we solve the\nPoisson-Boltzmann equation to model the solubility of the RBMs in aqueous\nenvironments. This methodology is expected to elucidate structural and\nfunctional differences in viral evolution and pathogenicity, likely improving\ntargeted treatments and vaccines.",
        "Palm vein recognition is an emerging biometric technology that offers\nenhanced security and privacy. However, acquiring sufficient palm vein data for\ntraining deep learning-based recognition models is challenging due to the high\ncosts of data collection and privacy protection constraints. This has led to a\ngrowing interest in generating pseudo-palm vein data using generative models.\nExisting methods, however, often produce unrealistic palm vein patterns or\nstruggle with controlling identity and style attributes. To address these\nissues, we propose a novel palm vein generation framework named PVTree. First,\nthe palm vein identity is defined by a complex and authentic 3D palm vascular\ntree, created using an improved Constrained Constructive Optimization (CCO)\nalgorithm. Second, palm vein patterns of the same identity are generated by\nprojecting the same 3D vascular tree into 2D images from different views and\nconverting them into realistic images using a generative model. As a result,\nPVTree satisfies the need for both identity consistency and intra-class\ndiversity. Extensive experiments conducted on several publicly available\ndatasets demonstrate that our proposed palm vein generation method surpasses\nexisting methods and achieves a higher TAR@FAR=1e-4 under the 1:1 Open-set\nprotocol. To the best of our knowledge, this is the first time that the\nperformance of a recognition model trained on synthetic palm vein data exceeds\nthat of the recognition model trained on real data, which indicates that palm\nvein image generation research has a promising future.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "A generative modeling framework is proposed that combines diffusion models\nand manifold learning to efficiently sample data densities on manifolds. The\napproach utilizes Diffusion Maps to uncover possible low-dimensional underlying\n(latent) spaces in the high-dimensional data (ambient) space. Two approaches\nfor sampling from the latent data density are described. The first is a\nscore-based diffusion model, which is trained to map a standard normal\ndistribution to the latent data distribution using a neural network. The second\none involves solving an It\\^o stochastic differential equation in the latent\nspace. Additional realizations of the data are generated by lifting the samples\nback to the ambient space using Double Diffusion Maps, a recently introduced\ntechnique typically employed in studying dynamical system reduction; here the\nfocus lies in sampling densities rather than system dynamics. The proposed\napproaches enable sampling high dimensional data densities restricted to\nlow-dimensional, a priori unknown manifolds. The efficacy of the proposed\nframework is demonstrated through a benchmark problem and a material with\nmultiscale structure.",
        "Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task.",
        "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances such as RankFlow and FS-LTR have introduced\ninteraction-aware training paradigms but still struggle to 1) align training\nobjectives with the goal of the entire cascade ranking (i.e., end-to-end\nrecall) and 2) learn effective collaboration patterns for different stages. To\naddress these challenges, we propose LCRON, which introduces a novel surrogate\nloss function derived from the lower bound probability that ground truth items\nare selected by cascade ranking, ensuring alignment with the overall objective\nof the system. According to the properties of the derived bound, we further\ndesign an auxiliary loss for each stage to drive the reduction of this bound,\nleading to a more robust and effective top-k selection. LCRON enables\nend-to-end training of the entire cascade ranking system as a unified network.\nExperimental results demonstrate that LCRON achieves significant improvement\nover existing methods on public benchmarks and industrial applications,\naddressing key limitations in cascade ranking training and significantly\nenhancing system performance.",
        "Ball lighting (BL) has been observed for centuries. There are large number of\nbooks, review articles, and original scientific papers devoted to different\naspects of BL phenomenon. Yet, the basic features of this phenomenon have never\nbeen explained by known physics. The main problem is the source which could\npower the dynamics of the BL. We advocate an idea that the dark matter in form\nof the axion quark nuggets (AQN) made of standard model quarks and gluons\n(similar to the old idea of the Witten's strangelets) could internally generate\nthe required power. The corresponding macroscopically large object in form of\nthe AQN behaves as {\\it chameleon}: it does not interact with the surrounding\nmaterial in dilute environment and serves as perfect cold DM candidate.\nHowever, AQN becomes strongly interacting object in sufficiently dense\nenvironment. The AQN model was invented long ago without any relation to the BL\nphysics. It was invented with a single motivation to explain the observed\nsimilarity $\\Omega_{\\rm DM}\\sim \\Omega_{\\rm visible}$ between visible and DM\ncomponents. This relation represents a very generic feature of this framework,\nnot sensitive to any parameters of the construction. However, with the same set\nof parameters being fixed long ago this model is capable to address the key\nelements of the BL phenomenology, including the source of the energy powering\nthe BL events. In particular, we argue that the visible size of BL, its typical\nlife time, the frequency of appearance , etc are all consistent with suggested\nproposal when BL represents a profound manifestation of the DM physics\nrepresented by the AQN objects. We also formulate a unique possible test which\ncan refute or unambiguously substantiate this unorthodox proposal on nature of\nBL.",
        "Covariate-adjusted response adaptive (CARA) designs have gained widespread\nadoption for their clear benefits in enhancing experimental efficiency and\nparticipant welfare. These designs dynamically adjust treatment allocations\nduring interim analyses based on participant responses and covariates collected\nduring the experiment. However, delayed responses can significantly compromise\nthe effectiveness of CARA designs, as they hinder timely adjustments to\ntreatment assignments when certain participant outcomes are not immediately\nobserved. In this manuscript, we propose a fully forward-looking CARA design\nthat dynamically updates treatment assignments throughout the experiment as\nresponse delay mechanisms are progressively estimated. Our design strategy is\ninformed by novel semiparametric efficiency calculations that explicitly\naccount for outcome delays in a multi-stage adaptive experiment. Through both\ntheoretical investigations and simulation studies, we demonstrate that our\nproposed design offers a robust solution for handling delayed outcomes in CARA\ndesigns, yielding significant improvements in both statistical power and\nparticipant welfare.",
        "We study a variational model in nonlinear elasticity allowing for cavitation\nwhich penalizes both the volume and the perimeter of the cavities.\nSpecifically, we investigate the approximation (in the sense of\n{\\Gamma}-convergence) of the energy by means of functionals defined on\nperforated domains. Perforations are introduced at flaw points where\nsingularities are expected and, hence, the corresponding deformations do not\nexhibit cavitation. Notably, those points are not prescribed but rather\nselected by the variational principle. Our analysis is motivated by the\nnumerical simulation of cavitation and extends previous results on models which\nsolely accounted for elastic energy but neglected contributions related to the\nformation of cavities.",
        "Differential privacy (DP) provides a principled approach to synthesizing data\n(e.g., loads) from real-world power systems while limiting the exposure of\nsensitive information. However, adversaries may exploit synthetic data to\ncalibrate cyberattacks on the source grids. To control these risks, we propose\nnew DP algorithms for synthesizing data that provide the source grids with both\ncyber resilience and privacy guarantees. The algorithms incorporate both normal\noperation and attack optimization models to balance the fidelity of synthesized\ndata and cyber resilience. The resulting post-processing optimization is\nreformulated as a robust optimization problem, which is compatible with the\nexponential mechanism of DP to moderate its computational burden.",
        "We study a tensor product in the category of effect algebras and in the\ncategory of partially ordered Abelian groups with order unit. We show that the\ntensor product preserves all the constructions that are essentially colimits\nover a connected diagram. Further, we prove the construction of a universal\ngroup for an effect algebra preserves all tensor products. We establish the\ncorresponding functor from the category of effect algebras to the category of\nunital Abelian po-groups as a strong monoidal functor. We note that the\ntechnique we use in establishing the result could be used in various similar\nsituations. Finally, we show that the tensor product of effect algebras does\nnot preserve the Riesz decomposition property, which was an open question for a\nwhile.",
        "A key challenge in tuning Model Predictive Control (MPC) cost function\nparameters is to ensure that the system performance stays consistently above a\ncertain threshold. To address this challenge, we propose a novel method,\nCOAT-MPC, Constrained Optimal Auto-Tuner for MPC. With every tuning iteration,\nCOAT-MPC gathers performance data and learns by updating its posterior belief.\nIt explores the tuning parameters' domain towards optimistic parameters in a\ngoal-directed fashion, which is key to its sample efficiency. We theoretically\nanalyze COAT-MPC, showing that it satisfies performance constraints with\narbitrarily high probability at all times and provably converges to the optimum\nperformance within finite time. Through comprehensive simulations and\ncomparative analyses with a hardware platform, we demonstrate the effectiveness\nof COAT-MPC in comparison to classical Bayesian Optimization (BO) and other\nstate-of-the-art methods. When applied to autonomous racing, our approach\noutperforms baselines in terms of constraint violations and cumulative regret\nover time.",
        "Qualifying new detectors in test beam environments presents a challenging\nsetting that requires stable operation of diverse devices, often employing\nmultiple data acquisition systems. Changes to these setups are frequent, such\nas using different reference detectors depending on the facility. Managing this\ncomplexity necessitates a system capable of controlling the data taking,\nmonitoring the experimental setup, facilitating seamless configuration, and\neasy integration of new devices. One aspect of such systems is network\nconfiguration. Many systems require fixed IP addresses for all machines\nparticipating in the data acquisition, which adds complexity for users. In this\npaper, a network protocol for network discovery tailored towards\nnetwork-distributed control and data acquisition systems is described.",
        "Machine learning models are increasingly used to produce predictions that\nserve as input data in subsequent statistical analyses. For example, computer\nvision predictions of economic and environmental indicators based on satellite\nimagery are used in downstream regressions; similarly, language models are\nwidely used to approximate human ratings and opinions in social science\nresearch. However, failure to properly account for errors in the machine\nlearning predictions renders standard statistical procedures invalid. Prior\nwork uses what we call the Predict-Then-Debias estimator to give valid\nconfidence intervals when machine learning algorithms impute missing variables,\nassuming a small complete sample from the population of interest. We expand the\nscope by introducing bootstrap confidence intervals that apply when the\ncomplete data is a nonuniform (i.e., weighted, stratified, or clustered) sample\nand to settings where an arbitrary subset of features is imputed. Importantly,\nthe method can be applied to many settings without requiring additional\ncalculations. We prove that these confidence intervals are valid under no\nassumptions on the quality of the machine learning model and are no wider than\nthe intervals obtained by methods that do not use machine learning predictions.",
        "AI safety is a rapidly growing area of research that seeks to prevent the\nharm and misuse of frontier AI technology, particularly with respect to\ngenerative AI (GenAI) tools that are capable of creating realistic and\nhigh-quality content through text prompts. Examples of such tools include large\nlanguage models (LLMs) and text-to-image (T2I) diffusion models. As the\nperformance of various leading GenAI models approaches saturation due to\nsimilar training data sources and neural network architecture designs, the\ndevelopment of reliable safety guardrails has become a key differentiator for\nresponsibility and sustainability. This paper presents a formalization of the\nconcept of computational safety, which is a mathematical framework that enables\nthe quantitative assessment, formulation, and study of safety challenges in\nGenAI through the lens of signal processing theory and methods. In particular,\nwe explore two exemplary categories of computational safety challenges in GenAI\nthat can be formulated as hypothesis testing problems. For the safety of model\ninput, we show how sensitivity analysis and loss landscape analysis can be used\nto detect malicious prompts with jailbreak attempts. For the safety of model\noutput, we elucidate how statistical signal processing and adversarial learning\ncan be used to detect AI-generated content. Finally, we discuss key open\nresearch challenges, opportunities, and the essential role of signal processing\nin computational AI safety.",
        "The uniqueness of global weak solutions to one-dimensional doubly degenerate\ncross-diffusion system is shown. The equations model the evolution of feeding\nbacterial populations in a malnourished environment. The key idea of the proof\nis applying anti-derivative of the sum of weak solutions to the system.",
        "This letter introduces a novel approach for predicting system frequency\nresponse and frequency nadir by leveraging modal information. It significantly\ndifferentiates from traditional methods rooted in the average system frequency\nmodel. The proposed methodology targets system modes associated with the slower\ndynamics of the grid, enabling precise predictions through modal decomposition\napplied to the full system model. This decomposition facilitates an analytical\nsolution for the frequency at the center of inertia, resulting in highly\naccurate predictions of both frequency response and nadir. Numerical results\nfrom a 39-bus, 10-machine test system verify the method's effectiveness and\naccuracy. This methodology represents a shift from observing a simplified\naverage system frequency response to a more detailed analysis focusing on\nsystem dynamics.",
        "We show that the non-relativistic (NR) limit of $D=10$ heterotic supergravity\nhas a finite gauge Lagrangian due to non-trivial cancellations of divergent\nparts coming from the Chern-Simons terms in the curvature of the $\\hat B$-field\nand the Yang-Mills Lagrangian. This is similar to what happens in bosonic\nsupergravity between the Ricci scalar, $\\hat R$, and the $- \\frac{1}{12} \\hat\nH^2$ term after taking the same limit. In this work we present the explicit\nform of the gauge transformations and curvatures after considering the NR limit\nand we compute the finite gauge Lagrangian in its covariant form. As an\ninteresting property, the Green-Schwarz mechanism for the two-form can be\ntrivialized in this limit. Terms equivalent to Chern-Simons contributions\nnaturally arise from the previous property.",
        "In the Correlation Clustering problem we are given $n$ nodes, and a\npreference for each pair of nodes indicating whether we prefer the two\nendpoints to be in the same cluster or not. The output is a clustering inducing\nthe minimum number of violated preferences. In certain cases, however, the\npreference between some pairs may be too important to be violated. The\nconstrained version of this problem specifies pairs of nodes that must be in\nthe same cluster as well as pairs that must not be in the same cluster (hard\nconstraints). The output clustering has to satisfy all hard constraints while\nminimizing the number of violated preferences.\n  Constrained Correlation Clustering is APX-Hard and has been approximated\nwithin a factor 3 by van Zuylen et al. [SODA '07] using $\\Omega(n^{3\\omega})$\ntime. In this work, using a more combinatorial approach, we show how to\napproximate this problem significantly faster at the cost of a slightly weaker\napproximation factor. In particular, our algorithm runs in $\\widetilde{O}(n^3)$\ntime and approximates Constrained Correlation Clustering within a factor 16.\n  To achieve our result we need properties guaranteed by a particular\ninfluential algorithm for (unconstrained) Correlation Clustering, the CC-PIVOT\nalgorithm. This algorithm chooses a pivot node $u$, creates a cluster\ncontaining $u$ and all its preferred nodes, and recursively solves the rest of\nthe problem. As a byproduct of our work, we provide a derandomization of the\nCC-PIVOT algorithm that still achieves the 3-approximation; furthermore, we\nshow that there exist instances where no ordering of the pivots can give a\n$(3-\\varepsilon)$-approximation, for any constant $\\varepsilon$.\n  Finally, we introduce a node-weighted version of Correlation Clustering,\nwhich can be approximated within factor 3 using our insights on Constrained\nCorrelation Clustering."
      ]
    }
  },
  {
    "id":2411.17342,
    "research_type":"applied",
    "start_id":"b35",
    "start_title":"Deep learning-based framework for automatic cranial defect reconstruction and implant modeling",
    "start_abstract":"This article presents a robust, fast, and fully automatic method for personalized cranial defect reconstruction implant modeling.We propose two-step deep learning-based using modified U-Net architecture to perform the reconstruction, dedicated iterative procedure improve geometry, followed by an generation of models ready 3-D printing. We cross-case augmentation based on imperfect image registration combining cases from different datasets. Additional ablation studies compare strategies other state-of-the-art methods.We evaluate three datasets introduced during AutoImplant 2021 challenge, organized jointly with MICCAI conference. quantitative evaluation Dice boundary coefficients, Hausdorff distance. The coefficient, 95th percentile distance averaged across all test sets, are 0.91, 0.94, 1.53 mm respectively. additional qualitative printing visualization in mixed reality confirm implant's usefulness.The proposes complete pipeline that enables one create model described is greatly extended version scored 1st place challenge tasks. freely release source code, which together open datasets, makes results reproducible. defects may enable manufacturing implants significantly shorter time, possibly allowing process directly given intervention. Moreover, we show usability further reduce surgery time.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Reconstruction of cranial defect with patient-specific implants: Four different cost-effective techniques"
      ],
      "abstract":[
        "Cranial defects secondary to trauma, surgery or pathological causes, result in large cranial imperfection, which affects the appearance of patient as well results sinking flap syndrome. Rehabilitation such a defect can be done using prosthetic options like custom-made polymethyl methacrylate (PMMA) prosthesis surgical outer table calvarial graft segments. It is usually observed that conventional moulage impression defective site most difficult task. The accuracy affected by impression, cast and techniques fabricating wax pattern. Orthodox method mark tentative outline make site. However, this an arbitrary offers challenges accurate replication borders defect. Recently, medical imaging digital modeling dentistry have paved way for dental practice additive manufacturing replacing manual subtractive procedures. use computerized tomography scan obtain 3 D image replica with rapid prototyping has markedly improved at margin defect\/prosthesis interface, resulting better fit optimal contour lending itself esthetic outcome. more reliable implant prosthesis, requires minimum adjustment when on OT table. These case reports compare rehabilitation PMMA methods technique. seen expensive but gives"
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Constraining the curvature-induced quantum gravity scales via gamma-ray\n  bursts",
        "Neural Chaos: A Spectral Stochastic Neural Operator",
        "Constructing balanced datasets for predicting failure modes in\n  structural systems under seismic hazards",
        "On rigid regular graphs and a problem of Babai and Pultr",
        "Groupoids, equivalence bibundles and bimodules for noncommutative\n  solenoids",
        "The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship",
        "Moduli of curves and moduli of sheaves",
        "Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman\n  model",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Human fields and their impact on brain waves A pilot study",
        "Highly efficient field-free switching by orbital Hall torque in a\n  MoS2-based device operating at room temperature",
        "Ergodic optimization for beta-transformations",
        "A non-homogeneous, non-stationary and path-dependent Markov anomalous\n  diffusion model",
        "Computational Complexity of Covering Colored Mixed Multigraphs with\n  Simple Degree Partitions",
        "Multi-scale Energy Release Events in the Quiet Sun: A Possible Source\n  for Coronal Heating",
        "Notes on a special order on $\\mathbb{Z}^\\infty$",
        "Wasserstein-based Kernels for Clustering: Application to Power\n  Distribution Graphs",
        "Non-stop Variability of Sgr A* using JWST at 2.1 and 4.8 micron\n  Wavelengths: Evidence for Distinct Populations of Faint and Bright Variable\n  Emission",
        "A tropical approach to rigidity: counting realisations of frameworks",
        "Spectral heat content for non-isotropic L\\'evy processes with weak lower\n  scaling condition",
        "Differentially Private Sequential Learning",
        "Local doping of an oxide semiconductor by voltage-driven splitting of\n  anti-Frenkel defects",
        "Revisiting Projection-Free Online Learning with Time-Varying Constraints",
        "Generic jet evaluation transversality of contact instantons against\n  contact distribution",
        "A 1.8 m class pathfinder Raman LIDAR for the Northern Site of the\n  Cherenkov Telescope Array Observatory -- Technical Design",
        "Benefits of Mutual Coupling in Dynamic Metasurface Antennas for\n  Optimizing Wireless Communications -- Theory and Experimental Validation",
        "On the nature of rubber wear",
        "Trotter error mitigation by error profiling with shallow quantum circuit",
        "Physics Informed Neural Networks for Learning the Horizon Size in\n  Bond-Based Peridynamic Models"
      ],
      "abstract":[
        "We constrain the parameters that govern curvature-induced quantum gravity\ntime-of-flight (TOF) effects. These TOF delays, which occur due to modified\ndispersion relations of particles in a vacuum, could be a phenomenological\nsignature of quantum gravity. Gamma-ray bursts (GRBs), short, high-energy\nevents from distant galaxies, offer a unique opportunity to impose\nobservational limits on TOF delays and, by extension, on the energy scales of\nquantum gravity. Using the standard Jacob-Piran relation, which assumes a\nlocally-flat spacetime, the analysis of quantum gravity-induced TOF effects\nestablishes a lower limit of approximately 10 Planck energies on the energy\nscale of these effects. However, curvature-induced quantum gravity effects may\nintroduce additional contributions. From current GRB observations, we find\nthat, at a 95% credibility level, in the symmetry-deformed scenario,\ncurvature-induced TOF effects may only arise at energies above 0.04 Planck\nenergy. If we consider only curvature-induced effects, this limit is an order\nof magnitude stronger. Observing more GRBs at different redshifts could improve\nthe constraints on the curvature-induced QG phenomena. However, given the\ncapabilities of current telescopes and the current understanding of GRBs, it is\nunlikely that these constraints will be significantly extended beyond the\npresent level.",
        "Building surrogate models with uncertainty quantification capabilities is\nessential for many engineering applications where randomness, such as\nvariability in material properties, is unavoidable. Polynomial Chaos Expansion\n(PCE) is widely recognized as a to-go method for constructing stochastic\nsolutions in both intrusive and non-intrusive ways. Its application becomes\nchallenging, however, with complex or high-dimensional processes, as achieving\naccuracy requires higher-order polynomials, which can increase computational\ndemands and or the risk of overfitting. Furthermore, PCE requires specialized\ntreatments to manage random variables that are not independent, and these\ntreatments may be problem-dependent or may fail with increasing complexity. In\nthis work, we adopt the spectral expansion formalism used in PCE; however, we\nreplace the classical polynomial basis functions with neural network (NN) basis\nfunctions to leverage their expressivity. To achieve this, we propose an\nalgorithm that identifies NN-parameterized basis functions in a purely\ndata-driven manner, without any prior assumptions about the joint distribution\nof the random variables involved, whether independent or dependent. The\nproposed algorithm identifies each NN-parameterized basis function\nsequentially, ensuring they are orthogonal with respect to the data\ndistribution. The basis functions are constructed directly on the joint\nstochastic variables without requiring a tensor product structure. This\napproach may offer greater flexibility for complex stochastic models, while\nsimplifying implementation compared to the tensor product structures typically\nused in PCE to handle random vectors. We demonstrate the effectiveness of the\nproposed scheme through several numerical examples of varying complexity and\nprovide comparisons with classical PCE.",
        "Accurate prediction of structural failure modes under seismic excitations is\nessential for seismic risk and resilience assessment. Traditional\nsimulation-based approaches often result in imbalanced datasets dominated by\nnon-failure or frequently observed failure scenarios, limiting the\neffectiveness in machine learning-based prediction. To address this challenge,\nthis study proposes a framework for constructing balanced datasets that include\ndistinct failure modes. The framework consists of three key steps. First,\ncritical ground motion features (GMFs) are identified to effectively represent\nground motion time histories. Second, an adaptive algorithm is employed to\nestimate the probability densities of various failure domains in the space of\ncritical GMFs and structural parameters. Third, samples generated from these\nprobability densities are transformed into ground motion time histories by\nusing a scaling factor optimization process. A balanced dataset is constructed\nby performing nonlinear response history analyses on structural systems with\nparameters matching the generated samples, subjected to corresponding\ntransformed ground motion time histories. Deep neural network models are\ntrained on balanced and imbalanced datasets to highlight the importance of\ndataset balancing. To further evaluate the framework's applicability, numerical\ninvestigations are conducted using two different structural models subjected to\nrecorded and synthetic ground motions. The results demonstrate the framework's\nrobustness and effectiveness in addressing dataset imbalance and improving\nmachine learning performance in seismic failure mode prediction.",
        "A graph is \\textit{rigid} if it only admits the identity endomorphism. We\nshow that for every $d\\ge 3$ there exist infinitely many mutually rigid\n$d$-regular graphs of arbitrary odd girth $g\\geq 7$. Moreover, we determine the\nminimum order of a rigid $d$-regular graph for every $d\\ge 3$. This provides\nstrong positive answers to a question of van der Zypen\n[https:\/\/mathoverflow.net\/q\/296483, https:\/\/mathoverflow.net\/q\/321108].\nFurther, we use our construction to show that every finite monoid is isomorphic\nto the endomorphism monoid of a regular graph. This solves a problem of Babai\nand Pultr [J. Comb.~Theory, Ser.~B, 1980].",
        "Let $p$ be a prime number and $\\mathcal{S}_p$ the $p$-solenoid. For\n$\\alpha\\in \\mathbb{R}\\times \\mathbb{Q}_p$ we consider in this paper a naturally\nassociated action groupoid $S_\\alpha:=\\mathbb{Z} [1\/p]\\ltimes_\\alpha\n\\mathcal{S}_p \\rightrightarrows \\mathcal{S}_p$ whose $C^*-$algebra is a model\nfor the noncommutative solenoid $\\mathcal{A}_\\alpha^\\mathscr{S}$ studied by\nLatremoli\\`ere and Packer. Following the geometric ideas of Connes and Rieffel\nto describe the Morita equivalences of noncommutative torus using the Kronecker\nfoliation on the torus, we give an explicit description of the\ngeometric\/topologic equivalence bibundle for groupoids $S_\\alpha$ and $S_\\beta$\nwhenever $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ are in the same orbit\nof the $GL_2(\\mathbb{Z}[1\/p])$ action by linear fractional transformations. As\na corollary, for $\\alpha,\\beta\\in \\mathbb{R}\\times \\mathbb{Q}_p$ as above we\nget an explicit description of the imprimitivity bimodules for the associated\nnoncommutative solenoids.",
        "This paper presents the AI Enabled Individual Entrepreneurship Theory (AIET),\na theoretical framework explaining how artificial intelligence technologies\ntransform individual entrepreneurial capability. The theory identifies two\nfoundational premises: knowledge democratization and resource requirements\nevolution. Through three core mechanisms skill augmentation, capital structure\ntransformation, and risk profile modification AIET explains how individuals can\nnow undertake entrepreneurial activities at scales previously requiring\nsignificant organizational infrastructure. The theory presents five testable\npropositions addressing the changing relationship between organizational size\nand competitive advantage, the expansion of individual entrepreneurial\ncapacity, the transformation of market entry barriers, the evolution of\ntraditional firm advantages, and the modification of entrepreneurial risk\nprofiles. Boundary conditions related to task characteristics and market\nconditions define the theory's scope and applicability. The framework suggests\nsignificant implications for entrepreneurship theory, organizational design,\nand market structure as AI capabilities continue to advance. This theory\nprovides a foundation for understanding the evolving landscape of\nentrepreneurship in an AI-enabled world.",
        "Relationships between moduli spaces of curves and sheaves on 3-folds are\npresented starting with the Gromov-Witten\/Donaldson-Thomas correspondence\nproposed more than 20 years ago with D. Maulik, N. Nekrasov, and A. Okounkov.\nThe descendent and relative correspondences as developed with A. Pixton in the\ncontext of stable pairs led to the proof of the correspondence for the\nCalabi-Yau quintic 3-fold. More recently, the study of correspondences in\nfamilies has played an important role in connection with other basic moduli\nproblems in algebraic geometry. The full conjectural framework is presented\nhere in the context of families of 3-folds. This article accompanies my lecture\nat the ICBS in July 2024.",
        "The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical\nmodel in lithium-ion battery research. Since it is a highly nonlinear model,\nits input-output relations are still poorly understood. Researchers therefore\noften employ sensitivity analyses to elucidate relative parametric importance\nfor certain use cases. However, some methods are ill-suited for the complexity\nof the model and appropriate methods often face the downside of only being\napplicable to scalar quantities of interest. We implement a novel framework for\nglobal sensitivity analysis of time-dependent model outputs and apply it to a\ndrive cycle simulation. We conduct a full and a subgroup sensitivity analysis\nto resolve lowly sensitive parameters and explore the model error when\nunimportant parameters are set to arbitrary values. Our findings suggest that\nthe method identifies insensitive parameters whose variations cause only small\ndeviations in the voltage response of the model. By providing the methodology,\nwe hope research questions related to parametric sensitivity for time-dependent\nquantities of interest, such as voltage responses, can be addressed more easily\nand adequately in simulative battery research and beyond.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "During brain function, groups of neurons fire synchronously. When these\ngroups are large enough, the resulting electrical signals can be measured on\nthe scalp using Electroencephalography (EEG). The amplitude of these signals\ncan be significant depending on the size and synchronization of the neural\nactivity. EEG waves exhibit distinct patterns based on the brain's state, such\nas whether it is asleep, awake, engaged in mental calculations, or performing\nother cognitive functions. Additionally, these patterns can be modified by\nexternal factors, such as transcranial magnetic stimulation (TMS). TMS involves\nbringing an antenna that generates variable electromagnetic fields close to\nspecific areas of the skull to treat certain pathologies. Given that the human\nbody naturally generates magnetic fields, a question arises: Can these fields\ninfluence the EEG by modulating neuronal function, causing a resonance effect,\nor through some unknown interaction? This study investigated whether\napproaching the palm of the hand to the top of the head (Intervention) could\ninduce effects in the EEG. Power Spectral Density (PSD) was obtained for the 30\nseconds preceding the intervention (PSD_pre) and the final 30 seconds of the\nintervention (PSD_last). The exact Wilcoxon signed-rank test suggests that the\nmedian of PSD_pre is greater than the median of PSD_last at the 95% confidence\nlevel (p-value = 0.004353). In contrast, in the control group, the test\nindicates that at the 95% confidence level (p-value = 0.7667), the median of\nPSD_pre is not greater than the median of PSD_last.",
        "Charge-to-spin and spin-to-charge conversion mechanisms in high spin-orbit\nmaterials are the new frontier of memory devices. They operate via spin-orbit\ntorque (SOT) switching of a magnetic electrode, driven by an applied charge\ncurrent. In this work, we propose a novel memory device based on the\nsemiconducting two-dimensional centrosymmetric transition metal dichalcogenide\n(TMD) MoS2, that operates as a SOT device in the writing process and a spin\nvalve in the reading process. We demonstrate that stable voltage states at room\ntemperature can be deterministically controlled by a switching current density\nas low as 3.2x10^4 A\/cm^2 even in zero field. An applied field 50-100 Oe can be\nused as a further or alternative control parameter for the state switching. Ab\ninitio calculations of spin Hall effect (SHE) and orbital Hall effect (OHE)\nindicate that the latter is the only one responsible for the generation of the\nSOT in the magnetic electrode. The large value of OHC in bulk MoS2 makes our\ndevice competitive in terms of energetic efficiency and could be integrated in\nTMD heterostructures to design memory devices with multiple magnetization\nstates for non-Boolean computation.",
        "Ergodic optimization for beta-transformations $T_\\beta(x)= \\beta x \\pmod 1$\nis developed. If $\\beta>1$ is a beta-number, or such that the orbit-closure of\n$1$ is not minimal, we show that the Typically Periodic Optimization Conjecture\nholds, establishing that there exists an open dense set of H\\\"{o}lder\ncontinuous functions such that for each function in this set, there exists a\nunique maximizing measure, this measure is supported on a periodic orbit, and\nthe periodic locking property holds. It follows that typical periodic\noptimization is typical among the class of beta-transformations: it holds for a\nset of parameters $\\beta>1$ that is residual, and has full Lebesgue measure.",
        "A novel probabilistic framework for modelling anomalous diffusion is\npresented. The resulting process is Markovian, non-homogeneous, non-stationary,\nnon-ergodic, and state-dependent. The fundamental law governing this process is\ndriven by two opposing forces: one proportional to the current state,\nrepresenting the intensity of autocorrelation or contagion, and another\ninversely proportional to the elapsed time, acting as a damping function. The\ninterplay between these forces determines the diffusion regime, characterized\nby the ratio of their proportionality coefficients. This framework encompasses\nvarious regimes, including subdiffusion, Brownian non-Gaussian, superdiffusion,\nballistic, and hyperballistic behaviours. The hyperballistic regime emerges\nwhen the correlation force dominates over damping, whereas a balance between\nthese mechanisms results in a ballistic regime, which is also stationary.\nCrucially, non-stationarity is shown to be necessary for regimes other than\nballistic. The model's ability to describe hyperballistic phenomena has been\ndemonstrated in applications such as epidemics, software reliability, and\nnetwork traffic. Furthermore, deviations from Gaussianity are explored and\nviolations of the Central Limit Theorem are highlighted, supported by\ntheoretical analysis and simulations. It will also be shown that the model\nexhibits a strong autocorrelation structure due to a position dependent jump\nprobability.",
        "The notion of graph covers (also referred to as locally bijective\nhomomorphisms) plays an important role in topological graph theory and has\nfound its computer science applications in models of local computation. For a\nfixed target graph $H$, the {\\sc $H$-Cover} problem asks if an input graph $G$\nallows a graph covering projection onto $H$. Despite the fact that the quest\nfor characterizing the computational complexity of {\\sc $H$-Cover} had been\nstarted more than 30 years ago, only a handful of general results have been\nknown so far.\n  In this paper, we present a complete characterization of the computational\ncomplexity of covering coloured graphs for the case that every equivalence\nclass in the degree partition of the target graph has at most two vertices. We\nprove this result in a very general form. Following the lines of current\ndevelopment of topological graph theory, we study graphs in the most relaxed\nsense of the definition. In particular, we consider graphs that are mixed (they\nmay have both directed and undirected edges), may have multiple edges, loops,\nand semi-edges. We show that a strong P\/NP-complete dichotomy holds true in the\nsense that for each such fixed target graph $H$, the {\\sc $H$-Cover} problem is\neither polynomial-time solvable for arbitrary inputs, or NP-complete even for\nsimple input graphs.",
        "The coronal heating problem remains one of the most challenging questions in\nsolar physics. The energy driving coronal heating is widely understood to be\nassociated with convective motions below the photosphere. Recent\nhigh-resolution observations reveal that photospheric magnetic fields in the\nquiet Sun undergo complex and rapid evolution. These photospheric dynamics are\nexpected to be reflected in the coronal magnetic field. Motivated by these\ninsights, our research aims to explore the relationship between magnetic energy\nand coronal heating. By combining observations from Solar Orbiter and SDO with\na magnetic field extrapolation technique, we estimate the magnetic free energy\nof multi-scale energy release events in the quiet Sun. Interestingly, our\nresults reveal a strong correlation between the evolution of free energy and\nthe integrated intensity of extreme ultraviolet emission at 171 \\AA~in these\nevents. We quantitatively assess the potential energy flux budget of these\nevents to evaluate their contribution to coronal heating. Our study implies a\nlink between photospheric magnetic field evolution and coronal temperature\nvariations, paving the way for further research into similar phenomena.",
        "In 1958, Helson and Lowdenslager extended the theory of analytic functions to\na general class of groups with ordered duals. In this context, analytic\nfunctions on such a group $G$ are defined as the integrable functions whose\nFourier coefficients lie in the positive semigroup of the dual of $G$. In this\npaper, we found some applications of their theory to infinite-dimensional\ncomplex analysis. Specifically, we considered a special order on\n$\\mathbb{Z}^\\infty$ and corresponding analytic continuous functions on\n$\\mathbb{T}^\\omega$, which serves as the counterpart of the disk algebra in\ninfinitely many variables setting. By characterizing its maximal ideals, we\nhave generalized the following theorem to the infinite-dimensional case: For a\npositive function $w$ that is integrable and log-integrable on $\\mathbb{T}^d$,\nthere exists an outer function $g$ such that $w=|g|^2$ if and only if the\nsupport of $\\hat{\\log w}$ is a subset of $\\mathbb{N}^d\\cap (-\\mathbb{N})^d$.\nFurthermore, we have found the counterpart of the above function algebra in the\nclosed right half-plane, and the representing measures of each point in the\nright half-plane for this algebra. As an application of the order, we provided\na new proof of the infinite-dimensional Szeg\\\"{o}'s theorem.",
        "Many data clustering applications must handle objects that cannot be\nrepresented as vector data. In this context, the bag-of-vectors representation\ncan be leveraged to describe complex objects through discrete distributions,\nand the Wasserstein distance can effectively measure the dissimilarity between\nthem. Additionally, kernel methods can be used to embed data into feature\nspaces that are easier to analyze. Despite significant progress in data\nclustering, a method that simultaneously accounts for distributional and\nvectorial dissimilarity measures is still lacking. To tackle this gap, this\nwork explores kernel methods and Wasserstein distance metrics to develop a\ncomputationally tractable clustering framework. The compositional properties of\nkernels allow the simultaneous handling of different metrics, enabling the\nintegration of both vectors and discrete distributions for object\nrepresentation. This approach is flexible enough to be applied in various\ndomains, such as graph analysis and image processing. The framework consists of\nthree main components. First, we efficiently approximate pairwise Wasserstein\ndistances using multiple reference distributions. Second, we employ kernel\nfunctions based on Wasserstein distances and present ways of composing kernels\nto express different types of information. Finally, we use the kernels to\ncluster data and evaluate the quality of the results using scalable and\ndistance-agnostic validity indices. A case study involving two datasets of 879\nand 34,920 power distribution graphs demonstrates the framework's effectiveness\nand efficiency.",
        "We present first results of JWST Cycle 1 and 2 observations of Sgr A* using\nNIRCam taken simultaneously at 2.1 and 4.8 micron for a total of ~48 hours over\nseven different epochs in 2023 and 2024. We find correlated variability at 2.1\nand 4.8 micron in all epochs, continual short-time scale (a few seconds)\nvariability and epoch-to-epoch variable emission implying long-term ( ~days to\nmonths) variability of Sgr A*. A highlight of this analysis is the evidence for\nsub-minute, horizon-scale time variability of Sgr A*, probing inner accretion\ndisk size scales. The power spectra of the light curves in each observing epoch\nalso indicate long-term variable emission. With continuous observations, JWST\ndata suggest that the flux of Sgr A* is fluctuating constantly. The flux\ndensity correlation exhibits a distinct break in the slope at ~3 mJy at 2.1\nmicron. The analysis indicates two different processes contributing to the\nvariability of Sgr A*. Brighter emission trends towards shallower spectral\nindices than the fainter emission. Cross correlation of the light curves\nindicates for the first time, a time delay of 3 - 40 sec in the 4.8 micron\nvariability with respect to 2.1 micron. This phase shift leads to loops in\nplots of flux density vs spectral index as the emission rises and falls.\nModeling suggests that the synchrotron emission from the evolving,\nage-stratified electron population reproduces the shape of the observed light\ncurves with a direct estimate of the magnetic field strengths in the range\nbetween 40-90 G, and upper cutoff energy, E_c, between 420 and 720 MeV.",
        "A realisation of a graph in the plane as a bar-joint framework is rigid if\nthere are finitely many other realisations, up to isometries, with the same\nedge lengths. Each of these finitely-many realisations can be seen as a\nsolution to a system of quadratic equations prescribing the distances between\npairs of points. For generic realisations, the size of the solution set depends\nonly on the underlying graph so long as we allow for complex solutions. We\nprovide a characterisation of the realisation number - that is the cardinality\nof this complex solution set - of a minimally rigid graph. Our characterisation\nuses tropical geometry to express the realisation number as an intersection of\nBergman fans of the graphic matroid. As a consequence, we derive a\ncombinatorial upper bound on the realisation number involving the Tutte\npolynomial. Moreover, we provide computational evidence that our upper bound is\nusually an improvement on the mixed volume bound.",
        "In this paper, we study the small-time asymptotic behavior of symmetric, but\nnot necessarily isotropic, L\\'evy processes with weak lower scaling condition\nnear zero on its L\\'evy density. Our main result, Theorem 2.1, extends and\ngeneralizes key findings in \\cite{KP24} and \\cite{PS22} by encompassing\nnon-isotropic L\\'evy processes and providing a unified proof that includes the\ncritical case in which the one-dimensional projection of the underlying\nprocesses is non-integrable. In particular, the main result recovers\n\\cite[Theorem 1.1]{PS22} for both $\\alpha\\in (1,2)$ and $\\alpha=1$ cases and\nprovide a robust proof that can be applied to study the small-time asymptotic\nbehavior of the spectral heat content for other interesting examples discussed\nin Section 4.",
        "In a differentially private sequential learning setting, agents introduce\nendogenous noise into their actions to maintain privacy. Applying this to a\nstandard sequential learning model leads to different outcomes for continuous\nvs. binary signals. For continuous signals with a nonzero privacy budget, we\nintroduce a novel smoothed randomized response mechanism that adapts noise\nbased on distance to a threshold, unlike traditional randomized response, which\napplies uniform noise. This enables agents' actions to better reflect both\nprivate signals and observed history, accelerating asymptotic learning speed to\n$\\Theta_{\\epsilon}(\\log(n))$, compared to $\\Theta(\\sqrt{\\log(n)})$ in the\nnon-private regime where privacy budget is infinite. Moreover, in the\nnon-private setting, the expected stopping time for the first correct decision\nand the number of incorrect actions diverge, meaning early agents may make\nmistakes for an unreasonably long period. In contrast, under a finite privacy\nbudget $\\epsilon \\in (0,1)$, both remain finite, highlighting a stark contrast\nbetween private and non-private learning. Learning with continuous signals in\nthe private regime is more efficient, as smooth randomized response enhances\nthe log-likelihood ratio over time, improving information aggregation.\nConversely, for binary signals, differential privacy noise hinders learning, as\nagents tend to use a constant randomized response strategy before an\ninformation cascade forms, reducing action informativeness and hampering the\noverall process.",
        "Layered oxides exhibit high ionic mobility and chemical flexibility,\nattracting interest as cathode materials for lithium-ion batteries and the\npairing of hydrogen production and carbon capture. Recently, layered oxides\nemerged as highly tunable semiconductors. For example, by introducing\nanti-Frenkel defects, the electronic hopping conductance in hexagonal\nmanganites was increased locally by orders of magnitude. Here, we demonstrate\nlocal acceptor and donor doping in Er(Mn,Ti)O$_3$, facilitated by the splitting\nof such anti-Frenkel defects under applied d.c. voltage. By combining density\nfunctional theory calculations, scanning probe microscopy, atom probe\ntomography, and scanning transmission electron microscopy, we show that the\noxygen defects readily move through the layered crystal structure, leading to\nnano-sized interstitial-rich (p-type) and vacancy-rich (n-type) regions. The\nresulting pattern is comparable to dipolar npn-junctions and stable on the\ntimescale of days. Our findings reveal the possibility of temporarily\nfunctionalizing oxide semiconductors at the nanoscale, giving additional\nopportunities for the field of oxide electronics and the development of\ntransient electronics in general.",
        "We investigate constrained online convex optimization, in which decisions\nmust belong to a fixed and typically complicated domain, and are required to\napproximately satisfy additional time-varying constraints over the long term.\nIn this setting, the commonly used projection operations are often\ncomputationally expensive or even intractable. To avoid the time-consuming\noperation, several projection-free methods have been proposed with an\n$\\mathcal{O}(T^{3\/4} \\sqrt{\\log T})$ regret bound and an $\\mathcal{O}(T^{7\/8})$\ncumulative constraint violation (CCV) bound for general convex losses. In this\npaper, we improve this result and further establish \\textit{novel} regret and\nCCV bounds when loss functions are strongly convex. The primary idea is to\nfirst construct a composite surrogate loss, involving the original loss and\nconstraint functions, by utilizing the Lyapunov-based technique. Then, we\npropose a parameter-free variant of the classical projection-free method,\nnamely online Frank-Wolfe (OFW), and run this new extension over the\nonline-generated surrogate loss. Theoretically, for general convex losses, we\nachieve an $\\mathcal{O}(T^{3\/4})$ regret bound and an $\\mathcal{O}(T^{3\/4} \\log\nT)$ CCV bound, both of which are order-wise tighter than existing results. For\nstrongly convex losses, we establish new guarantees of an\n$\\mathcal{O}(T^{2\/3})$ regret bound and an $\\mathcal{O}(T^{5\/6})$ CCV bound.\nMoreover, we also extend our methods to a more challenging setting with bandit\nfeedback, obtaining similar theoretical findings. Empirically, experiments on\nreal-world datasets have demonstrated the effectiveness of our methods.",
        "For a given coorientable contact manifold $(M,\\Xi)$ with contact distribution\n$\\Xi$, we consider its contact forms $\\lambda$ with $\\ker \\lambda = \\Xi$, and\nthe associated contact triads $(M,\\lambda, J)$. For a generic choice of contact\nform $\\lambda$, we prove the (0-jet) the interior and boundary evaluation maps,\nand the 1-jet transversality of contact instantons (against contact\ndistribution, for example).",
        "This paper presents the technical design of the pathfinder Barcelona Raman\nLIDAR (pBRL) for the northern site of the Cherenkov Telescope Array Observatory\n(CTAO-N) located at the Roque de los Muchachos Observatory (ORM). The pBRL is\ndeveloped for continuous atmospheric characterization, essential for correcting\nhigh-energy gamma-ray observations captured by Imaging Atmospheric Cherenkov\nTelescopes (IACTs). The LIDAR consists of a steerable telescope with a 1.8 m\nparabolic mirror and a pulsed Nd:YAG laser with frequency doubling and\ntripling. It emits at wavelengths of 355 nm and 532 nm to measure aerosol\nscattering and extinction through two elastic and Raman channels. Built upon a\nformer Cherenkov Light Ultraviolet Experiment (CLUE) telescope, the pBRL's\ndesign includes a Newtonian mirror configuration, a coaxial laser beam, a\nnear-range system, a liquid light guide and a custom-made polychromator. During\na one-year test at the ORM, the stability of the LIDAR and\nsemi-remote-controlled operations were tested. This pathfinder leads the way to\ndesigning a final version of a CTAO Raman LIDAR which will provide real-time\natmospheric monitoring and, as such, ensure the necessary accuracy of\nscientific data collected by the CTAO-N telescope array.",
        "Dynamic metasurface antennas (DMAs) are a promising embodiment of\nnext-generation reconfigurable antenna technology to realize base stations and\naccess points with reduced cost and power consumption. A DMA is a thin\nstructure patterned on its front with reconfigurable radiating metamaterial\nelements (meta-atoms) that are excited by waveguides or cavities. Mutual\ncoupling between the meta-atoms can result in a strongly non-linear dependence\nof the DMA's radiation pattern on the configuration of its meta-atoms. However,\nbesides the obvious algorithmic challenges of working with physics-compliant\nDMA models, it remains unclear how mutual coupling in DMAs influences the\nability to achieve a desired wireless functionality. In this paper, we provide\ntheoretical, numerical and experimental evidence that strong mutual coupling in\nDMAs increases the radiation pattern sensitivity to the DMA configuration and\nthereby boosts the available control over the radiation pattern, improving the\nability to tailor the radiation pattern to the requirements of a desired\nwireless functionality. Counterintuitively, we hence encourage next-generation\nDMA implementations to enhance (rather than suppress) mutual coupling, in\ncombination with suitable physics-compliant modeling and optimization. We\nexpect the unveiled mechanism by which mutual coupling boosts the radiation\npattern control to also apply to other reconfigurable antenna systems based on\ntunable lumped elements.",
        "Rubber wear results from the removal of small (micrometer-sized) rubber\nparticles through crack propagation. In this study, we investigate the wear\nbehavior of Styrene-Butadiene Rubber (SBR) and Natural Rubber (NR) sliding on\ntwo different concrete surfaces under dry and wet conditions. Experiments were\nconducted at low sliding speeds ($\\approx 1 \\ {\\rm mm\/s}$) to minimize\nfrictional heating and hydrodynamic effects. For two SBR compounds, we observe\nsignificantly higher wear rates in water compared to the dry state, with\nenhancement factors of $1.5-2.5$ for a low-glass-transition-temperature SBR\ncompound ($T_{\\rm g} = -50^\\circ {\\rm C}$) and approximately $4$ for a\nhigher-glass-transition compound ($T_{\\rm g} = -7^\\circ {\\rm C}$). In contrast,\nthe NR compound showed no wear in water at low nominal contact pressures\n($\\sigma_0 \\approx 0.12$, $0.16$, and $0.25 \\ {\\rm MPa}$), while at higher\npressures ($\\sigma_0 \\approx 0.36$ and $0.49 \\ {\\rm MPa}$), the wear rates in\ndry and wet states were similar. The experimental results are analyzed using a\nrecently developed rubber wear theory. The findings provide insights into the\nmechanisms of rubber wear under varying environmental and mechanical\nconditions, highlighting the influence of material properties, interfacial\neffects, and applied pressures on wear behavior.",
        "Understanding the dynamics of quantum systems is crucial in many areas of\nphysics, but simulating many-body systems presents significant challenges due\nto the large Hilbert space to navigate and the exponential growth of\ncomputational overhead. Quantum computers offer a promising platform to\novercome these challenges, particularly for simulating the time evolution with\nHamiltonians. Trotterization is a widely used approach among available\nalgorithms in this regard, and well suited for near-term quantum devices.\nHowever, it introduces algorithmic Trotter errors due to the non-commutativity\nof Hamiltonian components. Several techniques such as multi-product formulas\nhave been developed to mitigate Trotter errors, but often require deep quantum\ncircuits, which can introduce additional physical errors. In this work, we\npropose a resource-efficient scheme to reduce the algorithmic Trotter error\nwith relatively shallow circuit depth. We develop a profiling method by\nintroducing an auxiliary parameter to estimate the error effects in expectation\nvalues, enabling significant error suppression with a fixed number of Trotter\nsteps. Our approach offers an efficient way of quantum simulation on near-term\nquantum processors with shallow circuits.",
        "This paper broaches the peridynamic inverse problem of determining the\nhorizon size of the kernel function in a one-dimensional model of a linear\nmicroelastic material. We explore different kernel functions, including\nV-shaped, distributed, and tent kernels. The paper presents numerical\nexperiments using PINNs to learn the horizon parameter for problems in one and\ntwo spatial dimensions. The results demonstrate the effectiveness of PINNs in\nsolving the peridynamic inverse problem, even in the presence of challenging\nkernel functions. We observe and prove a one-sided convergence behavior of the\nStochastic Gradient Descent method towards a global minimum of the loss\nfunction, suggesting that the true value of the horizon parameter is an\nunstable equilibrium point for the PINN's gradient flow dynamics."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Trends in Phase II Trials for Cancer Therapies",
    "start_abstract":"Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "E(n) Equivariant Graph Neural Networks"
      ],
      "abstract":[
        "This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights\n  LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Rational Functions on the Projective Line from a Computational Viewpoint",
        "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and\n  Earth Surface Analysis",
        "Solar flares as electron accelerators: toward a resolution of the\n  acceleration efficiency issue",
        "Connectivity of Coxeter group Morse boundaries",
        "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
        "Privacy Bills of Materials: A Transparent Privacy Information Inventory\n  for Collaborative Privacy Notice Generation in Mobile App Development",
        "Enhancing Transformers for Generalizable First-Order Logical Entailment",
        "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models",
        "Monotonicity results in half spaces for quasilinear elliptic equations\n  involving a singular term",
        "On Fair Ordering and Differential Privacy",
        "Comparative Analysis of Efficient Adapter-Based Fine-Tuning of\n  State-of-the-Art Transformer Models",
        "Feedback cooling of fermionic atoms in optical lattices",
        "A Constraint-Preserving Neural Network Approach for Solving Mean-Field\n  Games Equilibrium",
        "Adaptive Mixture of Experts Learning for Robust Audio Spoofing Detection",
        "Model Fusion via Neuron Transplantation",
        "Ringworlds and Dyson spheres can be stable",
        "Parametrization Framework for the Deceleration Parameter in Scalar Field\n  Dark Energy Model",
        "KeBaB: $k$-mer based breaking for finding super-maximal exact matches",
        "The Transition from Centralized Machine Learning to Federated Learning\n  for Mental Health in Education: A Survey of Current Methods and Future\n  Directions",
        "Holes in silicon are heavier than expected: transport properties of\n  extremely high mobility electrons and holes in silicon MOSFETs",
        "Uniform Lyndon interpolation for the pure logic of necessitation with a\n  modal reduction principle",
        "Computing the $p$-Laplacian eigenpairs of signed graphs",
        "Global Lipschitz and Sobolev estimates for the Monge-Amp\\`ere\n  eigenfunctions of general bounded convex domains",
        "Academic Literature Recommendation in Large-scale Citation Networks\n  Enhanced by Large Language Models",
        "OminiControl2: Efficient Conditioning for Diffusion Transformers",
        "Differential inclusion systems with double phase competing operators,\n  convection, and mixed boundary conditions",
        "Supervised contrastive learning for cell stage classification of animal\n  embryos"
      ],
      "abstract":[
        "Large Language Models (LLMs) have demonstrated great potential as evaluators\nof NLG systems, allowing for high-quality, reference-free, and multi-aspect\nassessments. However, existing LLM-based metrics suffer from two major\ndrawbacks: reliance on proprietary models to generate training data or perform\nevaluations, and a lack of fine-grained, explanatory feedback. In this paper,\nwe introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation\nmetric that provides accurate explanations based on error spans. OpeNLGauge is\navailable as a two-stage ensemble of larger open-weight LLMs, or as a small\nfine-tuned evaluation model, with confirmed generalizability to unseen tasks,\ndomains and aspects. Our extensive meta-evaluation shows that OpeNLGauge\nachieves competitive correlation with human judgments, outperforming\nstate-of-the-art models on certain tasks while maintaining full reproducibility\nand providing explanations more than twice as accurate.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We explore the moduli space $\\mathcal{M}_d^1$ of degree $d \\geq 3$ rational\nfunctions on the projective line using a machine learning approach, focusing on\nautomorphism group classification. For $d = 3$, where $\\mathcal{M}_3^1 =\n{\\mathbb P}_{\\mathbf{w}}^5 ({\\mathbb Q})$ with weights $\\mathbf{w} = (2, 2, 3,\n3, 4, 6)$, we generate a dataset of 2,078,697 rational functions over $\\Q$ with\nnaive height $\\leq 4$. Initial coefficient-based models achieved high overall\naccuracy but struggled with minority classes due to extreme class imbalance. By\nusing invariants $\\xi_0, \\ldots, \\xi_5$ as features in a Random Forest\nclassifier, we achieved approximately 99.992\\% accuracy, mirroring successes in\ngenus 2 curves \\cite{2024-03}. This highlights the transformative role of\ninvariants in arithmetic dynamics, yet for $d > 3$, unknown generators of\n$\\mathcal{R}_{(d+1, d-1)}$ pose scalability challenges. Our framework bridges\ndata-driven and algebraic methods, with potential extensions to higher degrees\nand $\\mathcal{M}_d^2$.",
        "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https:\/\/github.com\/masseygeo\/earthscape.",
        "A major open issue concerning the active Sun is the effectiveness with which\nmagnetic reconnection accelerates electrons in flares. A paper published by\n{\\em{Nature}} in 2022 used microwave observations to conclude that the Sun is\nan almost ideal accelerator, energizing nearly all electrons within a coronal\nvolume to nonthermal energies. Shortly thereafter, a paper published in\n{\\em{Astrophysical Journal Letters}} used hard X-ray measurements \\emph{of the\nsame event} to reach the contradictory conclusion that less than 1\\% of the\navailable electrons were accelerated. Here we address this controversy by using\nspatially resolved observations of hard X-ray emission and a spectral inversion\nmethod to determine the evolution of the electron spectrum throughout the\nflare. So we estimated the density of the medium where electrons accelerate\nand, from this, the ratio of accelerated to ambient electron densities. Results\nshow that this ratio never exceeds a percent or so in the cases analyzed.",
        "We study the connectivity of Morse boundaries of Coxeter groups. We define\ntwo conditions on the defining graph of a Coxeter group: wide-avoidant and\nwide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,\naffine-free Coxeter groups have connected and locally connected Morse\nboundaries. On the other hand, one-ended Coxeter groups that are not\nwide-avoidant and not wide have disconnected Morse boundary. For the\nright-angled case, we get a full characterization: a one-ended right-angled\nCoxeter group has connected, non-empty Morse boundary if and only if it is\nwide-avoidant. Along the way we characterize Morse geodesic rays in affine-free\nCoxeter groups as those that spend uniformly bounded time in cosets of wide\nspecial subgroups.",
        "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
        "Privacy regulations mandate that developers must provide authentic and\ncomprehensive privacy notices, e.g., privacy policies or labels, to inform\nusers of their apps' privacy practices. However, due to a lack of knowledge of\nprivacy requirements, developers often struggle to create accurate privacy\nnotices, especially for sophisticated mobile apps with complex features and in\ncrowded development teams. To address these challenges, we introduce Privacy\nBills of Materials (PriBOM), a systematic software engineering approach that\nleverages different development team roles to better capture and coordinate\nmobile app privacy information. PriBOM facilitates transparency-centric privacy\ndocumentation and specific privacy notice creation, enabling traceability and\ntrackability of privacy practices. We present a pre-fill of PriBOM based on\nstatic analysis and privacy notice analysis techniques. We demonstrate the\nperceived usefulness of PriBOM through a human evaluation with 150 diverse\nparticipants. Our findings suggest that PriBOM could serve as a significant\nsolution for providing privacy support in DevOps for mobile apps.",
        "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
        "With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps:\/\/github.com\/CoderChen01\/LVLMSarcasmAnalysis",
        "We consider positive solutions to $\\displaystyle -\\Delta_p\nu=\\frac{1}{u^\\gamma}+f(u)$ under zero Dirichlet condition in the half space.\nExploiting a prio-ri estimates and the moving plane technique, we prove that\nany solution is monotone increasing in the direction orthogonal to the\nboundary.",
        "In blockchain systems, fair transaction ordering is crucial for a trusted and\nregulation-compliant economic ecosystem. Unlike traditional State Machine\nReplication (SMR) systems, which focus solely on liveness and safety,\nblockchain systems also require a fairness property. This paper examines these\nproperties and aims to eliminate algorithmic bias in transaction ordering\nservices.\n  We build on the notion of equal opportunity. We characterize transactions in\nterms of relevant and irrelevant features, requiring that the order be\ndetermined solely by the relevant ones. Specifically, transactions with\nidentical relevant features should have an equal chance of being ordered before\none another. We extend this framework to define a property where the greater\nthe distance in relevant features between transactions, the higher the\nprobability of prioritizing one over the other.\n  We reveal a surprising link between equal opportunity in SMR and Differential\nPrivacy (DP), showing that any DP mechanism can be used to ensure fairness in\nSMR. This connection not only enhances our understanding of the interplay\nbetween privacy and fairness in distributed computing but also opens up new\nopportunities for designing fair distributed protocols using well-established\nDP techniques.",
        "In this work, we investigate the efficacy of various adapter architectures on\nsupervised binary classification tasks from the SuperGLUE benchmark as well as\na supervised multi-class news category classification task from Kaggle.\nSpecifically, we compare classification performance and time complexity of\nthree transformer models, namely DistilBERT, ELECTRA, and BART, using\nconventional fine-tuning as well as nine state-of-the-art (SoTA) adapter\narchitectures. Our analysis reveals performance differences across adapter\narchitectures, highlighting their ability to achieve comparable or better\nperformance relative to fine-tuning at a fraction of the training time. Similar\nresults are observed on the new classification task, further supporting our\nfindings and demonstrating adapters as efficient and flexible alternatives to\nfine-tuning. This study provides valuable insights and guidelines for selecting\nand implementing adapters in diverse natural language processing (NLP)\napplications.",
        "We discuss the preparation of topological insulator states with fermionic\nultracold atoms in optical lattices by means of measurement-based Markovian\nfeedback control. The designed measurement and feedback operators induce an\neffective dissipative channel that stabilizes the desired insulator state,\neither in an exact way or approximately in the case where additional\nexperimental constraints are assumed. Successful state preparation is\ndemonstrated in one-dimensional insulators as well as for Haldane's Chern\ninsulator, by calculating the fidelity between the target ground state and the\nsteady state of the feedback-modified master equation. The fidelity is obtained\nvia time evolution of the system with moderate sizes. For larger 2D systems, we\ncompare the mean occupation of the single-particle eigenstates for the ground\nand steady state computed through mean-field kinetic equations.",
        "Neural network-based methods have demonstrated effectiveness in solving\nhigh-dimensional Mean-Field Games (MFG) equilibria, yet ensuring mathematically\nconsistent density-coupled evolution remains a major challenge. This paper\nproposes the NF-MKV Net, a neural network approach that integrates\nprocess-regularized normalizing flow (NF) with state-policy-connected\ntime-series neural networks to solve MKV FBSDEs and their associated\nfixed-point formulations of MFG equilibria. The method first reformulates MFG\nequilibria as MKV FBSDEs, embedding density evolution into equation\ncoefficients within a probabilistic framework. Neural networks are then\nemployed to approximate value functions and their gradients. To enforce\nvolumetric invariance and temporal continuity, NF architectures impose loss\nconstraints on each density transfer function.",
        "In audio spoofing detection, most studies rely on clean datasets, making\nmodels susceptible to real-world post-processing attacks, such as channel\ncompression and noise. To overcome this challenge, we propose the Adaptive\nMixture of Experts Learning (AMEL) framework, which enhances resilience by\nleveraging attack-specific knowledge and adapting dynamically to varied attack\nconditions. Specifically, AMEL utilizes Attack-Specific Experts (ASE)\nfine-tuned with Low-Rank Adaptation (LoRA), enabling each expert to target\nspecific post-processing patterns while requiring only 1.12\\% of the parameters\nneeded for full fine-tuning. Furthermore, we introduce Dynamic Expert\nAggregation (DEA), which adaptively selects and integrates expert knowledge to\nenhance the robustness of spoofing detection. Experimental results demonstrate\nthat AMEL significantly enhances robustness by improving noise resilience and\nexhibiting greater adaptability to previously unseen post-processing methods\ncompared to models relying on full fine-tuning. Additionally, our framework\noutperforms both single expert and simple average ensemble under various mixed\nattacks, demonstrating its superior robustness and adaptability in managing\ncomplex, real-world conditions.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In his 1856 Adams Prize essay, James Clark Maxwell demonstrated that Saturn's\nrings cannot be comprised of a uniform rigid body. This is a consequence of the\ntwo-body gravitational interaction between a ring and planet resulting in\ninstability. Similarly, it is also known that a so-called Dyson sphere\nencompassing a single star would be unstable due to Newton's shell theorem. A\nsurprising finding is reported here that both a ring and a sphere (shell) can\nbe stable in the restricted three-body problem. First, if two primary masses\nare considered in orbit about their common centre of mass, a large, uniform,\ninfinitesimal ring enclosing the smaller of the masses can in principle be\nstable under certain conditions. Similarly, a Dyson sphere can, be stable, if\nthe sphere encloses the smaller of the two primary masses, again under certain\nconditions. These findings extend Maxwell's results on the dynamics of rings\nand have an interesting bearing on so-called Ringworlds and Dyson spheres from\nfiction. Moreover, the existence of passively stable orbits for such\nlarge-scale structures may have implications for so-called techno-signatures in\nsearch for extra-terrestrial intelligence studies.",
        "We propose a Friedmann-Lemaitre-Robertson-Walker cosmological model with a\nscalar field that represents dark energy. A new parametrization of the\ndeceleration parameter is introduced of the form $q = 1 + \\eta (1 + \\mu\na^{\\eta})$ where $\\eta$ and $\\mu$ are model parameters. and the compatibility\nof the model is constrained by recent observational datasets, including cosmic\nchronometers, Pantheon+ and Baryon Acoustic Observations. By considering a\nvariable deceleration parameter, we address the expansion history of the\nuniverse, providing a viable description of the transition from deceleration to\nacceleration. Using the Markov Chain Monte Carlo method, the parameters of the\nmodel are constrained and we examine the cosmological parameters. A comparison\nis then made with the $\\Lambda$CDM model using the latest observations. We\nexamine the history of the main cosmological parameters, such as the\ndeceleration parameter, jerk parameter, snap parameter, density parameter, and\nequation-of-state parameter, by constraining and interpreting them to reveal\ninsights into what has been dubbed \"dynamical dark energy\" under the\nassumptions made above. Our method provides a framework that is independent of\nthe model to explore dark energy, leading to a deeper and more subtle\nunderstanding of the mechanisms driving late-time cosmic acceleration.",
        "Suppose we have a tool for finding super-maximal exact matches (SMEMs) and we\nwant to use it to find all the long SMEMs between a noisy long read $P$ and a\nhighly repetitive pangenomic reference $T$. Notice that if $L \\geq k$ and the\n$k$-mer $P [i..i + k - 1]$ does not occur in $T$ then no SMEM of length at\nleast $L$ contains $P [i..i + k - 1]$. Therefore, if we have a Bloom filter for\nthe distinct $k$-mers in $T$ and we want to find only SMEMs of length $L \\geq\nk$, then when given $P$ we can break it into maximal substrings consisting only\nof $k$-mers the filter says occur in $T$ -- which we call pseudo-SMEMs -- and\nsearch only the ones of length at least $L$. If $L$ is reasonably large and we\ncan choose $k$ well then the Bloom filter should be small (because $T$ is\nhighly repetitive) but the total length of the pseudo-SMEMs we search should\nalso be small (because $P$ is noisy). Now suppose we are interested only in the\nlongest $t$ SMEMs of length at least $L$ between $P$ and $T$. Notice that once\nwe have found $t$ SMEMs of length at least $\\ell$ then we need only search for\nSMEMs of length greater than $\\ell$. Therefore, if we sort the pseudo-SMEMs\ninto non-increasing order by length, then we can stop searching once we have\nfound $t$ SMEMs at least as long as the next pseudo-SMEM we would search. Our\npreliminary experiments indicate that these two admissible heuristics may\nsignificantly speed up SMEM-finding in practice.",
        "Research has increasingly explored the application of artificial intelligence\n(AI) and machine learning (ML) within the mental health domain to enhance both\npatient care and healthcare provider efficiency. Given that mental health\nchallenges frequently emerge during early adolescence -- the critical years of\nhigh school and college -- investigating AI\/ML-driven mental health solutions\nwithin the education domain is of paramount importance. Nevertheless,\nconventional AI\/ML techniques follow a centralized model training architecture,\nwhich poses privacy risks due to the need for transferring students' sensitive\ndata from institutions, universities, and clinics to central servers. Federated\nlearning (FL) has emerged as a solution to address these risks by enabling\ndistributed model training while maintaining data privacy. Despite its\npotential, research on applying FL to analyze students' mental health remains\nlimited. In this paper, we aim to address this limitation by proposing a\nroadmap for integrating FL into mental health data analysis within educational\nsettings. We begin by providing an overview of mental health issues among\nstudents and reviewing existing studies where ML has been applied to address\nthese challenges. Next, we examine broader applications of FL in the mental\nhealth domain to emphasize the lack of focus on educational contexts. Finally,\nwe propose promising research directions focused on using FL to address mental\nhealth issues in the education sector, which entails discussing the synergies\nbetween the proposed directions with broader human-centered domains. By\ncategorizing the proposed research directions into short- and long-term\nstrategies and highlighting the unique challenges at each stage, we aim to\nencourage the development of privacy-conscious AI\/ML-driven mental health\nsolutions.",
        "The quality of the silicon-oxide interface plays a crucial role in\nfabricating reproducible silicon spin qubits. In this work we characterize\ninterface quality by performing mobility measurements on silicon Hall bars. We\nfind a peak electron mobility of nearly $40,000\\,\\text{cm}^2\/\\text{Vs}$ in a\ndevice with a $21\\,\\text{nm}$ oxide layer, and a peak hole mobility of about\n$2,000\\,\\text{cm}^2\/\\text{Vs}$ in a device with $8\\,\\text{nm}$ oxide, the\nlatter being the highest recorded mobility for a p-type silicon MOSFET. Despite\nthe high device quality, we note an order-of-magnitude difference in mobility\nbetween electrons and holes. By studying additional n-type and p-type devices\nwith identical oxides, and fitting to transport theory, we show that this\nmobility discrepancy is due to valence band nonparabolicity. The\nnonparabolicity endows holes with a density-dependent transverse effective mass\nranging from $0.6m_0$ to $0.7m_0$, significantly larger than the usually quoted\nbend-edge mass of $0.22m_0$. Finally, we perform magnetotransport measurements\nto extract momentum and quantum scattering lifetimes.",
        "We prove the uniform Lyndon interpolation property (ULIP) of some extensions\nof the pure logic of necessitation $\\mathbf{N}$. For any $m, n \\in \\mathbb{N}$,\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ is the logic obtained from $\\mathbf{N}$ by\nadding a single axiom $\\Box^n \\varphi \\to \\Box^m \\varphi$, which is a\n$\\Diamond$-free modal reduction principle, and a rule $\\frac{\\neg \\Box\n\\varphi}{\\neg \\Box \\Box \\varphi}$, which is required to make the logic complete\nwith respect to its Kripke-like semantics. We first introduce a sequent\ncalculus $\\mathbf{GN}^+\\mathbf{A}_{m,n}$ for $\\mathbf{N}^+\\mathbf{A}_{m,n}$ and\nshow that it enjoys cut elimination, proving Craig and Lyndon interpolation\nproperties as a consequence. We then construct an embedding of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ into classical propositional logic\n$\\mathbf{Cl}$, which is then used to prove ULIP of\n$\\mathbf{N}^+\\mathbf{A}_{m,n}$ by reducing it to that of $\\mathbf{Cl}$. We also\nprove ULIP of $\\mathbf{NRA}_{m,n} = \\mathbf{N} + \\Box^n \\varphi \\to \\Box^m\n\\varphi + \\frac{\\neg \\varphi}{\\neg \\Box \\varphi}$ with a similar method.",
        "As a nonlinear extension of the graph Laplacian, the graph $p$-Laplacian has\nvarious applications in many fields. Due to the nonlinearity, it is very\ndifficult to compute the eigenvalues and eigenfunctions of graph $p$-Laplacian.\nIn this paper, we establish the equivalence between the graph $p$-Laplacian\neigenproblem and the tensor eigenproblem when $p$ is even. Building on this\nresult, algorithms designed for tensor eigenproblems can be adapted to compute\nthe eigenpairs of the graph $p$-Laplacian. For general $p>1$, we give a fast\nand convergent algorithm to compute the largest eigenvalue and the\ncorresponding eigenfunction of the signless graph $p$-Laplacian. As an\napplication, we provide a new criterion to determine when a graph is not a\nsubgraph of another one, which outperforms existing criteria based on the\nlinear Laplacian and adjacency matrices. Our work highlights the deep\nconnections and numerous similarities between the spectral theories of tensors\nand graph $p$-Laplacians.",
        "We show that the Monge-Amp\\`ere eigenfunctions of general bounded convex\ndomains are globally Lipschitz. The same result holds for convex solutions to\ndegenerate Monge-Amp\\`ere equations of the form $\\det D^2 u =M|u|^p$ with zero\nboundary condition on general bounded convex domains in ${\\mathbb R}^n$ within\nthe sharp threshold $p>n-2$. As a consequence, we obtain global $W^{2, 1}$\nestimates for these solutions.",
        "Literature recommendation is essential for researchers to find relevant\narticles in an ever-growing academic field. However, traditional methods often\nstruggle due to data limitations and methodological challenges. In this work,\nwe construct a large citation network and propose a hybrid recommendation\nframework for scientific article recommendation. Specifically, the citation\nnetwork contains 190,381 articles from 70 journals, covering statistics,\neconometrics, and computer science, spanning from 1981 to 2022. The\nrecommendation mechanism integrates network-based citation patterns with\ncontent-based semantic similarities. To enhance content-based recommendations,\nwe employ text-embedding-3-small model of OpenAI to generate an embedding\nvector for the abstract of each article. The model has two key advantages:\ncomputational efficiency and embedding stability during incremental updates,\nwhich is crucial for handling dynamic academic databases. Additionally, the\nrecommendation mechanism is designed to allow users to adjust weights according\nto their preferences, providing flexibility and personalization. Extensive\nexperiments have been conducted to verify the effectiveness of our approach. In\nsummary, our work not only provides a complete data system for building and\nanalyzing citation networks, but also introduces a practical recommendation\nmethod that helps researchers navigate the growing volume of academic\nliterature, making it easier to find the most relevant and influential articles\nin the era of information overload.",
        "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
        "In this paper, a new framework for studying the existence of generalized or\nstrongly generalized solutions to a wide class of inclusion systems involving\ndouble-phase, possibly competing differential operators, convection, and mixed\nboundary conditions is introduced. The technical approach exploits Galerkin's\nmethod and a surjective theorem for multifunctions in finite dimensional\nspaces.",
        "Video microscopy, when combined with machine learning, offers a promising\napproach for studying the early development of in vitro produced (IVP) embryos.\nHowever, manually annotating developmental events, and more specifically cell\ndivisions, is time-consuming for a biologist and cannot scale up for practical\napplications. We aim to automatically classify the cell stages of embryos from\n2D time-lapse microscopy videos with a deep learning approach. We focus on the\nanalysis of bovine embryonic development using video microscopy, as we are\nprimarily interested in the application of cattle breeding, and we have created\na Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1)\nlow-quality images and bovine dark cells that make the identification of cell\nstages difficult, (2) class ambiguity at the boundaries of developmental\nstages, and (3) imbalanced data distribution. To address these challenges, we\nintroduce CLEmbryo, a novel method that leverages supervised contrastive\nlearning combined with focal loss for training, and the lightweight 3D neural\nnetwork CSN-50 as an encoder. We also show that our method generalizes well.\nCLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset\nand the publicly available NYU Mouse Embryos dataset."
      ]
    }
  },
  {
    "id":2411.04747,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"E(n) Equivariant Graph Neural Networks",
    "start_abstract":"This paper introduces a new model to learn graph neural networks equivariant rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. addition, whereas methods are limited equivariance on 3 dimensional spaces, is easily scaled higher-dimensional spaces. We demonstrate the effectiveness of method dynamical systems modelling, representation learning autoencoders predicting molecular properties.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Trends in Phase II Trials for Cancer Therapies"
      ],
      "abstract":[
        "Background: Drug combinations are the standard of care in cancer treatment. Identifying effective drug has become more challenging because increasing number drugs. However, a substantial drugs stumble at Phase III clinical trials despite exhibiting favourable efficacy earlier Phase. Methods: We analysed recent II comprising 2165 response rates to uncover trends therapies and used null model non-interacting agents infer synergistic antagonistic combinations. compared our latest dataset with previous assess progress therapy. Results: Targeted reach higher when combination cytotoxic identify four 10 based on observed expected rates. demonstrate that targeted have not significantly increased Conclusions: conclude either we making or rate measured by tumour shrinkage is reliable surrogate endpoint for agents."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "On the eternal non-Markovianity of qubit maps",
        "Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for\n  Quantum Error Correction",
        "Search for continuous gravitational wave signals from luminous dark\n  photon superradiance clouds with LVK O3 observations",
        "Asymptotics for multiple $q$-orthogonal polynomials from the RHP",
        "Scalable First-order Method for Certifying Optimal k-Sparse GLMs",
        "Towards Transparent and Accurate Plasma State Monitoring at JET",
        "Quark number susceptibility and conserved charge fluctuation for\n  (2+1)-flavor QCD with M\\\"obius domain wall fermions",
        "Galaxy-cluster-stacked Fermi-LAT III: substructure and radio-relic\n  counterparts",
        "Erosion of a dense molecular core by a strong outflow from a massive\n  protostar",
        "Reporting on pTP sublimation during evaporation deposition",
        "Parameter Invariance Analysis of Moment Equations Using\n  Dulmage-Mendelsohn Decomposition",
        "Singularity-Based Consistent QML Estimation of Multiple Breakpoints in\n  High-Dimensional Factor Models",
        "Spectral Analysis and Invariant Measure in Studies of the Dynamics of\n  the Hemostasis of a Blood Vessel",
        "The Evolution of Hypervelocity Supernova Survivors and the Outcomes of\n  Interacting Double White Dwarf Binaries",
        "$K$-theoretic pullbacks for Lagrangians on derived critical loci",
        "Flux-tunable parity-protected qubit based on a single full-shell\n  nanowire Josephson junction",
        "Dollarized Economies in Latin America. An Inflationary Analysis of Pre,\n  During and Post Pandemic",
        "van der Waals epitaxy of $\\alpha$-MoO$_3$ films on f-mica by pulsed\n  sputter deposition",
        "Uniqueness of Dirac-harmonic maps from a compact surface with boundary",
        "Multi-Hazard Bayesian Hierarchical Model for Damage Prediction",
        "Symmetrical bipolar electrobending deformation in acceptor-doped\n  piezoceramics",
        "Smoothing surfaces on fourfolds",
        "The Atiyah-Schmid formula for reductive groups",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Computable $K$-theory for $\\mathrm{C}^*$-algebras: UHF algebras",
        "Topological flow data analysis for transient flow patterns: a\n  graph-based approach",
        "Transport equations for Osgood velocity fields",
        "Fault-Tolerant Qudit Gate Optimization in Solid-State Quantum Memory",
        "Lattice stitching by eigenvector continuation for Holstein polaron"
      ],
      "abstract":[
        "As is well known, unital Pauli maps can be eternally non-CP-divisible. In\ncontrast, here we show that in the case of non-unital maps, eternal\nnon-Markovianity in the non-unital part is ruled out. In the unital case, the\neternal non-Markovianity can be obtained by a convex combination of two\ndephasing semigroups, but not all three of them. We study these results and the\nramifications arising from them.",
        "Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error\ncorrection decoding because of its accuracy. However, many believe that it is\ndifficult, if possible at all, to achieve the microsecond latency requirement\nposed by superconducting qubits. This work presents the first publicly known\nMWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding\nlatency. Micro Blossom employs a heterogeneous architecture that carefully\npartitions a state-of-the-art MWPM decoder between software and a programmable\naccelerator with parallel processing units, one of each vertex\/edge of the\ndecoding graph. On a surface code with code distance $d$ and a circuit-level\nnoise model with physical error rate $p$, Micro Blossom's accelerator employs\n$O(d^3)$ parallel processing units to reduce the worst-case latency from\n$O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to\n$O(p^2 d^2+1)$ when $p \\ll 1$.\n  We report a prototype implementation of Micro Blossom using FPGA. Measured at\n$d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of\n$0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the\nfirst publicly known hardware-accelerated exact MWPM decoder, and the decoding\nlatency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder\nimplementations reported in the literature.",
        "Superradiance clouds of kinetically-mixed dark photons around spinning black\nholes can produce observable multi-messenger electromagnetic and gravitational\nwave signals. The cloud generates electric fields of up to a\nTeravolt-per-meter, which lead to a cascade production of charged particles,\nyielding a turbulent quasi-equilibrium plasma around the black hole, and\nresulting in electromagnetic fluxes ranging from supernova to pulsar-like\nluminosities. For stellar mass black holes, such systems resemble millisecond\npulsars and are expected to emit pulsating radio waves and continuous\ngravitational waves (CWs) within the LIGO-Virgo-KAGRA (LVK) sensitivity band.\nWe select 44 sources with approximately coincident frequencies or positive\nfrequency drifts from existing pulsar catalogs as potential candidates of\nlong-lasting superradiance clouds around old galactic black holes. For a subset\nof 34 sources that are well measured and have not been previously targeted, we\nperform the first search for CW emission in LVK data from the third observing\nrun. We find no evidence of a CW signal and place 95% confidence level upper\nlimits on the emitted strain amplitude. We interpret these results, together\nwith limits from previous searches, in terms of the underlying dark photon\ntheory by performing an analysis of the expected signals from superradiance\nclouds from galactic black holes. We find that, even for moderately spinning\nblack holes, the absence of an observed CW signal disfavors a discrete set of\ndark photon masses between about $10^{-13}$ $\\rm{eV}\/c^2$ and $10^{-12}$\n$\\rm{eV}\/c^2$ and kinetic mixing couplings in the range of $10^{-9}$-$10^{-7}$,\nsubject to assumptions about the properties of the black hole population and\nthe cloud's electromagnetic emission.",
        "We deduce the asymptotic behaviour of a broad class of multiple\n$q$-orthogonal polynomials as their degree tends to infinity. We achieve this\nby rephrasing multiple $q$-orthogonal polynomials as part of a solution to a\nRiemann Hilbert Problem (RHP). In particular, we study multiple $q$-orthogonal\npolynomials of the first kind (see [12]), which are Type II orthogonal\npolynomials with weights given by\n  \\begin{equation}\n  w_1(x) = x^\\alpha \\omega(x)d_qx,\\qquad w_2(x) = x^\\beta \\omega(x)d_qx,\n\\nonumber \\end{equation} which satisfy the constraint \\begin{equation}\\nonumber\n  |\\omega(q^{2n})-1| = \\mathcal{O}(q^{2n}), \\end{equation} as $n\\to \\infty$.\nUsing $q$-calculus we obtain detailed asymptotics for these polynomials from\nthe RHP. This class of polynomials studied was chosen in part to their\nconnection to the work of [11,12], concerning the irrationality of $\\zeta_q(1)$\nand $\\zeta_q(2)$. To conduct our asymptotic analysis we will require the\nfollowing added restrictions on $w_1(x)$ and $w_2(x)$: $\\alpha \\notin\n\\mathbb{Z}$, $\\beta \\notin \\mathbb{Z}$ and $\\alpha \\neq \\beta \\mod \\mathbb{Z}$.\nThese restrictions are necessary for the asymptotic analysis but not the\nstatement of multiple $q$-orthogonal polynomials as solutions to a RHP.\n  The author wishes to extend special thanks to Prof. Walter Van Assche, who\nmotivated this studied and provided valuable discussion.",
        "This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.",
        "Controlling and monitoring plasma within a tokamak device is complex and\nchallenging. Plasma off-normal events, such as disruptions, are hindering\nsteady-state operation. For large devices, they can even endanger the machine's\nintegrity and it represents in general one of the most serious concerns for the\nexploitation of the tokamak concept for future power plants. Effective plasma\nstate monitoring carries the potential to enable an understanding of such\nphenomena and their evolution which is crucial for the successful operation of\ntokamaks. This paper presents the application of a transparent and data-driven\nmethodology to monitor the plasma state in a tokamak. Compared to previous\nstudies in the field, supervised and unsupervised learning techniques are\ncombined. The dataset consisted of 520 expert-validated discharges from JET.\nThe goal was to provide an interpretable plasma state representation for the\nJET operational space by leveraging multi-task learning for the first time in\nthe context of plasma state monitoring. When evaluated as disruption\npredictors, a sequence-based approach showed significant improvements compared\nto the state-based models. The best resulting network achieved a promising\ncross-validated success rate when combined with a physical indicator and\naccounting for nearby instabilities. Qualitative evaluations of the learned\nlatent space uncovered operational and disruptive regions as well as patterns\nrelated to learned dynamics and global feature importance. The applied\nmethodology provides novel possibilities for the definition of triggers to\nswitch between different control scenarios, data analysis, and learning as well\nas exploring latent dynamics for plasma state monitoring. It also showed\npromising quantitative and qualitative results with warning times suitable for\navoidance purposes and distributions that are consistent with known physical\nmechanisms.",
        "We present quark number susceptibilities and conserved charge fluctuations\nfor (2+1)-flavor QCD using M\\\"obius Domain Wall fermions with a pion mass of\n\\(135~\\rm{MeV}\\). Our results are compared with hadron resonance gas models\nbelow the QCD transition temperature and with \\(\\mathcal{O}(g^2)\\) perturbation\ntheory at high temperatures. Additionally, we compare our findings with results\nfrom staggered fermion discretizations. Furthermore, we also present results of\nleading order Kurtosis of electric charge and strangeness fluctuations.",
        "Faint $\\gamma$-ray signatures emerge in Fermi-LAT data stacked scaled to the\ncharacteristic $R_{500}$ radii of MCXC galaxy clusters. This third paper in a\nseries shows a $4.3\\sigma$ excess of discrete 4FGL-DR4 catalog $\\gamma$-ray\nsources at the $r<1.5R_{500}$ radii of 205 clusters, coincident with an $r\\sim\nR_{500}$ diffuse $2.6\\sigma$ excess of 1-100 GeV emission from 75 high-latitude\nclusters. The source excess becomes highly ($>5\\sigma$) significant when\nconsidering the substantial ($3.4\\sigma$) and unexpectedly rapid quenching of\n$\\gamma$-ray sources just inside the virial shock. The excess sources show\nradial, spectral, and luminosity distributions better matching radio-relic\ncounterparts or substructure than present tentative classifications as\nblazar-candidates. Their spectral distribution is bimodal: flat-spectrum\nsources are consistent with enhanced hadronic emission behind weak, Mach\n$\\sim2$ shocks, while softer sources may be phoenix counterparts.",
        "We present Atacama Large Millimeter\/submillimeter Array Band 3 observations\nof N$_2$H$^+$ (1-0) and CH$_3$CN (5-4), as well as Band 7 observations of the\nH$_2$CO molecular line emissions from the protostellar system GGD 27-MM2(E).\nThrough position-velocity diagrams along and across the outflow axis, we study\nthe kinematics and structure of the outflow. We also fit extracted spectra of\nthe CH$_3$CN emission to obtain the physical conditions of the gas. We use the\nresults to discuss the impact of the outflow on its surroundings. We find that\nN$_2$H$^+$ emission traces a dense molecular cloud surrounding GGD 27-MM2(E).\nWe estimate that the mass of this cloud is $\\sim$13.3-26.5 M$_\\odot$. The\nmolecular cloud contains an internal cavity aligned with the H$_2$CO-traced\nmolecular outflow. The outflow, also traced by $\\mathrm{CH_3 CN}$, shows\nevidence of a collision with a molecular core (MC), as indicated by the\ndistinctive increases in the distinct physical properties of the gas such as\nexcitation temperature, column density, line width, and velocity. This\ncollision results in an X-shape structure in the northern part of the outflow\naround the position of the MC, which produces spray-shocked material downstream\nin the north of MC as observed in position-velocity diagrams both along and\nacross of the outflow axis. The outflow has a mass of 1.7-2.1 M$_\\odot$, a\nmomentum of 7.8-10.1 M$_\\odot$ km s$^{-1}$, a kinetic energy of 5.0-6.6$\\times\n10^{44}$ erg, and a mass loss rate of 4.9--6.0$\\times10^{-4}$ M$_\\odot$\nyr$^{-1}$. The molecular outflow from GGD 27-MM2(E) significantly perturbs and\nerodes its parent cloud, compressing the gas of sources such as MC and ALMA 12.\nThe feedback from this powerful protostellar outflow contributes to maintain\nthe turbulence in the surrounding area.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Living organisms maintain stable functioning amid environmental fluctuations\nthrough homeostasis, a mechanism that preserves a system's behavior despite\nchanges in environmental conditions. To elucidate homeostasis in stochastic\nbiochemical reactions, theoretical tools for assessing population-level\ninvariance under parameter perturbations are crucial. In this paper, we propose\na systematic method for identifying the stationary moments that remain\ninvariant under parameter perturbations by leveraging the structural properties\nof the stationary moment equations. A key step in this development is\naddressing the underdetermined nature of moment equations, which has\ntraditionally made it difficult to characterize how stationary moments depend\non system parameters. To overcome this, we utilize the Dulmage-Mendelsohn (DM)\ndecomposition of the coefficient matrix to extract welldetermined subequations\nand reveal their hierarchical structure. Leveraging this structure, we identify\nstationary moments whose partial derivatives with respect to parameters are\nstructurally zero, facilitating the exploration of fundamental constraints that\ngovern homeostatic behavior in stochastic biochemical systems.",
        "This paper investigates the estimation of high-dimensional factor models in\nwhich factor loadings undergo an unknown number of structural changes over\ntime. Given that a model with multiple changes in factor loadings can be\nobservationally indistinguishable from one with constant loadings but varying\nfactor variances, this reduces the high-dimensional structural change problem\nto a lower-dimensional one. Due to the presence of multiple breakpoints, the\nfactor space may expand, potentially causing the pseudo factor covariance\nmatrix within some regimes to be singular. We define two types of breakpoints:\n{\\bf a singular change}, where the number of factors in the combined regime\nexceeds the minimum number of factors in the two separate regimes, and {\\bf a\nrotational change}, where the number of factors in the combined regime equals\nthat in each separate regime. Under a singular change, we derive the properties\nof the small eigenvalues and establish the consistency of the QML estimators.\nUnder a rotational change, unlike in the single-breakpoint case, the pseudo\nfactor covariance matrix within each regime can be either full rank or\nsingular, yet the QML estimation error for the breakpoints remains stably\nbounded. We further propose an information criterion (IC) to estimate the\nnumber of breakpoints and show that, with probability approaching one, it\naccurately identifies the true number of structural changes. Monte Carlo\nsimulations confirm strong finite-sample performance. Finally, we apply our\nmethod to the FRED-MD dataset, identifying five structural breaks in factor\nloadings between 1959 and 2024.",
        "A mathematical model of atherosclerosis of a blood vessel is advanced with\nregard for the entry of low-density lipoproteins (LDLs) into blood. For the\nfirst time, the influence of cytokines on the inflammation of a blood vessel at\nthe formation of atherosclerotic plaques is taken into account. With the help\nof the expansion in a Fourier series and the calculation of an invariant\nmeasure, the scenario of the appearance of strange attractors depending on a\nchange in the parameter of the dissipation of cholesterol is studied. The\nconclusion is made about the interconnection of the dynamics of the metabolic\nprocess in a blood vascular system and its physical state.",
        "The recent prediction and discovery of hypervelocity supernova survivors has\nprovided strong evidence that the \"dynamically driven double-degenerate\ndouble-detonation\" (D6) Type Ia supernova scenario occurs in Nature. In this\nmodel, the accretion stream from the secondary white dwarf in a double white\ndwarf binary strikes the primary white dwarf violently enough to trigger a\nhelium shell detonation, which in turn triggers a carbon\/oxygen core\ndetonation. If the secondary white dwarf survives the primary's explosion, it\nwill be flung away as a hypervelocity star. While previous work has shown that\nthe hotter observed D6 stars can be broadly understood as secondaries whose\nouter layers have been heated by their primaries' explosions, the properties of\nthe cooler D6 stars have proven difficult to reproduce. In this paper, we show\nthat the cool D6 stars can be explained by the Kelvin-Helmholtz contraction of\nhelium or carbon\/oxygen white dwarfs that underwent significant mass loss and\ncore heating prior to and during the explosion of their white dwarf companions.\nWe find that the current population of known D6 candidates is consistent with\n~2% of Type Ia supernovae leaving behind a hypervelocity surviving companion.\nWe also calculate the evolution of hot, low-mass oxygen\/neon stars and find\nreasonable agreement with the properties of the LP 40-365 class of\nhypervelocity survivors, suggesting that these stars are the kicked remnants of\nnear-Chandrasekhar-mass oxygen\/neon white dwarfs that were partially disrupted\nby oxygen deflagrations. We use these results as motivation for schematic\ndiagrams showing speculative outcomes of interacting double white dwarf\nbinaries, including long-lived merger remnants, Type Ia supernovae, and several\nkinds of peculiar transients.",
        "Given a regular function $\\phi$ on a smooth stack, and a $(-1)$-shifted\nLagrangian $M$ on the derived critical locus of $\\phi$, under fairly general\nhypotheses, we construct a pullback map from the Grothendieck group of coherent\nmatrix factorizations of $\\phi$ to that of coherent sheaves on $M$. This map\nsatisfies a functoriality property with respect to the composition of\nLagrangian correspondences, as well as the usual bivariance and base-change\nproperties.\n  We provide three applications of the construction, one in the definition of\nquantum $K$-theory of critical loci (Landau-Ginzburg models), paving the way to\ngeneralize works of Okounkov school from Nakajima quiver varieties to quivers\nwith potentials, one in establishing a degeneration formula for $K$-theoretic\nDonaldson-Thomas theory of local Calabi-Yau 4-folds, the other in confirming a\n$K$-theoretic version of Joyce-Safronov conjecture.",
        "Leveraging the higher harmonics content of the Josephson potential in a\nsuperconducting circuit offers a promising route in the search for new qubits\nwith increased protection against decoherence. In this work, we demonstrate how\nthe flux tunability of a hybrid semiconductor-superconductor Josephson junction\nbased on a single full-shell nanowire enables this possibility. Near one flux\nquantum, $\\Phi\\approx \\Phi_0=h\/2e$, we find that the qubit system can be tuned\nfrom a gatemon regime to a parity-protected regime with qubit eigenstates\nlocalized in phase space in the $0$ and $\\pi$ minima of the Josephson potential\n($\\cos 2\\varphi_0$). Estimates of qubit coherence and relaxation times due to\ndifferent noise sources are presented.",
        "Given the hyperinflation that most of the Latin American countries suffered\nin the 90 and their decision towards adopting dollarization and in most cases\nkeeping their own currency, this paper analyzes the effectiveness of\ndollarization as a protective mechanism against economic disruptions in Latin\nAmerican countries. It assesses the context that led Latin American dollarized\ncountries to dollarize and analyzes CPI, GDP, and the poverty rates pre,\nduring, and postpandemic in Latin American countries, considering those that\nare dollarized and those that are not, and evaluating its relation to the US.\nInterviews were carried out with experts in the field. It assesses the\nadvantages and disadvantages of dollarization regarding global crises. The data\nwas compared and analyzed to check if there were patterns that support the\npaper objective which is that dollarization might serve as a protective\nmechanism against economic disruption. It was found that dollarization protects\nthe economy against inflation, however, it does not fully protect the economy\nwhen considering economic performance and poverty. In conclusion, this research\nconcludes that dollarization does not completely serve as a protective\nmechanism against economic disruptions nonetheless, it found that a bigger role\nis played by domestic policies and government action.",
        "This study examines the growth characteristics and structural properties of\n$\\alpha$-MoO$_3$ thin films with thicknesses ranging from 2.5 to 160 nm,\ndeposited on f-mica and c-sapphire substrates at 400 {\\deg}C. X-ray diffraction\nanalysis reveals that the films are predominantly orthorhombic $\\alpha$-MoO$_3$\nwith a preferred 0k0 orientation along the out-of-plane direction on both\nsubstrates. The d-spacing for the 060 reflection shows a slight reduction with\nincreased thickness, particularly on f-mica, which suggests minimal\nout-of-plane strain in the film and a stabilization of lattice parametes over\nlarger thicknesses. Furthermore, full-width at half maximum measurements\nindicate improved stacking and crystal quality on f-mica compared to\nc-sapphire. The films on f-mica exhibit epitaxial growth with specific\norientation relationships, while films on c-sapphire display a fiber texture.\nThe near-thickness-independent nature of the peak positions on f-mica suggests\nstable lattice parameters and reduced strain accumulation, could be attributed\nto the van der Waals epitaxy. These results highlight the role of substrate\nchoice in $\\alpha$-MoO$_3$ film growth and minimizing strain, providing\nvaluable insights into the tuning of thin-film properties.",
        "As a commutative version of the supersymmetric nonlinear sigma model,\nDirac-harmonic maps from Riemann surfaces were introduced fifteen years ago.\nThey are critical points of an unbounded conformally invariant functional\ninvolving two fields, a map from a Riemann surface into a Riemannian manifold\nand a section of a Dirac bundle which is the usual spinor bundle twisted with\nthe pull-back of the tangent bundle of the target by the map. As solutions to a\ncoupled nonlinear elliptic system, the existence and regularity theory of\nDirac-harmonic maps has already received much attention, while the general\nuniqueness theory has not been established yet. For uncoupled Dirac-harmonic\nmaps, the map components are harmonic maps. Since the uniqueness theory of\nharmonic maps from a compact surface with boundary is known, it is sufficient\nto consider the uniqueness of the spinor components, which are solutions to the\ncorresponding boundary value problems for a nonlinear Dirac equation. In\nparticular, when the map components belong to $W^{1,p}$ with $p>2$, the spinor\ncomponents are uniquely determined by boundary values and map components. For\ncoupled Dirac-harmonic maps, the map components are not harmonic maps. So the\nuniqueness problem is more difficult to solve. In this paper, we study the\nuniqueness problem on a compact surface with boundary. More precisely, we prove\nthe energy convexity for weakly Dirac-harmonic maps from the unit disk with\nsmall energy. This yields the first uniqueness result about Dirac-harmonic maps\nfrom a surface conformal to the unit disk with small energy and arbitrary\nboundary values.",
        "A fundamental theoretical limitation undermines current disaster risk models:\nexisting approaches suffer from two critical constraints. First, conventional\ndamage prediction models remain predominantly deterministic, relying on fixed\nparameters established through expert judgment rather than learned from data.\nSecond, probabilistic frameworks are fundamentally restricted by their\nunderlying assumption of hazard independence, which directly contradicts the\nobserved reality of cascading and compound disasters. By relying on fixed\nexpert parameters and treating hazards as independent phenomena, these models\ndangerously misrepresent the true risk landscape. This work addresses this\nchallenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM),\nwhich reconceptualizes the classical risk equation beyond its deterministic\norigins. The model's core theoretical contribution lies in reformulating a\nclassical risk formula as a fully probabilistic model that naturally\naccommodates hazard interactions through its hierarchical structure while\npreserving the traditional hazard-exposure-vulnerability framework. Using\ntropical cyclone damage data (1952-2020) from the Philippines as a test case,\nwith out-of-sample validation on recent events (2020-2022), the model\ndemonstrates significant empirical advantages. Key findings include a reduction\nin damage prediction error by 61% compared to a single-hazard model, and 80%\ncompared to a benchmark deterministic model. This corresponds to an improvement\nin damage estimation accuracy of USD 0.8 billion and USD 2 billion,\nrespectively. The improved accuracy enables more effective disaster risk\nmanagement across multiple domains, from optimized insurance pricing and\nnational resource allocation to local adaptation strategies, fundamentally\nimproving society's capacity to prepare for and respond to disasters.",
        "Since 2022, large apparent strains (>1%) with highly asymmetrical\nstrain-electric field (S-E) curves have been reported in various thin\npiezoceramic materials, attributed to a bidirectional electric-field-induced\nbending (electrobending) deformation, which consistently produces convex\nbending along the negative electric field direction. In this study, we report a\nnovel unidirectional electrobending behavior in acceptor-doped K0.5Na0.5NbO3\nceramics, where convex bending always occurs along the pre-poling direction\nregardless of the direction of the applied electric field. This unique\ndeformation is related to the reorientation of the defect dipoles in one\nsurface layer during the pre-poling process, resulting in an asymmetrical\ndistribution of defect dipoles in the two surface layers. The synergistic\ninteraction between ferroelectric domains and defect dipoles in the surface\nlayers induces this unidirectional electrobending, as evidenced by a\nbutterfly-like symmetrical bipolar S-E curve with a giant apparent strain of\n3.2%. These findings provide new insights into defect engineering strategies\nfor developing advanced piezoelectric materials with large electroinduced\ndisplacements.",
        "If $\\mathcal E, \\mathcal F$ are vector bundles of ranks $r-1,r$ on a smooth\nfourfold $X$ and $\\mathcal{Hom}(\\mathcal E,\\mathcal F)$ is globally generated,\nit is well known that the general map $\\phi: \\mathcal E \\to \\mathcal F$ is\ninjective and drops rank along a smooth surface. Chang improved on this with a\nfiltered Bertini theorem. We strengthen these results by proving variants in\nwhich (a) $\\mathcal F$ is not a vector bundle and (b) $\\mathcal{Hom}(\\mathcal\nE,\\mathcal F)$ is not globally generated. As an application, we give examples\nof even linkage classes of surfaces on $\\mathbb P^4$ in which all integral\nsurfaces are smoothable, including the linkage classes associated with the\nHorrocks-Mumford surface.",
        "We prove the Atiyah-Schmid formula for tempered and projective tempered\nrepresentations of reductive groups.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "We initiate the study of the effective content of $K$-theory for\n$\\mathrm{C}^*$-algebras. We prove that there are computable functors which\nassociate, to a computably enumerable presentation of a $\\mathrm{C}^*$-algebra\n$\\boldA$, computably enumerable presentations of the abelian groups\n$K_0(\\boldA)$ and $K_1(\\boldA)$. When $\\boldA$ is stably finite, we show that\nthe positive cone of $K_0(\\boldA)$ is computably enumerable. We strengthen the\nresults in the case that $\\boldA$ is a UHF algebra by showing that the\naforementioned presentation of $K_0(\\boldA)$ is actually computable. In the UHF\ncase, we also show that $\\boldA$ has a computable presentation precisely when\n$K_0(\\boldA)$ has a computable presentation, which in turn is equivalent to the\nsupernatural number of $\\boldA$ being lower semicomputable; we give an example\nthat shows that this latter equivalence cannot be improved to requiring that\nthe supernatural number of $\\boldA$ is computable. Finally, we prove that every\nUHF algebra is computably categorical.",
        "We introduce a time-series analysis method for transient two-dimensional flow\npatterns based on Topological Flow Data Analysis (TFDA), a new approach to\ntopological data analysis. TFDA identifies local topological flow structures\nfrom an instantaneous streamline pattern and describes their global connections\nas a unique planar tree and its string representation. With TFDA, the evolution\nof two-dimensional flow patterns is reduced to a discrete dynamical system\nrepresented as a transition graph between topologically equivalent streamline\npatterns. We apply this method to study the lid-driven cavity flow at Reynolds\nnumbers ranging from $Re=14000$ to $Re=16000$, a benchmark problem in fluid\ndynamics data analysis. Our approach reveals the transition from periodic to\nchaotic flow at a critical Reynolds number when the reduced dynamical system is\nmodelled as a Markov process on the transition graph. Additionally, we perform\nan observational causal inference to analyse changes in local flow patterns at\nthe cavity corners and discuss differences with a standard interventional\nsensitivity analysis. This work demonstrates the potential of TFDA-based\ntime-series analysis for uncovering complex dynamical behaviours in fluid flow\ndata.",
        "We consider the transport equation with a velocity field satisfying the\nOsgood condition. The weak formulation is not meaningful in the usual Lebesgue\nsense, meaning that the usual DiPerna--Lions treatment of the problem is not\napplicable {(in particular, the divergence of the velocity might be\nunbounded)}. Instead, we use Riemann--Stieltjes integration to interpret the\nweak formulation, leading to a well-posedness theory in regimes not covered by\nexisting works. The most general results are for the one-dimensional problem,\nwith generalisations to multiple dimensions in the particular case of\nlog-Lipschitz velocities.",
        "Achieving scalable, fault-tolerant quantum computation requires quantum\nmemory architectures that minimize error correction overhead while preserving\ncoherence. This work presents a framework for high-dimensional qudit memory in\n153Eu:Y2SiO5, integrating three core mechanisms: (i) non-destructive syndrome\nextraction, using spin-echo sequences to encode error syndromes without direct\nmeasurement; (ii) adaptive quantum Fourier transform (QFT) for error\nidentification, leveraging frequency-space transformations to reduce gate\ncomplexity; and (iii) coset-based fault-tolerant correction, factorizing large\nstabilizer-like unitaries into modular operations to confine error propagation.\nBy combining generalized stabilizer formalism, Weyl-Heisenberg operators, and\nfinite-group coset decompositions, we develop a qudit error correction scheme\noptimized for solid-state quantum memory. This approach circumvents\nresource-intensive multi-qubit concatenation, enabling scalable, long-lived\nquantum storage with efficient state retrieval and computational redundancy.\nThese results provide a pathway toward practical fault-tolerant architectures\nfor rare-earth-ion-doped quantum memories.",
        "Simulations of lattice particle - phonon systems are fundamentally restricted\nby the exponential growth of the number of quantum states with the lattice\nsize. Here, we demonstrate an algorithm that constructs the lowest eigenvalue\nand eigenvector for the Holstein model in extended lattices from eigenvalue\nproblems for small, independent lattice segments. This leads to exponential\nreduction of the computational Hilbert space and allows applications of\nvariational quantum algorithms to particle - phonon interactions in large\nlattices. We illustrate that the ground state of the Holstein polaron in the\nentire range of electron - phonon coupling, from weak to strong, and the lowest\nphonon frequency ($\\omega\/t = 0.1$) considered by numerical calculations to\ndate can be obtained from a sequence of up to four-site problems. When combined\nwith quantum algorithms, the present approach leads to a dramatic reduction of\nrequired quantum resources. We show that the ground state of the Holstein\npolaron in a lattice with 100 sites and 32 site phonons can be computed by a\nvariational quantum eigensolver with 11 qubits."
      ]
    }
  },
  {
    "id":2411.11513,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data",
    "start_abstract":"Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, massive data sets generated by NGS\u2014the Genome pilot alone includes nearly five terabases\u2014make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated Indeed, many professionals limited in scope ease with which they can answer scientific questions complexity accessing manipulating produced these machines. Here, we discuss Analysis Toolkit (GATK), a structured programming framework designed to development efficient next-generation sequencers using functional philosophy MapReduce. The GATK provides small but rich set access patterns that encompass majority tool needs. Separating specific calculations from common management infrastructure enables us optimize correctness, stability, CPU memory efficiency enable distributed shared parallelization. We highlight capabilities describing implementation application robust, scale-tolerant like coverage calculators single nucleotide polymorphism (SNP) calling. conclude developers analysts quickly easily write NGS tools, have been incorporated into large-scale projects Project Cancer Atlas.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Rethinking the Inception Architecture for Computer Vision"
      ],
      "abstract":[
        "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety tasks. Since 2014 very deep convolutional started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend translate immediate quality tasks (as long as enough labeled data is provided training), efficiency low parameter count still enabling factors use cases such mobile big-data scenarios. Here we exploring ways scale up that aim utilizing added computation efficiently possible by suitably factorized convolutions aggressive regularization. We benchmark our methods on ILSVRC 2012 classification challenge validation set demonstrate over art: 21:2% top-1 5:6% top-5 error single frame evaluation using network with 5 billion multiply-adds per inference less than 25 million parameters. With an ensemble 4 models multi-crop evaluation, report 3:5% 17:3% 3:6% official test set."
      ],
      "categories":[
        "BioInformatics"
      ]
    },
    "list":{
      "title":[
        "Modeling and stability analysis of live systems with time-varying\n  dimension",
        "Physics-Informed Neural Network Surrogate Models for River Stage\n  Prediction",
        "Robustness tests for biomedical foundation models should tailor to\n  specification",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Quantum induced superradiance",
        "Computerized Assessment of Motor Imitation for Distinguishing Autism in\n  Video (CAMI-2DNet)",
        "Social Media for Activists: Reimagining Safety, Content Presentation,\n  and Workflows",
        "OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary",
        "Moving Plasma Structures and Possible Driving Mechanisms of Solar\n  Microflares Observed with High-Resolution Coronal Imaging",
        "Grid-level impacts of renewable energy on thermal generation:\n  efficiency, emissions and flexibility",
        "Stable Hypergraph Matching in Unimodular Hypergraphs",
        "The rise of stochasticity in physics",
        "Inspecting the Representation Manifold of Differentially-Private Text",
        "Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large\n  Language Models",
        "Hyperspectral image reconstruction by deep learning with super-Rayleigh\n  speckles",
        "A Steerable Deep Network for Model-Free Diffusion MRI Registration",
        "Short-time Variational Mode Decomposition",
        "Spherically symmetric electrically counterpoised dust either collapses\n  or disperses",
        "Backcasting the Optimal Decisions in Transport Systems: An Example with\n  Electric Vehicle Purchase Incentives",
        "Revisiting Convolution Architecture in the Realm of DNA Foundation\n  Models",
        "Thermal and baryon density modifications to the $\\sigma$-boson\n  propagator: A road to describe the transfer of vorticity to spin in a nuclear\n  environment in relativistic heavy-ion collisions",
        "Iterative Motion Planning in Multi-agent Systems with Opportunistic\n  Communication under Disturbance",
        "REdiSplats: Ray Tracing for Editable Gaussian Splatting",
        "A Hierarchical Region-Based Approach for Efficient Multi-Robot\n  Exploration",
        "A note on Ordered Ruzsa-Szemer\\'edi graphs",
        "CMamba: Learned Image Compression with State Space Models",
        "Regularized higher-order Taylor approximation methods for nonlinear\n  least-squares",
        "Maximum likelihood estimation in the sparse Rasch model",
        "Benchmarking Classical, Deep, and Generative Models for Human Activity\n  Recognition"
      ],
      "abstract":[
        "A major limitation of the classical control theory is the assumption that the\nstate space and its dimension do not change with time. This prevents analyzing\nand even formalizing the stability and control problems for open multi-agent\nsystems whose agents may enter or leave the network, industrial processes where\nthe sensors or actuators may be exchanged frequently, smart grids, etc. In this\nwork, we propose a framework of live systems that covers a rather general class\nof systems with a time-varying state space. We argue that input-to-state\nstability is a proper stability notion for this class of systems, and many of\nthe classic tools and results, such as Lyapunov methods and superposition\ntheorems, can be extended to this setting.",
        "This work investigates the feasibility of using Physics-Informed Neural\nNetworks (PINNs) as surrogate models for river stage prediction, aiming to\nreduce computational cost while maintaining predictive accuracy. Our primary\ncontribution demonstrates that PINNs can successfully approximate HEC-RAS\nnumerical solutions when trained on a single river, achieving strong predictive\naccuracy with generally low relative errors, though some river segments exhibit\nhigher deviations.\n  By integrating the governing Saint-Venant equations into the learning\nprocess, the proposed PINN-based surrogate model enforces physical consistency\nand significantly improves computational efficiency compared to HEC-RAS. We\nevaluate the model's performance in terms of accuracy and computational speed,\ndemonstrating that it closely approximates HEC-RAS predictions while enabling\nreal-time inference.\n  These results highlight the potential of PINNs as effective surrogate models\nfor single-river hydrodynamics, offering a promising alternative for\ncomputationally efficient river stage forecasting. Future work will explore\ntechniques to enhance PINN training stability and robustness across a more\ngeneralized multi-river model.",
        "Existing regulatory frameworks for biomedical AI include robustness as a key\ncomponent but lack detailed implementational guidance. The recent rise of\nbiomedical foundation models creates new hurdles in testing and certification\ngiven their broad capabilities and susceptibility to complex distribution\nshifts. To balance test feasibility and effectiveness, we suggest a\npriority-based, task-oriented approach to tailor robustness evaluation\nobjectives to a predefined specification. We urge concrete policies to adopt a\ngranular categorization of robustness concepts in the specification. Our\napproach promotes the standardization of risk assessment and monitoring, which\nguides technical developments and mitigation efforts.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "Superradiance, the phenomenon enabling energy extraction through radiation\namplification, is not universal to all black holes. We show that semi-classical\nbackreaction can induce superradiance, even when absent at the classical level.\nSpecifically, we compute the quasinormal modes of a massless scalar field\nprobing a family of rotating `quantum' black holes in three-dimensional anti-de\nSitter space, accounting for all orders of backreaction due to quantum\nconformal matter. A subset of these modes is identified as superradiant,\nleading to the formation of quantum black hole `bombs'. All such quantum black\nholes have curvature singularities shrouded by horizons. Thus, while\nbackreaction enforces cosmic censorship, it also renders the black holes\ndynamically unstable. Further, we find all thermally unstable black holes are\ndynamically unstable, though the converse does not hold generally. Our findings\nthus suggest a semiclassical version of the Gubser-Mitra conjecture on black\nhole stability. This motivates us to propose a stability criterion for quantum\nblack holes.",
        "Motor imitation impairments are commonly reported in individuals with autism\nspectrum conditions (ASCs), suggesting that motor imitation could be used as a\nphenotype for addressing autism heterogeneity. Traditional methods for\nassessing motor imitation are subjective, labor-intensive, and require\nextensive human training. Modern Computerized Assessment of Motor Imitation\n(CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video\ndata, are less subjective. However, they rely on labor-intensive data\nnormalization and cleaning techniques, and human annotations for algorithm\ntraining. To address these challenges, we propose CAMI-2DNet, a scalable and\ninterpretable deep learning-based approach to motor imitation assessment in\nvideo data, which eliminates the need for data normalization, cleaning and\nannotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a\nmotion encoding that is disentangled from nuisance factors such as body shape\nand camera views. To learn a disentangled representation, we employ synthetic\ndata generated by motion retargeting of virtual characters through the\nreshuffling of motion, body shape, and camera views, as well as real\nparticipant data. To automatically assess how well an individual imitates an\nactor, we compute a similarity score between their motion encodings, and use it\nto discriminate individuals with ASCs from neurotypical (NT) individuals. Our\ncomparative analysis demonstrates that CAMI-2DNet has a strong correlation with\nhuman scores while outperforming CAMI-2D in discriminating ASC vs NT children.\nMoreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater\npracticality by operating directly on video data and without the need for\nad-hoc data normalization and human annotations.",
        "Social media is central to activists, who use it internally for coordination\nand externally to reach supporters and the public. To date, the HCI community\nhas not explored activists' perspectives on future social media platforms. In\ninterviews with 14 activists from an environmental and a queer-feminist\nmovement in Germany, we identify activists' needs and feature requests for\nfuture social media platforms. The key finding is that on- and offline safety\nis their main need. Based on this, we make concrete proposals to improve safety\nmeasures. Increased control over content presentation and tools to streamline\nactivist workflows are also central to activists. We make concrete design and\nresearch recommendations on how social media platforms and the HCI community\ncan contribute to improved safety and content presentation, and how activists\nthemselves can reduce their workload.",
        "Out-of-distribution (OOD) detection remains challenging for deep learning\nmodels, particularly when test-time OOD samples differ significantly from\ntraining outliers. We propose OODD, a novel test-time OOD detection method that\ndynamically maintains and updates an OOD dictionary without fine-tuning. Our\napproach leverages a priority queue-based dictionary that accumulates\nrepresentative OOD features during testing, combined with an informative inlier\nsampling strategy for in-distribution (ID) samples. To ensure stable\nperformance during early testing, we propose a dual OOD stabilization mechanism\nthat leverages strategically generated outliers derived from ID data. To our\nbest knowledge, extensive experiments on the OpenOOD benchmark demonstrate that\nOODD significantly outperforms existing methods, achieving a 26.0% improvement\nin FPR95 on CIFAR-100 Far OOD detection compared to the state-of-the-art\napproach. Furthermore, we present an optimized variant of the KNN-based OOD\ndetection framework that achieves a 3x speedup while maintaining detection\nperformance.",
        "Solar microflares are ubiquitous in the solar corona, yet their driving\nmechanisms remain a subject of ongoing debate. Using high-resolution coronal\nobservations from the Solar Orbiter's Extreme Ultraviolet Imager (EUI), we\nidentified about a dozen distinct moving plasma structures (hereafter, `` tiny\nejections'') originating from the centers of three homologous microflares out\nof four successive events. These tiny ejections propagate roughly perpendicular\nto the flaring loops. They often originate as dot-like structures with a length\nscale of approximately $10^{3}$ km. While these initial dot-like shapes are\nobservable in EUI images, they remain undetectable in the images captured by\nthe Atmospheric Imaging Assembly onboard the Solar Dynamics Observatory. As\nthey propagate, these dot-like structures consistently evolve into loop-like\nformations, possibly due to the heating of the surrounding magnetic field.\nRather than being generated by a series of flux rope eruptions, the tiny\nejections appear to result from small-angle magnetic reconnections within a\nbipolar field. Thus, the microflares associated with these ejections may be\ndriven by magnetic reconnection within braided fields, a process similar to the\nproposed nanoflare mechanism and distinct from the standard large-scale flare\nmodel.",
        "Wind and solar generation constitute an increasing share of electricity\nsupply globally. We find that this leads to shifts in the operational dynamics\nof thermal power plants. Using fixed effects panel regression across seven\nmajor U.S. balancing authorities, we analyze the impact of renewable generation\non coal, natural gas combined cycle plants, and natural gas combustion\nturbines. Wind generation consistently displaces thermal output, while effects\nfrom solar vary significantly by region, achieving substantial displacement in\nareas with high solar penetration such as the California Independent System\nOperator but limited impacts in coal reliant grids such as the Midcontinent\nIndependent System Operator. Renewable energy sources effectively reduce carbon\ndioxide emissions in regions with flexible thermal plants, achieving\ndisplacement effectiveness as high as one hundred and two percent in the\nCalifornia Independent System Operator and the Electric Reliability Council of\nTexas. However, in coal heavy areas such as the Midcontinent Independent System\nOperator and the Pennsylvania New Jersey Maryland Interconnection,\ninefficiencies from ramping and cycling reduce carbon dioxide displacement to\nas low as seventeen percent and often lead to elevated nitrogen oxides and\nsulfur dioxide emissions. These findings underscore the critical role of grid\ndesign, fuel mix, and operational flexibility in shaping the emissions benefits\nof renewables. Targeted interventions, including retrofitting high emitting\nplants and deploying energy storage, are essential to maximize emissions\nreductions and support the decarbonization of electricity systems.",
        "We study the NP-hard Stable Hypergraph Matching (SHM) problem and its\ngeneralization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M)\nproblem, and investigate their computational properties under various\nstructural constraints. Our study is motivated by the fact that Scarf's Lemma\n(Scarf, 1967) together with a result of Lov\\'asz (1972) guarantees the\nexistence of a stable matching whenever the underlying hypergraph is normal.\nFurthermore, if the hypergraph is unimodular (i.e., its incidence matrix is\ntotally unimodular), then even a stable $b$-matching is guaranteed to exist.\nHowever, no polynomial-time algorithm is known for finding a stable matching or\n$b$-matching in unimodular hypergraphs.\n  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are\ntractable such as laminar hypergraphs or so-called subpath hypergraphs with\nbounded-size hyperedges; for the latter case, even a maximum-weight stable\n$b$-matching can be found efficiently. We complement our algorithms by showing\nthat optimizing over stable matchings is NP-hard even in laminar hypergraphs.\nAs a practically important special case of SH$b$M for unimodular hypergraphs,\nwe investigate a tripartite stable matching problem with students, schools, and\ncompanies as agents, called the University Dual Admission problem, which models\nreal-world scenarios in higher education admissions.\n  Finally, we examine a superclass of subpath hypergraphs that are normal but\nnecessarily not unimodular, namely subtree hypergraphs where hyperedges\ncorrespond to subtrees of a tree. We establish that for such hypergraphs,\nstable matchings can be found in polynomial time but, in the setting with\ncapacities, finding a stable $b$-matching is NP-hard.",
        "In the last 175 years, the physical understanding of\n  nature has seen a revolutionary change. Until about 1850, Newton's\n  theory and the mechanical world view derived from it provided the\n  dominant view of the physical world, later supplemented by Maxwell's\n  theory of the electromagnetic field. That approach was entirely\n  deterministic and free of probabilistic concepts. In contrast to\n  that conceptual edifice, today many fields of physics are governed\n  by probabilistic concepts. Statistical mechanics in its classical or\n  quantum version and random-matrix theory are obvious\n  examples. Quantum mechanics is an intrinsically statistical\n  theory. Classical chaos and its quantum manifestations also require\n  a stochastic approach. The paper describes how a combination of\n  discoveries and conceptual problems undermined the mechanical world\n  view, led to novel concepts, and shaped the modern understanding of\n  physics.",
        "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.",
        "Unsafe prompts pose significant safety risks to large language models (LLMs).\nExisting methods for detecting unsafe prompts rely on data-driven fine-tuning\nto train guardrail models, necessitating significant data and computational\nresources. In contrast, recent few-shot gradient-based methods emerge,\nrequiring only few safe and unsafe reference prompts. A gradient-based approach\nidentifies unsafe prompts by analyzing consistent patterns of the gradients of\nsafety-critical parameters in LLMs. Although effective, its restriction to\ndirectional similarity (cosine similarity) introduces ``directional bias'',\nlimiting its capability to identify unsafe prompts. To overcome this\nlimitation, we introduce GradCoo, a novel gradient co-occurrence analysis\nmethod that expands the scope of safety-critical parameter identification to\ninclude unsigned gradient similarity, thereby reducing the impact of\n``directional bias'' and enhancing the accuracy of unsafe prompt detection.\nComprehensive experiments on the widely-used benchmark datasets ToxicChat and\nXStest demonstrate that our proposed method can achieve state-of-the-art (SOTA)\nperformance compared to existing methods. Moreover, we confirm the\ngeneralizability of GradCoo in detecting unsafe prompts across a range of LLM\nbase models with various sizes and origins.",
        "Ghost imaging via sparsity constraints (GISC) spectral camera modulates the\nthree-dimensional (3D) hyperspectral image into a two-dimensional (2D)\ncompressive image with speckles in a single shot. It obtains a 3D hyperspectral\nimage (HSI) by reconstruction algorithms. The rapid development of deep\nlearning has provided a new method for 3D HSI reconstruction. Moreover, the\nimaging performance of the GISC spectral camera can be improved by optimizing\nthe speckle modulation. In this paper, we propose an end-to-end GISCnet with\nsuper-Rayleigh speckle modulation to improve the imaging quality of the GISC\nspectral camera. The structure of GISCnet is very simple but effective, and we\ncan easily adjust the network structure parameters to improve the image\nreconstruction quality. Relative to Rayleigh speckles, our super-Rayleigh\nspeckles modulation exhibits a wealth of detail in reconstructing 3D HSIs.\nAfter evaluating 648 3D HSIs, it was found that the average peak\nsignal-to-noise ratio increased from 27 dB to 31 dB. Overall, the proposed\nGISCnet with super-Rayleigh speckle modulation can effectively improve the\nimaging quality of the GISC spectral camera by taking advantage of both\noptimized super-Rayleigh modulation and deep-learning image reconstruction,\ninspiring joint optimization of light-field modulation and image reconstruction\nto improve ghost imaging performance.",
        "Nonrigid registration is vital to medical image analysis but remains\nchallenging for diffusion MRI (dMRI) due to its high-dimensional,\norientation-dependent nature. While classical methods are accurate, they are\ncomputationally demanding, and deep neural networks, though efficient, have\nbeen underexplored for nonrigid dMRI registration compared to structural\nimaging. We present a novel, deep learning framework for model-free, nonrigid\nregistration of raw diffusion MRI data that does not require explicit\nreorientation. Unlike previous methods relying on derived representations such\nas diffusion tensors or fiber orientation distribution functions, in our\napproach, we formulate the registration as an equivariant diffeomorphism of\nposition-and-orientation space. Central to our method is an\n$\\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while\npreserving the geometric properties of a raw dMRI's domain. We introduce a new\nloss function based on the maximum mean discrepancy in Fourier space,\nimplicitly matching ensemble average propagators across images. Experimental\nresults on Human Connectome Project dMRI data demonstrate competitive\nperformance compared to state-of-the-art approaches, with the added advantage\nof bypassing the overhead for estimating derived representations. This work\nestablishes a foundation for data-driven, geometry-aware dMRI registration\ndirectly in the acquisition space.",
        "Variational mode decomposition (VMD) and its extensions like Multivariate VMD\n(MVMD) decompose signals into ensembles of band-limited modes with narrow\ncentral frequencies. These methods utilize Fourier transformations to shift\nsignals between time and frequency domains. However, since Fourier\ntransformations span the entire time-domain signal, they are suboptimal for\nnon-stationary time series.\n  We introduce Short-Time Variational Mode Decomposition (STVMD), an innovative\nextension of the VMD algorithm that incorporates the Short-Time Fourier\ntransform (STFT) to minimize the impact of local disturbances. STVMD segments\nsignals into short time windows, converting these segments into the frequency\ndomain. It then formulates a variational optimization problem to extract\nband-limited modes representing the windowed data. The optimization aims to\nminimize the sum of the bandwidths of these modes across the windowed data,\nextending the cost functions used in VMD and MVMD. Solutions are derived using\nthe alternating direction method of multipliers, ensuring the extraction of\nmodes with narrow bandwidths.\n  STVMD is divided into dynamic and non-dynamic types, depending on whether the\ncentral frequencies vary with time. Our experiments show that non-dynamic STVMD\nis comparable to VMD with properly sized time windows, while dynamic STVMD\nbetter accommodates non-stationary signals, evidenced by reduced mode function\nerrors and tracking of dynamic central frequencies. This effectiveness is\nvalidated by steady-state visual-evoked potentials in electroencephalogram\nsignals.",
        "We explore the dynamical evolution of spherically symmetric objects made of\nelectrically counterpoised dust in general relativity. It has been claimed that\nthese objects are in neutral equilibrium and, therefore, that black hole\nmimickers made of electrically counterpoised dust are feasible. Here we show\nthat if a velocity is imparted to the fluid elements, no matter how small, the\nevolution leads either to a black hole or to the dispersion of the fluid.\nFurthermore, in the case of collapse, the resulting object is necessarily an\nextremal Reissner-Nordstr\\\"om black hole.",
        "This study represents a first attempt to build a backcasting methodology to\nidentify the optimal policy roadmaps in transport systems. In this methodology,\ndesired objectives are set by decision makers at a given time horizon, and then\nthe optimal combinations of policies to achieve these objectives are computed\nas a function of time (i.e., ``backcasted''). This approach is illustrated on\nthe transportation sector by considering a specific subsystem with a single\npolicy decision. The subsystem describes the evolution of the passenger car\nfleet within a given region and its impact on greenhouse gas emissions. The\noptimized policy is a monetary incentive for the purchase of electric vehicles\nwhile minimizing the total budget of the state and achieving a desired CO$_2$\ntarget. A case study applied to Metropolitan France is presented to illustrate\nthe approach. Additionally, alternative policy scenarios are also analyzed to\nprovide further insights.",
        "In recent years, a variety of methods based on Transformer and state space\nmodel (SSM) architectures have been proposed, advancing foundational DNA\nlanguage models. However, there is a lack of comparison between these recent\napproaches and the classical architecture convolutional networks (CNNs) on\nfoundation model benchmarks. This raises the question: are CNNs truly being\nsurpassed by these recent approaches based on transformer and SSM\narchitectures? In this paper, we develop a simple but well-designed CNN-based\nmethod termed ConvNova. ConvNova identifies and proposes three effective\ndesigns: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch\nframework for gating mechanisms. Through extensive empirical experiments, we\ndemonstrate that ConvNova significantly outperforms recent methods on more than\nhalf of the tasks across several foundation model benchmarks. For example, in\nhistone-related tasks, ConvNova exceeds the second-best method by an average of\n5.8%, while generally utilizing fewer parameters and enabling faster\ncomputation. In addition, the experiments observed findings that may be related\nto biological characteristics. This indicates that CNNs are still a strong\ncompetitor compared to Transformers and SSMs. We anticipate that this work will\nspark renewed interest in CNN-based methods for DNA foundation models.",
        "In the context of the description of how the vortical motion, produced in\nperipheral heavy-ion collisions, is transferred to the spin of hadrons, we\ncompute the $\\sigma$-meson propagator at finite temperature and baryon density.\nThis propagator encodes the properties of a medium consisting mainly of\nnucleons{, and can be used to model the main interactions between hadrons} in\nthe corona region of the reaction. We compute the one-loop $\\sigma$ self-energy\nin an approximation that accounts for the large nucleon mass. From the real\npart of the self-energy, we find the dispersion relation and show that the\n$\\sigma$-mass receives a non-negligible thermal and baryon chemical dependent\n{contribution}. From the imaginary part, we also compute the spectral density,\nwhich we show to contain a piece coming from the branch cut associated with\nLandau damping. We also present approximations for the dispersion relation and\nthe residue at the pole in the small- and large-momentum regimes and complement\nthe calculation, providing the sum rules satisfied by the propagator. This\nstudy aims to determine one of the elements needed to compute how the vortical\nmotion in the corona region of the reaction is transferred to the spin of\n$\\Lambda$ hyperons that can interact with nucleons by $\\sigma$-meson exchange.",
        "In complex multi-agent systems involving heterogeneous teams, uncertainty\narises from numerous sources like environmental disturbances, model\ninaccuracies, and changing tasks. This causes planned trajectories to become\ninfeasible, requiring replanning. Further, different communication\narchitectures used in multi-agent systems give rise to asymmetric knowledge of\nplanned trajectories across the agents. In such systems, replanning must be\ndone in a communication-aware fashion. This paper establishes the conditions\nfor synchronization and feasibility in epistemic planning scenarios introduced\nby opportunistic communication architectures. We also establish conditions on\ntask satisfaction based on quantified recoverability of disturbances in an\niterative planning scheme. We further validate these theoretical results\nexperimentally in a UAV--UGV task assignment problem.",
        "Gaussian Splatting (GS) has become one of the most important neural rendering\nalgorithms. GS represents 3D scenes using Gaussian components with trainable\ncolor and opacity. This representation achieves high-quality renderings with\nfast inference. Regrettably, it is challenging to integrate such a solution\nwith varying light conditions, including shadows and light reflections, manual\nadjustments, and a physical engine. Recently, a few approaches have appeared\nthat incorporate ray-tracing or mesh primitives into GS to address some of\nthese caveats. However, no such solution can simultaneously solve all the\nexisting limitations of the classical GS. Consequently, we introduce\nREdiSplats, which employs ray tracing and a mesh-based representation of flat\n3D Gaussians. In practice, we model the scene using flat Gaussian distributions\nparameterized by the mesh. We can leverage fast ray tracing and control\nGaussian modification by adjusting the mesh vertices. Moreover, REdiSplats\nallows modeling of light conditions, manual adjustments, and physical\nsimulation. Furthermore, we can render our models using 3D tools such as\nBlender or Nvdiffrast, which opens the possibility of integrating them with all\nexisting 3D graphics techniques dedicated to mesh representations.",
        "Multi-robot autonomous exploration in an unknown environment is an important\napplication in robotics.Traditional exploration methods only use information\naround frontier points or viewpoints, ignoring spatial information of unknown\nareas. Moreover, finding the exact optimal solution for multi-robot task\nallocation is NP-hard, resulting in significant computational time consumption.\nTo address these issues, we present a hierarchical multi-robot exploration\nframework using a new modeling method called RegionGraph. The proposed approach\nmakes two main contributions: 1) A new modeling method for unexplored areas\nthat preserves their spatial information across the entire space in a weighted\ngraph called RegionGraph. 2) A hierarchical multi-robot exploration framework\nthat decomposes the global exploration task into smaller subtasks, reducing the\nfrequency of global planning and enabling asynchronous exploration. The\nproposed method is validated through both simulation and real-world\nexperiments, demonstrating a 20% improvement in efficiency compared to existing\nmethods.",
        "A recent breakthrough of Behnezhad and Ghafari [FOCS 2024] and subsequent\nwork of Assadi, Khanna, and Kiss [SODA 2025] gave algorithms for the fully\ndynamic $(1-\\varepsilon)$-approximate maximum matching problem whose runtimes\nare determined by a purely combinatorial quantity: the maximum density of\nOrdered Ruzsa-Szemer\\'edi (ORS) graphs. We say a graph $G$ is an $(r,t)$-ORS\ngraph if its edges can be partitioned into $t$ matchings $M_1,M_2, \\ldots, M_t$\neach of size $r$, such that for every $i$, $M_i$ is an induced matching in the\nsubgraph $M_{i} \\cup M_{i+1} \\cup \\cdots \\cup M_t$. This is a relaxation of the\nextensively-studied notion of a Ruzsa-Szemer\\'edi (RS) graph, the difference\nbeing that in an RS graph each $M_i$ must be an induced matching in $G$.\n  In this note, we show that these two notions are roughly equivalent.\nSpecifically, let $\\mathrm{ORS}(n)$ be the largest $t$ such that there exists\nan $n$-vertex ORS-$(\\Omega(n), t)$ graph, and define $\\mathrm{RS}(n)$\nanalogously. We show that if $\\mathrm{ORS}(n) \\ge \\Omega(n^c)$, then for any\nfixed $\\delta > 0$, $\\mathrm{RS}(n) \\ge \\Omega(n^{c(1-\\delta)})$. This resolves\na question of Behnezhad and Ghafari.",
        "Learned Image Compression (LIC) has explored various architectures, such as\nConvolutional Neural Networks (CNNs) and transformers, in modeling image\ncontent distributions in order to achieve compression effectiveness. However,\nachieving high rate-distortion performance while maintaining low computational\ncomplexity (\\ie, parameters, FLOPs, and latency) remains challenging. In this\npaper, we propose a hybrid Convolution and State Space Models (SSMs) based\nimage compression framework, termed \\textit{CMamba}, to achieve superior\nrate-distortion performance with low computational complexity. Specifically,\nCMamba introduces two key components: a Content-Adaptive SSM (CA-SSM) module\nand a Context-Aware Entropy (CAE) module. First, we observed that SSMs excel in\nmodeling overall content but tend to lose high-frequency details. In contrast,\nCNNs are proficient at capturing local details. Motivated by this, we propose\nthe CA-SSM module that can dynamically fuse global content extracted by SSM\nblocks and local details captured by CNN blocks in both encoding and decoding\nstages. As a result, important image content is well preserved during\ncompression. Second, our proposed CAE module is designed to reduce spatial and\nchannel redundancies in latent representations after encoding. Specifically,\nour CAE leverages SSMs to parameterize the spatial content in latent\nrepresentations. Benefiting from SSMs, CAE significantly improves spatial\ncompression efficiency while reducing spatial content redundancies. Moreover,\nalong the channel dimension, CAE reduces inter-channel redundancies of latent\nrepresentations via an autoregressive manner, which can fully exploit prior\nknowledge from previous channels without sacrificing efficiency. Experimental\nresults demonstrate that CMamba achieves superior rate-distortion performance.",
        "In this paper, we develop a regularized higher-order Taylor based method for\nsolving composite (e.g., nonlinear least-squares) problems. At each iteration,\nwe replace each smooth component of the objective function by a higher-order\nTaylor approximation with an appropriate regularization, leading to a\nregularized higher-order Taylor approximation (RHOTA) algorithm. We derive\nglobal convergence guarantees for RHOTA algorithm. In particular, we prove\nstationary point convergence guarantees for the iterates generated by RHOTA,\nand leveraging a Kurdyka-{\\L}ojasiewicz (KL) type property of the objective\nfunction, we derive improved rates depending on the KL parameter. When the\nTaylor approximation is of order $2$, we present an efficient implementation of\nRHOTA algorithm, demonstrating that the resulting nonconvex subproblem can be\neffectively solved utilizing standard convex programming tools. Furthermore, we\nextend the scope of our investigation to include the behavior and efficacy of\nRHOTA algorithm in handling systems of nonlinear equations and optimization\nproblems with nonlinear equality constraints deriving new rates under improved\nconstraint qualifications conditions. Finally, we consider solving the phase\nretrieval problem with a higher-order proximal point algorithm, showcasing its\nrapid convergence rate for this particular application. Numerical simulations\non phase retrieval and output feedback control problems also demonstrate the\nefficacy and performance of the proposed methods when compared to some\nstate-of-the-art optimization methods and software.",
        "The Rasch model has been widely used to analyse item response data in\npsychometrics and educational assessments. When the number of individuals and\nitems are large, it may be impractical to provide all possible responses. It is\ndesirable to study sparse item response experiments. Here, we propose to use\nthe Erd\\H{o}s\\textendash R\\'enyi random sampling design, where an individual\nresponds to an item with low probability $p$. We prove the uniform consistency\nof the maximum likelihood estimator %by developing a leave-one-out method for\nthe Rasch model when both the number of individuals, $r$, and the number of\nitems, $t$, approach infinity. Sampling probability $p$ can be as small as\n$\\max\\{\\log r\/r, \\log t\/t\\}$ up to a constant factor, which is a fundamental\nrequirement to guarantee the connection of the sampling graph by the theory of\nthe Erd\\H{o}s\\textendash R\\'enyi graph. The key technique behind this\nsignificant advancement is a powerful leave-one-out method for the Rasch model.\nWe further establish the asymptotical normality of the MLE by using a simple\nmatrix to approximate the inverse of the Fisher information matrix. The\ntheoretical results are corroborated by simulation studies and an analysis of a\nlarge item-response dataset.",
        "Human Activity Recognition (HAR) has gained significant importance with the\ngrowing use of sensor-equipped devices and large datasets. This paper evaluates\nthe performance of three categories of models : classical machine learning,\ndeep learning architectures, and Restricted Boltzmann Machines (RBMs) using\nfive key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and\nBerkeley MHAD). We assess various models, including Decision Trees, Random\nForests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs),\nusing metrics such as accuracy, precision, recall, and F1-score for a\ncomprehensive comparison. The results show that CNN models offer superior\nperformance across all datasets, especially on the Berkeley MHAD. Classical\nmodels like Random Forest do well on smaller datasets but face challenges with\nlarger, more complex data. RBM-based models also show notable potential,\nparticularly for feature learning. This paper offers a detailed comparison to\nhelp researchers choose the most suitable model for HAR tasks."
      ]
    }
  },
  {
    "id":2411.11513,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Rethinking the Inception Architecture for Computer Vision",
    "start_abstract":"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety tasks. Since 2014 very deep convolutional started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend translate immediate quality tasks (as long as enough labeled data is provided training), efficiency low parameter count still enabling factors use cases such mobile big-data scenarios. Here we exploring ways scale up that aim utilizing added computation efficiently possible by suitably factorized convolutions aggressive regularization. We benchmark our methods on ILSVRC 2012 classification challenge validation set demonstrate over art: 21:2% top-1 5:6% top-5 error single frame evaluation using network with 5 billion multiply-adds per inference less than 25 million parameters. With an ensemble 4 models multi-crop evaluation, report 3:5% 17:3% 3:6% official test set.",
    "start_categories":[
      "BioInformatics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data"
      ],
      "abstract":[
        "Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, massive data sets generated by NGS\u2014the Genome pilot alone includes nearly five terabases\u2014make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated Indeed, many professionals limited in scope ease with which they can answer scientific questions complexity accessing manipulating produced these machines. Here, we discuss Analysis Toolkit (GATK), a structured programming framework designed to development efficient next-generation sequencers using functional philosophy MapReduce. The GATK provides small but rich set access patterns that encompass majority tool needs. Separating specific calculations from common management infrastructure enables us optimize correctness, stability, CPU memory efficiency enable distributed shared parallelization. We highlight capabilities describing implementation application robust, scale-tolerant like coverage calculators single nucleotide polymorphism (SNP) calling. conclude developers analysts quickly easily write NGS tools, have been incorporated into large-scale projects Project Cancer Atlas."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Canonical Energy-Momentum Tensor of Abelian Fields",
        "Split Gibbs Discrete Diffusion Posterior Sampling",
        "Toward Generalized Image Quality Assessment: Relaxing the Perfect\n  Reference Quality Assumption",
        "A note on multiplicative roots of multivariable formal power series",
        "On total transitivity of graphs",
        "Effects of non-parallelism on standard and magnetorheological\n  measurements",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human\n  Videos for Zero-Shot Robotic Manipulation",
        "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
        "Measurement of the Quantum Efficiency of Electrode Materials for VUV\n  Photons in Liquid Xenon",
        "General Coded Computing: Adversarial Settings",
        "Status and prospect of weak radiative hyperon decays",
        "LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding",
        "Higher Axion Strings",
        "MultiResolution Low-Rank Regularization of Dynamic Imaging Problems",
        "Multi-column Compton Camera of stacked Si pixel sensors for sub-degree\n  angular resolution",
        "Self-similar Features in Sub-secondary Breakup of a Droplet and Ligament\n  Mediated Fragmentation under Extreme Conditions",
        "Statistical Distributions for Transient Transport",
        "A Thermodynamic Theory of Proximity Ferroelectricity",
        "Federated Learning for Diffusion Models",
        "Layer-Resolved Quantum Transport in Twisted Bilayer Graphene:\n  Counterflow and Machine Learning Predictions",
        "Separation Axioms Among US",
        "Dominance Regions of Pursuit-evasion Games in Non-anticipative\n  Information Patterns",
        "Rate distortion dimension and ergodic decomposition for\n  $\\mathbb{R}^d$-actions",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "LLM Embeddings for Deep Learning on Tabular Data",
        "Energy-efficient Merging of Connected and Automated Vehicles using\n  Control Barrier Functions",
        "Moderate deviations in first-passage percolation for bounded weights"
      ],
      "abstract":[
        "In this tutorial, we provide the natural derivation of symmetrical,\ngauge-invariant canonical energy-momentum tensor for the abelian gauge field,\ni.e., the electromagnetic field.",
        "We study the problem of posterior sampling in discrete-state spaces using\ndiscrete diffusion models. While posterior sampling methods for continuous\ndiffusion models have achieved remarkable progress, analogous methods for\ndiscrete diffusion models remain challenging. In this work, we introduce a\nprincipled plug-and-play discrete diffusion posterior sampling algorithm based\non split Gibbs sampling, which we call SG-DPS. Our algorithm enables\nreward-guided generation and solving inverse problems in discrete-state spaces.\nWe demonstrate that SG-DPS converges to the true posterior distribution on\nsynthetic benchmarks, and enjoys state-of-the-art posterior sampling\nperformance on a range of benchmarks for discrete data, achieving up to 2x\nimproved performance compared to existing baselines.",
        "Full-reference image quality assessment (FR-IQA) generally assumes that\nreference images are of perfect quality. However, this assumption is flawed due\nto the sensor and optical limitations of modern imaging systems. Moreover,\nrecent generative enhancement methods are capable of producing images of higher\nquality than their original. All of these challenge the effectiveness and\napplicability of current FR-IQA models. To relax the assumption of perfect\nreference image quality, we build a large-scale IQA database, namely DiffIQA,\ncontaining approximately 180,000 images generated by a diffusion-based image\nenhancer with adjustable hyper-parameters. Each image is annotated by human\nsubjects as either worse, similar, or better quality compared to its reference.\nBuilding on this, we present a generalized FR-IQA model, namely Adaptive\nFidelity-Naturalness Evaluator (A-FINE), to accurately assess and adaptively\ncombine the fidelity and naturalness of a test image. A-FINE aligns well with\nstandard FR-IQA when the reference image is much more natural than the test\nimage. We demonstrate by extensive experiments that A-FINE surpasses standard\nFR-IQA models on well-established IQA datasets and our newly created DiffIQA.\nTo further validate A-FINE, we additionally construct a super-resolution IQA\nbenchmark (SRIQA-Bench), encompassing test images derived from ten\nstate-of-the-art SR methods with reliable human quality annotations. Tests on\nSRIQA-Bench re-affirm the advantages of A-FINE. The code and dataset are\navailable at https:\/\/tianhewu.github.io\/A-FINE-page.github.io\/.",
        "Suppose that we are given a formal power series of many variables with\ncoefficients in $\\mathbb{R}$ (or $\\mathbb{C}$) and we want to compute its\n$n$-th (multiplicative) root. As can be expected coefficients of the root have\nto satisfy a system of infinitely many equations. We present such a system of\nequations that strictly corresponds with the system for $n$-th of a formal\npower series of one variable. With help of an example we show that the case of\nformal power series of many variables is very different from the one variable\ncase with respect to the existence of roots.",
        "Let $G=(V, E)$ be a graph where $V$ and $E$ are the vertex and edge sets,\nrespectively. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{dominates} $B$ if every vertex of $B$ is adjacent to at least one vertex\nof $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. In this article, we study a variation of transitive partition,\nnamely \\emph{total transitive partition}. The total transitivity $Tr_t(G)$ is\nequal to the maximum order of a vertex partition $\\pi = \\{V_1, V_2, \\ldots,\nV_k\\}$ of $G$ obtained by repeatedly removing a total dominating set from $G$,\nuntil no vertices remain. Thus, $V_1$ is a total dominating set of $G$, $V_2$\nis a total dominating set of the graph $G_1=G-V_1$, and, in general, $V_{i+1}$\nis a total dominating set in the graph $G_i=G-\\bigcup_{j=1}^i V_i$. A vertex\npartition of order $Tr_t(G)$ is called $Tr_t$-partition. The \\textsc{Maximum\nTotal Transitivity Problem} is to find a total transitive partition of a given\ngraph with the maximum number of parts. First, we characterize split graphs\nwith total transitivity equal to $1$ and $\\omega(G)-1$. Moreover, for the split\ngraph $G$ and $1\\leq p\\leq \\omega(G)-1$, we give some necessary conditions for\n$Tr_t(G)=p$. Furthermore, we show that the decision version of this problem is\nNP-complete for bipartite graphs. On the positive side, we design a\npolynomial-time algorithm to solve \\textsc{Maximum Total Transitivity Problem}\nin trees.",
        "Human blood has a complex composition and unique rheological properties,\nmaking it challenging to measure accurately. In addition to this, its\nmechanical properties may be influenced by external magnetic fields, which,\ndespite being a characteristic of significant interest in the development of\nnew treatment therapies, remains relatively unexplored. To achieve an accurate\nmagnetorheological description of blood, the employed equipment must achieve\naccurate results taking into account its low viscous and elastic character.\nHowever, low and inconsistent apparent-viscosity values were observed\nsystematically in a rotational rheometer equipped with a magnetorheological\ncell, without the applied magnetic field. In this work, a parametric study was\nconducted, experimentally and numerically, to evaluate this error source.\nSteady shear measurements were carried out with low-viscosity Newtonian fluids\nwith two geometries: a parallel-plate, at different gap heights, and a\ncone-plate. An additional standard bottom plate for non-magnetic testing was\nalso employed for comparison. The standard bottom plate returned constant\nviscosities near the expected values, whereas the plate attached to the\nmagnetorheological cell showed a clear decrease of measured viscosity with\nparallel-plate gap reduction and an increase in cone-plate-measured viscosity.\nNumerical results corroborated the experimental observations, pointing towards\nan inclination of the bottom magnetic plate which can significantly affect the\nflow. Additional experimental and numerical work was conducted to evaluate the\neffects of the setup imperfection on magnetorheological measurements, unveiling\nmagnetorheology's deep dependence on the geometric characteristics.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "Future robots are envisioned as versatile systems capable of performing a\nvariety of household tasks. The big question remains, how can we bridge the\nembodiment gap while minimizing physical robot learning, which fundamentally\ndoes not scale well. We argue that learning from in-the-wild human videos\noffers a promising solution for robotic manipulation tasks, as vast amounts of\nrelevant data already exist on the internet. In this work, we present VidBot, a\nframework enabling zero-shot robotic manipulation using learned 3D affordance\nfrom in-the-wild monocular RGB-only human videos. VidBot leverages a pipeline\nto extract explicit representations from them, namely 3D hand trajectories from\nvideos, combining a depth foundation model with structure-from-motion\ntechniques to reconstruct temporally consistent, metric-scale 3D affordance\nrepresentations agnostic to embodiments. We introduce a coarse-to-fine\naffordance learning model that first identifies coarse actions from the pixel\nspace and then generates fine-grained interaction trajectories with a diffusion\nmodel, conditioned on coarse actions and guided by test-time constraints for\ncontext-aware interaction planning, enabling substantial generalization to\nnovel scenes and embodiments. Extensive experiments demonstrate the efficacy of\nVidBot, which significantly outperforms counterparts across 13 manipulation\ntasks in zero-shot settings and can be seamlessly deployed across robot systems\nin real-world environments. VidBot paves the way for leveraging everyday human\nvideos to make robot learning more scalable.",
        "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.",
        "Light dark matter searches using ionization signals in dual-phase liquid\nxenon (LXe) time projection chambers (TPCs) are limited by low-energy\nionization backgrounds, including those from the photoelectric effect on the\nelectrodes. To address this, we measured the quantum efficiency (QE) of various\nelectrode materials for vacuum ultraviolet (VUV) photons in LXe, including\nplatinum (Pt), stainless steel (SUS304), and magnesium fluoride\n(MgF$_{2}$)-coated aluminum (Al). Our results show that MgF$_{2}$-coated Al\nexhibits the lowest QE among the tested materials. The QE for VUV photons with\na mean wavelength of 179.5~nm was measured to be $(7.2 \\pm 2.3) \\times\n10^{-5}$, corresponding to a reduction by a factor of 4.4 compared to SUS304, a\ncommonly used electrode material in direct dark matter experiments with LXe.\nThese findings suggest that employing low-QE electrodes can help mitigate\nphotoelectric-induced backgrounds, potentially improving the sensitivity of LXe\nTPCs in light dark matter searches.",
        "Conventional coded computing frameworks are predominantly tailored for\nstructured computations, such as matrix multiplication and polynomial\nevaluation. Such tasks allow the reuse of tools and techniques from algebraic\ncoding theory to improve the reliability of distributed systems in the presence\nof stragglers and adversarial servers.\n  This paper lays the foundation for general coded computing, which extends the\napplicability of coded computing to handle a wide class of computations. In\naddition, it particularly addresses the challenging problem of managing\nadversarial servers. We demonstrate that, in the proposed scheme, for a system\nwith $N$ servers, where $\\mathcal{O}(N^a)$, $a \\in [0,1)$, are adversarial, the\nsupremum of the average approximation error over all adversarial strategies\ndecays at a rate of $N^{\\frac{6}{5}(a-1)}$, under minimal assumptions on the\ncomputing tasks. Furthermore, we show that within a general framework, the\nproposed scheme achieves optimal adversarial robustness, in terms of maximum\nnumber of adversarial servers it can tolerate. This marks a significant step\ntoward practical and reliable general coded computing. Implementation results\nfurther validate the effectiveness of the proposed method in handling various\ncomputations, including inference in deep neural networks.",
        "Weak radiative hyperon decays represent a rich interplay between weak\ninteractions and the internal structure of baryons, offering profound insights\ninto Quantum Chromodynamics and weak interactions. Recent experimental\nobservations, particularly from BESIII, have revealed deviations from\ntheoretical predictions. These deviations could signal new physics or the need\nfor refined theoretical models incorporating intermediate resonance effects.\nThis review discusses recent theoretical advancements and key experimental\nfindings, focusing on recent measurements from BESIII and their implications\nfor strong interactions and baryon structure.",
        "Current advanced long-context language models offer great potential for\nreal-world software engineering applications. However, progress in this\ncritical domain remains hampered by a fundamental limitation: the absence of a\nrigorous evaluation framework for long code understanding. To gap this\nobstacle, we propose a long code understanding benchmark LONGCODEU from four\naspects (8 tasks) to evaluate LCLMs' long code understanding ability required\nfor practical applications, including code unit perception, intra-code unit\nunderstanding, inter-code unit relation understanding, and long code\ndocumentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6\ngeneral models and 3 code models). Our experimental results reveal key\nlimitations in current LCLMs' capabilities for long code understanding.\nParticularly, the performance of LCLMs drops dramatically when the long code\nlength is greater than 32K, falling far short of their claimed 128K-1M context\nwindows. In the four aspects, inter-code unit relation understanding is the\nmost challenging for LCLMs. Our study provides valuable insights for optimizing\nLCLMs and driving advancements in software engineering.",
        "We study the minimal requirements to obtain axion strings for axions with\nexponentially good quality. These ingredients appear in theories where an axion\ncoming from a higher-form gauge field mixes with the phase of a complex scalar\nfield in a situation that resembles higher-groups. The resulting axion is\nperturbatively massless and inherits a high-quality shift symmetry from the\nglobal higher-form symmetry while being compatible with a post-inflationary\naxion scenario. Due to differences and resemblances with both,\nextra-dimensional and field theory axions, we call this field the higher axion.\nTo this end, we study a toy model on a 5-dimensional manifold with boundary.\nThe boundary hosts the complex scalar that provides axion strings through\nstandard mechanisms. In addition, we study how these scenarios may arise in\nheterotic string theory and type II string compactifications.",
        "MultiResolution Low-Rank decomposition is formulated for regularization of\ndynamic image sequences. The decomposition applies a local low-rank\ndecomposition on a sequence of discrete wavelet transforms. Its effective\nformulation as a regularization functional is discussed and numerically tested\nfor dynamic X-ray tomography in comparison to other low-rank methods. The\nresults suggest it is similar to traditional locally low-rank decomposition but\nproduces less severe block artifacts.",
        "The Compton camera is a sensitive imaging detector for soft gamma-rays.\nCompton Reconstruction can not only give imaging capability but also remove\nbackground events to achieve good sensitivity. However, the angular resolution\nis in principle limited to several degrees. In this paper, we propose a novel\nconcept of Compton camera incorporating shadow effects. We consider\nmulti-column Compton camera (MCCC), consisting of stacked Si pixel sensors.\nEach of columns is separated each other to create shadow effects. This design\nachieves an angular resolution of less than 1 degree within around 1 degree\nfrom the center of the field-of-view by just modifying a conventional\nSi-stacked Compton camera and keeping advantages (wide field-of-view and good\nsensitivity) of conventional Compton camera. Here we validated the concept of\nproposed Compton camera through Monte-Carlo simulation. MCCC with 1 m column\nheight, 0.5 mm pixel size, 100 layers, and 10 columns for the 1-D direction can\ndistinguish two sources separated by 0.1 degree with 0.6M Compton-reconstructed\nevents.",
        "Droplet formation is relevant in many applications spanning natural and\nartificial settings. Comprehending droplet aerobreakup or air-assisted\nsecondary atomization is challenging, especially in high-speed flow scenarios.\nThis entails multi-scale interface deformations with intricate wave dynamics\nthat conform to a non-linear cascade. In the present study, we look into\nshockwave-induced breakups and associated intermediate processes happening at\nsmaller spatiotemporal scales across the disintegrating droplet interface at\ndifferent Weber numbers ($We \\sim 10^3$). We observe the undulations to follow\nbreakup patterns that resemble a scaled-down version of a secondary atomization\nevent. These sub-secondary breakup processes end with corrugated ligaments that\ngenerate the final daughter droplets. The size distribution of these droplets\nis estimated using a Depth from Defocus (DFD) technique. These illustrate the\ntransient nature of aerobreakup, where the normalized statistics in subsequent\ntime periods and different $We$ are observed to follow a universal\ndistribution. This conforms to a gamma distribution where the associated fit\nparameters agree well with the coefficients determined from ligament shape\nfactors, corresponding to the limit associated with most extreme corrugations.\nScaling laws based on $We$ are deduced for the averaged statistics using a high\nenergy chaotic breakup mechanism. These observations reinforce the idea of a\nself-similar mechanism for catastrophic aerobreakup of a droplet.",
        "This paper introduces the use of statistical distributions based on transport\ndifferential equations for clear distinction of transport modes within\ntransient kinetic experiments. More specifically,novel techniques are developed\nfor the transient data obtained through the Temporal Analysis of Products (TAP)\nreactor. The methodology allows distinguishing between two domains of diffusion\ntransport in heterogeneous catalytic systems, i.e., Knudsen and non-Knudsen\ndiffusion, using statistical fingerprints, and finding the transition domain.\nTwo distribution parameters were obtained that directly result in coefficients\nthat correspond to the concentration and the rate of transport. Using a linear\nrelationship between the rate and concentration coefficients, Knudsen diffusion\nis revealed when the rate of transport is constant and non-Knudsen diffusion is\nconfirmed when the rate of transport coefficient is a function of the\nconcentration coefficient. As a result, accurate transport information is\nobtained while in the presence of instrument drift or noise while investigating\nhigher pressure pulse responses. As such, experiments where the influence of\ngas phase reactions can be more directly studied.",
        "Proximity ferroelectricity has recently been reported as a new design\nparadigm for inducing ferroelectricity, where a non-ferroelectric polar\nmaterial becomes a ferroelectric by interfacing with a thin ferroelectric\nlayer. Strongly polar materials, such as AlN and ZnO, which were previously\nunswitchable with an external field below their dielectric breakdown fields,\ncan now be switched with practical coercive fields when they are in intimate\nproximity to a switchable ferroelectric. Here, we develop a general\nLandau-Ginzburg theory of proximity ferroelectricity in multilayers of\nnon-ferroelectrics and ferroelectrics to analyze their switchability and\ncoercive fields. The theory predicts regimes of both \"proximity switching\"\nwhere the multilayers collectively switch, as well as \"proximity suppression\"\nwhere they collectively do not switch. The mechanism of the proximity\nferroelectricity is an internal electric field determined by the polarization\nof the layers and their relative thickness in a self-consistent manner that\nrenormalizes the double-well ferroelectric potential to lower the steepness of\nthe switching barrier. Further reduction in the coercive field emerges from\ncharged defects in the bulk that act as nucleation centers. The application of\nthe theory to proximity ferroelectricity in Alx-1ScxN\/AlN and Zn1-xMgxO\/ZnO\nbilayers is demonstrated. The theory further predicts that multilayers of\ndielectric\/ferroelectric and paraelectric\/ferroelectric layers can potentially\nresult in induced ferroelectricity in the dielectric or paraelectric layers,\nresulting in the entire stack being switched, an exciting avenue for new\ndiscoveries. This thawing of \"frozen ferroelectrics\", paraelectrics and\npotentially dielectrics, promises a large class of new ferroelectrics with\nexciting prospects for previously unrealizable domain-patterned optoelectronic\nand memory technologies.",
        "Diffusion models are powerful generative models that can produce highly\nrealistic samples for various tasks. Typically, these models are constructed\nusing centralized, independently and identically distributed (IID) training\ndata. However, in practical scenarios, data is often distributed across\nmultiple clients and frequently manifests non-IID characteristics. Federated\nLearning (FL) can leverage this distributed data to train diffusion models, but\nthe performance of existing FL methods is unsatisfactory in non-IID scenarios.\nTo address this, we propose FedDDPM-Federated Learning with Denoising Diffusion\nProbabilistic Models, which leverages the data generative capability of\ndiffusion models to facilitate model training. In particular, the server uses\nwell-trained local diffusion models uploaded by each client before FL training\nto generate auxiliary data that can approximately represent the global data\ndistribution. Following each round of model aggregation, the server further\noptimizes the global model using the auxiliary dataset to alleviate the impact\nof heterogeneous data on model performance. We provide a rigorous convergence\nanalysis of FedDDPM and propose an enhanced algorithm, FedDDPM+, to reduce\ntraining overheads. FedDDPM+ detects instances of slow model learning and\nperforms a one-shot correction using the auxiliary dataset. Experimental\nresults validate that our proposed algorithms outperform the state-of-the-art\nFL algorithms on the MNIST, CIFAR10 and CIFAR100 datasets.",
        "The layer-resolved quantum transport response of a twisted bilayer graphene\ndevice is investigated by driving a current through the bottom layer and\nmeasuring the induced voltage in the top layer. Devices with four- and\neight-layer differentiated contacts were analyzed, revealing that in a\nnanoribbon geometry (four contacts), a counterflow current emerges in the top\nlayer, while in a square-junction configuration (eight contacts), this\ncounterflow is accompanied by a transverse, or Hall, component. These effects\npersist despite weak coupling to contacts, onsite disorder, and variations in\ndevice size. The observed counterflow response indicates a circulating\ninterlayer current, which generates an in-plane magnetic moment excited by the\ninjected current. Finally, due to the intricate relationship between the\nelectrical layer response, energy, and twist angle, a clusterized machine\nlearning model was trained, validated, and tested to predict various\nconductances.",
        "A standard introductory result is that Hausdorff spaces have the property US,\nthat is, each convergent sequence has a unique limit. This paper explores\nseveral existing and new characterizations of separation axioms that are\nstrictly weaker than $T_2$ but strictly stronger than US.",
        "The evader's dominance region is an important concept and the foundation of\ngeometric methods for pursuit-evasion games. This article mainly reveals the\nrelevant properties of the evader's dominance region, especially in\nnon-anticipative information patterns. We can use these properties to research\npursuit-evasion games in non-anticipative information patterns. The core\nproblem is under what condition the pursuer has a non-anticipative strategy to\nprevent the evader leaving its initial dominance region before being captured\nregardless of the evader's strategy. We first define the evader's dominance\nregion by the shortest path distance, and we rigorously prove for the first\ntime that the initial dominance region of the evader is the reachable region of\nthe evader in the open-loop sense. Subsequently, we prove that there exists a\nnon-anticipative strategy by which the pursuer can capture the evader before\nthe evader leaves its initial dominance region's closure in the absence of\nobstacles. For cases with obstacles, we provide a counter example to illustrate\nthat such a non-anticipative strategy does not always exist, and provide a\nnecessary condition for the existence of such strategy. Finally, we consider a\nscenario with a single corner obstacle and provide a sufficient condition for\nthe existence of such a non-anticipative strategy. At the end of this article,\nwe discuss the application of the evader's dominance region in target defense\ngames. This article has important reference significance for the design of\nnon-anticipative strategies in pursuit-evasion games with obstacles.",
        "Rate distortion dimension describes the theoretical limit of lossy data\ncompression methods as the distortion bound goes to zero. It was originally\nintroduced in the context of information theory, and recently it was discovered\nthat it has an intimate connection to Gromov's theory of mean dimension of\ndynamical systems. This paper studies the behavior of rate distortion dimension\nof $\\mathbb{R}^d$-actions under ergodic decomposition. Our main theorems\nprovide natural convexity and concavity of upper and lower rate distortion\ndimensions under convex combination of invariant probability measures. We also\npresent examples which clarify the validity and limitations of the theorems.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "Tabular deep-learning methods require embedding numerical and categorical\ninput features into high-dimensional spaces before processing them. Existing\nmethods deal with this heterogeneous nature of tabular data by employing\nseparate type-specific encoding approaches. This limits the cross-table\ntransfer potential and the exploitation of pre-trained knowledge. We propose a\nnovel approach that first transforms tabular data into text, and then leverages\npre-trained representations from LLMs to encode this data, resulting in a\nplug-and-play solution to improv ing deep-learning tabular methods. We\ndemonstrate that our approach improves accuracy over competitive models, such\nas MLP, ResNet and FT-Transformer, by validating on seven classification\ndatasets.",
        "Highway merges present difficulties for human drivers and automated vehicles\ndue to incomplete situational awareness and a need for a structured\n(precedence, order) environment, respectively. In this paper, an unstructured\nmerge algorithm is presented for connected and automated vehicles. There is\nneither precedence nor established passing order through the merge point. The\nalgorithm relies on Control Barrier Functions for safety (collision avoidance)\nand for coordination that arises from exponential instability of\nstall-equilibria in the inter-agent space. A Monte Carlo simulation comparison\nto a first-in-first-out approach shows improvement in traffic flow and a\nsignificant energy efficiency benefit.",
        "We investigate the moderate and large deviations in first-passage percolation\n(FPP) with bounded weights on $\\mathbb{Z}^d$ for $d \\geq 2$. Write\n$T(\\mathbf{x}, \\mathbf{y})$ for the first-passage time and denote by\n$\\mu(\\mathbf{u})$ the time constant in direction $\\mathbf{u}$. In this paper,\nwe establish that, if one assumes that the sublinear error term $T(\\mathbf{0},\nN\\mathbf{u}) - N\\mu(\\mathbf{u})$ is of order $N^\\chi$, then under some\nunverified (but widely believed) assumptions, for $\\chi < a < 1$,\n\\begin{align*}\n  &\\mathbb{P}\\bigl(T(\\mathbf{0}, N\\mathbf{u}) > N\\mu(\\mathbf{u}) + N^a\\bigr) =\n\\exp{\\Big(-\\,N^{\\frac{d(1+o(1))}{1-\\chi}(a-\\chi)}\\Big)},\n  &\\mathbb{P}\\bigl(T(\\mathbf{0}, N\\mathbf{u}) < N\\mu(\\mathbf{u}) - N^a\\bigr) =\n\\exp{\\Big(-\\,N^{\\frac{1+o(1)}{1-\\chi}(a-\\chi)}\\Big)}, \\end{align*} with\naccompanying estimates in the borderline case $a=1$. Moreover, the exponents\n$\\frac{d}{1-\\chi}$ and $\\frac{1}{1-\\chi}$ also appear in the asymptotic\nbehavior near $0$ of the rate functions for upper and lower tail large\ndeviations. Notably, some of our estimates are established rigorously without\nrelying on any unverified assumptions. Our main results highlight the interplay\nbetween fluctuations and the decay rates of large deviations, and bridge the\ngap between these two regimes.\n  A key ingredient of our proof is an improved concentration via multi-scale\nanalysis for several moderate deviation estimates, a phenomenon that has\npreviously appeared in the contexts of two-dimensional last-passage percolation\nand two-dimensional rotationally invariant FPP."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Human-Centric Interfaces for Ambient Intelligence",
    "start_abstract":"To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence.",
    "start_categories":[
      "physics.app-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Speaker Diarization with LSTM"
      ],
      "abstract":[
        "For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Multi-modal Speech Enhancement with Limited Electromyography Channels",
        "Spline Quantile Regression",
        "Custom Loss Functions in Fuel Moisture Modeling",
        "Interior control for surfaces with positive scalar curvature and its\n  application",
        "Estimates for short character sums evaluated at homogeneous polynomials",
        "An Explainable Pipeline for Machine Learning with Functional Data",
        "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Data-driven geometric parameter optimization for PD-GMRES",
        "Diagrammatic Categories which arise from Representation Graphs",
        "Detecting Heel Strike and toe off Events Using Kinematic Methods and\n  LSTM Models",
        "Approximation properties of neural ODEs",
        "Objective Metrics for Human-Subjects Evaluation in Explainable\n  Reinforcement Learning",
        "Unraveling Pedestrian Fatality Patterns: A Comparative Study with\n  Explainable AI",
        "Duoidal R-Matrices",
        "Dark Deceptions in DHCP: Dismantling Network Defenses",
        "Numerical Schemes for Signature Kernels",
        "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
        "A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders",
        "Nonabelian Yang-Mills-Higgs and Plateau's problem in codimension three",
        "Multi-layer RIS on Edge: Communication, Computation and Wireless Power\n  Transfer",
        "Causal Learning for Heterogeneous Subgroups Based on Nonlinear Causal\n  Kernel Clustering",
        "Spiking Neural Network Accelerator Architecture for Differential-Time\n  Representation using Learned Encoding",
        "Fast and Cheap Covariance Smoothing",
        "Phase variation and angular momentum of the Riemann, and, Dirichlet Xi\n  functions",
        "On the ADM mass of critical area-normalized capacitors",
        "Assortative Marriage and Geographic Sorting",
        "CPVis: Evidence-based Multimodal Learning Analytics for Evaluation in\n  Collaborative Programming",
        "Measuring Political Preferences in AI Systems: An Integrative Approach"
      ],
      "abstract":[
        "Speech enhancement (SE) aims to improve the clarity, intelligibility, and\nquality of speech signals for various speech enabled applications. However,\nair-conducted (AC) speech is highly susceptible to ambient noise, particularly\nin low signal-to-noise ratio (SNR) and non-stationary noise environments.\nIncorporating multi-modal information has shown promise in enhancing speech in\nsuch challenging scenarios. Electromyography (EMG) signals, which capture\nmuscle activity during speech production, offer noise-resistant properties\nbeneficial for SE in adverse conditions. Most previous EMG-based SE methods\nrequired 35 EMG channels, limiting their practicality. To address this, we\npropose a novel method that considers only 8-channel EMG signals with acoustic\nsignals using a modified SEMamba network with added cross-modality modules. Our\nexperiments demonstrate substantial improvements in speech quality and\nintelligibility over traditional approaches, especially in extremely low SNR\nsettings. Notably, compared to the SE (AC) approach, our method achieves a\nsignificant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under\nmismatched conditions, highlighting its robustness.",
        "Quantile regression is a powerful tool capable of offering a richer view of\nthe data as compared to linear-squares regression. Quantile regression is\ntypically performed individually on a few quantiles or a grid of quantiles\nwithout considering the similarity of the underlying regression coefficients at\nnearby quantiles. When needed, an ad hoc post-processing procedure such as\nkernel smoothing is employed to smooth the estimated coefficients across\nquantiles and thereby improve the performance of these estimates. This paper\nintroduces a new method, called spline quantile regression (SQR), that unifies\nquantile regression with quantile smoothing and jointly estimates the\nregression coefficients across quantiles as smoothing splines. We discuss the\ncomputation of the SQR solution as a linear program (LP) using an\ninterior-point algorithm. We also experiment with some gradient algorithms that\nrequire less memory than the LP algorithm. The performance of the SQR method\nand these algorithms is evaluated using simulated and real-world data.",
        "Fuel moisture content (FMC) is a key predictor for wildfire rate of spread\n(ROS). Machine learning models of FMC are being used more in recent years,\naugmenting or replacing traditional physics-based approaches. Wildfire rate of\nspread (ROS) has a highly nonlinear relationship with FMC, where small\ndifferences in dry fuels lead to large differences in ROS. In this study,\ncustom loss functions that place more weight on dry fuels were examined with a\nvariety of machine learning models of FMC. The models were evaluated with a\nspatiotemporal cross-validation procedure to examine whether the custom loss\nfunctions led to more accurate forecasts of ROS. Results show that the custom\nloss functions improved accuracy for ROS forecasts by a small amount. Further\nresearch would be needed to establish whether the improvement in ROS forecasts\nleads to more accurate real-time wildfire simulations.",
        "Let $M^n$, $n\\in\\{3,4,5\\}$, be a closed aspherical $n$-manifold and $S\\subset\nM$ a subset consisting of disjoint incompressible embedded closed aspherical\nsubmanifolds (possibly with different dimensions). When $n =3,4$, we show that\n$M\\setminus S$ cannot admit any complete metric with positive scalar curvature.\nWhen $n=5$, we obtain the same result either when $S$ contains a submanifold of\ncodimension 1 or 2, or when $S$ itself is a connected submaifold of codimension\n$\\ge 3.$ The key ingredient is a new interior control for the extrinsic\ndiameter of surfaces with positive scalar curvature.",
        "Let $p$ be a prime. We prove bounds on short Dirichlet character sums\nevaluated at a class of homogeneous polynomials in arbitrary dimensions. In\nevery dimension, this bound is nontrivial for sums over boxes with side lengths\nas short as $p^{1\/4 + \\kappa}$ for any $\\kappa>0$. Our methods capitalize on\nthe relationship between characters mod $p$ and characters over finite field\nextensions as well as bounds on the multiplicative energy of sets in products\nof finite fields.",
        "Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.",
        "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "Restarted GMRES is a robust and widely used iterative solver for linear\nsystems. The control of the restart parameter is a key task to accelerate\nconvergence and to prevent the well-known stagnation phenomenon. We focus on\nthe Proportional-Derivative GMRES (PD-GMRES), which has been derived using\ncontrol-theoretic ideas in [Cuevas N\\'u\\~nez, Schaerer, and Bhaya (2018)] as a\nversatile method for modifying the restart parameter. Several variants of a\nquadtree-based geometric optimization approach are proposed to find a best\nchoice of PD-GMRES parameters. We show that the optimized PD-GMRES performs\nwell across a large number of matrix types and we observe superior performance\nas compared to major other GMRES-based iterative solvers. Moreover, we propose\nan extension of the PD-GMRES algorithm to further improve performance by\ncontrolling the range of values for the restart parameter.",
        "The main result of this paper utilizes the representation graph of a group\n$G$, $R(V,G)$, and gives a general construction of a diagrammatic category\n$\\mathbf{Dgrams}_{R(V,G)}$. The proof of the main theorem shows that, given\nexplicit criteria, there is an equivalence of categories between a quotient\ncategory of $\\mathbf{Dgrams}_{R(V,G)}$ and a full subcategory of\n$G-\\textbf{mod}$ with objects being the tensor products of finitely many\nirreducible $G$-modules.",
        "Accurate gait event detection is crucial for gait analysis, rehabilitation,\nand assistive technology, particularly in exoskeleton control, where precise\nidentification of stance and swing phases is essential. This study evaluated\nthe performance of seven kinematics-based methods and a Long Short-Term Memory\n(LSTM) model for detecting heel strike and toe-off events across 4363 gait\ncycles from 588 able-bodied subjects. The results indicated that while the Zeni\net al. method achieved the highest accuracy among kinematics-based approaches,\nother methods exhibited systematic biases or required dataset-specific tuning.\nThe LSTM model performed comparably to Zeni et al., providing a data-driven\nalternative without systematic bias. These findings highlight the potential of\ndeep learning-based approaches for gait event detection while emphasizing the\nneed for further validation in clinical populations and across diverse gait\nconditions. Future research will explore the generalizability of these methods\nin pathological populations, such as individuals with post-stroke conditions\nand knee osteoarthritis, as well as their robustness across varied gait\nconditions and data collection settings to enhance their applicability in\nrehabilitation and exoskeleton control.",
        "We study the approximation properties of shallow neural networks whose\nactivation function is defined as the flow of a neural ordinary differential\nequation (neural ODE) at the final time of the integration interval. We prove\nthe universal approximation property (UAP) of such shallow neural networks in\nthe space of continuous functions. Furthermore, we investigate the\napproximation properties of shallow neural networks whose parameters are\nrequired to satisfy some constraints. In particular, we constrain the Lipschitz\nconstant of the flow of the neural ODE to increase the stability of the shallow\nneural network, and we restrict the norm of the weight matrices of the linear\nlayers to one to make sure that the restricted expansivity of the flow is not\ncompensated by the increased expansivity of the linear layers. For this\nsetting, we prove approximation bounds that tell us the accuracy to which we\ncan approximate a continuous function with a shallow neural network with such\nconstraints. We prove that the UAP holds if we consider only the constraint on\nthe Lipschitz constant of the flow or the unit norm constraint on the weight\nmatrices of the linear layers.",
        "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
        "Road fatalities pose significant public safety and health challenges\nworldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian\ncrashes due to disparities in physical and performance characteristics. This\nstudy employs explainable artificial intelligence (XAI) to identify key factors\ncontributing to pedestrian fatalities across the five U.S. states with the\nhighest crash rates (2018-2022). It compares them to the five states with the\nlowest fatality rates. Using data from the Fatality Analysis Reporting System\n(FARS), the study applies machine learning techniques-including Decision Trees,\nGradient Boosting Trees, Random Forests, and XGBoost-to predict contributing\nfactors to pedestrian fatalities. To address data imbalance, the Synthetic\nMinority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive\nExplanations (SHAP) values enhance model interpretability. The results indicate\nthat age, alcohol and drug use, location, and environmental conditions are\nsignificant predictors of pedestrian fatalities. The XGBoost model outperformed\nothers, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of\n92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian\nfatalities are more common in mid-block locations and areas with poor\nvisibility, with older adults and substance-impaired individuals at higher\nrisk. These insights can inform policymakers and urban planners in implementing\ntargeted safety measures, such as improved lighting, enhanced pedestrian\ninfrastructure, and stricter traffic law enforcement, to reduce fatalities and\nimprove public safety.",
        "In this note, we define an analogue of R-matrices for bialgebras in the\nsetting of a monad that is opmonoidal over two tensor products. Analogous to\nthe classical case, such structures bijectively correspond to duoidal\nstructures on the Eilenberg--Moore category of the monad. Further, we\ninvestigate how a cocommutative version of this lifts the linearly distributive\nstructure of a normal duoidal category.",
        "This paper explores vulnerabilities in the Dynamic Host Configuration\nProtocol (DHCP) and their implications on the Confidentiality, Integrity, and\nAvailability (CIA) Triad. Through an analysis of various attacks, including\nDHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits,\nthe paper provides a taxonomic classification of threats, assesses risks, and\nproposes appropriate controls. The discussion also highlights the dangers of\nVPN decloaking through DHCP exploits and underscores the importance of\nsafeguarding network infrastructures. By bringing awareness to the TunnelVision\nexploit, this paper aims to mitigate risks associated with these prevalent\nvulnerabilities.",
        "Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps:\/\/github.com\/FrancescoPiatti\/polysigkernel.",
        "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.",
        "We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.",
        "We investigate the asymptotic behavior of the\n$\\mathrm{SU}(2)$-Yang-Mills-Higgs energy $E(\\Phi,A)=\\int_M|d_A\\Phi|^2+|F_A|^2$\nin the large mass limit, proving convergence to the codimension-three area\nfunctional in the sense of De Giorgi's $\\Gamma$-convergence. More precisely,\nfor a compact manifold with boundary $M$ and any family of pairs\n$\\Phi_m\\in\\Omega^0(M;\\mathfrak{su}(2))$ and $A_m\\in\n\\Omega^1(M;\\mathfrak{su}(2))$ indexed by a mass parameter $m\\to\\infty$,\nsatisfying $$E(\\Phi_m,A_m)\\leq\nCm\\quad\\text{and}\\quad\\lim_{m\\to\\infty}\\frac{1}{m}\\int_M(m-|\\Phi_m|)^2=0,$$ we\nprove that the $(n-3)$-currents dual to $\\frac{1}{2\\pi\nm}\\mathrm{tr}(d_{A_m}\\Phi_m\\wedge F_{A_m})$ converge subsequentially to a\nrelative integral $(n-3)$-cycle $T$ of mass \\begin{equation}\n  \\mathbb{M}(T)\\leq \\liminf_{m\\to\\infty}\\frac{1}{4\\pi m}E(\\Phi_m,A_m),\n\\end{equation} and show conversely that any integral $(n-3)$-current $T$ with\n$[T]=0\\in H_{n-3}(M,\\partial M;\\mathbb{Z})$ admits such an approximation, with\nequality in the above inequality. In the special case of pairs $(\\Phi_m,A_m)$\nsatisfying the generalized monopole equation $*d_{A_m}\\Phi_m=F_{A_m}\\wedge\n\\Theta$ for a calibration form $\\Theta\\in \\Omega^{n-3}(M)$, we deduce that the\nlimit $\\nu=\\lim_{m\\to\\infty}\\frac{1}{2\\pi m}|d_{A_m}\\Phi_m|^2$ of the Dirichlet\nenergy measures satisfies $\\nu\\leq |T|$, with equality if and only if $T$ is\ncalibrated by $\\Theta$, giving evidence for predictions of Donaldson-Segal in\nthe settings of $G_2$-manifolds and Calabi-Yau $3$-folds.",
        "The rapid expansion of Internet of Things (IoT) and its integration into\nvarious applications highlight the need for advanced communication,\ncomputation, and energy transfer techniques. However, the traditional\nhardware-based evolution of communication systems faces challenges due to\nexcessive power consumption and prohibitive hardware cost. With the rapid\nadvancement of reconfigurable intelligent surface (RIS), a new approach by\nparallel stacking a series of RIS, i.e., multi-layer RIS, has been proposed.\nBenefiting from the characteristics of scalability, passivity, low cost, and\nenhanced computation capability, multi-layer RIS is a promising technology for\nfuture massive IoT scenarios. Thus, this article proposes a multi-layer\nRIS-based universal paradigm at the network edge, enabling three functions,\ni.e., multiple-input multiple-output (MIMO) communication, computation, and\nwireless power transfer (WPT). Starting by picturing the possible applications\nof multi-layer RIS, we explore the potential signal transmission links, energy\ntransmission links, and computation processes in IoT scenarios, showing its\nability to handle on-edge IoT tasks and associated green challenges. Then,\nthese three key functions are analyzed respectively in detail, showing the\nadvantages of the proposed scheme, compared with the traditional hardware-based\nscheme. To facilitate the implementation of this new paradigm into reality, we\nlist the dominant future research directions at last, such as inter-layer\nchannel modeling, resource allocation and scheduling, channel estimation, and\nedge training. It is anticipated that multi-layer RIS will contribute to more\nenergy-efficient wireless networks in the future by introducing a revolutionary\nparadigm shift to an all-wave-based approach.",
        "Due to the challenge posed by multi-source and heterogeneous data collected\nfrom diverse environments, causal relationships among features can exhibit\nvariations influenced by different time spans, regions, or strategies. This\ndiversity makes a single causal model inadequate for accurately representing\ncomplex causal relationships in all observational data, a crucial consideration\nin causal learning. To address this challenge, the nonlinear Causal Kernel\nClustering method is introduced for heterogeneous subgroup causal learning,\nhighlighting variations in causal relationships across diverse subgroups. The\nmain component for clustering heterogeneous subgroups lies in the construction\nof the $u$-centered sample mapping function with the property of unbiased\nestimation, which assesses the differences in potential nonlinear causal\nrelationships in various samples and supported by causal identifiability\ntheory. Experimental results indicate that the method performs well in\nidentifying heterogeneous subgroups and enhancing causal learning, leading to a\nreduction in prediction error.",
        "Spiking Neural Networks (SNNs) have garnered attention over recent years due\nto their increased energy efficiency and advantages in terms of operational\ncomplexity compared to traditional Artificial Neural Networks (ANNs). Two\nimportant questions when implementing SNNs are how to best encode existing data\ninto spike trains and how to efficiently process these spike trains in\nhardware. This paper addresses both of these problems by incorporating the\nencoding into the learning process, thus allowing the network to learn the\nspike encoding alongside the weights. Furthermore, this paper proposes a\nhardware architecture based on a recently introduced differential-time\nrepresentation for spike trains allowing decoupling of spike time and\nprocessing time. Together these contributions lead to a feedforward SNN using\nonly Leaky-Integrate and Fire (LIF) neurons that surpasses 99% accuracy on the\nMNIST dataset while still being implementable on medium-sized FPGAs with\ninference times of less than 295us.",
        "We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and\nefficient algorithm for estimating covariance tensors with large observational\nsizes. TReK extends the conjugate gradient method to incorporate range\nrestrictions, enabling its use in a variety of covariance smoothing\napplications. By leveraging matrix-level operations, it achieves significant\nimprovements in both computational speed and memory cost, improving over\nexisting methods by an order of magnitude. TReK ensures finite-step convergence\nin the absence of rounding errors and converges fast in practice, making it\nwell-suited for large-scale problems. The algorithm is also highly flexible,\nsupporting a wide range of forward and projection tensors.",
        "The concept of angular momentum is used to find new RH equivalence\nstatements, and, generalize some known results from Riemann to Dirichlet\nprimitive Xi functions",
        "In this note, we prove mass-capacity inequalities for asymptotically flat\nmanifolds whose boundary capacity potential satisfies an overdetermined\nproblem, referred to as critical area-normalized capacitors. As a consequence,\nwe obtain uniqueness results for the Schwarzschild metric, from which\nimprovements in the uniqueness theorems for spin asymptotically flat spacetimes\ncontaining a connected photon surface, as well as for spin asymptotically flat\nstatic manifolds with boundary are obtained.",
        "Between 1980 and 2000, the U.S. experienced a significant rise in geographic\nsorting and educational homogamy, with college graduates increasingly\nconcentrating in high-skill cities and marrying similarly educated spouses. We\ndevelop and estimate a spatial equilibrium model with local labor, housing, and\nmarriage markets, incorporating a marriage matching framework with transferable\nutility. Using the model, we estimate trends in assortative preferences,\nquantify the interplay between marital and geographic sorting, and assess their\ncombined impact on household inequality. Welfare analyses show that after\naccounting for marriage, the college well-being gap grew substantially more\nthan the college wage gap.",
        "As programming education becomes more widespread, many college students from\nnon-computer science backgrounds begin learning programming. Collaborative\nprogramming emerges as an effective method for instructors to support novice\nstudents in developing coding and teamwork abilities. However, due to limited\nclass time and attention, instructors face challenges in monitoring and\nevaluating the progress and performance of groups or individuals. To address\nthis issue, we collect multimodal data from real-world settings and develop\nCPVis, an interactive visual analytics system designed to assess student\ncollaboration dynamically. Specifically, CPVis enables instructors to evaluate\nboth group and individual performance efficiently. CPVis employs a novel\nflower-based visual encoding to represent performance and provides time-based\nviews to capture the evolution of collaborative behaviors. A within-subject\nexperiment (N=22), comparing CPVis with two baseline systems, reveals that\nusers gain more insights, find the visualization more intuitive, and report\nincreased confidence in their assessments of collaboration.",
        "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment."
      ]
    }
  },
  {
    "id":2412.00319,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Speaker Diarization with LSTM",
    "start_abstract":"For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and diarization applications. However, mirroring rise of deep learning in various domains, neural network embeddings, also known as <i xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\">d-vectors<\/i> , have consistently demonstrated superior performance. In this paper, we build on success d-vector systems to develop a new diarization. Specifically, combine LSTM-based embeddings with recent work non-parametric clustering obtain state-of-the-art system. Our system is evaluated three standard public datasets, suggesting that offer significant advantages over traditional systems. We achieved 12.0% error rate NIST SRE 2000 CALLHOME, while our model trained out-of-domain data from voice search logs.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Human-Centric Interfaces for Ambient Intelligence"
      ],
      "abstract":[
        "To create truly effective human-centric ambient intelligence systems both engineering and computing methods are needed. This is the first book to bridge data processing and intelligent reasoning methods for the creation of human-centered ambient intelligence systems. Interdisciplinary in nature, the book covers topics such as multi-modal interfaces, human-computer interaction, smart environments and pervasive computing, addressing principles, paradigms, methods and applications. This book will be an ideal reference for university researchers, R&amp;D engineers, computer engineers, and graduate students working in signal, speech and video processing, multi-modal interfaces, human-computer interaction and applications of ambient intelligence."
      ],
      "categories":[
        "physics.app-ph"
      ]
    },
    "list":{
      "title":[
        "Einstein-Maxwell-Dilaton Wormholes that meet the Energy Conditions",
        "On the comparison principle for a nonlocal infinity Laplacian",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "$e$-product of distributions, with applications",
        "A nonlinear model of shearable elastic rod from an origami-like\n  microstructure displaying folding and faulting",
        "Galaxy infall models for arbitrary velocity directions",
        "Analysis of $q_\\mathrm{rec}^2$-distribution for $B\\to K M_X$ and $B\\to\n  K^* M_X$ decays in a scalar-mediator dark-matter scenario",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Geometric Gauss Sums and Gross-Koblitz Formulas over Function Fields",
        "Solar irradiance statistical analysis in Mexico City from 2018 to 2021",
        "On the axially symmetric solutions to the spatially homogeneous Landau\n  equation",
        "Electroweak baryogenesis from charged current anomalies in $B$ meson\n  decays",
        "Ferri- and Ferro-Electric Switching in Spontaneously Chiral Polar Liquid\n  Crystals",
        "A general quasilinear elliptic problem with variable exponents and\n  Neumann boundary conditions for image processing",
        "Limit theorems for the fluctuation of the mixed elephant random walk in\n  the superdiffusive case",
        "Sequential Change Point Detection via Denoising Score Matching",
        "Emergence of the polydeterminant in QCD",
        "Infinitely many solutions for a boundary Yamabe problem",
        "Euclid: Early Release Observations -- Interplay between dwarf galaxies\n  and their globular clusters in the Perseus galaxy cluster",
        "The Spectroscopy of Kerr-Einstein-Maxwell-Dilaton-Axion: Exact\n  Quasibound States, Scalar Cloud, Horizon's Boson Statistics and Superradiance",
        "Skew shapes, Ehrhart positivity and beyond",
        "Multiplicative chaos measure for multiplicative functions: the\n  $L^1$-regime",
        "Non-linear Quantum Monte Carlo",
        "Differential topology of the spaces of asymptotically stable vector\n  fields and Lyapunov functions",
        "Orbital Depot Location Optimization for Satellite Constellation\n  Servicing with Low-Thrust Transfers",
        "Multigrid Preconditioning for FD-DLM Method in Elliptic Interface\n  Problems",
        "Dirichlet Spaces In Balls And Half-spaces of $\\R^n$",
        "Disjointly non-singular operators and various topologies on Banach\n  lattices",
        "Exact Parent Hamiltonians for All Landau Level States in a Half-flux\n  Lattice"
      ],
      "abstract":[
        "One of the latest predictions of Einstein's theory is the existence of\nWormholes (WH). In this work, we present exact solutions of the\nEinstein-Maxwell-Dilaton equations representing traversable Wormholes. These\nsolutions satisfy the energy conditions and have a ring singularity satisfying\nthe cosmic censorship of WHs, i.e. we show that, as in previous solutions,\ngeodesics cannot touch the singularity. We find that the most optimal input\nregions for the first class of solutions traversing these wormholes are near\nthe poles and near the equatorial plane for the second class. We also find that\nthe solution associated with the first class is physically feasible, while for\nthe second class it presents the problem of not being asymptotically flat when\nconsidering a dilatonic-type scalar field. Finally, we give examples of\nrealistic astrophysical objects that could fulfill these conditions.",
        "In this article, we prove the uniqueness of viscosity solutions to\n$\\mathcal{L}_{\\infty} u =f$ in $\\Omega$, where $\\mathcal{L}_{\\infty}$ denotes\nthe nonlocal infinity Laplace operator, $\\Omega$ a bounded domain, and $f$ a\ncontinuous functions such that $f \\leq 0$. Uniqueness is established through a\ncomparison principle.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "We consider and reformulate a recent definition of multiplication between\ndistributions. We show that this definition can be adopted, in particular, to\nprove biorthonormality of some distributions arising when looking to the\n(generalized) eigenvalues of a specific non self-adjoint number-like operator,\nconsidered in connection with the recently introduced {\\em weak pseudo-bosons}.\nSeveral examples are discussed in details.",
        "A new continuous model of shearable rod, subject to large elastic\ndeformation, is derived from nonlinear homogenization of a one-dimensional\nperiodic microstructured chain. As particular cases, the governing equations\nreduce to the Euler elastica and to the shearable elastica known as 'Engesser',\nthat has been scarcely analysed so far. The microstructure that is homogenized\nis made up of elastic hinges and four-bar linkages, which may be realized in\npractice using origami joints. The equivalent continuous rod is governed by a\nDifferential-Algebraic system of nonlinear Equations (DAE), containing an\ninternal length ratio, and showing a surprisingly rich mechanical landscape,\nwhich involves a twin sequence of bifurcation loads, separated by a\n'transition' mode. The latter occurs, for simply supported and cantilever rods\nin a 'bookshelf-like' mode and in a mode involving faulting (formation of a\nstep in displacement), respectively. The postcritical response of the simply\nsupported rod exhibits the emergence of folding, an infinite curvature\noccurring at a point of the rod axis, developing into a curvature jump at\nincreasing load. Faulting and folding, excluded for both Euler and Reissner\nmodels and so far unknown in the rod theory, represent 'signatures' revealing\nthe origami design of the microstructure. These two features are shown to be\nassociated with bifurcations and, in particular folding, with a secondary\nbifurcation of the corresponding discrete chain when the number of elements is\nodd. Beside the intrinsic theoretical relevance to the field of structural\nmechanics, our results can be applied to various technological contexts\ninvolving highly compliant mechanisms, such as the achievement of objective\ntrajectories with soft robot arms through folding and localized displacement of\norigami-inspired or multi-material mechanisms.",
        "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. Peculiar motions on the\nsky are only measured for a few cases. With increasingly detailed observations,\nthe assumption that line-of-sight velocities suffice for an accurate and\nprecise reconstruction of galaxy kinematics needs to be re-investigated and the\nimpact of perpendicular velocities to be quantified. We analyse the motion of\ntwo galaxies with arbitrary velocities, determine their mutual velocity on an\narbitrary background, and compare this general relative velocity to the one\nfrom line-of-sight components only. The latter are known as ``minor and major\ninfall models'' established by Karachentsev and Kashibadze (2006). Our\nderivations reveal that the infall models approximate the radial velocity\nbetween two galaxies by two different projections employing different\ninformation about the system. For galaxies with small angular separations, all\ninfall models agree that the radial velocity is the difference of their\nline-of-sight velocities. For larger angles, the minor infall model is mostly\nsuitable when perpendicular velocity components are negligible and there is no\ninformation about the tangential velocity of the binary. The major infall model\nis best suitable when the motion is mainly radial and symmetry assumptions\ncancel the tangential and one perpendicular component. The latter often\nrequires to transition from galaxy binaries to groups or clusters, as we show\nquantitatively. We give an encompassing overview how the infall models over-\nand under-estimate general binary or $N$-body motions. We quantify the impact\nof perpendicular velocity components, sparse sampling, and deviations of the\ntracer-galaxies from the motion in an embedding gravitational potential which\nare related to the angular momentum of the structure. (abridged)",
        "We demonstrate that the scalar-mediator dark-matter scenario is consistent\nwith the experimental data on the decay $B\\to K M_X$ and provides a good\ndescription of the shape of the observed excess. Within this scenario, the\ninteraction with dark-matter particles leads to approximately the same excess\nin $\\Gamma(B\\to K^* M_X)$ and $\\Gamma(B\\to K M_X)$ compared to the Standard\nModel; also the differential distributions of the excess events are similar in\nshape in the variable $q_\\mathrm{rec}^2$ measured by experiment.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "In this paper, we prove the Gross-Koblitz-Thakur formulas relating special\n$v$-adic gamma values to the newly introduced geometric Gauss sums in the\nfunction field setting. These are analogous to those for the $p$-adic gamma\nfunction in the classical setting due to Gross-Koblitz and the $v$-adic\narithmetic gamma function over function fields due to Thakur. For these new\nGauss sums, we establish their key arithmetic properties, including the\nuniformity of absolute values and prime factorizations. We also determine their\nsigns at infinite places, and derive two analogs of the Hasse-Davenport\nrelations.",
        "Solar radiation is made up of three components of electromagnetic waves:\ninfrared, visible and ultraviolet. The infrared component is the cause of\nthermal energy, the visible spectrum allows to see through the eyes and the\nultraviolet component is the most energetic and damaging. Solar radiation has\nseveral benefits, such as helping to synthesize vitamin D in the skin, favors\nblood circulation, among others benefits for the human body. In the Earth, it\nis the main source of energy for agriculture, also used as an alternative\nsource of energy to hydrocarbons, through solar cells. The solar irradiance\nrepresents the surface power density with units W\/m$^2$ in SI. Too much\nexposure can cause damage and an increase in value over the time can be can be\nalso damaging. In this work it was used an open data base provided by\nSecretar\\'ia del Medio Ambiente, from which a statistical analysis was\nperformed of the solar irradiance values measured at various meteorological\nstations in Mexico City and the so-called metropolitan area, from 2018 to 2021.\nThis analysis was carried out per years, months and days. From the solar\nirradiance values distributions, it was obtained the averages, maximums and\nmeans were it was found there was no variation in the solar irradiance values\nover this period of years.",
        "In this paper, we consider the spatially homogeneous Landau equation, which\nis a variation of the Boltzmann equation in the grazing collision limit. For\nthe Landau equation for hard potentials in the style of Desvillettes-Villani\n(Comm. Partial Differential Equations, 2000), we provide the proof of the\nexistence of axisymmetric measure-valued solution for any axisymmetric\n$\\mathcal{P}_p(\\mathbb{R}^3)$ initial profile for any $p\\ge 2$. Moreover, we\nprove that if the initial data is not a single Dirac mass, then the solution\ninstantaneously becomes analytic for any time $t>0$ in the hard potential case.\nIn the soft potential and the Maxwellian molecule cases, we show that there are\nno solutions whose support is contained in a fixed line even for any given\nline-concentrated data.",
        "We demonstrate for the first time that new physics explaining the long\nstanding charged $B$ meson anomalies, $R(D^{(*)})$, can be the source of CP\nviolation that explains the observed baryon asymmetry of the universe (BAU). We\nconsider the general two Higgs doublet model with complex Yukawa couplings and\ncompute the BAU in the semiclassical formalism, using a novel analytic\napproximation for the latter. After imposing constraints from both flavor\nobservables and the electron electric dipole moment (eEDM), we find that a\nsignificant BAU can still be generated for a variety of benchmark points in the\nparameter space, assuming the occurrence of a sufficiently strong first order\nelectroweak phase transition. These scenarios, which explain both the\n$R(D^{(*)})$ flavor anomalies and the BAU, can be probed with future eEDM\nexperiments and Higgs factories measurements.",
        "The recent discovery of spontaneous chiral symmetry breaking has demonstrated\nthe possibility of discovering the exotic textures of ferromagnetic systems in\nliquid crystalline fluid ferro-electrics. We show that the polar smectic\nmesophase exhibited by the first molecule discovered to exhibit a spontaneously\nchiral ferroelectric nematic phase is also helical has a strongly varied\ntextural morphology depending in its thermal history and phase ordering.\nElectro-optic studies demonstrate that the two spontaneously chiral phases\nexhibit field induced phase transitions. For the nematic variant, this process\nis threshold-less and has no hysteresis while for the smectic it has a clear\nthreshold and shows hysteresis meaning this phase exhibits pseudo-ferrielectric\nswitching, the first of its kind for ferroelectric nematic like phases. We show\nthat helix formation can be both 1st and 2nd order but when it is 1st it is\naccompanied by pre-transitional helix formation in the preceding ferroelectric\nnematic phase.",
        "The aim of this paper is to state and prove existence and uniqueness results\nfor a general elliptic problem with homogeneous Neumann boundary conditions,\noften associated with image processing tasks like denoising. The novelty is\nthat we surpass the lack of coercivity of the Euler-Lagrange functional with an\ninnovative technique that has at its core the idea of showing that the minimum\nof the energy functional over a subset of the space $W^{1,p(x)}(\\Omega)$\ncoincides with the global minimum. The obtained existence result applies to\nmultiple-phase elliptic problems under remarkably weak assumptions.",
        "Motivated by the previous results by Coletti--de Lima--Gava--Luiz (2020) and\nShiozawa (2022), we study the fluctuation of the mixed elephant random walk in\nthe superdiffusive case, and prove the Central Limit Theorem and the Law of\nIterated Logarithm with subtracting a random drift.",
        "Sequential change-point detection plays a critical role in numerous\nreal-world applications, where timely identification of distributional shifts\ncan greatly mitigate adverse outcomes. Classical methods commonly rely on\nparametric density assumptions of pre- and post-change distributions, limiting\ntheir effectiveness for high-dimensional, complex data streams. This paper\nproposes a score-based CUSUM change-point detection, in which the score\nfunctions of the data distribution are estimated by injecting noise and\napplying denoising score matching. We consider both offline and online versions\nof score estimation. Through theoretical analysis, we demonstrate that\ndenoising score matching can enhance detection power by effectively controlling\nthe injected noise scale. Finally, we validate the practical efficacy of our\nmethod through numerical experiments on two synthetic datasets and a real-world\nearthquake precursor detection task, demonstrating its effectiveness in\nchallenging scenarios.",
        "A generalization of the determinant appears in particle physics in effective\nLagrangian interaction terms that model the chiral anomaly in Quantum\nChromodynamics (PRD 97 (2018) 9, 091901 PRD 109 (2024) 7, L071502), in\nparticular in connection to mesons. This \\textit{polydeterminant function},\nknown in the mathematical literature as a mixed discriminant, associates $N$\ndistinct $N\\times N$ complex matrices into a complex number and reduces to the\nusual determinant when all matrices are taken as equal. Here, we explore the\nmain properties of the polydeterminant applied to (quantum) fields by using a\nformalism and a language close to high-energy physics approaches. We discuss\nits use as a tool to write down novel Lagrangian terms and present an explicit\nillustrative model for mesons. Finally, the extension of the polydeterminant as\na function of tensors is shown.",
        "We consider the classical geometric problem of prescribing the scalar and the\nboundary mean curvature in the unit ball endowed with the standard Euclidean\nmetric. We will deal with the case of negative scalar curvature showing the\nexistence of infinitely many non-radial positive solutions when the dimension\nis larger or equal to 5. This is the first result of existence of solutions in\nthe case of negative prescribed scalar curvature problem in higher dimensions.",
        "We present an analysis of globular clusters (GCs) of dwarf galaxies in the\nPerseus galaxy cluster to explore the relationship between dwarf galaxy\nproperties and their GCs. Our focus is on GC numbers ($N_{\\rm GC}$) and GC\nhalf-number radii ($R_{\\rm GC}$) around dwarf galaxies, and their relations\nwith host galaxy stellar masses ($M_*$), central surface brightnesses\n($\\mu_0$), and effective radii ($R_{\\rm e}$). Interestingly, we find that at a\ngiven stellar mass, $R_{\\rm GC}$ is almost independent of the host galaxy\n$\\mu_0$ and $R_{\\rm e}$, while $R_{\\rm GC}\/R_{\\rm e}$ depends on $\\mu_0$ and\n$R_{\\rm e}$; lower surface brightness and diffuse dwarf galaxies show $R_{\\rm\nGC}\/R_{\\rm e}\\approx 1$ while higher surface brightness and compact dwarf\ngalaxies show $R_{\\rm GC}\/R_{\\rm e}\\approx 1.5$-$2$. This means that for dwarf\ngalaxies of similar stellar mass, the GCs have a similar median extent;\nhowever, their distribution is different from the field stars of their host.\nAdditionally, low surface brightness and diffuse dwarf galaxies on average have\na higher $N_{\\rm GC}$ than high surface brightness and compact dwarf galaxies\nat any given stellar mass. We also find that UDGs (ultra-diffuse galaxies) and\nnon-UDGs have similar $R_{\\rm GC}$, while UDGs have smaller $R_{\\rm GC}\/R_{\\rm\ne}$ (typically less than 1) and 3-4 times higher $N_{\\rm GC}$ than non-UDGs.\nExamining nucleated and not-nucleated dwarf galaxies, we find that for\n$M_*>10^8M_{\\odot}$, nucleated dwarf galaxies seem to have smaller $R_{\\rm GC}$\nand $R_{\\rm GC}\/R_{\\rm e}$, with no significant differences between their\n$N_{\\rm GC}$, except at $M_*<10^8M_{\\odot}$ where the nucleated dwarf galaxies\ntend to have a higher $N_{\\rm GC}$. Lastly, we explore the stellar-to-halo mass\nratio (SHMR) of dwarf galaxies and conclude that the Perseus cluster dwarf\ngalaxies follow the expected SHMR at $z=0$ extrapolated down to\n$M_*=10^6M_{\\odot}$.",
        "In the present study, we investigate the quasibound states, scalar cloud and\nsuperradiance of relativistic scalar fields bound to a rotating black hole in\nEinstein-Maxwell-Dilaton-Axion theory (Kerr-EMDA). We present the exact\neigensolutions of the governing Klein-Gordon equation in the black hole\nbackground. By imposing boundary conditions on the quasibound states, we are\nable to find the exact complex quasibound state frequencies of the\ncorresponding radial wave functions in terms of the confluent Heun polynomial.\nConsidering light scalar field limit of the obtained solution, we investigate\nthe scalar-black hole resonance configuration known as the scalar cloud. In\naddition, we obtain analytic relation between light scalar mass and black hole\nspin for scalar cloud. We explore a boson distribution function by linearly\nexpanding the radial wave function near the black hole's event horizon.\nMoreover, by applying the Damour-Ruffini method, this allows us to calculate\nthe Hawking radiation flux. In the final section, we consider propagating wave\nin a slowly rotating Kerr-EMDA black hole for bosons having much larger Compton\nwavelength comparing to the size of rotating black hole. This condition allows\nus to use the asymptotic matching technique to calculate the amplification\nfactor for scalar fields in the Kerr-EMDA black hole. We present the dependence\nof amplification factor on black hole parameters by graphical analysis.",
        "A classical result by Kreweras (1965) allows one to compute the number of\nplane partitions of a given skew shape and bounded parts as certain\ndeterminants. We prove that these determinants expand as polynomials with\nnonnegative coefficients. This result can be reformulated in terms of order\npolynomials of cell posets of skew shapes, and explains important positivity\nphenomena about the Ehrhart polynomials of shard polytopes, matroids, and order\npolytopes. Among other applications, we generalize a positivity statement from\nSchubert calculus by Fomin and Kirillov (1997) from straight shapes to skew\nshapes. We show that all shard polytopes are Ehrhart positive and, stronger,\nthat all fence posets, including the zig-zag poset, have order polynomials with\nnonnegative coefficients. We present a more general method for proving\npositivity which reduces to showing positivity of the linear terms of the order\npolynomials and we state conjectures on other positive classes of posets.",
        "Let $\\alpha$ be a Steinhaus random multiplicative function. For a wide class\nof multiplicative functions $f$ we construct a multiplicative chaos measure\narising from the Dirichlet series of $\\alpha f$, in the whole $L^1$-regime. Our\nmethod does not rely on the thick point approach or Gaussian approximation, and\nuses a modified second moment method with the help of an approximate Girsanov\ntheorem. We also employ the idea of weak convergence in $L^r$ to show that the\nlimiting measure is independent of the choice of the approximation schemes, and\nthis may be seen as a non-Gaussian analogue of Shamov's characterisation of\nmultiplicative chaos.\n  Our class of $f$-s consists of those for which the mean value of $|f(p)|^2$\nlies in $(0,1)$. In particular, it includes the indicator of sums of two\nsquares. As an application of our construction, we establish a generalised\ncentral limit theorem for the (normalised) sums of $\\alpha f$, with random\nvariance determined by the total mass of our measure.",
        "The mean of a random variable can be understood as a $\\textit{linear}$\nfunctional on the space of probability distributions. Quantum computing is\nknown to provide a quadratic speedup over classical Monte Carlo methods for\nmean estimation. In this paper, we investigate whether a similar quadratic\nspeedup is achievable for estimating $\\textit{non-linear}$ functionals of\nprobability distributions. We propose a quantum-inside-quantum Monte Carlo\nalgorithm that achieves such a speedup for a broad class of non-linear\nestimation problems, including nested conditional expectations and stochastic\noptimization. Our algorithm improves upon the direct application of the quantum\nmultilevel Monte Carlo algorithm introduced by An et al.. The existing lower\nbound indicates that our algorithm is optimal up polylogarithmic factors. A key\ninnovation of our approach is a new sequence of multilevel Monte Carlo\napproximations specifically designed for quantum computing, which is central to\nthe algorithm's improved performance.",
        "We study the topology of the space of all smooth asymptotically stable vector\nfields on $\\mathbb{R}^n$, as well as the space of all proper smooth Lyapunov\nfunctions for such vector fields. We prove that both spaces are path-connected\nand simply connected when $n\\neq 4,5$ and weakly contractible when $n\\leq 3$.\nMoreover, both spaces have the weak homotopy type of the nonlinear Grassmannian\nof submanifolds of $\\mathbb{R}^n$ diffeomorphic to the $n$-disc.\n  The proofs rely on Lyapunov theory and differential topology, such as the\nwork of Smale and Perelman on the generalized Poincar\\'{e} conjecture and\nresults of Smale, Cerf, and Hatcher on the topology of diffeomorphism groups of\ndiscs. Applications include a partial answer to a question of Conley, a\nparametric Hartman-Grobman theorem for nonyperbolic but asymptotically stable\nequilibria, and a parametric Morse lemma for degenerate minima. We also study\nthe related topics of hyperbolic equilibria, Morse minima, and relative\nhomotopy groups of the space of asymptotically stable vector fields inside the\nspace of those vanishing at a single point.",
        "This paper addresses the critical problem of co-optimizing the optimal\nlocations for orbital depots and the sequence of in-space servicing for a\nsatellite constellation. While most traditional studies used network\noptimization for this problem, assuming a fixed set of discretized nodes in the\nnetwork (i.e., a limited number of depot location candidates), this work is\nunique in that it develops a method to optimize the depot location in\ncontinuous space. The problem is formulated as mixed-integer nonlinear\nprogramming, and we propose a solution methodology that iteratively solves two\ndecoupled problems: one using mixed-integer linear programming and the other\nusing nonlinear programming with an analytic transfer solution. To demonstrate\nthe effectiveness of our approach, we apply this methodology to a case study\ninvolving a GPS satellite constellation. Numerical experiments confirm the\nstability of our proposed solutions.",
        "We investigate the performance of multigrid preconditioners for solving\nlinear systems arising from finite element discretizations of elliptic\ninterface problems using the Fictitious Domain with Distributed Lagrange\nMultipliers (FD-DLM) formulation. Numerical experiments are conducted using\ncontinuous and discontinuous finite element spaces for the Lagrange multiplier.\nResults indicate that multigrid is a promising preconditioner for problems in\nthe FD-DLM formulation.",
        "The present paper studies the Dirichlet spaces in balls and upper-half\nEuclidean spaces. As main results, we give identical characterizations of the\nDirichlet norms in the respective contexts as for the classical 2-D disc case\nproved by Douglas and Ahlfors.",
        "We continue the study of dispersed subspaces and disjointly non-singular\n(DNS) operators on Banach lattices using topological methods. In particular, we\nprovide a simple proof of the fact that in an order continuous Banach lattice\nan operator is DNS if and only if it is $n$-DNS, for some $n\\in\\mathbb{N}$. We\ncharacterize Banach lattices with order continuous dual in terms of dispersed\nsubspaces and absolute weak topology. We also connect these topics with the\nrecently launched study of phase retrieval in Banach lattices.",
        "Realizing topological flat bands with tailored single-particle Hilbert spaces\nis a critical step toward exploring many-body phases, such as those featuring\nanyonic excitations. One prominent example is the Kapit-Mueller model, a\nvariant of the Harper-Hofstadter model that stabilizes lattice analogs of the\nlowest Landau level states. The Kapit-Mueller model is constructed based on the\nPoisson summation rule, an exact lattice sum rule for coherent states. In this\nwork, we consider higher Landau-level generalizations of the Poisson summation\nrule, from which we derive families of parent Hamiltonians on a half-flux\nlattice which have exact flat bands whose flatband wavefunctions are lattice\nversion of higher Landau level states. Focusing on generic Bravais lattices\nwith only translation and inversion symmetries, we discuss how these symmetries\nenforced gaplessness and singular points for odd Landau level series, and how\nto achieve fully gapped parent Hamiltonians by mixing even and odd series. Our\nmodel points to a large class of tight-binding models with suitable energetic\nand quantum geometries that are potentially useful for realizing non-Abelian\nfractionalized states when interactions are included. The model exhibits fast\ndecay hopping amplitudes, making it potentially realizable with neutral atoms\nin optical lattices."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Turning single-molecule localization microscopy into a quantitative bioanalytical tool",
    "start_abstract":"Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "A framework for evaluating the performance of SMLM cluster analysis algorithms"
      ],
      "abstract":[
        "This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length\n  Video Generation",
        "Out-of-distribution generalisation for learning quantum channels with\n  low-energy coherent states",
        "Formalising the intentional stance 2: a coinductive approach",
        "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep\n  Reinforcement Learning",
        "MastermindEval: A Simple But Scalable Reasoning Benchmark",
        "Critical Unstable Qubits: an Application to $B^0\\bar{B}^0$-Meson System",
        "MobileSteward: Integrating Multiple App-Oriented Agents with\n  Self-Evolution to Automate Cross-App Instructions",
        "Simple Hamiltonians for Matrix Product State models",
        "A Reinforcement Learning Approach to Quiet and Safe UAM Traffic\n  Management",
        "Learning Visual Proxy for Compositional Zero-Shot Learning",
        "k-LLMmeans: Summaries as Centroids for Interpretable and Scalable\n  LLM-Based Text Clustering",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "An overview of regularity results for the Laplacian and $p$-Laplacian in\n  metric spaces",
        "Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video\n  Classification",
        "Detectability, Riccati Equations, and the Game-Based Control of\n  Discrete-Time MJLSs with the Markov Chain on a Borel Space",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Distribution Matching for Self-Supervised Transfer Learning",
        "Neural Reflectance Fields for Radio-Frequency Ray Tracing",
        "PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology\n  Report Generation",
        "Q-NL Verifier: Leveraging Synthetic Data for Robust Knowledge Graph\n  Question Answering",
        "Optimal-Reference Excited State Methods: Static Correlation at\n  Polynomial Cost with Single-Reference Coupled-Cluster Approaches",
        "Solving the Traveling Salesman Problem via Different Quantum Computing\n  Architectures",
        "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
        "Impact of photoevaporative winds in chemical models of externally\n  irradiated protoplanetary disks",
        "Gravitational waves from the E-model inflation with Gauss-Bonnet\n  correction",
        "Vector-Quantized Vision Foundation Models for Object-Centric Learning",
        "On the positivity of light-ray operators",
        "LiteChain: A Lightweight Blockchain for Verifiable and Scalable\n  Federated Learning in Massive Edge Networks"
      ],
      "abstract":[
        "Diffusion models are successful for synthesizing high-quality videos but are\nlimited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained\nfootage (e.g. over minutes) still remains an open research question. In this\npaper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers),\na new diffusion model specialized for long video generation. MALT Diffusion (or\njust MALT) handles long videos by subdividing them into short segments and\ndoing segment-level autoregressive generation. To achieve this, we first\npropose recurrent attention layers that encode multiple segments into a compact\nmemory latent vector; by maintaining this memory vector over time, MALT is able\nto condition on it and continuously generate new footage based on a long\ntemporal context. We also present several training techniques that enable the\nmodel to generate frames over a long horizon with consistent quality and\nminimal degradation. We validate the effectiveness of MALT through experiments\non long video benchmarks. We first perform extensive analysis of MALT in\nlong-contextual understanding capability and stability using popular long video\nbenchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video\ngeneration on UCF-101, outperforming the previous state-of-the-art of 648.4.\nFinally, we explore MALT's capabilities in a text-to-video generation setting\nand show that it can produce long videos compared with recent techniques for\nlong text-to-video generation.",
        "When experimentally learning the action of a continuous variable quantum\nprocess by probing it with inputs, there will often be some restriction on the\ninput states used. One experimentally simple way to probe the channel is using\nlow-energy coherent states. Learning a quantum channel in this way presents\ndifficulties, due to the fact that two channels may act similarly on low energy\ninputs but very differently for high energy inputs. They may also act similarly\non coherent state inputs but differently on non-classical inputs. Extrapolating\nthe behaviour of a channel for more general input states from its action on the\nfar more limited set of low energy coherent states is a case of\nout-of-distribution generalisation. To be sure that such generalisation gives\nmeaningful results, one needs to relate error bounds for the training set to\nbounds that are valid for all inputs. We show that for any pair of channels\nthat act sufficiently similarly on low energy coherent state inputs, one can\nbound how different the input-output relations are for any (high energy or\nhighly non-classical) input. This proves out-of-distribution generalisation is\nalways possible for learning quantum channels using low energy coherent states.",
        "Given a stochastic process with inputs and outputs, how might its behaviour\nbe related to pursuit of a goal? We model this using 'transducers', objects\nthat capture only the external behaviour of a system and not its internal\nstate. A companion paper summarises our results for cognitive scientists; the\ncurrent paper gives formal definitions and proofs.\n  To formalise the concept of a system that behaves as if it were pursuing a\ngoal, we consider what happens when a transducer (a 'policy') is coupled to\nanother transducer that comes equipped with a success condition (a\n'teleo-environment'). An optimal policy is identified with a transducer that\nbehaves as if it were perfectly rational in the pursuit of a goal; our\nframework also allows us to model constrained rationality.\n  Optimal policies obey a version of Bellman's principle: a policy that's\noptimal in one time step will again be optimal in the next time step, but with\nrespect to a different teleo-environment (obtained from the original one by a\nmodified version of Bayesian filtering). This property sometimes also applies\nto the bounded-rational case; we give a sufficient condition.\n  A policy is deterministic if and only if there exists a teleo-environment for\nwhich it is uniquely optimal among the set of all policies; we relate this to\nclassical representation theorems from decision theory. This result need not\nhold in the bounded-rational case; we give an example related to the\nabsent-minded driver problem. The formalism is defined using coinduction,\nfollowing the style proposed by Czajka.",
        "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single\/multiple agents,\nsingle\/multiple backdoors, discrete\/continuous action spaces, and sparse\/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps:\/\/github.com\/maoubo\/UNIDOOR.",
        "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
        "We extend our previous work on a novel class of unstable qubits which we have\nidentified recently and called them Critical Unstable Qubits (CUQs). The\ncharacteristic property of CUQs is that the energy-level and decay-width\nvectors, ${\\bf E}$ and ${\\bf \\Gamma}$, are orthogonal to one another, and the\nkey parameter $r = |{\\bf \\Gamma}|\/|2{\\bf E}|$ is less than 1. Most remarkably,\nCUQs exhibit two atypical behaviours: (i) they display coherence-decoherence\noscillations in a co-decaying frame of the system described by a unit Bloch\nvector ${\\bf b}$, and (ii) the unit Bloch vector ${\\bf b}$ describing a pure\nCUQ sweeps out unequal areas during equal intervals of time, while rotating\nabout the vector ${\\bf E}$. The latter anharmonic phenomenon emerges beyond the\nusual oscillatory pattern due to the energy-level difference of the two-level\nquantum system, which governs an ordinary qubit. By making use of a Fourier\nseries decomposition, we define anharmonicity observables that quantify the\ndegree of non-sinusoidal oscillation of a CUQ. We apply the results of our\nformalism to the $B^0\\bar{B}^0$-meson system and derive, for the first time,\ngeneric upper limits on these new observables.",
        "Mobile phone agents can assist people in automating daily tasks on their\nphones, which have emerged as a pivotal research spotlight. However, existing\nprocedure-oriented agents struggle with cross-app instructions, due to the\nfollowing challenges: (1) complex task relationships, (2) diverse app\nenvironment, and (3) error propagation and information loss in multi-step\nexecution. Drawing inspiration from object-oriented programming principles, we\nrecognize that object-oriented solutions is more suitable for cross-app\ninstruction. To address these challenges, we propose a self-evolving\nmulti-agent framework named MobileSteward, which integrates multiple\napp-oriented StaffAgents coordinated by a centralized StewardAgent. We design\nthree specialized modules in MobileSteward: (1) Dynamic Recruitment generates a\nscheduling graph guided by information flow to explicitly associate tasks among\napps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each\nequipped with app-specialized expertise to address the diversity between apps.\n(3) Adjusted Evaluation conducts evaluation to provide reflection tips or\ndeliver key information, which alleviates error propagation and information\nloss during multi-step execution. To continuously improve the performance of\nMobileSteward, we develop a Memory-based Self-evolution mechanism, which\nsummarizes the experience from successful execution, to improve the performance\nof MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench)\nin the real-world environment to evaluate the agents' capabilities of solving\ncomplex cross-app instructions. Experimental results demonstrate that\nMobileSteward achieves the best performance compared to both single-agent and\nmulti-agent frameworks, highlighting the superiority of MobileSteward in better\nhandling user instructions with diverse complexity.",
        "Matrix Product States (MPS) and Tensor Networks provide a general framework\nfor the construction of solvable models. The best-known example is the\nAffleck-Kennedy-Lieb-Tasaki (AKLT) model, which is the ground state of a 2-body\nnearest-neighbor parent Hamiltonian. We show that such simple parent\nHamiltonians for MPS models are, in fact, much more prevalent than hitherto\nknown: The existence of a single example with a simple Hamiltonian for a given\nchoice of dimensions already implies that any generic MPS with those dimensions\npossesses an equally simple Hamiltonian. We illustrate our finding by\ndiscussing a number of models with nearest-neighbor parent Hamiltonians, which\ngeneralize the AKLT model on various levels.",
        "Urban air mobility (UAM) is a transformative system that operates various\nsmall aerial vehicles in urban environments to reshape urban transportation.\nHowever, integrating UAM into existing urban environments presents a variety of\ncomplex challenges. Recent analyses of UAM's operational constraints highlight\naircraft noise and system safety as key hurdles to UAM system implementation.\nFuture UAM air traffic management schemes must ensure that the system is both\nquiet and safe. We propose a multi-agent reinforcement learning approach to\nmanage UAM traffic, aiming at both vertical separation assurance and noise\nmitigation. Through extensive training, the reinforcement learning agent learns\nto balance the two primary objectives by employing altitude adjustments in a\nmulti-layer UAM network. The results reveal the tradeoffs among noise impact,\ntraffic congestion, and separation. Overall, our findings demonstrate the\npotential of reinforcement learning in mitigating UAM's noise impact while\nmaintaining safe separation using altitude adjustments",
        "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions by leveraging knowledge from seen compositions.\nExisting methods align textual prototypes with visual features through\nVision-Language Models (VLMs), but they face two key limitations: (1) modality\ngaps hinder the discrimination of semantically similar composition pairs, and\n(2) single-modal textual prototypes lack fine-grained visual cues, creating\nbottlenecks in VLM-based CZSL. In this paper, we introduce Visual Proxy\nLearning, a novel approach that facilitates the learning of distinct visual\ndistributions, effectively reducing the modality gap and improving\ncompositional generalization performance. Specifically, we initialize visual\nproxies for various attributes, objects, and their compositions using text\nrepresentations. By optimizing the visual space, we capture fine-grained visual\ncues and guide the learning of more discriminative visual representations for\nattributes, objects and compositions. Furthermore, we propose an effective\nCross-Modal Joint Learning (CMJL) strategy that imposes cross-modal constraints\nbetween the original text-image space and the fine-grained visual space. This\napproach not only boosts generalization for previously unseen composition pairs\nbut also sharpens the discrimination of similar pairs, fostering more robust\nand precise learning. Extensive experiments demonstrate state-of-the-art\nperformance in closed-world scenarios and competitive open-world results across\nfour established CZSL benchmarks, validating the effectiveness of our approach\nin advancing compositional generalization.",
        "We introduce k-LLMmeans, a novel modification of the k-means clustering\nalgorithm that utilizes LLMs to generate textual summaries as cluster\ncentroids, thereby capturing contextual and semantic nuances often lost when\nrelying on purely numerical means of document embeddings. This modification\npreserves the properties of k-means while offering greater interpretability:\nthe cluster centroid is represented by an LLM-generated summary, whose\nembedding guides cluster assignments. We also propose a mini-batch variant,\nenabling efficient online clustering for streaming text data and providing\nreal-time interpretability of evolving cluster centroids. Through extensive\nsimulations, we show that our methods outperform vanilla k-means on multiple\nmetrics while incurring only modest LLM usage that does not scale with dataset\nsize. Finally, We present a case study showcasing the interpretability of\nevolving cluster centroids in sequential text streams. As part of our\nevaluation, we compile a new dataset from StackExchange, offering a benchmark\nfor text-stream clustering.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "We review some regularity results for the Laplacian and $p$-Laplacian in\nmetric measure spaces. The focus is mainly on interior H\\\"older, Lipschitz and\nsecond-regularity estimates and on spaces supporting a Poincar\\'e inequality or\nhaving Ricci curvature bounded below.",
        "Automated viewpoint classification in echocardiograms can help\nunder-resourced clinics and hospitals in providing faster diagnosis and\nscreening when expert technicians may not be available. We propose a novel\napproach towards echocardiographic viewpoint classification. We show that\ntreating viewpoint classification as video classification rather than image\nclassification yields advantage. We propose a CNN-GRU architecture with a novel\ntemporal feature weaving method, which leverages both spatial and temporal\ninformation to yield a 4.33\\% increase in accuracy over baseline image\nclassification while using only four consecutive frames. The proposed approach\nincurs minimal computational overhead. Additionally, we publish the Neonatal\nEchocardiogram Dataset (NED), a professionally-annotated dataset providing\nsixteen viewpoints and associated echocardipgraphy videos to encourage future\nwork and development in this field. Code available at:\nhttps:\/\/github.com\/satchelfrench\/NED",
        "In this paper, detectability is first put forward for discrete-time Markov\njump linear systems with the Markov chain on a Borel space ($\\Theta$,\n$\\mathcal{B}(\\Theta)$). Under the assumption that the unforced system is\ndetectable, a stability criterion is established relying on the existence of\nthe positive semi-definite solution to the generalized Lyapunov equation. It\nplays a key role in seeking the conditions that guarantee the existence and\nuniqueness of the maximal solution and the stabilizing solution for a class of\ngeneral coupled algebraic Riccati equations (coupled-AREs). Then the\nnonzero-sum game-based control problem is tackled, and Nash equilibrium\nstrategies are achieved by solving four integral coupled-AREs. As an\napplication of the Nash game approach, the infinite horizon mixed\n$H_{2}\/H_{\\infty}$ control problem is studied, along with its solvability\nconditions. These works unify and generalize those set up in the case where the\nstate space of the Markov chain is restricted to a finite or countably infinite\nset. Finally, some examples are included to validate the developed results,\ninvolving a practical example of the solar thermal receiver.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
        "Ray tracing is widely employed to model the propagation of radio-frequency\n(RF) signal in complex environment. The modelling performance greatly depends\non how accurately the target scene can be depicted, including the scene\ngeometry and surface material properties. The advances in computer vision and\nLiDAR make scene geometry estimation increasingly accurate, but there still\nlacks scalable and efficient approaches to estimate the material reflectivity\nin real-world environment. In this work, we tackle this problem by learning the\nmaterial reflectivity efficiently from the path loss of the RF signal from the\ntransmitters to receivers. Specifically, we want the learned material\nreflection coefficients to minimize the gap between the predicted and measured\npowers of the receivers. We achieve this by translating the neural reflectance\nfield from optics to RF domain by modelling both the amplitude and phase of RF\nsignals to account for the multipath effects. We further propose a\ndifferentiable RF ray tracing framework that optimizes the neural reflectance\nfield to match the signal strength measurements. We simulate a complex\nreal-world environment for experiments and our simulation results show that the\nneural reflectance field can successfully learn the reflection coefficients for\nall incident angles. As a result, our approach achieves better accuracy in\npredicting the powers of receivers with significantly less training data\ncompared to existing approaches.",
        "The interpretation of histopathology cases underlies many important\ndiagnostic and treatment decisions in medicine. Notably, this process typically\nrequires pathologists to integrate and summarize findings across multiple\nslides per case. Existing vision-language capabilities in computational\npathology have so far been largely limited to small regions of interest, larger\nregions at low magnification, or single whole-slide images (WSIs). This limits\ninterpretation of findings that span multiple high-magnification regions across\nmultiple WSIs. By making use of Gemini 1.5 Flash, a large multimodal model\n(LMM) with a 1-million token context window, we demonstrate the ability to\ngenerate bottom-line diagnoses from up to 40,000 768x768 pixel image patches\nfrom multiple WSIs at 10X magnification. This is the equivalent of up to 11\nhours of video at 1 fps. Expert pathologist evaluations demonstrate that the\ngenerated report text is clinically accurate and equivalent to or preferred\nover the original reporting for 68% (95% CI: [60%, 76%]) of multi-slide\nexamples with up to 5 slides. While performance decreased for examples with 6\nor more slides, this study demonstrates the promise of leveraging the\nlong-context capabilities of modern LMMs for the uniquely challenging task of\nmedical report generation where each case can contain thousands of image\npatches.",
        "Question answering (QA) requires accurately aligning user questions with\nstructured queries, a process often limited by the scarcity of high-quality\nquery-natural language (Q-NL) pairs. To overcome this, we present Q-NL\nVerifier, an approach to generating high-quality synthetic pairs of queries and\nNL translations. Our approach relies on large language models (LLMs) to\ngenerate semantically precise natural language paraphrases of structured\nqueries. Building on these synthetic Q-NL pairs, we introduce a learned\nverifier component that automatically determines whether a generated paraphrase\nis semantically equivalent to the original query. Our experiments with the\nwell-known LC-QuAD 2.0 benchmark show that Q-NL Verifier generalizes well to\nparaphrases from other models and even human-authored translations. Our\napproach strongly aligns with human judgments across varying query complexities\nand outperforms existing NLP metrics in assessing semantic correctness. We also\nintegrate the verifier into QA pipelines, showing that verifier-filtered\nsynthetic data has significantly higher quality in terms of translation\ncorrectness and enhances NL to Q translation accuracy. Lastly, we release an\nupdated version of the LC-QuAD 2.0 benchmark containing our synthetic Q-NL\npairs and verifier scores, offering a new resource for robust and scalable QA.",
        "Accurate yet efficient modeling of chemical systems with pronounced static\ncorrelation in their excited states remains a significant challenge in quantum\nchemistry, as most electronic structure methods that can adequately capture\nstatic correlation scale factorially with system size. Researchers are often\nleft with no option but to use more affordable methods that may lack the\naccuracy required to model critical processes in photochemistry such as\nphotolysis, photocatalysis, and non-adiabatic relaxation. A great deal of work\nhas been dedicated to refining single-reference descriptions of static\ncorrelation in the ground state via ``addition-by-subtraction'' coupled cluster\nmethods such as pair coupled cluster with double substitutions (pCCD),\nsinglet-paired CCD (CCD0), triplet-paired CCD (CCD1), and CCD with frozen\nsinglet- or triplet-paired amplitudes (CCDf0\/CCDf1). By combining wave\nfunctions derived from these methods with the intermediate state representation\n(ISR), we gain insights into the extensibility of single-reference coupled\ncluster theory's coverage of static correlation to the excited state problem.\nOur CCDf1-ISR(2) approach is robust in the face of static correlation and\nprovides enough dynamical correlation to accurately predict excitation energies\nto within about 0.2~eV in small organic molecules. We also highlight distinct\nadvantages of the Hermitian ISR construction, such as the avoidance of\npathological failures of equation-of-motion methods for excited state potential\nenergy surface topology. Our results prompt us to continue exploring optimal\nsingle-reference theories (excited state approaches that leverage dependence on\nthe initial reference wave function) as a potentially economical approach to\nthe excited state static correlation problem.",
        "We study the application of emerging photonic and quantum computing\narchitectures to solving the Traveling Salesman Problem (TSP), a well-known\nNP-hard optimization problem. We investigate several approaches: Simulated\nAnnealing (SA), Quadratic Unconstrained Binary Optimization (QUBO-Ising)\nmethods implemented on quantum annealers and Optical Coherent Ising Machines,\nas well as the Quantum Approximate Optimization Algorithm (QAOA) and the\nQuantum Phase Estimation (QPE) algorithm on gate-based quantum computers.\n  QAOA and QPE were tested on the IBM Quantum platform. The QUBO-Ising method\nwas explored using the D-Wave quantum annealer, which operates on\nsuperconducting Josephson junctions, and the QCI Dirac machine, a nonlinear\noptoelectronic Ising machine. Gate-based quantum computers demonstrated\naccurate results for small TSP instances in simulation. However, real quantum\ndevices are hindered by noise and limited scalability. Circuit complexity grows\nwith problem size, restricting performance to TSP instances with a maximum of 6\nnodes.\n  In contrast, Ising-based architectures show improved scalability for larger\nproblem sizes. SQUID-based Ising machines can handle TSP instances with up to\n12 nodes, while nonlinear optoelectronic Ising machines extend this capability\nto 18 nodes. Nevertheless, the solutions tend to be suboptimal due to hardware\nlimitations and challenges in achieving ground state convergence as the problem\nsize increases. Despite these limitations, Ising machines demonstrate\nsignificant time advantages over classical methods, making them a promising\ncandidate for solving larger-scale TSPs efficiently.",
        "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.",
        "Most stars form in dense clusters within high-mass star-forming regions,\nwhere protoplanetary disks may be exposed to intense UV radiation from nearby\nmassive stars. While previous studies have typically focused on isolated\nsources in low-mass regions, recent observational campaigns have started to\nprobe the chemistry of irradiated disks in unprecedented detail. Interpreting\nthis data requires complex chemical models, yet few studies have examined these\ndisks' chemistry, and none have incorporated the photoevaporative wind launched\nby external UV fields into their physical structure. In this study, we\npost-process radiation hydrodynamics simulations of externally irradiated\nprotoplanetary disks using the thermochemical code DALI, comparing models with\nand without the wind to assess its impact on disk chemistry. Results show that\nUV radiation is rapidly attenuated by the disk in both cases. However, thermal\nre-emission from the wind at longer wavelengths enhances disk heating,\nincreasing the gas-phase abundances of some key volatiles. Synthetic line\nfluxes vary by orders of magnitude between wind and windless models, primarily\ndue to emission from the wind itself rather than abundance variations within\nthe disk. Our findings demonstrate that the photoevaporative wind significantly\ninfluences the physical and chemical structure, and observational\ncharacteristics, of externally irradiated disks. We conclude that incorporating\nthe wind into chemical models is essential for accurately predicting chemical\nabundances, interpreting observations, and ultimately understanding planet\nformation in these common yet complex environments.",
        "In this work, we study the generation of gravitational waves in the E-model\ninflation with the scalar field non-minimally coupled to the Gauss-Bonnet term.\nConsidering a wall-crossing behavior in the moduli space, we parameterize the\ncoupling coefficient $\\xi$ as a step-like function, then if\n$V_{,\\phi}\\xi_{,\\phi}>0$, the Gauss-Bonnet term dominate the inflation\ndynamics, causing a short rapid-decline phase of the inflaton, and for\nappropriate parameter spaces, the mode equation of tensor perturbations\ndevelops a transient growing solution. This process generates a peak in the\ntensor perturbation power spectrum, corresponding to a peak in the\ngravitational wave energy spectrum around the nanohertz frequency band. Further\nmore, we investigate the feasibility of generating double peaks in the\ngravitational wave spectrum using a double-step coupling, For certain parameter\nchoices, one peak lies near nanohertz frequencies, while the other is around\nmillihertz frequencies. Consequently, these gravitational waves can be observed\nby the pulsar timing array and the space-based gravitational wave detectors\nsuch as LISA, simultaneously.",
        "Decomposing visual scenes into objects, as humans do, facilitates modeling\nobject relations and dynamics. Object-Centric Learning (OCL) achieves this by\naggregating image or video feature maps into object-level feature vectors,\nknown as \\textit{slots}. OCL's self-supervision via reconstructing the input\nfrom slots struggles with complex textures, thus many methods employ Vision\nFoundation Models (VFMs) to extract feature maps with better objectness.\nHowever, using VFMs merely as feature extractors does not fully unlock their\npotential. We propose Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO), where\nVFM features are extracted to facilitate object-level information aggregation\nand further quantized to strengthen supervision in reconstruction. Our VVO\nunifies OCL representatives into a concise architecture. Experiments\ndemonstrate that VVO not only outperforms mainstream methods on object\ndiscovery tasks but also benefits downstream tasks like visual prediction and\nreasoning. The source code is available in the supplement.",
        "We consider light-ray operators $\\mathcal{L}_{2n} = \\int\\mathrm{d} x^+\n(x^+)^{2n}T_{++}$, where $x^+$ is a null coordinate and $n$ a positive integer,\nin QFT in Minkowski spacetime in arbitrary dimensions. These operators are\ngeneralizations of the average null energy operator, which is positive. We give\na proof that the light-ray operators are positive in a non-minimally coupled\nbut otherwise free scalar field theory, and we present various arguments that\nshow that $\\mathcal{L}_2$ is positive semi-definite in two-dimensional\nconformal field theories. However, we are also able to construct reasonable\nstates which contradict these results by exploiting an infrared loophole in our\nproof. To resolve the resulting tension, we conjecture that the light-ray\noperators are positive in a more restrictive set of states. These states\nsatisfy stronger conditions than the Hadamard condition, and have the\ninterpretation of states that can be physically prepared. Our proposal is\nnontrivial even in two-dimensional CFT.",
        "Leveraging blockchain in Federated Learning (FL) emerges as a new paradigm\nfor secure collaborative learning on Massive Edge Networks (MENs). As the scale\nof MENs increases, it becomes more difficult to implement and manage a\nblockchain among edge devices due to complex communication topologies,\nheterogeneous computation capabilities, and limited storage capacities.\nMoreover, the lack of a standard metric for blockchain security becomes a\nsignificant issue. To address these challenges, we propose a lightweight\nblockchain for verifiable and scalable FL, namely LiteChain, to provide\nefficient and secure services in MENs. Specifically, we develop a distributed\nclustering algorithm to reorganize MENs into a two-level structure to improve\ncommunication and computing efficiency under security requirements. Moreover,\nwe introduce a Comprehensive Byzantine Fault Tolerance (CBFT) consensus\nmechanism and a secure update mechanism to ensure the security of model\ntransactions through LiteChain. Our experiments based on Hyperledger Fabric\ndemonstrate that LiteChain presents the lowest end-to-end latency and on-chain\nstorage overheads across various network scales, outperforming the other two\nbenchmarks. In addition, LiteChain exhibits a high level of robustness against\nreplay and data poisoning attacks."
      ]
    }
  },
  {
    "id":2412.00173,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"A framework for evaluating the performance of SMLM cluster analysis algorithms",
    "start_abstract":"This analysis compares the performance of seven algorithms for cluster analysis of single-molecule localization microscopy data. The results provide a framework for comparing these types of methods and point users to the best tools. Single-molecule localization microscopy (SMLM) generates data in the form of coordinates of localized fluorophores. Cluster analysis is an attractive route for extracting biologically meaningful information from such data and has been widely applied. Despite a range of cluster analysis algorithms, there exists no consensus framework for the evaluation of their performance. Here, we use a systematic approach based on two metrics to score the success of clustering algorithms in simulated conditions mimicking experimental data. We demonstrate the framework using seven diverse analysis algorithms: DBSCAN, ToMATo, KDE, FOCAL, CAML, ClusterViSu and SR-Tesseler. Given that the best performer depended on the underlying distribution of localizations, we demonstrate an analysis pipeline based on statistical similarity measures that enables the selection of the most appropriate algorithm, and the optimized analysis parameters for real SMLM data. We propose that these standard simulated conditions, metrics and analysis pipeline become the basis for future analysis algorithm development and evaluation.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Turning single-molecule localization microscopy into a quantitative bioanalytical tool"
      ],
      "abstract":[
        "Single-molecule localization microscopy (SMLM) generates super-resolution images by serially detecting individual fluorescent molecules. The power of SMLM, however, goes beyond images: biologically relevant information can be extracted from the mathematical relationships between the positions of the fluorophores in space and time. Here we review the history of SMLM and how recent progress in methods for spatial point analysis has enabled quantitative measurement of SMLM data, providing insights into biomolecule patterning, clustering and oligomerization in biological systems."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Mechanism of Shape Symmetry Breaking in Surfactant Mediated Crystal\n  Growth",
        "A New Interpretation for the Hot Corona in Active Galactic Nuclei",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "Comparative Time-Series Analysis of Hip and Shoulder Rotation in\n  Baseball Bat Swings",
        "Nearly tight weighted 2-designs in complex projective spaces of every\n  dimension",
        "On the viability of higher order theories",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Exceptional-point-controlled mode interaction in three-dimensional\n  microcavities represented by generalized Husimi functions",
        "Proxy Control Barrier Functions: Integrating Barrier-Based and\n  Lyapunov-Based Safety-Critical Control Design",
        "New exact spatially localized solutions of the (3 + 1) -dimensional\n  nonlinear non-dissipative quasi-geostrophic potential vorticity equation for\n  an exponential atmosphere",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "Efficient detection of entanglement by stimulated disentanglement",
        "Fractional Sobolev spaces related to an ultraparabolic operator",
        "Geography of irreducible 4-manifolds with order two fundamental group",
        "Evaluation codes arising from symmetric polynomials",
        "Intrinsic vs. Extrinsic Magnetic Transitions in Sr3Ru2O7 films",
        "Risk-Adjusted learning curve assessment using comparative probability\n  metrics",
        "High-Frequency Market Manipulation Detection with a Markov-modulated\n  Hawkes process",
        "Optimizer-Dependent Generalization Bound for Quantum Neural Networks",
        "First-ish Order Methods: Hessian-aware Scalings of Gradient Descent",
        "Comparative study of small-scale magnetic fields on $\\xi$ Boo A using\n  optical and near-infrared spectroscopy",
        "Remarks on ghost resonances",
        "Ecosystem Evolution and Drivers across the Tibetan Plateau and\n  Surrounding Regions",
        "Approximation of High-Dimensional Gibbs Distributions with Functional\n  Hierarchical Tensors",
        "Impermanent loss and Loss-vs-Rebalancing II",
        "Redefining $Q$ for multi-component discs of stars and gas",
        "Entropic optimal transport with congestion aversion Application to\n  relocation of drones",
        "A Million Three-body Binaries Caught by Gaia",
        "On the $\\mathcal{F}$-multicolor Tur\\'{a}n number of hypergraph graphs"
      ],
      "abstract":[
        "We present a dynamical model of crystal growth, in which it is possible to\nreliably achieve asymmetric products, beginning from symmetric initial\nconditions and growing within an isotropic environment. The asymmetric growth\nis the result of a positive feedback mechanism that amplifies the effect of\nthermal fluctuations in the coverage of surfactants on the growing crystalline\nfacets. Within our simple model, we are able to understand the kinetic and\nthermodynamic factors involved in both the onset of symmetry breaking and the\npersistence of anisotropic growth. We demonstrate that the mechanism is general\nby studying models with increasing complexity. We argue that this mechanism of\nsymmetry breaking underpins observations of colloidal, seed-mediated syntheses\nof single crystalline metal nanorods capped with strongly interacting\nsurfactants. The parameters within our model are related to experimental\nobservables such as the concentration, hydrophobicity, and binding strength of\nthe surfactants, which suggests a potential route to optimize the yield of\nasymmetric products in colloidal nanoparticle syntheses.",
        "This work attempts to provide a new interpretation for the hot corona in\nactive galactic nuclei (AGNs). A thin\n  parabolic magnetic reconnection layer, anchored at the innermost disk and\nextending along the boundary of the\n  magnetic tower for a few tens of gravitational radii, serves as a hard-X-ray\nsource above the disk. Within this\n  reconnection layer, the tearing instability leads to the formation of a chain\nof plasmoids, which contain relativistic\n  electrons that generate X-ray radiation through inverse-Compton (IC)\nscattering of soft photons emitted by the\n  accretion disk. Based on previous theoretical works and numerical\nsimulations, we develop a heuristic framework\n  to parameterize the geometry and magnetization of the reconnection layer, as\nwell as to compute both the power of\n  the IC-scattering radiation and the height of the reconnection layer. Our\nmodel allows for a quantitative\n  investigation of the relation between the height of the corona and the X-ray\nradiation luminosity, which can be\n  directly compared against the observed relation from X-ray reverberation\nmapping of individual AGNs. The\n  theoretical results are in good agreement with the observations of IRAS\n13224-3809, indicating the validation of\n  our model.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
        "We use dense Sidon sets to construct small weighted projective 2-designs.\nThis represents quantitative progress on Zauner's conjecture.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Non-Hermitian photonics has attracted significant interest and influences\nseveral key areas such as optical metamaterials, laser physics, and nonlinear\noptics. While non-Hermitian effects have been widely addressed in\ntwo-dimensional systems, we focus on realistic three-dimensional devices. To\nthis end we generalize established phase space methods from mesoscopic optics\nand introduce Husimi functions for three-dimensional systems that deepen the\ninsight and access to the mode morphology and their dynamics. We illustrate\nthat four-dimensional Husimi functions can be represented using a specific\nprojection in two dimensions and illustrate it for (conical) cylindrical\ncavities. The non-Hermitian character of the intrinsically open photonic\nsystems is in particular revealed when examining the TE and TM polarization\ncharacter of the resonance modes. Unlike the 2D case, polarization is not\nconserved in three-dimensional cavities, and we use generalized Husimi function\nto represent the interaction of polarization modes. We find their dynamics to\nbe ruled by a network of exceptional points in the parameter space spanned by\nthe refractive index and the cavity geometry tilt angle. This approach not only\nenhances our understanding of cavity modes but also aids in the design of more\nefficient photonic devices and systems.",
        "This work introduces a novel Proxy Control Barrier Function (PCBF) scheme\nthat integrates barrier-based and Lyapunov-based safety-critical control\nstrategies for strict-feedback systems with potentially unknown dynamics. The\nproposed method employs a modular design procedure, decomposing the original\nsystem into a proxy subsystem and a virtual tracking subsystem that are\ncontrolled by the control barrier function (CBF)-based and Lyapunov-based\ncontrollers, respectively. By integrating these separately designed\ncontrollers, the overall system's safety is ensured. Moreover, a new\nfilter-based disturbance observer is utilized to design a PCBF-based safe\ncontroller for strict-feedback systems subject to mismatched disturbances. This\napproach broadens the class of systems to which CBF-based methods can be\napplied and significantly simplifies CBF construction by requiring only the\nmodel of the proxy subsystem. The effectiveness of the proposed method is\ndemonstrated through numerical simulations.",
        "New exact spatially localized stationary solutions against the background of\na zonal flow are found for the (3+1)-dimensional nonlinear non-dissipative\nquasi-geostrophic potential vorticity equation, which describes Rossby waves\nand vortices in an exponential atmosphere. In total, three solutions are\npresented. The nonlinear boundary conditions with a flat bottom and a rigid lid\ngenerate an infinite discrete set of baroclinic modes for each solution. The\nsolutions show the possibility of existence of baroclinic dipoles in the\nexponential atmosphere, similar to baroclinic dipoles in the ocean. It is shown\nthat: a) a pair of vortices in the baroclinic dipole appears and disappears\nwhen the velocity of stationary motion changes; b) the baroclinic dipoles show\nthe ability to transfer warm or cold air depending on the polarity of the\nvortices in the dipole.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Standard detection of entanglement relies on local measurements of the\nindividual particles, evaluating their correlations in post-processing. For\ntime-energy entangled photons, either times $(t_{1},t_{2})$ or energies\n$(E_{1},E_{2})$ are measured, but not both due to the mutual quantum\nuncertainty, providing only partial information of the entanglement. In\nprinciple, a global detector could recover the complete information of\nentanglement in a single shot if it could measure the combined correlated\nvariables $(t_{1}-t_{2})$ and $(E_{1}+E_{2})$ without measuring the individual\nenergies or times. Such a global measurement is possible using the reverse\ndisentangling interaction, like sum-frequency generation (SFG), but nonlinear\ninteractions at the single-photon level are exceedingly inefficient. We\novercome this barrier by stimulating the nonlinear SFG interaction with a\nstrong pump, thereby measuring both the energy-sum (SFG spectrum) and the\ntime-difference (response to group delay\/dispersion) simultaneously and\nefficiently. We generate bi-photons with extreme time-energy entanglement\n(octave-spanning spectrum of 113THz) and measure a relative uncertainty between\ntime-difference and energy-sum of\n$\\Delta(t_1-t_2)\\Delta(E_1+E_2)\\approx\\!2\\!\\cdot\\!10^{-13}h$, violating the\nclassical bound by more than 12 orders of magnitude. The presented coherent SFG\ndramatically enhances the detection SNR compared to standard methods since it\nideally rejects erroneous coincidences in both time and energy, paving the way\nfor sensing applications, such as quantum illumination (radar) and more.",
        "We propose a functional framework of fractional Sobolev spaces for a class of\nultra-parabolic Kolmogorov type operators satisfying the weak H\\\"ormander\ncondition. We characterize these spaces as real interpolation of natural order\nintrinic Sobolev spaces recently introduced in [27], and prove continuous\nembeddings into $L^p$ and intrinsic H\\\"older spaces from [24]. These embeddings\nnaturally extend the standard Euclidean ones, coherently with the homogeneous\nstructure of the associated Kolmogorov group. Our approach to interpolation is\nbased on approximation of intrinsically regular functions, the latter heavily\nrelying on integral estimates of the intrinsic Taylor remainder. The embeddings\nexploit the aforementioned interpolation property and the corresponding\nembeddings of natural order intrinsic spaces.",
        "Let $R$ be a closed, oriented topological 4-manifold whose Euler\ncharacteristic and signature are denoted by $e$ and $\\sigma$. We show that if\n$R$ has order two $\\pi_1$, odd intersection form, and $2e + 3\\sigma \\geq 0$,\nthen for all but seven $(e, \\sigma)$ coordinates, $R$ admits an irreducible\nsmooth structure. We accomplish this by performing a variety of operations on\nirreducible simply-connected 4-manifolds to build 4-manifolds with order two\n$\\pi_1$. These techniques include torus surgeries, symplectic fiber sums,\nrational blow-downs, and numerous constructions of Lefschetz fibrations,\nincluding a new approach to equivariant fiber summing.",
        "Datta and Johnsen (Des. Codes and Cryptogr., {\\bf{91}} (2023), 747-761)\nintroduced a new family of evalutation codes in an affine space of dimension\n$\\ge 2$ over a finite field $\\mathbb{F}_q$ where linear combinations of\nelementary symmetric polynomials are evaluated on the set of all points with\npairwise distinct coordinates. In this paper, we propose a generalization by\ntaking low dimensional linear systems of symmetric polynomials. Computation for\nsmall values of $q=7,9$ shows that carefully chosen generalized Datta-Johnsen\ncodes $\\left[\\frac{1}{2}q(q-1),3,d\\right]$ have minimum distance $d$ equal to\nthe optimal value minus 1.",
        "In scientific research, both positive and negative results play crucial role\nin advancing the field. Negative results provide valuable insights that can\nguide future experiments and prevent repeated failures. Here we present our\ngrowth attempts of Sr3Ru2O7 thin films using the hybrid molecular beam epitaxy.\nX-ray diffraction suggests nominally phase-pure films. A combination of\nmagnetoresistance and magnetization measurements exhibits an onset of\nferromagnetism at 170 K and 100 K, along with a metamagnetic-like transition at\n40 K. These results could initially be interpreted as intrinsic behavior of\nstrain-engineered Sr3Ru2O7 films. However, detailed microstructural analysis\nreveals intergrowths of Sr2RuO4, Sr4Ru3O10, and SrRuO3 phases, dispersed\nthroughout the film. Our findings suggest that the Sr3Ru2O7 films are likely\nparamagnetic, with the observed ferromagnetism arising from the Sr4Ru3O10 and\nSrRuO3 phases. Our results highlight the need for detailed microstructural\nanalysis when interpreting new material properties influenced by strain and\nmicrostructure.",
        "Surgical learning curves are graphical tools used to evaluate a trainee's\nprogress in the early stages of their career and determine whether they have\nachieved proficiency after completing a specified number of surgeries.\nCumulative sum (CUSUM) techniques are commonly used to assess learning curves\ndue to their simplicity, but they face criticism for relying on fixed\nperformance thresholds and lacking interpretability. This paper introduces a\nrisk-adjusted surgical learning curve assessment (SLCA) method that focuses on\nestimation rather than hypothesis testing, as seen in CUSUM methods. The method\nis designed to accommodate right-skewed outcomes, such as surgery durations,\ncharacterized by the Weibull distribution. To evaluate the learning process,\nthe SLCA approach estimates comparative probability metrics that assess the\nlikelihood of a clinically important difference between the trainee's\nperformance and a standard. Expecting improvement over time, we use weighted\nestimating equations to give greater weight to recent outcomes. Compared to\nCUSUM methods, SLCA offers enhanced interpretability, avoids reliance on\nexternally defined performance levels, and emphasizes assessing clinical\nequivalence or noninferiority. We demonstrate the method's effectiveness\nthrough a colorectal surgery dataset case study and a numerical study.",
        "This work focuses on a self-exciting point process defined by a Hawkes-like\nintensity and a switching mechanism based on a hidden Markov chain. Previous\nworks in such a setting assume constant intensities between consecutive events.\nWe extend the model to general Hawkes excitation kernels that are piecewise\nconstant between events. We develop an expectation-maximization algorithm for\nthe statistical inference of the Hawkes intensities parameters as well as the\nstate transition probabilities. The numerical convergence of the estimators is\nextensively tested on simulated data. Using high-frequency cryptocurrency data\non a top centralized exchange, we apply the model to the detection of anomalous\nbursts of trades. We benchmark the goodness-of-fit of the model with the\nMarkov-modulated Poisson process and demonstrate the relevance of the model in\ndetecting suspicious activities.",
        "Quantum neural networks (QNNs) play a pivotal role in addressing complex\ntasks within quantum machine learning, analogous to classical neural networks\nin deep learning. Ensuring consistent performance across diverse datasets is\ncrucial for understanding and optimizing QNNs in both classical and quantum\nmachine learning tasks, but remains a challenge as QNN's generalization\nproperties have not been fully explored. In this paper, we investigate the\ngeneralization properties of QNNs through the lens of learning algorithm\nstability, circumventing the need to explore the entire hypothesis space and\nproviding insights into how classical optimizers influence QNN performance. By\nestablishing a connection between QNNs and quantum combs, we examine the\ngeneral behaviors of QNN models from a quantum information theory perspective.\nLeveraging the uniform stability of the stochastic gradient descent algorithm,\nwe propose a generalization error bound determined by the number of trainable\nparameters, data uploading times, dataset dimension, and classical optimizer\nhyperparameters. Numerical experiments validate this comprehensive\nunderstanding of QNNs and align with our theoretical conclusions. As the first\nexploration into understanding the generalization capability of QNNs from a\nunified perspective of design and training, our work offers practical insights\nfor applying QNNs in quantum machine learning.",
        "Gradient descent is the primary workhorse for optimizing large-scale problems\nin machine learning. However, its performance is highly sensitive to the choice\nof the learning rate. A key limitation of gradient descent is its lack of\nnatural scaling, which often necessitates expensive line searches or heuristic\ntuning to determine an appropriate step size. In this paper, we address this\nlimitation by incorporating Hessian information to scale the gradient\ndirection. By accounting for the curvature of the function along the gradient,\nour adaptive, Hessian-aware scaling method ensures a local unit step size\nguarantee, even in nonconvex settings. Near a local minimum that satisfies the\nsecond-order sufficient conditions, our approach achieves linear convergence\nwith a unit step size. We show that our method converges globally under a\nsignificantly weaker version of the standard Lipschitz gradient smoothness\nassumption. Even when Hessian information is inexact, the local unit step size\nguarantee and global convergence properties remain valid under mild conditions.\nFinally, we validate our theoretical results empirically on a range of convex\nand nonconvex machine learning tasks, showcasing the effectiveness of the\napproach.",
        "Magnetic field investigations of Sun-like stars, using Zeeman splitting of\nnon-polarised spectra, in the optical and H-band have found significantly\ndifferent magnetic field strengths for the same stars, the cause of which is\ncurrently unknown. We aim to further investigate this issue by systematically\nanalysing the magnetic field of $\\xi$ Boo A, a magnetically active G7 dwarf,\nusing spectral lines at different wavelengths. We used polarised radiative\ntransfer accounting for the departures from local thermodynamic equilibrium to\ngenerate synthetic spectra. To find the magnetic field strengths in the\noptical, H-band, and K-band, we employed MCMC sampling analysis of\nhigh-resolution spectra observed with the spectrographs CRIRES$^+$, ESPaDOnS,\nNARVAL, and UVES. We also determine the formation depth of different lines by\ncalculating the contribution functions for each line employed in the analysis.\nWe find that the magnetic field strength discrepancy between lines in the\noptical and H-band persists even when treating the different wavelength regions\nconsistently. In addition, the magnetic measurements derived from the K-band\nappear to more closely align with the optical. The H-band appears to yield\nmagnetic field strengths $\\sim$ 0.4 kG with a statistically significant\nvariation while the optical and K-band is stable at $\\sim$ 0.6 kG for\nobservations spanning about two decades. The contribution functions reveal that\nthe optical lines form at a significantly higher altitude in the photosphere\ncompared to those in the H- and K-band. While we find that the discrepancy\nremains, the variation of formation depths could indicate that the disagreement\nbetween magnetic field measurements obtained at different wavelengths is linked\nto the variation of the magnetic field along the line of sight and between\ndifferent structures, such as star spots and faculae, in the stellar\nphotosphere.",
        "In this paper we study various aspects of ghost resonances: the resummation\nthat leads to the dressed propagator, the poles locations, the analytic\ncontinuation into the second Riemann sheet and the spectral representations in\nboth first and second sheets. In particular, we show that for real masses above\nthe multiparticle threshold the ghost propagator has a pair of complex\nconjugate poles in the first sheet, unlike the case of an ordinary unstable\nresonance which has no pole in the first sheet but a complex conjugate pair in\nthe second sheet. Mathematical and physical implications of this feature are\ndiscussed. We also clarify an important point regarding the two absorptive\ncontributions of a ghost propagator in the narrow-width approximation.\nFurthermore, we argue that finite-time quantum field theories are needed to\nconsistently derive the dressed ghost propagator and capture the true physical\nproperties of ghost resonances. Throughout the work, different prescriptions to\ndefine the ghost propagator on the real axis are considered: Feynman,\nanti-Feynman and fakeon prescriptions.",
        "The Tibetan Plateau (TP) and surrounding regions, vital to global energy and\nwater cycles, are profoundly influenced by climate change and anthropogenic\nactivities. Despite widespread attention to vegetation greening across the\nregion since the 1980s, its underlying mechanisms remain poorly understood.\nThis study employs the eigen microstates method to quantify vegetation greening\ndynamics using long-term remote sensing and reanalysis data. We identify two\ndominant modes that collectively explain more than 61% of the vegetation\ndynamics. The strong seasonal heterogeneity in the southern TP, primarily\ndriven by radiation and agricultural activities, is reflected in the first\nmode, which accounts for 46.34% of the variance. The second mode, which\nexplains 15% of the variance, is closely linked to deep soil moisture (SM3, 28\ncm to 1 m). Compared to precipitation and surface soil moisture (SM1 and SM2, 0\nto 28 cm), our results show that deep soil moisture exerts a stronger and more\nimmediate influence on vegetation growth, with a one-month response time. This\nstudy provides a complexity theory-based framework to quantify vegetation\ndynamics and underscores the critical influence of deep soil moisture on\ngreening patterns in the TP.",
        "The numerical representation of high-dimensional Gibbs distributions is\nchallenging due to the curse of dimensionality manifesting through the\nintractable normalization constant calculations. This work addresses this\nchallenge by performing a particle-based high-dimensional parametric density\nestimation subroutine, and the input to the subroutine is Gibbs samples\ngenerated by leveraging advanced sampling techniques. Specifically, to generate\nGibbs samples, we employ ensemble-based annealed importance sampling, a\npopulation-based approach for sampling multimodal distributions. These samples\nare then processed using functional hierarchical tensor sketching, a\ntensor-network-based density estimation method for high-dimensional\ndistributions, to obtain the numerical representation of the Gibbs\ndistribution. We successfully apply the proposed approach to complex\nGinzburg-Landau models with hundreds of variables. In particular, we show that\nthe approach proposed is successful at addressing the metastability issue under\ndifficult numerical cases.",
        "This paper examines the relationship between impermanent loss (IL) and\nloss-versus-rebalancing (LVR) in automated market makers (AMMs). Our main focus\nis on statistical properties, the impact of fees, the role of block times, and,\nrelated to the latter, the continuous time limit. We find there are three\nrelevant regimes: (i) very short times where LVR and IL are identical; (ii)\nintermediate time where LVR and IL show distinct distribution functions but are\nconnected via the central limit theorem exhibiting the same expectation value;\n(iii) long time behavior where both the distribution functions and averages are\ndistinct. Subsequently, we study how fees change this dynamics with a special\nfocus on competing time scales like block times and 'arbitrage times'.",
        "We point out a fundamental mismatch in the $Q$ stability parameter for\nGalactic discs: Toomre's $Q = 1$ defines the boundary between axisymmetric\nstability\/instability, while simulations, observations, and theoretical\nexpectations apply $Q$ in the region $Q > 1$ as a measure for spiral activity\n(e.g. swing amplification), for which $Q$ has not been designed. We suggest to\nredefine $Q$ to keep $Q = 1$ as the stability boundary, but to equally yield a\nconsistent map between $Q$ and the maximum swing amplification factor. Using\nthe Goldreich-Lynden-Bell formalism, we find that particularly the $Q$ for gas\ndiscs has been mismatched, and should be redefined to close to the square of\nthe traditional definition. We provide new formulations of $Q$ for simple,\ntwo-component, and multi-component discs, including a discussion of vertically\nextended discs, providing a simple iterative formula for which we also provide\ncode. We find $Q \\approx 1.58$ for the Solar Neighbourhood under our\ndefinition, closer to results from simulations. We compare the Milky Way and\nM74, showing that, consistent with observations, the theory suggests a higher\n$m$ number for the Milky Way (arguing against a 2-arm pattern) for\nstellar-dominated patterns. Gas instability arises at much smaller scales ($m\n\\gtrsim 10$), and we link both M74's gas pattern and local spurs in the Milky\nWay to this gas instability rather than stellar spiral arms.",
        "We present a mathematical framework for tempo-spatial entropic optimal\ntransport, motivated by the problem of efficiently routing drones back to\nlogistics centers. To address collision risk, we incorporate a convex penalty\nterm into the transport model. We propose the Sinkhorn-Frank-Wolfe algorithm, a\nnumerically efficient method with theoretical convergence guarantees, and\ndemonstrate its effectiveness through experiments on synthetic datasets. Our\napproach provides a foundation for optimizing large-scale autonomous drone\nlogistics while ensuring safe and efficient transportation.",
        "Gaia observations have revealed over a million stellar binary candidates\nwithin ~1 kpc of the Sun, predominantly characterized by orbital separations\n>10^3 AU and eccentricities >0.7. The prevalence of such wide, eccentric\nbinaries has proven challenging to explain through canonical binary formation\nchannels. However, recent advances in our understanding of three-body binary\nformation (3BBF) -- new binary assembly by the gravitational scattering of\nthree unbound bodies -- have shown that 3BBF in star clusters can efficiently\ngenerate wide, highly eccentric binaries. We further explore this possibility\nby constructing a semi-analytic model of the Galactic binary population in the\nsolar neighborhood, originating from 3BBF in star clusters. The model relies on\n3BBF scattering experiments to determine how the 3BBF rate and resulting binary\nproperties scale with local stellar density and velocity dispersion. We then\nmodel the Galactic star cluster population, incorporating up-to-date\nprescriptions for the Galaxy's star formation history as well as the birth\nproperties and internal evolution of its star clusters. Finally, we account for\nbinary destruction induced by perturbations from stellar interactions before\ncluster escape and and for subsequent changes to binary orbital elements by\ndynamical interactions in the Galactic field. Without any explicit fine-tuning,\nour model closely reproduces both the total number of Gaia's wide binaries and\ntheir separation distribution, and qualitatively matches the eccentricity\ndistribution, suggesting that 3BBF may be an important formation channel for\nthese enigmatic systems.",
        "Recently, Imolay, Karl, Nazy and V\\'{a}li explored a generalization of\nTur\\'{a}n's forbidden subgraph problem and Ruzsa-Szrmer\\'{e}di $(6,3)$-problem.\nThey specifically studied the following question: for two graphs $F$ and $G$,\ndetermine the maximum number of edge-disjoint copies of $F$ in a set of $n$\nvertices such that there is no copies of $G$ whose edges come from different\n$F$-copies. The maximum number is denoted by $ex_F(n,G)$ and is called the {\\em\n$F$-multicolor Tur\\'{a}n number} of $G$. One of their main results is that\n$ex_F(n,G)=o(n^2)$ if and only if there exists a homomorphism from $G$ to $F$.\nWe generalize the result to uniformly hypergraph using main tools of Hypergraph\nRegularity Lemma and Counting Lemma."
      ]
    }
  }
]