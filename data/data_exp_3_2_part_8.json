[
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm",
    "start_abstract":"The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A method for real-time optimal heliostat aiming strategy generation via deep learning"
      ],
      "abstract":[
        "Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Attention Pruning: Automated Fairness Repair of Language Models via\n  Surrogate Simulated Annealing",
        "LLM Reasoner and Automated Planner: A new NPC approach",
        "CollabLLM: From Passive Responders to Active Collaborators",
        "DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale\n  Traveling Salesman Problem",
        "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
        "Advice for Diabetes Self-Management by ChatGPT Models: Challenges and\n  Recommendations",
        "A Driver Advisory System Based on Large Language Model for High-speed\n  Train",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
        "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
        "Human-Alignment Influences the Utility of AI-assisted Decision Making",
        "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for\n  Skin Cancer Treatment",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "Uncovering the Iceberg in the Sea: Fundamentals of Pulse Shaping and\n  Modulation Design for Random ISAC Signals",
        "Efficient Image Restoration via Latent Consistency Flow Matching",
        "Quantum Computer Controlled by Superconducting Digital Electronics at\n  Millikelvin Temperature",
        "ESPARGOS: An Ultra Low-Cost, Realtime-Capable Multi-Antenna WiFi Channel\n  Sounder",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Graph Neural Network Flavor Tagger and measurement of\n  $\\mathrm{sin}2\\beta$ at Belle II",
        "Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation",
        "Channel Resolvability Using Multiplicative Weight Update Algorithm",
        "DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco\n  Addiction Prevention",
        "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
        "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image\n  Models?",
        "A Survey on the Optimization of Large Language Model-based Agents",
        "Algorithmic Clustering based on String Compression to Extract P300\n  Structure in EEG Signals",
        "Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series",
        "Decentralized Online Ensembles of Gaussian Processes for Multi-Agent\n  Systems"
      ],
      "abstract":[
        "This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active\/inactive) and their corresponding fairness\/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.",
        "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase.",
        "Large Language Models are typically trained with next-turn rewards, limiting\ntheir ability to optimize for long-term interaction. As a result, they often\nrespond passively to ambiguous or open-ended user requests, failing to help\nusers reach their ultimate intents and leading to inefficient conversations. To\naddress these limitations, we introduce CollabLLM, a novel and general training\nframework that enhances multiturn human-LLM collaboration. Its key innovation\nis a collaborative simulation that estimates the long-term contribution of\nresponses using Multiturn-aware Rewards. By reinforcement fine-tuning these\nrewards, CollabLLM goes beyond responding to user requests, and actively\nuncovers user intent and offers insightful suggestions-a key step towards more\nhuman-centered AI. We also devise a multiturn interaction benchmark with three\nchallenging tasks such as document creation. CollabLLM significantly\noutperforms our baselines with averages of 18.5% higher task performance and\n46.3% improved interactivity by LLM judges. Finally, we conduct a large user\nstudy with 201 judges, where CollabLLM increases user satisfaction by 17.6% and\nreduces user spent time by 10.4%.",
        "This paper proposes a dual divide-and-optimize algorithm (DualOpt) for\nsolving the large-scale traveling salesman problem (TSP). DualOpt combines two\ncomplementary strategies to improve both solution quality and computational\nefficiency. The first strategy is a grid-based divide-and-conquer procedure\nthat partitions the TSP into smaller sub-problems, solving them in parallel and\niteratively refining the solution by merging nodes and partial routes. The\nprocess continues until only one grid remains, yielding a high-quality initial\nsolution. The second strategy involves a path-based divide-and-optimize\nprocedure that further optimizes the solution by dividing it into sub-paths,\noptimizing each using a neural solver, and merging them back to progressively\nimprove the overall solution. Extensive experiments conducted on two groups of\nTSP benchmark instances, including randomly generated instances with up to\n100,000 nodes and real-world datasets from TSPLIB, demonstrate the\neffectiveness of DualOpt. The proposed DualOpt achieves highly competitive\nresults compared to 10 state-of-the-art algorithms in the literature. In\nparticular, DualOpt achieves an improvement gap up to 1.40% for the largest\ninstance TSP100K with a remarkable 104x speed-up over the leading heuristic\nsolver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB\nbenchmarks, confirming its capability to tackle diverse real-world TSP\napplications.",
        "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
        "Given their ability for advanced reasoning, extensive contextual\nunderstanding, and robust question-answering abilities, large language models\nhave become prominent in healthcare management research. Despite adeptly\nhandling a broad spectrum of healthcare inquiries, these models face\nsignificant challenges in delivering accurate and practical advice for chronic\nconditions such as diabetes. We evaluate the responses of ChatGPT versions 3.5\nand 4 to diabetes patient queries, assessing their depth of medical knowledge\nand their capacity to deliver personalized, context-specific advice for\ndiabetes self-management. Our findings reveal discrepancies in accuracy and\nembedded biases, emphasizing the models' limitations in providing tailored\nadvice unless activated by sophisticated prompting techniques. Additionally, we\nobserve that both models often provide advice without seeking necessary\nclarification, a practice that can result in potentially dangerous advice. This\nunderscores the limited practical effectiveness of these models without human\noversight in clinical settings. To address these issues, we propose a\ncommonsense evaluation layer for prompt evaluation and incorporating\ndisease-specific external memory using an advanced Retrieval Augmented\nGeneration technique. This approach aims to improve information quality and\nreduce misinformation risks, contributing to more reliable AI applications in\nhealthcare settings. Our findings seek to influence the future direction of AI\nin healthcare, enhancing both the scope and quality of its integration.",
        "With the rapid development of China high-speed railway, drivers face\nincreasingly significant technical challenges during operations, such as fault\nhandling. Currently, drivers depend on the onboard mechanic when facing\ntechnical issues, for instance, traction loss or sensor faults. This dependency\ncan hinder effective operation, even lead to accidents, while waiting for\nfaults to be addressed. To enhance the accuracy and explainability of actions\nduring fault handling, an Intelligent Driver Advisory System (IDAS) framework\nbased on a large language model (LLM) named IDAS-LLM, is introduced. Initially,\ndomain-fine-tuning of the LLM is performed using a constructed railway\nknowledge question-and-answer dataset to improve answer accuracy in\nrailway-related questions. Subsequently, integration of the Retrieval-augmented\nGeneration (RAG) architecture is pursued for system design to enhance the\nexplainability of generated responses. Comparative experiments are conducted\nusing the constructed railway driving knowledge assessment dataset. Results\nindicate that domain-fine-tuned LLMs show an improvement in answer accuracy by\nan average of 10%, outperforming some current mainstream LLMs. Additionally,\nthe inclusion of the RAG framework increases the average recall rate of\nquestion-and-answer sessions by about 4%. Finally, the fault handling\ncapability of IDAS-LLM is demonstrated through simulations of real operational\nscenarios, proving that the proposed framework has practical application\nprospects.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely.",
        "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
        "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.",
        "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "Integrated Sensing and Communications (ISAC) is expected to play a pivotal\nrole in future 6G networks. To maximize time-frequency resource utilization, 6G\nISAC systems must exploit data payload signals, that are inherently random, for\nboth communication and sensing tasks. This paper provides a comprehensive\nanalysis of the sensing performance of such communication-centric ISAC signals,\nwith a focus on modulation and pulse shaping design to reshape the statistical\nproperties of their auto-correlation functions (ACFs), thereby improving the\ntarget ranging performance. We derive a closed-form expression for the\nexpectation of the squared ACF of random ISAC signals, considering arbitrary\nmodulation bases and constellation mappings within the Nyquist pulse shaping\nframework. The structure is metaphorically described as an ``iceberg hidden in\nthe sea\", where the ``iceberg'' represents the squared mean of the ACF of\nrandom ISAC signals, that is determined by the pulse shaping filter, and the\n``sea level'' characterizes the corresponding variance, caused by the\nrandomness of the data payload. Our analysis shows that, for QAM\/PSK\nconstellations with Nyquist pulse shaping, Orthogonal Frequency Division\nMultiplexing (OFDM) achieves the lowest ranging sidelobe level across all lags.\nBuilding on these insights, we propose a novel Nyquist pulse shaping design to\nenhance the sensing performance of random ISAC signals. Numerical results\nvalidate our theoretical findings, showing that the proposed pulse shaping\nsignificantly reduces ranging sidelobes compared to conventional root-raised\ncosine (RRC) pulse shaping, thereby improving the ranging performance.",
        "Recent advances in generative image restoration (IR) have demonstrated\nimpressive results. However, these methods are hindered by their substantial\nsize and computational demands, rendering them unsuitable for deployment on\nedge devices. This work introduces ELIR, an Efficient Latent Image Restoration\nmethod. ELIR operates in latent space by first predicting the latent\nrepresentation of the minimum mean square error (MMSE) estimator and then\ntransporting this estimate to high-quality images using a latent consistency\nflow-based model. Consequently, ELIR is more than 4x faster compared to the\nstate-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also\nmore than 4x smaller, making it well-suited for deployment on\nresource-constrained edge devices. Comprehensive evaluations of various image\nrestoration tasks show that ELIR achieves competitive results, effectively\nbalancing distortion and perceptual quality metrics while offering improved\nefficiency in terms of memory and computation.",
        "Current superconducting quantum computing platforms face significant scaling\nchallenges, as individual signal lines are required for control of each qubit.\nThis wiring overhead is a result of the low level of integration between\ncontrol electronics at room temperature and qubits operating at millikelvin\ntemperatures, which raise serious doubts among technologists about whether\nutility-scale quantum computers can be built. A promising alternative is to\nutilize cryogenic, superconducting digital control electronics that coexist\nwith qubits. Here, we report the first multi-qubit system integrating this\ntechnology. The system utilizes digital demultiplexing, breaking the linear\nscaling of control lines to number of qubits. We also demonstrate single-qubit\nfidelities above 99%, and up to 99.9%. This work is a critical step forward in\nrealizing highly scalable chip-based quantum computers.",
        "Multi-antenna channel sounding is a technique for measuring the propagation\ncharacteristics of electromagnetic waves that is commonly employed for\nparameterizing channel models. Channel sounders are usually custom-built from\nmany Software Defined Radio receivers, making them expensive to procure and\ndifficult to operate, which constrains the set of users to a few specialized\nscientific institutions and industrial research laboratories. Recent\ndevelopments in Joint Communications and Sensing (JCaS) extend the possible\nuses of channel data to applications like human activity recognition, human\npresence detection, user localization and wireless Channel Charting, all of\nwhich are of great interest to security researchers, experts in industrial\nautomation and others. However, due to a lack of affordable, easy-to-use and\ncommercially available multi-antenna channel sounders, those scientific\ncommunities can be hindered by their lack of access to wireless channel\nmeasurements. To lower the barrier to entry for channel sounding, we develop an\nultra low-cost measurement hardware platform based on mass-produced WiFi chips,\nwhich is easily affordable to research groups and even hobbyists.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "We present GFlaT, a new algorithm that uses a graph-neural-network to\ndetermine the flavor of neutral B mesons produced in $\\mathrm{\\Upsilon(4S)}$\ndecays. We evaluate its performance using $B$ decays to flavor-specific\nhadronic final states reconstructed in a $362$ $\\mathrm{fb}^{-1}$ sample of\nelectron-positron collisions recorded at the $\\mathrm{\\Upsilon(4S)}$ resonance\nwith the Belle II detector at the SuperKEKB collider. We achieve an effective\ntagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first\nuncertainty is statistical and the second systematic, which is $18\\%$ better\nthan the previous Belle II algorithm. Demonstrating the algorithm, we use $B^0\n\\to J\/\\psi K_\\mathrm{S}^0$ decays to measure the direct and mixing-induced CP\nviolation parameters, $C = (-0.035 \\pm 0.026 \\pm 0.013)$ and $S = (0.724 \\pm\n0.035 \\pm 0.014)$, from which we obtain $\\beta = (23.2 \\pm 1.5 \\pm\n0.6)^{\\circ}$.",
        "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. A state-of-the-art domain adaptation technique is Generative Pseudo\nLabeling (GPL). GPL uses synthetic query generation and initially mined hard\nnegatives to distill knowledge from cross-encoder to dense retrievers in the\ntarget domain. In this paper, we analyze the documents retrieved by the\ndomain-adapted model and discover that these are more relevant to the target\nqueries than those of the non-domain-adapted model. We then propose refreshing\nthe hard-negative index during the knowledge distillation phase to mine better\nhard negatives. Our remining R-GPL approach boosts ranking performance in 13\/14\nBEIR datasets and 9\/12 LoTTe datasets. Our contributions are (i) analyzing hard\nnegatives returned by domain-adapted and non-domain-adapted models and (ii)\napplying the GPL training with and without hard-negative re-mining in LoTTE and\nBEIR datasets.",
        "We study the channel resolvability problem, which is used to prove strong\nconverse of identification via channel. Channel resolvability has been solved\nby only random coding in the literature. We prove channel resolvability using\nthe multiplicative weight update algorithm. This is the first approach to\nchannel resolvability using non-random coding.",
        "While tobacco advertising innovates at unprecedented speed, traditional\nsurveillance methods remain frozen in time, especially in the context of social\nmedia. The lack of large-scale, comprehensive datasets and sophisticated\nmonitoring systems has created a widening gap between industry advancement and\npublic health oversight. This paper addresses this critical challenge by\nintroducing Tobacco-1M, a comprehensive dataset of one million tobacco product\nimages with hierarchical labels spanning 75 product categories, and DEFEND, a\nnovel foundation model for tobacco product understanding. Our approach\nintegrates a Feature Enhancement Module for rich multimodal representation\nlearning, a Local-Global Visual Coherence mechanism for detailed feature\ndiscrimination, and an Enhanced Image-Text Alignment strategy for precise\nproduct characterization. Experimental results demonstrate DEFEND's superior\nperformance, achieving 83.1% accuracy in product classification and 73.8% in\nvisual question-answering tasks, outperforming existing methods by significant\nmargins. Moreover, the model exhibits robust zero-shot learning capabilities\nwith 45.6% accuracy on novel product categories. This work provides regulatory\nbodies and public health researchers with powerful tools for monitoring\nemerging tobacco products and marketing strategies, potentially revolutionizing\napproaches to tobacco control and public health surveillance.",
        "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
        "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...",
        "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https:\/\/github.com\/YoungDubbyDu\/LLM-Agent-Optimization.",
        "P300 is an Event-Related Potential widely used in Brain-Computer Interfaces,\nbut its detection is challenging due to inter-subject and temporal variability.\nThis work introduces a clustering methodology based on Normalized Compression\nDistance (NCD) to extract the P300 structure, ensuring robustness against\nvariability. We propose a novel signal-to-ASCII transformation to generate\ncompression-friendly objects, which are then clustered using a hierarchical\ntree-based method and a multidimensional projection approach. Experimental\nresults on two datasets demonstrate the method's ability to reveal relevant\nP300 structures, showing clustering performance comparable to state-of-the-art\napproaches. Furthermore, analysis at the electrode level suggests that the\nmethod could assist in electrode selection for P300 detection. This\ncompression-driven clustering methodology offers a complementary tool for EEG\nanalysis and P300 identification.",
        "Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and\/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.",
        "Flexible and scalable decentralized learning solutions are fundamentally\nimportant in the application of multi-agent systems. While several recent\napproaches introduce (ensembles of) kernel machines in the distributed setting,\nBayesian solutions are much more limited. We introduce a fully decentralized,\nasymptotically exact solution to computing the random feature approximation of\nGaussian processes. We further address the choice of hyperparameters by\nintroducing an ensembling scheme for Bayesian multiple kernel learning based on\nonline Bayesian model averaging. The resulting algorithm is tested against\nBayesian and frequentist methods on simulated and real-world datasets."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A method for real-time optimal heliostat aiming strategy generation via deep learning",
    "start_abstract":"Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm"
      ],
      "abstract":[
        "The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "A global approach for generalized semi-infinte programs with polyhedral\n  parameter sets",
        "Pareto sensitivity, most-changing sub-fronts, and knee solutions",
        "Extension of Controllability Score to Infinite-Dimensional Systems",
        "La M\\'ethode du Gradient Proxim\\'e",
        "Inertial Bregman Proximal Gradient under Partial Smoothness",
        "Price and Assortment Optimization under the Multinomial Logit Model with\n  Opaque Products",
        "A Stochastic Linear-Quadratic Leader-Follower Differential Game with\n  Elephant Memory",
        "Distributionally Fair Peer-to-Peer Electricity Trading",
        "Set-valued evenly convex functions: characterizations and c-conjugacy",
        "On the acceleration of gradient methods: the triangle steepest descent\n  method",
        "A Linear Complexity Algorithm for Optimal Transport Problem with\n  Log-type Cost",
        "Regularity of the Product of Two Relaxed Cutters with Relaxation\n  Parameters Beyond Two",
        "Scalable Second-Order Optimization Algorithms for Minimizing Low-rank\n  Functions",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "A hybrid pressure formulation of the face-centred finite volume method\n  for viscous laminar incompressible flows",
        "Chemically-Accurate Prediction of the Ionisation Potential of Helium\n  Using a Quantum Processor",
        "Testing the QCD formation time with reconstructed parton splittings",
        "Cauchy Random Features for Operator Learning in Sobolev Space",
        "Grid-based exoplanet atmospheric mass loss predictions through neural\n  network",
        "Hyper-neutron stars from an ab initio calculation",
        "The Method of ${\\cal M}_{n}$-Extension: The KdV Equation",
        "Stronger Constraints on Primordial Black Holes as Dark Matter Derived\n  from the Thermal Evolution of the Intergalactic Medium over the Last Twelve\n  Billion Years",
        "Impulsive mixing of stellar populations in dwarf spheroidal galaxies",
        "A Flux-Tunable cavity for Dark matter detection",
        "Apparent teleportation of indistinguishable particles",
        "Long Lived Quasinormal Modes of Regular and Extreme Black Holes",
        "Positive Feedback: How a Synergy Between the Streaming Instability and\n  Dust Coagulation Forms Planetesimals",
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$"
      ],
      "abstract":[
        "This paper studies generalized semi-infinite programs (GSIPs) defined with\npolyhedral parameter sets. Assume these GSIPs are given by polynomials. We\npropose a new approach to solve them as a disjunctive program. This approach is\nbased on the Kurash-Kuhn-Tucker (KKT) conditions of the robust constraint and a\ntechnique called partial Lagrange multiplier expressions. We summarize a\nsemidefinite algorithm and study its convergence properties. Numerical\nexperiments are given to show the efficiency of our method. In addition, we\nchecked its performance in gemstone cutting and robust control applications.",
        "When dealing with a multi-objective optimization problem, obtaining a\ncomprehensive representation of the Pareto front can be computationally\nexpensive. Furthermore, identifying the most representative Pareto solutions\ncan be difficult and sometimes ambiguous. A popular selection are the so-called\nPareto knee solutions, where a small improvement in any objective leads to a\nlarge deterioration in at least one other objective. In this paper, using\nPareto sensitivity, we show how to compute Pareto knee solutions according to\ntheir verbal definition of least maximal change. We refer to the resulting\napproach as the sensitivity knee (snee) approach, and we apply it to\nunconstrained and constrained problems. Pareto sensitivity can also be used to\ncompute the most-changing Pareto sub-fronts around a Pareto solution, where the\npoints are distributed along directions of maximum change, which could be of\ninterest in a decision-making process if one is willing to explore solutions\naround a current one. Our approach is still restricted to scalarized methods,\nin particular to the weighted-sum or epsilon-constrained methods, and require\nthe computation or approximations of first- and second-order derivatives. We\ninclude numerical results from synthetic problems that illustrate the benefits\nof our approach.",
        "Centrality analysis in dynamical network systems is essential for\nunderstanding system behavior. In finite-dimensional settings, controllability\nscores -- namely, the Volumetric Controllability Score (VCS) and the Average\nEnergy Controllability Score (AECS) -- are defined as the unique solutions of\nspecific optimization problems. In this work, we extend these concepts to\ninfinite-dimensional systems by formulating analogous optimization problems.\nMoreover, we prove that these optimization problems have optimal solutions\nunder weak assumptions, and that both VCS and AECS remain unique in the\ninfinite-dimensional context under appropriate assumptions. The uniqueness of\nthe controllability scores is essential to use them as a centrality measure,\nsince it not only reflects the importance of each state in the dynamical\nnetwork but also provides a consistent basis for interpretation and comparison\nacross different researchers. Finally, we illustrate the behavior of VCS and\nAECS with a numerical experiment based on the heat equation.",
        "English version of abstract for \"The Proximal Gradient Method\": The proximal\ngradient method is a splitting algorithm for the minimization of the sum of two\nconvex functions, one of which is smooth. It has applications in areas such as\nmechanics, inverse problems, machine learning, image reconstruction,\nvariational inequalities, statistics, operations research, and optimal\ntransportation. Its formalism encompasses a wide variety of numerical methods\nin optimization such as gradient descent, projected gradient, iterative\nthresholding, alternating projections, the constrained Landweber method, as\nwell as various algorithms in statistics and sparse data analysis. This paper\naims at providing an account of the main properties of the proximal gradient\nmethod and to discuss some of its applications. -- -- -- -- -- -\n  R\\'esum\\'e : La m\\'ethode du gradient proxim\\'e est un algorithme\nd'\\'eclatement pour la minimisation de la somme de deux fonctions convexes,\ndont l'une est lisse. Elle trouve des applications dans des domaines tels que\nla m\\'ecanique, le traitement du signal, les probl\\`emes inverses,\nl'apprentissage automatique, la reconstruction d'images, les in\\'equations\nvariationnelles, les statistiques, la recherche op\\'erationnelle et le\ntransport optimal. Son formalisme englobe une grande vari\\'et\\'e de m\\'ethodes\nnum\\'eriques en optimisation, telles que la descente de gradient, le gradient\nprojet\\'e, la m\\'ethode de seuillage it\\'eratif, la m\\'ethode des projections\naltern\\'ees, la m\\'ethode de Landweber contrainte, ainsi que divers algorithmes\nen statistique et en analyse parcimonieuse de donn\\'ees. Cet article vise \\`a\ndonner un aper\\c{c}u des principales propri\\'et\\'es de la m\\'ethode du gradient\nproxim\\'e et d'aborder certaines de ses applications.",
        "This work considers an Inertial version of Bregman Proximal Gradient\nalgorithm (IBPG) for minimizing the sum of two single-valued functions in\nfinite dimension. We suppose that one of the functions is proper, closed, and\nconvex but non-necessarily smooth whilst the second is a smooth enough function\nbut not necessarily convex. For the latter, we ask the smooth adaptable\nproperty (smad) with respect to some kernel or entropy which allows to remove\nthe very popular global Lipschitz continuity requirement on the gradient of the\nsmooth part. We consider the IBPG under the framework of the triangle scaling\nproperty (TSP) which is a geometrical property for which one can provably\nensure acceleration for a certain subset of kernel\/entropy functions in the\nconvex setting. Based on this property, we provide global convergence\nguarantees when the entropy is strongly convex under the framework of the\nKurdyka-\\L{}ojasiewicz (KL) property. Turning to the local convergence\nproperties, we show that when the nonsmooth part is partly smooth relative to a\nsmooth submanifold, IBPG has a finite activity identification property before\nentering a local linear convergence regime for which we establish a sharp\nestimate of the convergence rate. We report numerical simulations to illustrate\nour theoretical results on low complexity regularized phase retrieval.",
        "An opaque product is a product for which only partial information is\ndisclosed to the buyer at the time of purchase. Opaque products are common in\nsectors such as travel and online retail, where the car type or product color\nis hidden in the opaque product. Opaque products enable sellers to target\ncustomers who prefer a price discount in exchange for being flexible about the\nproduct they receive. In this paper, we integrate opaque products and\ntraditional products together into the multinomial logit (MNL) choice model and\nstudy the associated price and assortment optimization problems. For the price\noptimization problem, we surprisingly show that uniform pricing is optimal\nwhich implies it has the same optimal pricing solution and value as the\ntraditional MNL model. While adding an opaque product priced at the\nrevenue-maximizing price may enhance revenue given arbitrary traditional\nproduct prices, this advantage disappears when all prices are optimized\njointly. For the assortment optimization problem, we show that the\nrevenue-maximizing assortment is nested-by-valuation for uniformly priced\nproducts. For non-uniformly priced cases, we propose a natural\nnested-by-revenue-and-valuation heuristic that performs extremely well in an\nextensive numerical study.",
        "This paper is concerned with a stochastic linear-quadratic leader-follower\ndifferential game with elephant memory. The model is general in that the state\nequation for both the leader and the follower includes the elephant memory of\nthe state and the control, which are part of the diffusion term. Under certain\nassumptions, the state feedback representation of the open-loop Stackelberg\nstrategy is derived by introducing two Riccati equations and a special\nmatrix-valued equation. Finally, theoretical results are illustrated by means\nof an example concerning a dynamic advertising problem with elephant memory.",
        "Peer-to-peer energy trading platforms enable direct electricity exchanges\nbetween peers who belong to the same energy community. In a semi-decentralized\nsystem, a community manager adheres to grid restrictions while optimizing\nsocial welfare. However, with no further supervision, some peers can be\ndiscriminated against from participating in the electricity trades. To solve\nthis issue, this paper proposes an optimization-based mechanism to enable\ndistributionally fair peer-to-peer electricity trading. For the implementation\nof our mechanism, peers are grouped by energy poverty level. The proposed model\naims to redistribute the electricity trades to minimize the maximum Wasserstein\ndistance among the transaction distributions linked to the groups while\nlimiting the sacrifice level with a predefined parameter. We demonstrate the\neffectiveness of our proposal using the IEEE 33-bus distribution grid,\nsimulating an energy community with 1600 peers. Results indicate that up to\n70.1% of unfairness can be eliminated by using our proposed model, even\nachieving a full elimination when including a non-profit community photovoltaic\nplant.",
        "In this work we deal with set-valued functions with values in the power set\nof a separated locally convex space where a nontrivial pointed convex cone\ninduces a partial order relation. A set-valued function is evenly convex if its\nepigraph is an evenly convex set, i.e., it is the intersection of an arbitrary\nfamily of open half-spaces. In this paper we characterize evenly convex\nset-valued functions as the pointwise supremum of its set-valued e-affine\nminorants. Moreover, a suitable conjugation pattern will be developed for these\nfunctions, as well as the counterpart of the biconjugation Fenchel-Moreau\ntheorem.",
        "The gradient type of methods has been a competitive choice in solving large\nscale problems arising from various applications such as machine learning.\nHowever, there is still space to accelerate the gradient methods. To this end,\nin this paper, we pay attention to the cyclic steepest descent method (CSD),\nand prove that the CSD method has a gradient subsequence that is\nR-superlinearly convergent for the 2-dimensional strictly convex quadratic\ncase. Moreover, we propose a new gradient method called triangle steepest\ndescent method (TSD) which has a parameter $j$ to control the number of cycles.\nThis method is motivated by utilizing a geometric property of the steepest\ndescent method (SD) method to get around the zigzag behavior. We show that the\nTSD method is at least R-linearly convergent for strictly convex quadratic\nproblems. The advantage of the TSD method is that it is not sensitive to the\ncondition number of a strictly convex quadratic problem. For example, it\nperforms better than other competitive gradient methods when the condition\nnumber reaches 1e20 or 1e100 for some strictly convex quadratic problems.\nExtensive numerical results verify the efficiency of the TSD method compared to\nother types of gradient methods.",
        "In [Q. Liao et al., Commun. Math. Sci., 20(2022)], a linear-time Sinkhorn\nalgorithm is developed based on dynamic programming, which significantly\nreduces the computational complexity involved in solving optimal transport\nproblems. However, this algorithm is specifically designed for the\nWasserstein-1 metric. We are curious whether the preceding dynamic programming\nframework can be extended to tackle optimal transport problems with different\ntransport costs. Notably, two special kinds of optimal transport problems, the\nSinkhorn ranking and the far-field reflector and refractor problems, are\nclosely associated with the log-type transport costs. Interestingly, by\nemploying series rearrangement and dynamic programming techniques, it is\nfeasible to perform the matrix-vector multiplication within the Sinkhorn\niteration in linear time for this type of cost. This paper provides a detailed\nexposition of its implementation and applications, with numerical simulations\ndemonstrating the effectiveness and efficiency of our methods.",
        "We study the product of two relaxed cutters having a common fixed point. We\nassume that one of the relaxation parameters is greater than two so that the\ncorresponding relaxed cutter is no longer quasi-nonexpansive, but rather\ndemicontractive. We show that if both of the operators are (weakly\/linearly)\nregular, then under certain conditions, the resulting product inherits the same\ntype of regularity. We then apply these results to proving convergence in the\nweak, norm and linear sense of algorithms that employ such products.",
        "We present a random-subspace variant of cubic regularization algorithm that\nchooses the size of the subspace adaptively, based on the rank of the projected\nsecond derivative matrix. Iteratively, our variant only requires access to\n(small-dimensional) projections of first- and second-order problem derivatives\nand calculates a reduced step inexpensively. The ensuing method maintains the\noptimal global rate of convergence of (full-dimensional) cubic regularization,\nwhile showing improved scalability both theoretically and numerically,\nparticularly when applied to low-rank functions. When applied to the latter,\nour algorithm naturally adapts the subspace size to the true rank of the\nfunction, without knowing it a priori.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "This work presents a hybrid pressure face-centred finite volume (FCFV) solver\nto simulate steady-state incompressible Navier-Stokes flows. The method\nleverages the robustness, in the incompressible limit, of the hybridisable\ndiscontinuous Galerkin paradigm for compressible and weakly compressible flows\nto derive the formulation of a novel, low-order face-based discretisation. The\nincompressibility constraint is enforced in a weak sense, by introducing an\ninter-cell mass flux defined in terms of a new, hybrid variable, representing\nthe pressure at the cell faces. This results in a new hybridisation strategy\nwhere cell variables (velocity, pressure and deviatoric strain rate tensor) are\nexpressed as a function of velocity and pressure at the barycentre of the cell\nfaces. The hybrid pressure formulation provides first-order convergence of all\nvariables, including the stress, independently of cell type, stretching and\ndistortion. Numerical benchmarks of Navier-Stokes flows at low and moderate\nReynolds numbers, in two and three dimensions, are presented to evaluate\naccuracy and robustness of the method. In particular, the hybrid pressure\nformulation outperforms the FCFV method when convective effects are relevant,\nachieving accurate predictions on significantly coarser meshes.",
        "Quantum computers have the potential to revolutionise our understanding of\nthe microscopic behaviour of materials and chemical processes by enabling\nhigh-accuracy electronic structure calculations to scale more efficiently than\nis possible using classical computers. Current quantum computing hardware\ndevices suffer from the dual challenges of noise and cost, which raises the\nquestion of what practical value these devices might offer before full fault\ntolerance is achieved and economies of scale enable cheaper access. Here we\nexamine the practical value of noisy quantum computers as tools for\nhigh-accuracy electronic structure, by using a Quantinuum ion-trap quantum\ncomputer to predict the ionisation potential of helium. By combining a series\nof techniques suited for use with current hardware including qubit-efficient\nencoding coupled with chemical insight, low-cost variational optimisation with\nhardware-adapted quantum circuits, and moments-based corrections, we obtain an\nionisation potential of 24.5536 (+0.0011, -0.0005) eV, which agrees with the\nexperimentally measured value to within true chemical accuracy, and with high\nstatistical confidence. The methods employed here can be generalised to predict\nother properties and expand our understanding of the value that might be\nprovided by near-term quantum computers.",
        "In high-energy elementary collisions the space-time ordering of parton\nbranching processes is not accessible experimentally. In contrast, in heavy-ion\ncollisions, parton showers interact with a spatially extended dense medium.\nThis sets a reference length scale with respect to which the space-time\nordering may be analysed. Here, we explore the possibility of identifying\nexperimental signatures of the QCD formation time, $\\tau_f$, on the level of a\nsingle parton splitting. Since heavy flavour offers an additional handle on\ntracing the propagation of individual quarks through the medium, we focus on\nthe $g\\to c\\bar{c}$ splitting. Combining adapted versions of the\nCambridge-Aachen and FlavourCone jet finding algorithms with grooming\ntechniques, we show how the kinematics of such splittings can be reconstructed\nwith high fidelity using either final state partons or hadrons, and how the\nformation time distribution of parton splittings can be constructed therefrom.\nMedium modification leads to a characteristic modification of this $\\tau_f$\ndistribution. This effect can be used to construct experimentally-accessible\nratios of $\\tau_f$ distributions, in which the sensitivity of the medium\nmodification to the QCD formation time becomes measurable.",
        "Operator learning is the approximation of operators between infinite\ndimensional Banach spaces using machine learning approaches. While most\nprogress in this area has been driven by variants of deep neural networks such\nas the Deep Operator Network and Fourier Neural Operator, the theoretical\nguarantees are often in the form of a universal approximation property.\nHowever, the existence theorems do not guarantee that an accurate operator\nnetwork is obtainable in practice. Motivated by the recent kernel-based\noperator learning framework, we propose a random feature operator learning\nmethod with theoretical guarantees and error bounds. The random feature method\ncan be viewed as a randomized approximation of a kernel method, which\nsignificantly reduces the computation requirements for training. We provide a\ngeneralization error analysis for our proposed random feature operator learning\nmethod along with comprehensive numerical results. Compared to kernel-based\nmethod and neural network methods, the proposed method can obtain similar or\nbetter test errors across benchmarks examples with significantly reduced\ntraining times. An additional advantages it that our implementation is simple\nand does require costly computational resources, such as GPU.",
        "The fast and accurate estimation of planetary mass-loss rates is critical for\nplanet population and evolution modelling. We use machine learning (ML) for\nfast interpolation across an existing large grid of hydrodynamic upper\natmosphere models, providing mass-loss rates for any planet inside the grid\nboundaries with superior accuracy compared to previously published\ninterpolation schemes. We consider an already available grid comprising about\n11000 hydrodynamic upper atmosphere models for training and generate an\nadditional grid of about 250 models for testing purposes. We develop the ML\ninterpolation scheme (dubbed \"atmospheric Mass Loss INquiry frameworK\"; MLink)\nusing a Dense Neural Network, further comparing the results with what was\nobtained employing classical approaches (e.g. linear interpolation and radial\nbasis function-based regression). Finally, we study the impact of the different\ninterpolation schemes on the evolution of a small sample of carefully selected\nsynthetic planets. MLink provides high-quality interpolation across the entire\nparameter space by significantly reducing both the number of points with large\ninterpolation errors and the maximum interpolation error compared to previously\navailable schemes. For most cases, evolutionary tracks computed employing MLink\nand classical schemes lead to comparable planetary parameters at\nGyr-timescales. However, particularly for planets close to the top edge of the\nradius gap, the difference between the predicted planetary radii at a given age\nof tracks obtained employing MLink and classical interpolation schemes can\nexceed the typical observational uncertainties. Machine learning can be\nsuccessfully used to estimate atmospheric mass-loss rates from model grids\npaving the way to explore future larger and more complex grids of models\ncomputed accounting for more physical processes.",
        "The equation of state (EoS) of neutron matter plays a decisive role to\nunderstand the neutron star properties and the gravitational waves from neutron\nstar mergers. At sufficient densities, the appearance of hyperons generally\nsoftens the EoS, leading to a reduction in the maximum mass of neutron stars\nwell below the observed values of about 2 solar masses. Even though repulsive\nthree-body forces are known to solve this so-called ``hyperon puzzle'', so far\nperforming \\textit{ab initio} calculations with a substantial number of\nhyperons for neutron star properties has remained elusive. Starting from the\nnewly developed auxiliary field quantum Monte Carlo algorithm to simulate\nhyper-neutron matter (HNM) without any sign oscillations, we derive three\ndistinct EoSs by employing the state-of-the-art Nuclear Lattice Effective Field\nTheory. We include $N\\Lambda$, $\\Lambda\\Lambda$ two-body forces, $NN\\Lambda$,\nand $N\\Lambda\\Lambda$ three-body forces. Consequently, we determine essential\nastrophysical quantities such as the neutron star mass, radius, tidal\ndeformability, and the universal $I$-Love-$Q$ relation. The maximum mass,\nradius and tidal deformability of a $1.4M_\\odot$ neutron star are predicted to\nbe $2.17(1)(1)~M_\\odot$, $R_{1.4M\\odot}=13.10(1)(7)~$km, and\n$\\Lambda_{1.4M_\\odot}=597(5)(18)$, respectively, based on our most realistic\nEoS. These predictions are in good agreement with the latest astrophysical\nconstraints derived from observations of massive neutron stars, gravitational\nwaves, and joint mass-radius measurements. Also, for the first time in\n\\textit{ab initio} calculations, we investigate both non-rotating and rotating\nneutron star configurations. The results indicate that the impact of rotational\ndynamics on the maximum mass is small, regardless of whether hyperons are\npresent in the EoS or not.",
        "In this work we generalize ${\\cal M}_{2}$-extension that has been introduced\nrecently. For illustration we use the KdV equation. We present five different\n${\\cal M}_{3}$-extensions of the KdV equation and their recursion operators. We\ngive a compact form of ${\\cal M}_{n}$-extension of the KdV equation and\nrecursion operator of the coupled KdV system. The method of ${\\cal\nM}_{n}$-extension can be applied to any integrable scalar equation to obtain\nintegrable multi-field system of equations. We also present unshifted and\nshifted nonlocal reductions of an example of ${\\cal M}_{3}$-extension of KdV.",
        "Primordial black holes (PBHs) have been explored as potential dark matter\ncandidates, with various astrophysical observations placing upper limits on the\nfraction $f_\\mathrm{PBH}$ of dark matter in the form of PBHs. However, a\nlargely underutilized probe of PBH abundance is the temperature of the\nintergalactic medium (IGM), inferred from the thermal broadening of absorption\nlines in the Lyman-$\\alpha$ forest of quasar spectra. PBHs inject energy into\nthe IGM via Hawking radiation, altering its thermal evolution. In this work, we\nconstrain this energy injection by self-consistently modeling its interplay\nwith the cosmological ultraviolet background from galaxies and supermassive\nblack holes. Leveraging IGM temperature measurements spanning the past twelve\nbillion years ($z \\sim 0$ to $6$), we derive one of the most stringent\nconstraints on PBH-induced heating from light PBHs within the mass range\n$10^{15}\\unicode{x2013}10^{17}$ g. Specifically, for $M_\\mathrm{PBH} = 10^{16}$\ng, we find $f_\\mathrm{PBH} < 5 \\times 10^{-5}$ at 95% confidence, with the\nbound scaling approximately as $M_\\mathrm{PBH}^{4}$ at other masses. Our\ninclusion of helium reionization and low-redshift temperature measurements\nstrengthens previous IGM-based PBH constraints by an order of magnitude or\nmore. Compared to other existing limits, our result is among the strongest,\nsecond only to the constraints from the 511 keV line from the Galactic Centre,\nbut with distinct systematics. More broadly, this study highlights the IGM\nthermal history as a powerful and independent probe of beyond-standard-model\nphysics.",
        "We study the response of mono-energetic stellar populations with initially\nisotropic kinematics to impulsive and adiabatic changes to an underlying dark\nmatter potential. Half-light radii expand and velocity dispersions decrease as\nenclosed dark matter is removed. The details of this expansion and cooling\ndepend on the time scale on which the underlying potential changes. In the\nadiabatic regime, the product of half-light radius and average velocity\ndispersion is conserved. We show that the stellar populations maintain\ncentrally isotropic kinematics throughout their adiabatic evolution, and their\ndensities can be approximated by a family of analytical radial profiles.\nMetallicity gradients within the galaxy flatten as dark matter is slowly\nremoved. In the case of strong impulsive perturbations, stellar populations\ndevelop power-law-like density tails with radially biased kinematics. We show\nthat the distribution of stellar binding energies within the dark matter halo\nsubstantially widens after an impulsive perturbation, no matter the sign of the\nperturbation. This allows initially energetically separated stellar populations\nto mix, to the extent that previously chemo-dynamically distinct populations\nmay masquerade as a single population with large metallicity and energy spread.\nFinally, we show that in response to an impulsive perturbation, stellar\npopulations that are deeply embedded in cored dark matter halos undergo a\nseries of damped oscillations before reaching a virialised equilibrium state,\ndriven by inefficient phase mixing in the harmonic potentials of cored halos.\nThis slow return to equilibrium adds substantial systematic uncertainty to\ndynamical masses estimated from Jeans modeling or the virial theorem.",
        "Developing a dark matter detector with wide mass tunability is an immensely\ndesirable property, yet it is challenging due to maintaining strong\nsensitivity. Resonant cavities for dark matter detection have traditionally\nemployed mechanical tuning, moving parts around to change electromagnetic\nboundary conditions. However, these cavities have proven challenging to operate\nin sub-Kelvin cryogenic environments due to differential thermal contraction,\nlow heat capacities, and low thermal conductivities. Instead, we develop an\nelectronically tunable cavity architecture by coupling a superconducting 3D\nmicrowave cavity with a DC flux tunable SQUID. With a flux delivery system\nengineered to maintain high coherence in the cavity, we perform a hidden-photon\ndark matter search below the quantum-limited threshold. A microwave photon\ncounting technique is employed through repeated quantum non-demolition\nmeasurements using a transmon qubit. With this device, we perform a\nhidden-photon search with a dark count rate of around 64 counts\/s and constrain\nthe kinetic mixing angle to ${\\varepsilon}< 4\\times 10^{-13}$ in a tunable band\nfrom 5.672 GHz to 5.694 GHz. By coupling multimode tunable cavities to the\ntransmon, wider hidden-photon searching ranges are possible.",
        "Teleportation, introduced in science fiction literature, is an instantaneous\nchange of the position of a microscopic object. Two teleportation-like\nphenomena were predicted by quantum mechanics: quantum teleportation and,\nrecently, quantum particle teleportation. The former is investigated\nexperimentally and has applications in quantum communication and computing.\n  Here, we introduced the third teleportation-like phenomenon - an apparent\nteleportation. It seems to be a natural consequence of elementary particles and\nantiparticles of the Standard Model being indistinguishable. We give an example\nof a process leading to the apparent teleportation within a toy model of\nboson-like particles. It utilizes the local transport of particles and\nantiparticles and the local creation and annihilation of particle-antiparticle\npairs. Furthermore, we suggest a method to observe the apparent teleportation\nin nucleus-nucleus collisions at properly selected collision energy. The method\nrequires the measurement of correlations between momenta of charm and anticharm\nhadrons in collisions with a single $c\\bar{c}$ pair being produced. The\nultimate prediction following the apparent teleportation hypothesis is the\nuncorrelated emission of charm and anticharm hadrons. It can be tested by\ncontemporary experiments.\n  Observing the apparent teleportation would uncover the basic transport\nproperties of indistinguishable particles. In particular, the apparent\nteleportation may explain the rapid thermalisation of the system created in\ncollisions of two atomic nuclei. Theoretical and experimental efforts are\nneeded to observe the apparent teleportation processes and study their\nproperties.",
        "Recently, black hole models in a nonlinear modification of the Maxwell\nelectrodynamics were suggested, possessing simultaneously properties of an\nextreme charge and regularity (Bronnikov K. A., Phys. Rev. D, 110 (2024)\n024021). We study quasinormal modes of a massive scalar field around such black\nholes and show that they are characterized by a comparatively small damping\nrate, indicating the possible existence of arbitrarily long-lived quasinormal\nmodes, called quasi-resonances.",
        "One of the most important open questions in planet formation is how dust\ngrains in a protoplanetary disk manage to overcome growth barriers and form the\n$\\sim$100km planet building blocks that we call planetesimals. There appears to\nbe a gap between the largest grains that can be produce by coagulation, and the\nsmallest grains that are needed for the streaming instability (SI) to form\nplanetesimals. Here we explore a novel hypothesis: That dust coagulation and\nthe SI work in tandem. That they form a feedback loop where each one boosts the\naction of the other to bridge the gap between dust grains and planetesimals. We\ndevelop a semi-analytical model of dust concentration due to the SI, and an\nanalytic model of how the SI affects the fragmentation and radial drift\nbarriers. We then combine those to model our proposed feedback loop. In the\nfragmentation-limited regime, we find a powerful synergy between the SI and\ndust growth that drastically increases both grain sizes and densities. We find\nthat a midplane dust-to-gas ratio of $\\epsilon \\ge 0.3$ is a sufficient\ncondition for the feedback loop to reach the planetesimal-forming region for\nturbulence values $10^{-4} \\le \\alpha \\le 10^{-3}$ and grain sizes $0.01 \\le\n{\\rm St} \\le 0.1$. In contrast, the drift-limited regime only shows grain\ngrowth, without significant dust accumulation. Planet formation in the\ndrift-limited portion of the disk may require other processes (particle traps)\nto halt radial drift.",
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation",
    "start_abstract":"Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators.",
    "start_categories":[
      "physics.acc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Accelerating cavity fault prediction using deep learning at Jefferson laboratory"
      ],
      "abstract":[
        "Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "An Efficient Diffusion-based Non-Autoregressive Solver for Traveling\n  Salesman Problem",
        "A HEART for the environment: Transformer-Based Spatiotemporal Modeling\n  for Air Quality Prediction",
        "Near-optimal Regret Using Policy Optimization in Online MDPs with\n  Aggregate Bandit Feedback",
        "Primal-Dual Sample Complexity Bounds for Constrained Markov Decision\n  Processes with Multiple Constraints",
        "Filter, Obstruct and Dilute: Defending Against Backdoor Attacks on\n  Semi-Supervised Learning",
        "An Efficient Real Time DDoS Detection Model Using Machine Learning\n  Algorithms",
        "Exploring Representation-Aligned Latent Space for Better Generation",
        "ARMAX identification of low rank graphical models",
        "ATM-Net: Adaptive Termination and Multi-Precision Neural Networks for\n  Energy-Harvested Edge Intelligence",
        "Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward\n  Real-World Multi-Echelon Inventory Optimization",
        "On Rollouts in Model-Based Reinforcement Learning",
        "Training LLMs with MXFP4",
        "Contextual Linear Bandits with Delay as Payoff",
        "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
        "LexPro-1.0 Technical Report",
        "Observer-Based Data-Driven Consensus Control for Nonlinear Multi-Agent\n  Systems against DoS and FDI attacks",
        "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive\n  Impairment",
        "Generative AI & Changing Work: Systematic Review of Practitioner-led\n  Work Transformations through the Lens of Job Crafting",
        "Learning Code-Edit Embedding to Model Student Debugging Behavior",
        "Monochromatic graph decompositions and monochromatic piercing inspired\n  by anti-Ramsey colorings",
        "Select2Drive: Pragmatic Communications for Real-Time Collaborative\n  Autonomous Driving",
        "Determination of the density in the linear elastic wave equation",
        "Semicustom Frontend VLSI Design and Analysis of a 32-bit Brent-Kung\n  Adder in Cadence Suite",
        "DiffCLIP: Differential Attention Meets CLIP",
        "Splitting algorithms for paraxial and It\\^o-Schr\\\"odinger models of wave\n  propagation in random media",
        "Numerical homological regularities over positively graded algebras",
        "Provable Benefits of Task-Specific Prompts for In-context Learning",
        "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of\n  Frozen Language Models"
      ],
      "abstract":[
        "Recent advances in neural models have shown considerable promise in solving\nTraveling Salesman Problems (TSPs) without relying on much hand-crafted\nengineering. However, while non-autoregressive (NAR) approaches benefit from\nfaster inference through parallelism, they typically deliver solutions of\ninferior quality compared to autoregressive ones. To enhance the solution\nquality while maintaining fast inference, we propose DEITSP, a diffusion model\nwith efficient iterations tailored for TSP that operates in a NAR manner.\nFirstly, we introduce a one-step diffusion model that integrates the controlled\ndiscrete noise addition process with self-consistency enhancement, enabling\noptimal solution prediction through simultaneous denoising of multiple\nsolutions. Secondly, we design a dual-modality graph transformer to bolster the\nextraction and fusion of features from node and edge modalities, while further\naccelerating the inference with fewer layers. Thirdly, we develop an efficient\niterative strategy that alternates between adding and removing noise to improve\nexploration compared to previous diffusion methods. Additionally, we devise a\nscheduling framework to progressively refine the solution space by adjusting\nnoise levels, facilitating a smooth search for optimal solutions. Extensive\nexperiments on real-world and large-scale TSP instances demonstrate that DEITSP\nperforms favorably against existing neural approaches in terms of solution\nquality, inference latency, and generalization ability. Our code is available\nat $\\href{https:\/\/github.com\/DEITSP\/DEITSP}{https:\/\/github.com\/DEITSP\/DEITSP}$.",
        "Accurate and reliable air pollution forecasting is crucial for effective\nenvironmental management and policy-making. llull-environment is a\nsophisticated and scalable forecasting system for air pollution, inspired by\nprevious models currently operational in Madrid and Valladolid (Spain). It\ncontains (among other key components) an encoder-decoder convolutional neural\nnetwork to forecast mean pollution levels for four key pollutants (NO$_2$,\nO$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and\nother contextual features. This paper investigates the augmentation of this\nneural network with an attention mechanism to improve predictive accuracy. The\nproposed attention mechanism pre-processes tensors containing the input\nfeatures before passing them to the existing mean forecasting model. The\nresulting model is a combination of several architectures and ideas and can be\ndescribed as a \"Hybrid Enhanced Autoregressive Transformer\", or HEART. The\neffectiveness of the approach is evaluated by comparing the mean square error\n(MSE) across different attention layouts against the system without such a\nmechanism. We observe a significant reduction in MSE of up to 22%, with an\naverage of 7.5% across tested cities and pollutants. The performance of a given\nattention mechanism turns out to depend on the pollutant, highlighting the\ndifferences in their creation and dissipation processes. Our findings are not\nrestricted to optimizing air quality prediction models, but are applicable\ngenerally to (fixed length) time series forecasting.",
        "We study online finite-horizon Markov Decision Processes with adversarially\nchanging loss and aggregate bandit feedback (a.k.a full-bandit). Under this\ntype of feedback, the agent observes only the total loss incurred over the\nentire trajectory, rather than the individual losses at each intermediate step\nwithin the trajectory. We introduce the first Policy Optimization algorithms\nfor this setting. In the known-dynamics case, we achieve the first\n\\textit{optimal} regret bound of $\\tilde \\Theta(H^2\\sqrt{SAK})$, where $K$ is\nthe number of episodes, $H$ is the episode horizon, $S$ is the number of\nstates, and $A$ is the number of actions. In the unknown dynamics case we\nestablish regret bound of $\\tilde O(H^3 S \\sqrt{AK})$, significantly improving\nthe best known result by a factor of $H^2 S^5 A^2$.",
        "This paper addresses the challenge of solving Constrained Markov Decision\nProcesses (CMDPs) with $d > 1$ constraints when the transition dynamics are\nunknown, but samples can be drawn from a generative model. We propose a\nmodel-based algorithm for infinite horizon CMDPs with multiple constraints in\nthe tabular setting, aiming to derive and prove sample complexity bounds for\nlearning near-optimal policies. Our approach tackles both the relaxed and\nstrict feasibility settings, where relaxed feasibility allows some constraint\nviolations, and strict feasibility requires adherence to all constraints. The\nmain contributions include the development of the algorithm and the derivation\nof sample complexity bounds for both settings. For the relaxed feasibility\nsetting we show that our algorithm requires $\\tilde{\\mathcal{O}} \\left( \\frac{d\n|\\mathcal{S}| |\\mathcal{A}| \\log(1\/\\delta)}{(1-\\gamma)^3\\epsilon^2} \\right)$\nsamples to return $\\epsilon$-optimal policy, while in the strict feasibility\nsetting it requires $\\tilde{\\mathcal{O}} \\left( \\frac{d^3 |\\mathcal{S}|\n|\\mathcal{A}| \\log(1\/\\delta)}{(1-\\gamma)^5\\epsilon^2{\\zeta_{\\mathbf{c}}^*}^2}\n\\right)$ samples.",
        "Recent studies have verified that semi-supervised learning (SSL) is\nvulnerable to data poisoning backdoor attacks. Even a tiny fraction of\ncontaminated training data is sufficient for adversaries to manipulate up to\n90\\% of the test outputs in existing SSL methods. Given the emerging threat of\nbackdoor attacks designed for SSL, this work aims to protect SSL against such\nrisks, marking it as one of the few known efforts in this area. Specifically,\nwe begin by identifying that the spurious correlations between the backdoor\ntriggers and the target class implanted by adversaries are the primary cause of\nmanipulated model predictions during the test phase. To disrupt these\ncorrelations, we utilize three key techniques: Gaussian Filter, complementary\nlearning and trigger mix-up, which collectively filter, obstruct and dilute the\ninfluence of backdoor attacks in both data pre-processing and feature learning.\nExperimental results demonstrate that our proposed method, Backdoor Invalidator\n(BI), significantly reduces the average attack success rate from 84.7\\% to\n1.8\\% across different state-of-the-art backdoor attacks. It is also worth\nmentioning that BI does not sacrifice accuracy on clean data and is supported\nby a theoretical guarantee of its generalization capability.",
        "Distributed Denial of Service attacks have become a significant threat to\nindustries and governments leading to substantial financial losses. With the\ngrowing reliance on internet services, DDoS attacks can disrupt services by\noverwhelming servers with false traffic causing downtime and data breaches.\nAlthough various detection techniques exist, selecting an effective method\nremains challenging due to trade-offs between time efficiency and accuracy.\nThis research focuses on developing an efficient real-time DDoS detection\nsystem using machine learning algorithms leveraging the UNB CICDDoS2019 dataset\nincluding various traffic features. The study aims to classify DDoS and\nnon-DDoS traffic through various ML classifiers including Logistic Regression,\nK-Nearest Neighbors, Random Forest, Support Vector Machine, Naive Bayes. The\ndataset is preprocessed through data cleaning, standardization and feature\nselection techniques using Principal Component Analysis. The research explores\nthe performance of these algorithms in terms of precision, recall and F1-score\nas well as time complexity to create a reliable system capable of real-time\ndetection and mitigation of DDoS attacks. The findings indicate that RF,\nAdaBoost and XGBoost outperform other algorithms in accuracy and efficiency,\nmaking them ideal candidates for real-time applications.",
        "Generative models serve as powerful tools for modeling the real world, with\nmainstream diffusion models, particularly those based on the latent diffusion\nmodel paradigm, achieving remarkable progress across various tasks, such as\nimage and video synthesis. Latent diffusion models are typically trained using\nVariational Autoencoders (VAEs), interacting with VAE latents rather than the\nreal samples. While this generative paradigm speeds up training and inference,\nthe quality of the generated outputs is limited by the latents' quality.\nTraditional VAE latents are often seen as spatial compression in pixel space\nand lack explicit semantic representations, which are essential for modeling\nthe real world. In this paper, we introduce ReaLS (Representation-Aligned\nLatent Space), which integrates semantic priors to improve generation\nperformance. Extensive experiments show that fundamental DiT and SiT trained on\nReaLS can achieve a 15% improvement in FID metric. Furthermore, the enhanced\nsemantic latent space enables more perceptual downstream tasks, such as\nsegmentation and depth estimation.",
        "In large-scale systems, complex internal relationships are often present.\nSuch interconnected systems can be effectively described by low rank stochastic\nprocesses. When identifying a predictive model of low rank processes from\nsampling data, the rank-deficient property of spectral densities is often\nobscured by the inevitable measurement noise in practice. However, existing low\nrank identification approaches often did not take noise into explicit\nconsideration, leading to non-negligible inaccuracies even under weak noise. In\nthis paper, we address the identification issue of low rank processes under\nmeasurement noise. We find that the noisy measurement model admits a sparse\nplus low rank structure in latent-variable graphical models. Specifically, we\nfirst decompose the problem into a maximum entropy covariance extension\nproblem, and a low rank graphical estimation problem based on an autoregressive\nmoving-average with exogenous input (ARMAX) model. To identify the ARMAX low\nrank graphical models, we propose an estimation approach based on maximum\nlikelihood. The identifiability and consistency of this approach are proven\nunder certain conditions. Simulation results confirm the reliable performance\nof the entire algorithm in both the parameter estimation and noisy data\nfiltering.",
        "ATM-Net is a novel neural network architecture tailored for energy-harvested\nIoT devices, integrating adaptive termination points with multi-precision\ncomputing. It dynamically adjusts computational precision (32\/8\/4-bit) and\nnetwork depth based on energy availability via early exit points. An\nenergy-aware task scheduler optimizes the energy-accuracy trade-off.\nExperiments on CIFAR-10, PlantVillage, and TissueMNIST show ATM-Net achieves up\nto 96.93% accuracy while reducing power consumption by 87.5% with Q4\nquantization compared to 32-bit operations. The power-delay product improves\nfrom 13.6J to 0.141J for DenseNet-121 and from 10.3J to 0.106J for ResNet-18,\ndemonstrating its suitability for energy-harvesting systems.",
        "Multi-echelon inventory optimization (MEIO) is critical for effective supply\nchain management, but its inherent complexity can pose significant challenges.\nHeuristics are commonly used to address this complexity, yet they often face\nlimitations in scope and scalability. Recent research has found deep\nreinforcement learning (DRL) to be a promising alternative to traditional\nheuristics, offering greater versatility by utilizing dynamic decision-making\ncapabilities. However, since DRL is known to struggle with the curse of\ndimensionality, its relevance to complex real-life supply chain scenarios is\nstill to be determined. This thesis investigates DRL's applicability to MEIO\nproblems of increasing complexity. A state-of-the-art DRL model was replicated,\nenhanced, and tested across 13 supply chain scenarios, combining diverse\nnetwork structures and parameters. To address DRL's challenges with\ndimensionality, additional models leveraging graph neural networks (GNNs) and\nmulti-agent reinforcement learning (MARL) were developed, culminating in the\nnovel iterative multi-agent reinforcement learning (IMARL) approach. IMARL\ndemonstrated superior scalability, effectiveness, and reliability in optimizing\ninventory policies, consistently outperforming benchmarks. These findings\nconfirm the potential of DRL, particularly IMARL, to address real-world supply\nchain challenges and call for additional research to further expand its\napplicability.",
        "Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by\nlearning a model of the environment and generating synthetic rollouts from it.\nHowever, accumulated model errors during these rollouts can distort the data\ndistribution, negatively impacting policy learning and hindering long-term\nplanning. Thus, the accumulation of model errors is a key bottleneck in current\nMBRL methods. We propose Infoprop, a model-based rollout mechanism that\nseparates aleatoric from epistemic model uncertainty and reduces the influence\nof the latter on the data distribution. Further, Infoprop keeps track of\naccumulated model errors along a model rollout and provides termination\ncriteria to limit data corruption. We demonstrate the capabilities of Infoprop\nin the Infoprop-Dyna algorithm, reporting state-of-the-art performance in\nDyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing\nrollout length and data quality.",
        "Low precision (LP) datatypes such as MXFP4 can accelerate matrix\nmultiplications (GEMMs) and reduce training costs. However, directly using\nMXFP4 instead of BF16 during training significantly degrades model quality. In\nthis work, we present the first near-lossless training recipe that uses MXFP4\nGEMMs, which are $2\\times$ faster than FP8 on supported hardware. Our key\ninsight is to compute unbiased gradient estimates with stochastic rounding\n(SR), resulting in more accurate model updates. However, directly applying SR\nto MXFP4 can result in high variance from block-level outliers, harming\nconvergence. To overcome this, we use the random Hadamard tranform to\ntheoretically bound the variance of SR. We train GPT models up to 6.7B\nparameters and find that our method induces minimal degradation over\nmixed-precision BF16 training. Our recipe computes $>1\/2$ the training FLOPs in\nMXFP4, enabling an estimated speedup of $>1.3\\times$ over FP8 and $>1.7\\times$\nover BF16 during backpropagation.",
        "A recent work by Schlisselberg et al. (2024) studies a delay-as-payoff model\nfor stochastic multi-armed bandits, where the payoff (either loss or reward) is\ndelayed for a period that is proportional to the payoff itself. While this\ncaptures many real-world applications, the simple multi-armed bandit setting\nlimits the practicality of their results. In this paper, we address this\nlimitation by studying the delay-as-payoff model for contextual linear bandits.\nSpecifically, we start from the case with a fixed action set and propose an\nefficient algorithm whose regret overhead compared to the standard no-delay\ncase is at most $D\\Delta_{\\max}\\log T$, where $T$ is the total horizon, $D$ is\nthe maximum delay, and $\\Delta_{\\max}$ is the maximum suboptimality gap. When\npayoff is loss, we also show further improvement of the bound, demonstrating a\nseparation between reward and loss similar to Schlisselberg et al. (2024).\nContrary to standard linear bandit algorithms that construct least squares\nestimator and confidence ellipsoid, the main novelty of our algorithm is to\napply a phased arm elimination procedure by only picking actions in a\nvolumetric spanner of the action set, which addresses challenges arising from\nboth payoff-dependent delays and large action sets. We further extend our\nresults to the case with varying action sets by adopting the reduction from\nHanna et al. (2023). Finally, we implement our algorithm and showcase its\neffectiveness and superior performance in experiments.",
        "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
        "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.",
        "Existing data-driven control methods generally do not address False Data\nInjection (FDI) and Denial-of-Service (DoS) attacks simultaneously. This letter\nintroduces a distributed data-driven attack-resilient consensus problem under\nboth FDI and DoS attacks and proposes a data-driven consensus control\nframework, consisting of a group of comprehensive attack-resilient observers.\nThe proposed group of observers is designed to estimate FDI attacks, external\ndisturbances, and lumped disturbances, combined with a DoS attack compensation\nmechanism. A rigorous stability analysis of the approach is provided to ensure\nthe boundedness of the distributed neighborhood estimation consensus error. The\neffectiveness of the approach is validated through numerical examples involving\nboth leaderless consensus and leader-follower consensus, demonstrating\nsignificantly improved resilient performance compared to existing data-driven\ncontrol approaches.",
        "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.",
        "Widespread integration of Generative AI tools is transforming white-collar\nwork, reshaping how workers define their roles, manage their tasks, and\ncollaborate with peers. This has created a need to develop an overarching\nunderstanding of common worker-driven patterns around these transformations. To\nfill this gap, we conducted a systematic literature review of 23 studies from\nthe ACM Digital Library that focused on workers' lived-experiences and\npractitioners with GenAI. Our findings reveal that while many professionals\nhave delegated routine tasks to GenAI to focus on core responsibilities, they\nhave also taken on new forms of AI managerial labor to monitor and refine GenAI\noutputs. Additionally, practitioners have restructured collaborations,\nsometimes bypassing traditional peer and subordinate interactions in favor of\nGenAI assistance. These shifts have fragmented cohesive tasks into piecework\ncreating tensions around role boundaries and professional identity. Our\nanalysis suggests that current frameworks, like job crafting, need to evolve to\naddress the complexities of GenAI-driven transformations.",
        "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
        "Anti-Ramsey theory was initiated in 1975 by Erd\\H{o}s, Simonovits and S\\'os,\ninspiring hundreds of publications since then. The present work is the third\nand last piece of our trilogy in which we introduce a far-reaching\ngeneralization via the following two functions for any graph $G$ and family\n${\\cal F}$ of graphs:\n  If $K_2 \\in {\\cal F}$, let $f(n,G|{\\cal F})$ be the smallest integer $k$ such\nthat every edge coloring of $K_n$ with at least $k$ colors forces a copy of $G$\nin which all color classes are members of ${\\cal F}$.\n  If $K_2 \\notin {\\cal F}$, let $g(n,G|{\\cal F})$ be the largest integer $k$\nfor which there exists an edge coloring of $K_n$ using exactly $k$ colors, such\nthat every copy of $G$ contains an induced color class which is a member of\n${\\cal F}$.\n  We develop methods suitable for deriving asymptotically tight results for the\n$f$-function and the $g$-function for many combinations of $G$ and ${\\cal F}$.\n  The preceding parts of the trilogy are arXiv: 2405.19812 and 2408.04257,\npublished in Discrete Applied Math. Vol. 363 and Mathematics Vol. 12:23,\nrespectively.",
        "Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has\nwitnessed remarkable advancements in recent years, with pragmatic\ncommunications (PragComm) emerging as a promising paradigm for real-time\ncollaboration among vehicles and other agents.Simultaneously, extensive\nresearch has explored the interplay between collaborative perception and\ndecision-making in end-to-end driving frameworks.In this work, we revisit the\ncollaborative driving problem and propose the Select2Drive framework to\noptimize the utilization of limited computational and communication\nresources.Particularly, to mitigate cumulative latency in perception and\ndecision-making, Select2Drive introduces Distributed Predictive Perception\n(DPP) by formulating an active prediction paradigm and simplifies\nhigh-dimensional semantic feature prediction into computation cost-efficient,\nmotion-aware reconstruction. Given the \"less is more\" principle that a\nbroadened perceptual horizon possibly confuses the decision module rather than\ncontributing to it, Select2Drive utilizes Area-of-Importance-based PragComm\n(APC) to prioritize the communications of critical regions, thus boosting both\ncommunication efficiency and decision-making efficacy. Empirical evaluations on\nthe V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive\nachieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under\nlimited bandwidth (resp. pose error conditions). Moreover, it delivers at most\n14.68% and 31.76% enhancement in closed-loop driving scores and route\ncompletion rates, particularly in scenarios characterized by dense traffic and\nhigh-speed dynamics.",
        "We study the inverse boundary value problem for the linear elastic wave\nequation in three-dimensional isotropic medium. We show that both the Lam\\'e\nparameters and the density can be uniquely recovered from the boundary\nmeasurements under the strictly convex foliation condition.",
        "Adders are fundamental components in digital circuits, playing a crucial role\nin arithmetic operations within computing systems and many other applications.\nThis paper focuses on the design and simulation of a 32-bit Brent-Kung parallel\nprefix adder, which is recognized for its efficient carry propagation and\nlogarithmic delay characteristics. The Brent-Kung architecture balances\ncomputational speed and hardware complexity, making it suitable for high-speed\ndigital applications. The design is implemented using Verilog HDL and simulated\nusing Cadence Design Suite tools, including NCLaunch and Genus, to evaluate its\nperformance in terms of scalability, speed, and functional working. Comparative\nanalysis with traditional adder architectures highlights the advantages of the\nBrent-Kung adder for modern digital systems.",
        "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps:\/\/github.com\/hammoudhasan\/DiffCLIP.",
        "This paper introduces a full discretization procedure to solve wave beam\npropagation in random media modeled by a paraxial wave equation or an\nIt\\^o-Schr\\\"odinger stochastic partial differential equation. This method bears\nsimilarities with the phase screen method used routinely to solve such\nproblems. The main axis of propagation is discretized by a centered splitting\nscheme with step $\\Delta z$ while the transverse variables are treated by a\nspectral method after appropriate spatial truncation. The originality of our\napproach is its theoretical validity even when the typical wavelength $\\theta$\nof the propagating signal satisfies $\\theta\\ll\\Delta z$. More precisely, we\nobtain a convergence of order $\\Delta z$ in mean-square sense while the errors\non statistical moments are of order $(\\Delta z)^2$ as expected for standard\ncentered splitting schemes. This is a surprising result as splitting schemes\ntypically do not converge when $\\Delta z$ is not the smallest scale of the\nproblem. The analysis is based on equations satisfied by statistical moments in\nthe It\\^o-Schr\\\"odinger case and on integral (Duhamel) expansions for the\nparaxial model. Several numerical simulations illustrate and confirm the\ntheoretical findings.",
        "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
        "The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned\/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.",
        "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https:\/\/github.com\/fairyshine\/Chain-of-Tools ."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Accelerating cavity fault prediction using deep learning at Jefferson laboratory",
    "start_abstract":"Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation"
      ],
      "abstract":[
        "Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators."
      ],
      "categories":[
        "physics.acc-ph"
      ]
    },
    "list":{
      "title":[
        "New Generation Compact Linear Accelerator for Low-Current, Low-Energy\n  Multiple Applications",
        "Measurement of the mean excitation energy of liquid argon",
        "Using Convolutional Neural Networks to Accelerate 3D Coherent\n  Synchrotron Radiation Computations",
        "Polarized electron bunch refresh rates in an electron storage ring",
        "Concept of an Infrared FEL for the Chemical Dynamics Research Laboratory\n  at PETRA IV",
        "Stability Enhancement of a Self-Amplified Spontaneous Emission\n  Free-electron Laser with Bunching Containment",
        "On the influence of electron velocity spread on Compton FEL operation",
        "FCC-ee positron source from conventional to crystal-based",
        "Incoherent horizontal emittance growth due to the interplay of beam-beam\n  and longitudinal wakefield in crab-waist colliders",
        "Effects of Curved Superconducting Magnets on Beam Stability in a Compact\n  Ion Therapy Synchrotron",
        "Recent progress in high-temperature superconducting undulators",
        "Triple Evaporation of Bialkali Antimonide Photocathodes and\n  Photoemission Characterization at the PhoTEx Experiment",
        "Cyclotron Maser Cooling towards Coherent Particle Beams",
        "An exposition on the supersimplicity of certain expansions of the\n  additive group of the integers",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "A new local time-decoupled squared Wasserstein-2 method for training\n  stochastic neural networks to reconstruct uncertain parameters in dynamical\n  systems",
        "A Comprehensive Framework for Statistical Inference in Measurement\n  System Assessment Studies",
        "A Packaging Method for ALPIDE Integration Enabling Flexible and\n  Low-Material-Budget Designs",
        "FairUDT: Fairness-aware Uplift Decision Trees",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "Kauffman bracket skein module of the $(3,3,3,3)$-pretzel link exterior",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "The $D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $ decay from a $D_{s1}$ molecular\n  perspective",
        "Gravitational waves and primordial black holes from the T-model\n  inflation with Gauss-Bonnet correction",
        "Inverse Gaussian Distribution, Introduction and\n  Applications:Comprehensive Analysis of Power Plant Performance: A Study of\n  Combined Cycle and Nuclear Power Plant",
        "Personalized Convolutional Dictionary Learning of Physiological Time\n  Series",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A Comparison of Strategies to Embed Physics-Informed Neural Networks in\n  Nonlinear Model Predictive Control Formulations Solved via Direct\n  Transcription"
      ],
      "abstract":[
        "A new compact linear proton accelerator project (named LINAC 7) for multiple\nlow-current applications, designed and built in-house at the Beam Laboratory of\nthe University of the Basque Country (UPV\/EHU) is described. The project\ncombines the University, a research technology center and a private company\nwith the aim of designing and building a compact, low-current proton\naccelerator capable of accelerating particles up to 7 MeV. In this paper, we\npresent an overview of the accelerator design, summarize the progress and\ntesting of the components that have been built, and describe the components\nthat are being designed that will allow us to achieve the final desired energy\nof 7 MeV.",
        "The mean excitation energy (I-value) of liquid argon is a critical input for\nenergy estimation in neutrino oscillation experiments. It is measured to be\n$(205\\pm4)$\\,eV using the range of 402.2\\,MeV protons from the Fermilab Linac.\nThis compares to the author's recent evaluation of $(197\\pm 7)$\\,eV based on a\ncombination of an oscillator strength distribution analysis, gaseous argon\nrange measurements, sparse stopping power data on solid argon, and an\nextrapolation of data on the effect of phase from other substances. Using all\nsources of information, we recommend a value of $(203.0\\pm3.2)$\\,eV for liquid\nargon, which is significantly higher than 188\\,eV, from ICRU-37's gaseous argon\nevaluation, commonly used in Monte Carlo codes such as \\textsc{Geant4}.",
        "Calculating the effects of Coherent Synchrotron Radiation (CSR) is one of the\nmost computationally expensive tasks in accelerator physics. Here, we use\nconvolutional neural networks (CNN's), along with a latent conditional\ndiffusion (LCD) model, trained on physics-based simulations to speed up\ncalculations. Specifically, we produce the 3D CSR wakefields generated by\nelectron bunches in circular orbit in the steady-state condition. Two datasets\nare used for training and testing the models: wakefields generated by\nthree-dimensional Gaussian electron distributions and wakefields from a sum of\nup to 25 three-dimensional Gaussian distributions. The CNN's are able to\naccurately produce the 3D wakefields $\\sim 250-1000$ times faster than the\nnumerical calculations, while the LCD has a gain of a factor of $\\sim 34$. We\nalso test the extrapolation and out-of-distribution generalization ability of\nthe models. They generalize well on distributions with larger spreads than what\nthey were trained on, but struggle with smaller spreads.",
        "When polarized electron bunches are injected and circulated in a high-energy\nstorage ring, the polarization of the bunches relaxes to the asymptotic value\nof the radiative polarization, caused by the synchrotron radiation. Hence the\nbunches must be refreshed periodically, to maintain a predetermined\ntime-averaged value of the bunch polarization. In general, the refresh rates of\nthe \"up\" and \"down\" polarization bunches are different. We suggest an\nalternative policy. We point out that the total bunch refresh rate is almost\nindependent of the asymptotic level of the radiative polarization. We also note\nthat the true goal of so-called \"spin matching\" is to maximize the buildup time\nconstant (not the asymptotic level) of the radiative polarization. We suggest a\nscheme to equalize the refresh rates of the \"up\" and \"down\" polarization\nbunches, which may be (i) helpful for accelerator operations, and also (ii)\nreduce systematic errors in HEP experiments.",
        "We describe an infrared free electron laser (FEL), proposed as a part of a\nuser facility that also incorporates synchrotron-radiation beamlines for the\nPETRA IV. The FEL itself addresses the needs of the chemical sciences community\nfor a high-brightness, tunable source covering a broad region of the infrared\nspectrum - from 5 to 100 mkm. The user facility will allow, for the first time,\nthe integrated and simultaneous use of dedicated infrared FEL and\nsynchrotron-radiation beamlines for pump-probe experiments that will focus on\ngaining a rigorous molecular-level understanding of combustion and other\nenergetic molecular processes. These (pump-probe) requirements dictate the use\nof storage ring RF structures and cw operation. The technical approach adopted\nin FEL design uses an old PETRA III RF system and accelerating cavities. The\nprimary motivation for adopting this approach was to minimize facility costs.",
        "The self-amplified spontaneous emission (SASE) mechanism, the fundamental\noperating principle of numerous free-electron laser (FEL) facilities, is driven\nby electron beam shot noise and leads to significant fluctuations in the output\npulse energy. This study presents a robust method for improving pulse energy\nstability by incorporating a dispersion element that introduces longitudinal\ndispersion into the electron beam during the exponential growth phase of the\nSASE process. At this phase, the density modulation of the electron beam,\ncharacterized by the bunching factor, undergoes large fluctuations, resulting\nin substantial variations in the emitted radiation power. The introduction of\nlongitudinal dispersion allows for controlled manipulation of the bunching\ndistribution, suppressing fluctuations and enhancing pulse energy stability.\nThe stabilization mechanism is explained in this paper, and its impact on the\nradiation properties is analyzed for both the standard SASE scheme and advanced\nlasing setups, such as a two-stage lasing process for two-color pulse\ngeneration, with the initial stage operating in SASE mode.",
        "For the field amplitude, a nonlinear integro-differential equation is derived\nthat describes the operation of a Compton FEL in the presence of electron\nvelocity spread typical for modern facilities. Numerical solutions of the\nequation are in good agreement with particle simulations for the bunching\nfactor less than 0.6, reproduce the frequency detuning spectrum near its\nmaximum, and describe the amplification process up to saturation.",
        "The high-luminosity requirement in future lepton colliders imposes a need for\na high-intensity positron source. In the conventional scheme, positron beams\nare obtained by the conversion of bremsstrahlung photons into electron-positron\npairs through the interaction between a high-energy electron beam and a high-Z\namorphous target. One method to enhance the number of produced positrons is by\nboosting the incident electron beam power. However, the maximum heat load and\nthermo-mechanical stresses bearable by the target severely limit the beam power\nof the incident electrons. To overcome these limitations, an innovative\napproach using lattice coherent effects in oriented crystals appears promising.\nThis approach uses a single thick crystal that serves as a radiator and a\nconverter. In this paper, we investigate the application of this scheme as an\nalternative to the conventional positron source at the Future Circular Collider\n(FCC-ee). Simulations were carried out from the positron production stage to\nthe entrance of the damping ring to estimate the accepted positron yield. The\nresults demonstrate the advantages of the crystal-based positron source: it\nrequires thinner targets than the conventional scheme, resulting in a 14%\nreduction in the deposited power while achieving a 10% increase in accepted\npositron yield.",
        "In this paper, we investigate quadrupolar sychrobetatron resonances caused by\nbeam-beam collisions and their interplay with longitudinal wakefields in the\ncontext of crab-waist colliders. We present a comprehensive theoretical review\nof the established theory of sychrobetatron resonances and extend the formalism\nto explore horizontal sychrobetatron resonances specific to crab-waist\ncolliders. As a case study, we examine incoherent horizontal emittance growth\nat the SuperKEKB and demonstrate through simulations that the interplay between\nbeam-beam and longitudinal wakefields leads to a horizontal blowup of the bunch\nsize and that the study of the dynamics can be reduced to the\nhorizontal-longitudinal plane, independent of the motion in the vertical\ndimension. We present extensive simulation results using the codes BBWS,\nPyHEADTAIL and Xsuite, connect our analytical findings with these findings, and\npropose strategies to mitigate horizontal blowup.",
        "Superconducting, curved magnets can reduce accelerator footprints by\nproducing strong fields (>3T) and reducing the total number of magnets through\ntheir capability for combined-function multipolar fields, making them an\nattractive choice for applications such as heavy ion therapy. There exists the\nproblem that the effect of strongly curved harmonics and fringe fields on\ncompact accelerator beam dynamics is not well represented: existing approaches\nuse integrated cylindrical multipoles to describe and model the fields for beam\ndynamics studies, which are invalid in curved coordinate systems and assume\nindividual errors cancel out over the full machine. In the modelling of these\nmachines, the effect of strongly curved harmonics and fringe fields on compact\naccelerator beam dynamics needs to properly included. An alternative approach\nmust be introduced for capturing off-axis fields in a strongly curved magnet,\nwhich may affect long-term beam stability in a compact accelerator. In this\narticle, we investigate the impacts of deploying a curved canted-cosine-theta\n(CCT) superconducting magnet in a compact medical synchrotron for the first\ntime. We develop a method to analyse and characterise the 3D curved fields of\nan electromagnetic model of a CCT developed for the main bending magnets of a\n27m circumference carbon ion therapy synchrotron, designed within the Heavy Ion\nTherapy Research Integration Plus European project, and the CERN Next Ion\nMedical Machine Study (NIMMS). The fields are modelled in the compact\nsynchrotron in MAD-X\/PTC to study their effects on beam dynamics and long-term\nbeam stability. The insights gained through the methods presented allow for the\noptimisation of both magnet and synchrotron designs, with the potential to\nimpact the operational performance of future ion therapy facilities.",
        "Considerable effort has been devoted to the development of superconducting\nundulators (SCUs) intended for particle accelerator-based light sources,\nincluding synchrotrons and free electron laser (FEL) facilities. Recently, a\nhigh-temperature superconducting (HTS) undulator prototype, consisting of\nstaggered-array Re-Ba-Cu-O bulks, achieved an on-axis sinusoidal magnetic field\nprofile with a peak amplitude B$_0$ of 2.1 T and a period length of 10 mm,\nresulting in a deflection parameter K = 1.96. Such a short period HTS undulator\nnot only enables the generation of higher-energy photons, but also supports the\nconstruction of economically feasible and compact FELs with shorter linear\naccelerators (LINACs). This article provides a comprehensive review of recent\nadvances in the staggered-array bulk HTS undulator as well as other types of\nHTS undulators. Furthermore, it offers insights into the development of\nengineering HTS undulator prototypes designed for deployment in synchrotron and\nfree electron laser (FEL) facilities. We conclude by discussing opportunities\nfor and the challenges facing the use of HTS undulators in practical\napplications.",
        "The development of high-performance photocathodes is essential for generating\nhigh-brightness electron beams required by existing and future accelerators.\nThis work introduces a state-of-the-art triple evaporation growth system\ndesigned for bialkali antimonide photocathodes. By enabling the simultaneous\ndeposition of all three materials, this system significantly enhances vacuum\nstability and the reproducibility of photocathode fabrication. Complementing\nthis, the novel characterization system PhoTEx allows spatially and spectrally\nresolved measurements of key photocathode parameters, such as quantum\nefficiency (QE), mean transverse energy (MTE), reflectance and lifetime.\nCrucially, all measurements are performed within a single compact setup,\nwithout moving the sample, preserving ultra-high vacuum conditions. The\nspectral resolved measurement of the reflectance allows the investigation of\nthe color. Photocathode colorimetry may provide valuable insights into material\nhomogeneity and aging. A Na-K-Sb photocathode was grown using the triple\nevaporation method, achieving an initial QE of $5.5\\,\\%$ at $520\\,$nm. The\nphotocathode was characterized at PhoTEx over two months, demonstrating\nconsistent MTE measurements and a dataset with spectral response, reflectance\nand colorimetry data. Together, the triple evaporation growth system and PhoTEx\nmark a significant advancement in optimizing photocathodes with exceptional\nperformance, paving the way for brighter and more stable electron sources for\nnext-generation accelerator facilities.",
        "This article presents a new particle beam cooling scheme, namely cyclotron\nmaser cooling (CMC). Relativistic gyrating particles, forced by a solenoidal\nmagnetic field over some length of their trajectory, move in a helical path and\nundergo emission or absorption of radiations stimulated by a resonance RF\nfield. Theoretical and experimental investigations on electron beams indicate\nthat when the action of the RF field exceeds a critical value the beam jumps\nabruptly to a coherent radiative system undergoing CMC in which most electrons\nare accumulated to a discrete energy with the same gyration phase. The\nmechanism of CMC was proved to be an elementary cooling process that is common\nto dissipative systems consisting of a driving field and oscillators with a\nstable energy. It leads to the generation of a coherent beam of particles that\nprovides means to control miscellaneous particle induced reactions. For\nexample, CMC electrons would generate coherent X-ray and gamma-ray photons\nthrough coherent inverse Compton scattering of laser radiation.",
        "In this short note, we present a self-contained exposition of the\nsupersimplicity of certain expansions of the additive group of the integers,\nsuch as adding a generic predicate (due to Chatzidakis and Pillay), a predicate\nfor the square-free integers (due to Bhardwaj and Tran) or a predicate for the\nprime integers (due to Kaplan and Shelah, assuming Dickson's conjecture).",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "In this work, we propose and analyze a new local time-decoupled squared\nWasserstein-2 method for reconstructing the distribution of unknown parameters\nin dynamical systems. Specifically, we show that a stochastic neural network\nmodel, which can be effectively trained by minimizing our proposed local\ntime-decoupled squared Wasserstein-2 loss function, is an effective model for\napproximating the distribution of uncertain model parameters in dynamical\nsystems. Through several numerical examples, we showcase the effectiveness of\nour proposed method in reconstructing the distribution of parameters in\ndifferent dynamical systems.",
        "Measurement system analysis aims to quantify the variability in data\nattributable to the measurement system and evaluate its contribution to overall\ndata variability. This paper conducts a rigorous theoretical investigation of\nthe statistical methods used in such analyses, focusing on variance components\nand other critical parameters. While established techniques exist for\nsingle-variable cases, a systematic theoretical exploration of their properties\nhas been largely overlooked. This study addresses this gap by examining\nestimators for variance components and other key parameters in measurement\nsystem assessment, analyzing their statistical properties, and providing new\ninsights into their reliability, performance, and applicability.",
        "This work presents a novel solution for the packaging of ALPIDE chips that\nfacilitates non-planar assembly with a minimal material budget. This solution\nrepresents a technological advancement based on methodologies developed for the\nALICE ITS1 and the STAR tracker two decades ago. The core of this approach\ninvolves the use of flexible cables composed of aluminum and polyimide, with\nthicknesses on the order of tens of micrometers. These cables are connected to\nthe sensors using single-point Tape Automated Bonding (spTAB), which replaces\nthe traditional wire bonding technique that is suboptimal for curved\nintegrations. The spTAB bonding is achieved by creating openings in the\npolyimide layer, allowing aluminum wires to remain free-standing, which are\nthen connected to the sensor using pressure and ultrasonic energy. Extending\nthis concept, we have applied this approach to entire printed circuit boards\n(PCBs), resulting in a fully flexible packaging solution maintaining an\nultra-low material budget. This work introduces a prototype utilizing this\nmethod to bond an ALPIDE chip, proposing it as a viable option for future\ndesigns necessitating flexible packaging for both the chip and associated\nelectronics. The overall workflow, comprising microfabrication and assembly, is\ncarried out at the Fondazione Bruno Kessler and INFN TIFPA laboratories and\nwill be detailed to elucidate our procedures and demonstrate the applicability\nof our solution in future experimental setups. The proposed packaging features\na flexible PCB constructed from three stacked layers, each containing 20 $\\mu$m\nthick aluminum features and a 25 $\\mu$m thick polyimide substrate. These layers\ninclude a ground layer, a signal layer (encompassing both digital and analog\nsignals), and a local bonding layer (which substitutes wire bonding).",
        "Training data used for developing machine learning classifiers can exhibit\nbiases against specific protected attributes. Such biases typically originate\nfrom historical discrimination or certain underlying patterns that\ndisproportionately under-represent minority groups, such as those identified by\ntheir gender, religion, or race. In this paper, we propose a novel approach,\nFairUDT, a fairness-aware Uplift-based Decision Tree for discrimination\nidentification. FairUDT demonstrates how the integration of uplift modeling\nwith decision trees can be adapted to include fair splitting criteria.\nAdditionally, we introduce a modified leaf relabeling approach for removing\ndiscrimination. We divide our dataset into favored and deprived groups based on\na binary sensitive attribute, with the favored dataset serving as the treatment\ngroup and the deprived dataset as the control group. By applying FairUDT and\nour leaf relabeling approach to preprocess three benchmark datasets, we achieve\nan acceptable accuracy-discrimination tradeoff. We also show that FairUDT is\ninherently interpretable and can be utilized in discrimination detection tasks.\nThe code for this project is available https:\/\/github.com\/ara-25\/FairUDT",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "We show that the Kauffman bracket skein module of the $(3,3,3,3)$-pretzel\nlink exterior over $\\mathbb{Q}(q^{\\frac{1}{2}})$ is not finitely generated as a\nmodule over $\\mathbb{Q}(q^{\\frac{1}{2}})[t_1,t_2]$, where $t_1,t_2$ are the\nmeridians of two components. This disproves a finiteness conjecture proposed in\n2021.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "We conduct a theoretical study of the $ D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $\ndecay from the perspective that the $D_{s1}$ is a molecular state, built mostly\nfrom the $D^* K$ and $D_s^* \\eta$ components. The $D^*$ and $D_s^*$ mesons are\nallowed to decay into two pseudoscalars, with one of them merging with the\nother pseudoscalar that forms the $D_{s1}$ state, ultimately leading to the\n$\\pi^+ \\pi^- D_s$ final state. This results in a triangle diagram mechanism\nwhere all theoretical ingredients are well-known, leading to a free parameter\nframework. We evaluate the mass distributions of particle pairs and find good\nagreement with the experimental distributions of a recent LHCb experiment,\nproviding strong support to the molecular picture of the $D_{s1}(2460)$ state.\nWe also discuss the role played by the scalar mesons $f_0(500)$ and $f_0(980)$,\nat odds with the interpretation of the experimental analysis.",
        "Recently, the worldwide Pulsar Timing Array (PTA) collaborations detected a\nstochastic gravitational wave(GW) background in the nanohertz range, which may\noriginate from the early universe's inflationary phase. So in this work, we\ninvestigated induce GWs in the T-model inflation with Gauss-Bonnet coupling.\nConsider the scenario of traversing a domain wall in moduli space, we take the\ncoupling coefficient to be an approximately step function. Within suitable\nparameter regions, the model exhibits de Sitter fixed points, which allows\ninflation to undergo an ultra-slow-roll phase, which causes the power spectrum\nto exhibit a peak. Such a peak can induce nanohertz GWs, which provids an\nexplanation for the PTA observational data. Furthermore, we consider the case\nof multiple domain wall crossings, and adopting a double-step coupling\nfunction. In this case, the resulting GW spectrum has two peaks with\nfrequencies around \\(10^{-8} \\,\\text{Hz}\\) and \\(10^{-2}\\,\\text{Hz}\\),\nrespectively. Which can be observed by the PTA and the space GW detectors\nsimultaneously.Additionally, the reentry of the power spectrum peaks into the\nhorizon leads to the collapse into primordial black holes (PBHs). We calculate\nthe abundance of PBHs and found that the masses is in the range of \\(10^{-14}\n\\sim 10^{-13} M_\\odot\\) and around \\(10^{-2} M_\\odot\\) , which constitute\nsignificant components of the current dark matter.",
        "This paper presents a comprehensive analysis of power plant performance using\nthe inverse Gaussian (IG) distribution framework. We combine theoretical\nfoundations with practical applications, focusing on both combined cycle and\nnuclear power plant contexts. The study demonstrates the advantages of the IG\ndistribution in modeling right-skewed industrial data, particularly in power\ngeneration. Using the UCI Combined Cycle Power Plant Dataset, we establishthe\nsuperiority of IG-based models over traditional approaches through rigorous\nstatistical testing and model validation. The methodology developed here\nextends naturally to nuclear power plant applications, where similar\nstatistical patterns emerge in operational data. Our findings suggest that\nIG-based models provide more accurate predictions and better capture the\nunderlying physical processes in power generation systems.",
        "Human physiological signals tend to exhibit both global and local structures:\nthe former are shared across a population, while the latter reflect\ninter-individual variability. For instance, kinetic measurements of the gait\ncycle during locomotion present common characteristics, although idiosyncrasies\nmay be observed due to biomechanical disposition or pathology. To better\nrepresent datasets with local-global structure, this work extends Convolutional\nDictionary Learning (CDL), a popular method for learning interpretable\nrepresentations, or dictionaries, of time-series data. In particular, we\npropose Personalized CDL (PerCDL), in which a local dictionary models local\ninformation as a personalized spatiotemporal transformation of a global\ndictionary. The transformation is learnable and can combine operations such as\ntime warping and rotation. Formal computational and statistical guarantees for\nPerCDL are provided and its effectiveness on synthetic and real human\nlocomotion data is demonstrated.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "This study aims to benchmark candidate strategies for embedding neural\nnetwork (NN) surrogates in nonlinear model predictive control (NMPC)\nformulations that are subject to systems described with partial differential\nequations and that are solved via direct transcription (i.e., simultaneous\nmethods). This study focuses on the use of physics-informed NNs and\nphysics-informed convolutional NNs as the internal (surrogate) models within\nthe NMPC formulation. One strategy embeds NN models as explicit algebraic\nconstraints, leveraging the automatic differentiation (AD) of an algebraic\nmodelling language (AML) to evaluate the derivatives. Alternatively, the solver\ncan be provided with derivatives computed external to the AML via the AD\nroutines of the machine learning environment the NN is trained in. The three\nnumerical experiments considered in this work reveal that replacing mechanistic\nmodels with NN surrogates may not always offer computational advantages when\nsmooth activation functions are used in conjunction with a local nonlinear\nsolver (e.g., Ipopt), even with highly nonlinear systems. Moreover, in this\ncontext, the external function evaluation of the NN surrogates often\noutperforms the embedding strategies that rely on explicit algebraic\nconstraints, likely due to the difficulty in initializing the auxiliary\nvariables and constraints introduced by explicit algebraic reformulations."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A New Kind of Science",
    "start_abstract":"3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics.",
    "start_categories":[
      "cs.FL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory"
      ],
      "abstract":[
        "Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore."
      ],
      "categories":[
        "cs.NA"
      ]
    },
    "list":{
      "title":[
        "Global Semantic-Guided Sub-image Feature Weight Allocation in\n  High-Resolution Large Vision-Language Models",
        "Mobile Manipulation Instruction Generation from Multiple Images with\n  Automatic Metric Enhancement",
        "Indexing Join Inputs for Fast Queries and Maintenance",
        "Message Replication for Improving Reliability of LR-FHSS\n  Direct-to-Satellite IoT",
        "AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal\n  Conditions and Larger Scene",
        "DNRSelect: Active Best View Selection for Deferred Neural Rendering",
        "How is Google using AI for internal code migrations?",
        "Poisoned-MRAG: Knowledge Poisoning Attacks to Multimodal Retrieval\n  Augmented Generation",
        "PatchPilot: A Stable and Cost-Efficient Agentic Patching Framework",
        "Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo\n  Development Assessment",
        "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation",
        "Diffusion based Text-to-Music Generation with Global and Local Text\n  based Conditioning",
        "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set\n  Annotation: An Energy-Based Approach",
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces"
      ],
      "abstract":[
        "As the demand for high-resolution image processing in Large Vision-Language\nModels (LVLMs) grows, sub-image partitioning has become a popular approach for\nmitigating visual information loss associated with fixed-resolution processing.\nHowever, existing partitioning methods uniformly process sub-images, resulting\nin suboptimal image understanding. In this work, we reveal that the sub-images\nwith higher semantic relevance to the entire image encapsulate richer visual\ninformation for preserving the model's visual understanding ability. Therefore,\nwe propose the Global Semantic-guided Weight Allocator (GSWA) module, which\ndynamically allocates weights to sub-images based on their relative information\ndensity, emulating human visual attention mechanisms. This approach enables the\nmodel to focus on more informative regions, overcoming the limitations of\nuniform treatment. We integrate GSWA into the InternVL2-2B framework to create\nSleighVL, a lightweight yet high-performing model. Extensive experiments\ndemonstrate that SleighVL outperforms models with comparable parameters and\nremains competitive with larger models. Our work provides a promising direction\nfor more efficient and contextually aware high-resolution image processing in\nLVLMs, advancing multimodal system development.",
        "We consider the problem of generating free-form mobile manipulation\ninstructions based on a target object image and receptacle image. Conventional\nimage captioning models are not able to generate appropriate instructions\nbecause their architectures are typically optimized for single-image. In this\nstudy, we propose a model that handles both the target object and receptacle to\ngenerate free-form instruction sentences for mobile manipulation tasks.\nMoreover, we introduce a novel training method that effectively incorporates\nthe scores from both learning-based and n-gram based automatic evaluation\nmetrics as rewards. This method enables the model to learn the co-occurrence\nrelationships between words and appropriate paraphrases. Results demonstrate\nthat our proposed method outperforms baseline methods including representative\nmultimodal large language models on standard automatic evaluation metrics.\nMoreover, physical experiments reveal that using our method to augment data on\nlanguage instructions improves the performance of an existing multimodal\nlanguage understanding model for mobile manipulation.",
        "In database systems, joins are often expensive despite many years of research\nproducing numerous join algorithms. Precomputed and materialized join views\ndeliver the best query performance, whereas traditional indexes, used as\npre-sorted inputs for merge joins, permit very efficient maintenance. Neither\ntraditional indexes nor materialized join views require blocking phases, in\ncontrast to query-time sorting and transient indexes, e.g., hash tables in hash\njoins, that impose high memory requirements and possibly spill to temporary\nstorage.\n  Here, we introduce a hybrid of traditional indexing and materialized join\nviews. The *merged index* can be implemented with traditional b-trees, permits\nhigh-bandwidth maintenance using log-structured merge-forests, supports all\njoin types (inner joins, all outer joins, all semi joins), and enables\nnon-blocking query processing. Experiments across a wide range of scenarios\nconfirm its query performance comparable to materialized join views and\nmaintenance efficiency comparable to traditional indexes.",
        "Long-range frequency-hopping spread spectrum (LR-FHSS) promises to enhance\nnetwork capacity by integrating frequency hopping into existing Long Range Wide\nArea Networks (LoRaWANs). Due to its simplicity and scalability, LR-FHSS has\ngenerated significant interest as a potential candidate for direct-to-satellite\nIoT (D2S-IoT) applications. This paper explores methods to improve the\nreliability of data transfer on the uplink (i.e., from terrestrial IoT nodes to\nsatellite) of LR-FHSS D2S-IoT networks.\n  Because D2S-IoT networks are expected to support large numbers of potentially\nuncoordinated IoT devices per satellite,\nacknowledgment-cum-retransmission-aided reliability mechanisms are not suitable\ndue to their lack of scalability. We therefore leverage message-replication,\nwherein every application-layer message is transmitted multiple times to\nimprove the probability of reception without the use of receiver\nacknowledgments. We propose two message-replication schemes. One scheme is\nbased on conventional replication, where multiple replicas of a message are\ntransmitted, each as a separate link-layer frame. In the other scheme, multiple\ncopies of a message is included in the payload of a single link-layer frame. We\nshow that both techniques improve LR-FHSS reliability. Which method is more\nsuitable depends on the network's traffic characteristics. We provide\nguidelines to choose the optimal method.",
        "Compared to frame-based methods, computational neuromorphic imaging using\nevent cameras offers significant advantages, such as minimal motion blur,\nenhanced temporal resolution, and high dynamic range. The multi-view\nconsistency of Neural Radiance Fields combined with the unique benefits of\nevent cameras, has spurred recent research into reconstructing NeRF from data\ncaptured by moving event cameras. While showing impressive performance,\nexisting methods rely on ideal conditions with the availability of uniform and\nhigh-quality event sequences and accurate camera poses, and mainly focus on the\nobject level reconstruction, thus limiting their practical applications. In\nthis work, we propose AE-NeRF to address the challenges of learning event-based\nNeRF from non-ideal conditions, including non-uniform event sequences, noisy\nposes, and various scales of scenes. Our method exploits the density of event\nstreams and jointly learn a pose correction module with an event-based NeRF\n(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.\nTo generalize to larger scenes, we propose hierarchical event distillation with\na proposal e-NeRF network and a vanilla e-NeRF network to resample and refine\nthe reconstruction process. We further propose an event reconstruction loss and\na temporal loss to improve the view consistency of the reconstructed scene. We\nestablished a comprehensive benchmark that includes large-scale scenes to\nsimulate practical non-ideal conditions, incorporating both synthetic and\nchallenging real-world event datasets. The experimental results show that our\nmethod achieves a new state-of-the-art in event-based 3D reconstruction.",
        "Deferred neural rendering (DNR) is an emerging computer graphics pipeline\ndesigned for high-fidelity rendering and robotic perception. However, DNR\nheavily relies on datasets composed of numerous ray-traced images and demands\nsubstantial computational resources. It remains under-explored how to reduce\nthe reliance on high-quality ray-traced images while maintaining the rendering\nfidelity. In this paper, we propose DNRSelect, which integrates a reinforcement\nlearning-based view selector and a 3D texture aggregator for deferred neural\nrendering. We first propose a novel view selector for deferred neural rendering\nbased on reinforcement learning, which is trained on easily obtained rasterized\nimages to identify the optimal views. By acquiring only a few ray-traced images\nfor these selected views, the selector enables DNR to achieve high-quality\nrendering. To further enhance spatial awareness and geometric consistency in\nDNR, we introduce a 3D texture aggregator that fuses pyramid features from\ndepth maps and normal maps with UV maps. Given that acquiring ray-traced images\nis more time-consuming than generating rasterized images, DNRSelect minimizes\nthe need for ray-traced data by using only a few selected views while still\nachieving high-fidelity rendering results. We conduct detailed experiments and\nablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness\nof DNRSelect. The code will be released.",
        "In recent years, there has been a tremendous interest in using generative AI,\nand particularly large language models (LLMs) in software engineering; indeed\nthere are now several commercially available tools, and many large companies\nalso have created proprietary ML-based tools for their own software engineers.\nWhile the use of ML for common tasks such as code completion is available in\ncommodity tools, there is a growing interest in application of LLMs for more\nbespoke purposes. One such purpose is code migration.\n  This article is an experience report on using LLMs for code migrations at\nGoogle. It is not a research study, in the sense that we do not carry out\ncomparisons against other approaches or evaluate research questions\/hypotheses.\nRather, we share our experiences in applying LLM-based code migration in an\nenterprise context across a range of migration cases, in the hope that other\nindustry practitioners will find our insights useful. Many of these learnings\napply to any application of ML in software engineering. We see evidence that\nthe use of LLMs can reduce the time needed for migrations significantly, and\ncan reduce barriers to get started and complete migration programs.",
        "Multimodal retrieval-augmented generation (RAG) enhances the visual reasoning\ncapability of vision-language models (VLMs) by dynamically accessing\ninformation from external knowledge bases. In this work, we introduce\n\\textit{Poisoned-MRAG}, the first knowledge poisoning attack on multimodal RAG\nsystems. Poisoned-MRAG injects a few carefully crafted image-text pairs into\nthe multimodal knowledge database, manipulating VLMs to generate the\nattacker-desired response to a target query. Specifically, we formalize the\nattack as an optimization problem and propose two cross-modal attack\nstrategies, dirty-label and clean-label, tailored to the attacker's knowledge\nand goals. Our extensive experiments across multiple knowledge databases and\nVLMs show that Poisoned-MRAG outperforms existing methods, achieving up to 98\\%\nattack success rate with just five malicious image-text pairs injected into the\nInfoSeek database (481,782 pairs). Additionally, We evaluate 4 different\ndefense strategies, including paraphrasing, duplicate removal, structure-driven\nmitigation, and purification, demonstrating their limited effectiveness and\ntrade-offs against Poisoned-MRAG. Our results highlight the effectiveness and\nscalability of Poisoned-MRAG, underscoring its potential as a significant\nthreat to multimodal RAG systems.",
        "Recent research builds various patching agents that combine large language\nmodels (LLMs) with non-ML tools and achieve promising results on the\nstate-of-the-art (SOTA) software patching benchmark, SWE-Bench. Based on how to\ndetermine the patching workflows, existing patching agents can be categorized\nas agent-based planning methods, which rely on LLMs for planning, and\nhuman-based planning methods, which follow a pre-defined workflow. At a high\nlevel, agent-based planning methods achieve high patching performance but with\na high cost and limited stability. Human-based planning methods, on the other\nhand, are more stable and efficient but have key workflow limitations that\ncompromise their patching performance. In this paper, we propose PatchPilot, an\nagentic patcher that strikes a balance between patching efficacy, stability,\nand cost-efficiency. PatchPilot proposes a novel human-based planning workflow\nwith five components: reproduction, localization, generation, validation, and\nrefinement (where refinement is unique to PatchPilot). We introduce novel and\ncustomized designs to each component to optimize their effectiveness and\nefficiency. Through extensive experiments on the SWE-Bench benchmarks,\nPatchPilot shows a superior performance than existing open-source methods while\nmaintaining low cost (less than 1$ per instance) and ensuring higher stability.\nWe also conduct a detailed ablation study to validate the key designs in each\ncomponent.",
        "Deep learning models in medical imaging face dual challenges: domain shift,\nwhere models perform poorly when deployed in settings different from their\ntraining environment, and class imbalance, where certain disease conditions are\nnaturally underrepresented. We present Imbalance-Aware Domain Adaptation\n(IADA), a novel framework that simultaneously tackles both challenges through\nthree key components: (1) adaptive feature learning with class-specific\nattention mechanisms, (2) balanced domain alignment with dynamic weighting, and\n(3) adaptive threshold optimization. Our theoretical analysis establishes\nconvergence guarantees and complexity bounds. Through extensive experiments on\nembryo development assessment across four imaging modalities, IADA demonstrates\nsignificant improvements over existing methods, achieving up to 25.19\\% higher\naccuracy while maintaining balanced performance across classes. In challenging\nscenarios with low-quality imaging systems, IADA shows robust generalization\nwith AUC improvements of up to 12.56\\%. These results demonstrate IADA's\npotential for developing reliable and equitable medical imaging systems for\ndiverse clinical settings. The code is made public available at\n\\url{https:\/\/github.com\/yinghemedical\/imbalance-aware_domain_adaptation}",
        "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.",
        "Diffusion based Text-To-Music (TTM) models generate music corresponding to\ntext descriptions. Typically UNet based diffusion models condition on text\nembeddings generated from a pre-trained large language model or from a\ncross-modality audio-language representation model. This work proposes a\ndiffusion based TTM, in which the UNet is conditioned on both (i) a uni-modal\nlanguage model (e.g., T5) via cross-attention and (ii) a cross-modal\naudio-language representation model (e.g., CLAP) via Feature-wise Linear\nModulation (FiLM). The diffusion model is trained to exploit both a local text\nrepresentation from the T5 and a global representation from the CLAP.\nFurthermore, we propose modifications that extract both global and local\nrepresentations from the T5 through pooling mechanisms that we call mean\npooling and self-attention pooling. This approach mitigates the need for an\nadditional encoder (e.g., CLAP) to extract a global representation, thereby\nreducing the number of model parameters. Our results show that incorporating\nthe CLAP global embeddings to the T5 local embeddings enhances text adherence\n(KL=1.47) compared to a baseline model solely relying on the T5 local\nembeddings (KL=1.54). Alternatively, extracting global text embeddings directly\nfrom the T5 local embeddings through the proposed mean pooling approach yields\nsuperior generation quality (FAD=1.89) while exhibiting marginally inferior\ntext adherence (KL=1.51) against the model conditioned on both CLAP and T5 text\nembeddings (FAD=1.94 and KL=1.47). Our proposed solution is not only efficient\nbut also compact in terms of the number of parameters required.",
        "Active learning (AL), which iteratively queries the most informative examples\nfrom a large pool of unlabeled candidates for model training, faces significant\nchallenges in the presence of open-set classes. Existing methods either\nprioritize query examples likely to belong to known classes, indicating low\nepistemic uncertainty (EU), or focus on querying those with highly uncertain\npredictions, reflecting high aleatoric uncertainty (AU). However, they both\nyield suboptimal performance, as low EU corresponds to limited useful\ninformation, and closed-set AU metrics for unknown class examples are less\nmeaningful. In this paper, we propose an Energy-based Active Open-set\nAnnotation (EAOA) framework, which effectively integrates EU and AU to achieve\nsuperior performance. EAOA features a $(C+1)$-class detector and a target\nclassifier, incorporating an energy-based EU measure and a margin-based energy\nloss designed for the detector, alongside an energy-based AU measure for the\ntarget classifier. Another crucial component is the target-driven adaptive\nsampling strategy. It first forms a smaller candidate set with low EU scores to\nensure closed-set properties, making AU metrics meaningful. Subsequently,\nexamples with high AU scores are queried to form the final query set, with the\ncandidate set size adjusted adaptively. Extensive experiments show that EAOA\nachieves state-of-the-art performance while maintaining high query precision\nand low training overhead. The code is available at\nhttps:\/\/github.com\/chenchenzong\/EAOA.",
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory",
    "start_abstract":"Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore.",
    "start_categories":[
      "cs.NA"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A New Kind of Science"
      ],
      "abstract":[
        "3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics."
      ],
      "categories":[
        "cs.FL"
      ]
    },
    "list":{
      "title":[
        "On Constructing Finite Automata by Relational Programming",
        "Counting Abstraction for the Verification of Structured Parameterized\n  Networks",
        "An Automata-Based Method to Formalize Psychological Theories -- The Case\n  Study of Lazarus and Folkman's Stress Theory",
        "Omega-Regular Robustness",
        "The rIC3 Hardware Model Checker",
        "Active Learning Techniques for Pomset Recognizers",
        "Soundness of reset workflow nets",
        "Behaviorally Correct Learning from Informants",
        "Fined-Grained Complexity of Ambiguity Problems on Automata and Directed\n  Graphs",
        "Asynchronism in Cellular Automata",
        "Learning Automata with Name Allocation",
        "Lexicographic transductions of finite words",
        "Complexity of the Uniform Membership Problem for Hyperedge Replacement\n  Grammars",
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces"
      ],
      "abstract":[
        "We consider ways to construct a transducer for a given set of input word to\noutput symbol pairs. This is motivated by the need for representing game\nplaying programs in a low-level mathematical format that can be analyzed by\nalgebraic tools. This is different from the classical applications of finite\nstate automata, thus the usual optimization techniques are not directly\napplicable. Therefore, we use relational programming tools to find minimal\ntransducers realizing a given set of input-output pairs.",
        "We consider the verification of parameterized networks of replicated\n  processes whose architecture is described by hyperedge-replacement\n  graph grammars. Due to the undecidability of verification problems\n  such as reachability or coverability of a given configuration, in\n  which we count the number of replicas in each local state, we\n  develop two orthogonal verification techniques. We present a\n  counting abstraction able to produce, from a graph grammar\n  describing a parameterized system, a finite set of Petri nets that\n  over-approximate the behaviors of the original system. The counting\n  abstraction is implemented in a prototype tool, evalutated on a\n  non-trivial set of test cases. Moreover, we identify a decidable\n  fragment, for which the coverability problem is in 2EXPTIME\n  and PSPACE-hard.",
        "Formal models are important for theory-building, enhancing the precision of\npredictions and promoting collaboration. Researchers have argued that there is\na lack of formal models in psychology. We present an automata-based method to\nformalize psychological theories, i.e. to transform verbal theories into formal\nmodels. This approach leverages the tools of theoretical computer science for\nformal theory development, for verification, comparison, collaboration, and\nmodularity. We exemplify our method on Lazarus and Folkman's theory of stress,\nshowcasing a step-by-step modeling of the theory.",
        "Roughly speaking, a system is said to be robust if it can resist disturbances\nand still function correctly. For instance, if the requirement is that the\ntemperature remains in an allowed range $[l,h]$, then a system that remains in\na range $[l',h']\\subset[l,h]$ is more robust than one that reaches $l$ and $h$\nfrom time to time. In this example the initial specification is quantitative in\nnature, this is not the case in $\\omega$-regular properties. Still, it seems\nthere is a natural robustness preference relation induced by an\n$\\omega$-regular property. E.g. for a property requiring that every request is\neventually granted, one would say that a system where requests are granted two\nticks after they are issued is more robust than one in which requests are\nanswered ninety ticks after they are issued. In this work we manage to distill\na robustness preference relation that is induced by a given $\\omega$-regular\nlanguage. The relation is a semantic notion (agnostic to the given\nrepresentation of $L$) that satisfies some natural criteria.",
        "In this paper, we present rIC3, an efficient bit-level hardware model checker\nprimarily based on the IC3 algorithm. It boasts a highly efficient\nimplementation and integrates several recently proposed optimizations, such as\nthe specifically optimized SAT solver, dynamically adjustment of generalization\nstrategies, and the use of predicates with internal signals, among others. As a\nfirst-time participant in the Hardware Model Checking Competition, rIC3 was\nindependently evaluated as the best-performing tool, not only in the bit-level\ntrack but also in the word-level bit-vector track through bit-blasting. Our\nexperiments further demonstrate significant advancements in both efficiency and\nscalability. rIC3 can also serve as a backend for verifying industrial RTL\ndesigns using SymbiYosys. Additionally, the source code of rIC3 is highly\nmodular, with the IC3 algorithm module being particularly concise, making it an\nacademic platform that is easy to modify and extend.",
        "Pomsets are a promising formalism for concurrent programs based on partially\nordered sets. Among this class, series-parallel pomsets admit a convenient\nlinear representation and can be recognized by simple algebraic structures\nknown as pomset recognizers. Active learning consists in inferring a formal\nmodel of a recognizable language by asking membership and equivalence queries\nto a minimally adequate teacher (MAT). We improve existing learning algorithms\nfor pomset recognizers by 1. introducing a new counter-example analysis\nprocedure that is in the best case scenario exponentially more efficient than\nexisting methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to\nminimize the impact of exceedingly verbose counter-examples and remove\nredundant queries 3. designing a suitable finite test suite that ensures\ngeneral equivalence between two pomset recognizers by extending the well-known\nW-method.",
        "Workflow nets are a well-established variant of Petri nets for the modeling\nof process activities such as business processes. The standard correctness\nnotion of workflow nets is soundness, which comes in several variants. Their\ndecidability was shown decades ago, but their complexity was only identified\nrecently. In this work, we are primarily interested in two popular variants:\n$1$-soundness and generalised soundness.\n  Workflow nets have been extended with resets to model workflows that can,\ne.g., cancel actions. It has been known for a while that, for this extension,\nall variants of soundness, except possibly generalised soundness, are\nundecidable.\n  We complete the picture by showing that generalised soundness is also\nundecidable for reset workflow nets. We then blur this undecidability landscape\nby identifying a property, coined ``$1$-in-between soundness'', which lies\nbetween $1$-soundness and generalised soundness. It reveals an unusual\nnon-monotonic complexity behaviour: a decidable soundness property is in\nbetween two undecidable ones. This can be valuable in the algorithmic analysis\nof reset workflow nets, as our procedure yields an output of the form\n``$1$-sound'' or ``not generalised sound'' which is always correct.",
        "In inductive inference, we investigate the learnability of classes of formal\nlanguages. We are interested in what classes of languages are learnable in\ncertain learning settings. A class of languages is learnable, if there is a\nlearner that can identify all of its languages and satisfies the constraints of\nthe learning setting. To identify a language, a learner is presented with\ninformation about this very language. When learning from informants, this\ninformation consists of examples for numbers that are, and numbers that are not\nincluded in the target language. As more and more examples are presented, the\nlearner outputs a hypothesis sequence. To satisfy behaviorally correct\nidentification, this hypothesis sequence must eventually only list correct\nlabels for the target language. In this thesis, we compare the effects of a\nnumber of semantic learning restrictions on the learning capabilities for\nbehaviorally correct learning from informants.",
        "Two fundamental classes of finite automata are deterministic and\nnondeterministic ones (DFAs and NFAs). Natural intermediate classes arise from\nbounds on an NFA's allowed ambiguity, i.e. number of accepting runs per word:\nunambiguous, finitely ambiguous, and polynomially ambiguous finite automata. It\nis known that deciding whether a given NFA is unambiguous and whether it is\npolynomially ambiguous is possible in quadratic time, and deciding finite\nambiguity is possible in cubic time. We provide matching lower bounds showing\nthese running times to be optimal, assuming popular fine-grained complexity\nhypotheses.\n  We improve the upper bounds for unary automata, which are essentially\ndirected graphs with a source and a target. In this view, unambiguity asks\nwhether all walks from the source to the target have different lengths. The\nrunning time analysis of our algorithm reduces to bounding the entry-wise\n1-norm of a GCD matrix, yielding a near-linear upper bound. For finite and\npolynomial ambiguity, we provide simple linear-time algorithms in the unary\ncase.\n  Finally, we study the twins property for weighted automata over the tropical\nsemiring, which characterises the determinisability of unambiguous weighted\nautomata. It occurs naturally in our context as deciding the twins property is\nan intermediate step in determinisability algorithms for weighted automata with\nbounded ambiguity. We show that Allauzen and Mohri's quadratic-time algorithm\nchecking the twins property is optimal up to the same fine-grained hypotheses\nas for unambiguity. For unary automata, we show that the problem can be\nrephrased to whether all cycles in a weighted directed graph have the same\naverage weight and give a linear-time algorithm.",
        "This study introduces Skewed Fully Asynchronous Cellular Automata (SACA), a\nnovel update scheme in cellular automata that updates the states of only two\nconsecutive and adjacent cells, such as ci and ci+1, simultaneously at each\ntime step. The behavior and dynamics of elementary cellular automata (ECA)\nunder this scheme are analyzed and compared with those of synchronous and fully\nasynchronous update methods. The comparative analysis highlights a range of\nphenomena, including transitions in ECAs from convergent or non-reversible\ndynamics to reversible, divergent behavior. The divisibility of lattice size by\n2 or 4 is shown to have significant effects on the system dynamics, linked to\nthe presence or absence of atomicity. The study also explores the convergence\nof ECAs to all-zero or all-one point attractors under SACA, providing\ntheoretical insights that align with experimental findings.\n  Additionally, the research investigates the application of fully asynchronous\ncellular automata in solving clustering problems. Clustering is defined as\ngrouping objects with similar properties. The proposed method employs\nreversible asynchronous cellular automata to merge clusters iteratively based\non their closeness, continuing until the desired number of clusters is\nachieved. This approach leverages a small set of rules, leading to faster\nconvergence and efficiency in clustering tasks. The findings underscore the\npotential of asynchronous cellular automata as a versatile and effective\nframework for studying complex system dynamics and solving practical problems\nsuch as clustering.",
        "Automata over infinite alphabets have emerged as a convenient computational\nmodel for processing structures involving data, such as nonces in cryptographic\nprotocols or data values in XML documents. We introduce active learning methods\nfor bar automata, a species of automata that process finite data words\nrepresented as bar strings, which are words with explicit name binding letters.\nBar automata have pleasant algorithmic properties. We develop a framework in\nwhich every learning algorithm for standard deterministic or nondeterministic\nfinite automata over finite alphabets can be used to learn bar automata, with a\nquery complexity determined by that of the chosen learner. The technical key to\nour approach is the algorithmic handling of $\\alpha$-equivalence of bar\nstrings, which allows to bridge the gap between finite and infinite alphabets.\nThe principles underlying our framework are generic and also apply to bar\nB\\\"uchi automata and bar tree automata, leading to the first active learning\nmethods for data languages of infinite words and finite trees.",
        "Regular transductions over finite words have linear input-to-output growth.\nThis class of transductions enjoys many characterizations. Recently, regular\ntransductions have been extended by Boja\\'nczyk to polyregular transductions,\nwhich have polynomial growth, and are characterized by pebble transducers and\nMSO interpretations. Another class of interest is that of transductions defined\nby streaming string transducers or marble transducers, which have exponential\ngrowth and are incomparable with polyregular transductions.\n  In this paper, we consider MSO set interpretations (MSOSI) over finite words\nwhich were introduced by Colcombet and Loeding. MSOSI are a natural candidate\nfor the class of \"regular transductions with exponential growth\", and are\nrather well-behaved. However MSOSI lack, for now, two desirable properties that\nregular and polyregular transductions have. The first property is being\ndescribed by an automaton model, which is closely related to the second\nproperty of regularity preserving meaning preserving regular languages under\ninverse image. We first show that if MSOSI are (effectively) regularity\npreserving then any automatic $\\omega$-word has a decidable MSO theory, an\nalmost 20 years old conjecture of B\\'ar\\'any.\n  Our main contribution is the introduction of a class of transductions of\nexponential growth, which we call lexicographic transductions. We provide three\ndifferent presentations for this class: 1) as the closure of simple\ntransductions (recognizable transductions) under a single operator called\nmaplex; 2) as a syntactic fragment of MSOSI (but the regular languages are\ngiven by automata instead of formulas); 3) we give an automaton based model\ncalled nested marble transducers, which generalize both marble transducers and\npebble transducers. We show that this class enjoys many nice properties\nincluding being regularity preserving.",
        "We investigate complexity of the uniform membership problem for hyperedge\nreplacement grammars in comparison with other mildly context-sensitive grammar\nformalisms. It turns out that the complexity of the problem considered depends\nheavily on how one defines a hypergraph. There are two commonly used\ndefinitions in the field which differ in whether repetitions of attachment\nnodes of a hyperedge are allowed in a hypergraph or not. We show that, if\nrepetitions are allowed, then the problem under consideration is\nEXPTIME-complete even for string-generating hyperedge replacement grammars\nwhile it is NP-complete if repetitions are disallowed. We also prove that\nchecking whether a hyperedge replacement grammar is string-generating is\nEXPTIME-complete.",
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven",
    "start_abstract":"The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      ],
      "abstract":[
        "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "TRKM: Twin Restricted Kernel Machines for Classification and Regression",
        "Categorical Schr\\\"odinger Bridge Matching",
        "Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically\n  Consistent Neural Networks",
        "MUSS: Multilevel Subset Selection for Relevance and Diversity",
        "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth-\n  and First-Order Optimization",
        "Temperature-Annealed Boltzmann Generators",
        "The Tabular Foundation Model TabPFN Outperforms Specialized Time Series\n  Forecasting Models Based on Simple Features",
        "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning",
        "Value-Based Deep RL Scales Predictably",
        "FedEM: A Privacy-Preserving Framework for Concurrent Utility\n  Preservation in Federated Learning",
        "Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization",
        "Network-Wide Traffic Flow Estimation Across Multiple Cities with Global\n  Open Multi-Source Data: A Large-Scale Case Study in Europe and North America",
        "Scalable Trajectory-User Linking with Dual-Stream Representation\n  Networks",
        "Reward Models Identify Consistency, Not Causality",
        "On the Data-Driven Modeling of Price-Responsive Flexible Loads:\n  Formulation and Algorithm",
        "Using Large Language Models for Solving Thermodynamic Problems",
        "Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data",
        "Spherical Dense Text-to-Image Synthesis",
        "Expressive Music Data Processing and Generation",
        "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on\n  Code Naturalness",
        "Language-agnostic, automated assessment of listeners' speech recall\n  using large language models",
        "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
        "Temporal Context Awareness: A Defense Framework Against Multi-turn\n  Manipulation Attacks on Large Language Models",
        "Dimension of diagonal self-affine measures with exponentially separated\n  projections",
        "Global linearization without hyperbolicity",
        "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
        "Design of Bayesian Clinical Trials with Clustered Data and Multiple\n  Endpoints",
        "Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited\n  Communication"
      ],
      "abstract":[
        "Restricted kernel machines (RKMs) have considerably improved generalization\nin machine learning. Recent advancements explored various techniques within the\nRKM framework, integrating kernel functions with least squares support vector\nmachines (LSSVM) to mirror the energy function of restricted Boltzmann machines\n(RBM), leading to enhanced performance. However, RKMs may face challenges in\ngeneralization when dealing with unevenly distributed or complexly clustered\ndata. Additionally, as the dataset size increases, the computational burden of\nmanaging high-dimensional feature spaces can become substantial, potentially\nhindering performance in large-scale datasets. To address these challenges, we\npropose twin restricted kernel machine (TRKM). TRKM combines the benefits of\ntwin models with the robustness of the RKM framework to enhance classification\nand regression tasks. By leveraging the Fenchel-Young inequality, we introduce\na novel conjugate feature duality, allowing the formulation of classification\nand regression problems in terms of dual variables. This duality provides an\nupper bound to the objective function of the TRKM problem, resulting in a new\nmethodology under the RKM framework. The model uses an energy function similar\nto that of RBM, incorporating both visible and hidden variables corresponding\nto both classes. Additionally, the kernel trick is employed to map data into a\nhigh-dimensional feature space, where the model identifies an optimal\nseparating hyperplane using a regularized least squares approach. Experiments\non UCI and KEEL datasets confirm TRKM's superiority over baselines, showcasing\nits robustness and efficiency in handling complex data. Furthermore, We\nimplemented the TRKM model on the brain age dataset, demonstrating its efficacy\nin predicting brain age.",
        "The Schr\\\"odinger Bridge (SB) is a powerful framework for solving generative\nmodeling tasks such as unpaired domain translation. Most SB-related research\nfocuses on continuous data space $\\mathbb{R}^{D}$ and leaves open theoretical\nand algorithmic questions about applying SB methods to discrete data, e.g, on\nfinite spaces $\\mathbb{S}^{D}$. Notable examples of such sets $\\mathbb{S}$ are\ncodebooks of vector-quantized (VQ) representations of modern autoencoders,\ntokens in texts, categories of atoms in molecules, etc. In this paper, we\nprovide a theoretical and algorithmic foundation for solving SB in discrete\nspaces using the recently introduced Iterative Markovian Fitting (IMF)\nprocedure. Specifically, we theoretically justify the convergence of\ndiscrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop\na practical computational algorithm for SB which we call Categorical\nSchr\\\"odinger Bridge Matching (CSBM). We show the performance of CSBM via a\nseries of experiments with synthetic data and VQ representations of images.",
        "Neural networks are widely used as surrogate models but they do not guarantee\nphysically consistent predictions thereby preventing adoption in various\napplications. We propose a method that can enforce NNs to satisfy physical laws\nthat are nonlinear in nature such as enthalpy balances. Our approach, inspired\nby Picard successive approximations method, aims to enforce multiplicatively\nseparable constraints by sequentially freezing and projecting a set of the\nparticipating variables. We demonstrate our PicardKKThPINN for surrogate\nmodeling of a catalytic packed bed reactor for methanol synthesis. Our results\nshow that the method efficiently enforces nonlinear enthalpy and linear atomic\nbalances at machine-level precision. Additionally, we show that enforcing\nconservation laws can improve accuracy in data-scarce conditions compared to\nvanilla multilayer perceptron.",
        "The problem of relevant and diverse subset selection has a wide range of\napplications, including recommender systems and retrieval-augmented generation\n(RAG). For example, in recommender systems, one is interested in selecting\nrelevant items, while providing a diversified recommendation. Constrained\nsubset selection problem is NP-hard, and popular approaches such as Maximum\nMarginal Relevance (MMR) are based on greedy selection. Many real-world\napplications involve large data, but the original MMR work did not consider\ndistributed selection. This limitation was later addressed by a method called\nDGDS which allows for a distributed setting using random data partitioning.\nHere, we exploit structure in the data to further improve both scalability and\nperformance on the target application. We propose MUSS, a novel method that\nuses a multilevel approach to relevant and diverse selection. We provide a\nrigorous theoretical analysis and show that our method achieves a constant\nfactor approximation of the optimal objective. In a recommender system\napplication, our method can achieve the same level of performance as baselines,\nbut 4.5 to 20 times faster. Our method is also capable of outperforming\nbaselines by up to 6 percent points of RAG-based question answering accuracy.",
        "Zeroth-order (ZO) optimization is being recognized as a simple yet powerful\nalternative to standard backpropagation (BP)-based training. Notably, ZO\noptimization allows for training with only forward passes and (almost) the same\nmemory as inference, making it well-suited for edge devices with limited\ncomputing and memory resources. In this paper, we propose ZO-based on-device\nlearning (ODL) methods for full-precision and 8-bit quantized deep neural\nnetworks (DNNs), namely ElasticZO and ElasticZO-INT8. ElasticZO lies in the\nmiddle between pure ZO- and pure BP-based approaches, and is based on the idea\nto employ BP for the last few layers and ZO for the remaining layers.\nElasticZO-INT8 achieves integer arithmetic-only ZO-based training for the first\ntime, by incorporating a novel method for computing quantized ZO gradients from\ninteger cross-entropy loss values. Experimental results on the classification\ndatasets show that ElasticZO effectively addresses the slow convergence of\nvanilla ZO and shrinks the accuracy gap to BP-based training. Compared to\nvanilla ZO, ElasticZO achieves 5.2-9.5% higher accuracy with only 0.072-1.7%\nmemory overhead, and can handle fine-tuning tasks as well as full training.\nElasticZO-INT8 further reduces the memory usage and training time by 1.46-1.60x\nand 1.38-1.42x without compromising the accuracy. These results demonstrate a\nbetter tradeoff between accuracy and training cost compared to pure ZO- and\nBP-based approaches, and also highlight the potential of ZO optimization in\non-device learning.",
        "Efficient sampling of unnormalized probability densities such as the\nBoltzmann distribution of molecular systems is a longstanding challenge. Next\nto conventional approaches like molecular dynamics or Markov chain Monte Carlo,\nvariational approaches, such as training normalizing flows with the reverse\nKullback-Leibler divergence, have been introduced. However, such methods are\nprone to mode collapse and often do not learn to sample the full\nconfigurational space. Here, we present temperature-annealed Boltzmann\ngenerators (TA-BG) to address this challenge. First, we demonstrate that\ntraining a normalizing flow with the reverse Kullback-Leibler divergence at\nhigh temperatures is possible without mode collapse. Furthermore, we introduce\na reweighting-based training objective to anneal the distribution to lower\ntarget temperatures. We apply this methodology to three molecular systems of\nincreasing complexity and, compared to the baseline, achieve better results in\nalmost all metrics while requiring up to three times fewer target energy\nevaluations. For the largest system, our approach is the only method that\naccurately resolves the metastable states of the system.",
        "Foundation models have become popular in forecasting due to their ability to\nmake accurate predictions, even with minimal fine-tuning on specific datasets.\nIn this paper, we demonstrate how the newly released regression variant of\nTabPFN, a general tabular foundation model, can be applied to time series\nforecasting. We propose a straightforward approach, TabPFN-TS, which pairs\nTabPFN with simple feature engineering to achieve strong forecasting\nperformance. Despite its simplicity and with only 11M parameters, TabPFN-TS\noutperforms Chronos-Mini, a model of similar size, and matches or even slightly\noutperforms Chronos-Large, which has 65-fold more parameters. A key strength of\nour method lies in its reliance solely on artificial data during pre-training,\navoiding the need for large training datasets and eliminating the risk of\nbenchmark contamination.",
        "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset.",
        "Scaling data and compute is critical to the success of machine learning.\nHowever, scaling demands predictability: we want methods to not only perform\nwell with more compute or data, but also have their performance be predictable\nfrom small-scale runs, without running the large-scale experiment. In this\npaper, we show that value-based off-policy RL methods are predictable despite\ncommunity lore regarding their pathological behavior. First, we show that data\nand compute requirements to attain a given performance level lie on a Pareto\nfrontier, controlled by the updates-to-data (UTD) ratio. By estimating this\nfrontier, we can predict this data requirement when given more compute, and\nthis compute requirement when given more data. Second, we determine the optimal\nallocation of a total resource budget across data and compute for a given\nperformance and use it to determine hyperparameters that maximize performance\nfor a given budget. Third, this scaling behavior is enabled by first estimating\npredictable relationships between hyperparameters, which is used to manage\neffects of overfitting and plasticity loss unique to RL. We validate our\napproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI\ngym, and IsaacGym, when extrapolating to higher levels of data, compute,\nbudget, or performance.",
        "Federated Learning (FL) enables collaborative training of models across\ndistributed clients without sharing local data, addressing privacy concerns in\ndecentralized systems. However, the gradient-sharing process exposes private\ndata to potential leakage, compromising FL's privacy guarantees in real-world\napplications. To address this issue, we propose Federated Error Minimization\n(FedEM), a novel algorithm that incorporates controlled perturbations through\nadaptive noise injection. This mechanism effectively mitigates gradient leakage\nattacks while maintaining model performance. Experimental results on benchmark\ndatasets demonstrate that FedEM significantly reduces privacy risks and\npreserves model accuracy, achieving a robust balance between privacy protection\nand utility preservation.",
        "In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.",
        "Network-wide traffic flow, which captures dynamic traffic volume on each link\nof a general network, is fundamental to smart mobility applications. However,\nthe observed traffic flow from sensors is usually limited across the entire\nnetwork due to the associated high installation and maintenance costs. To\naddress this issue, existing research uses various supplementary data sources\nto compensate for insufficient sensor coverage and estimate the unobserved\ntraffic flow. Although these studies have shown promising results, the\ninconsistent availability and quality of supplementary data across cities make\ntheir methods typically face a trade-off challenge between accuracy and\ngenerality. In this research, we first time advocate using the Global Open\nMulti-Source (GOMS) data within an advanced deep learning framework to break\nthe trade-off. The GOMS data primarily encompass geographical and demographic\ninformation, including road topology, building footprints, and population\ndensity, which can be consistently collected across cities. More importantly,\nthese GOMS data are either causes or consequences of transportation activities,\nthereby creating opportunities for accurate network-wide flow estimation.\nFurthermore, we use map images to represent GOMS data, instead of traditional\ntabular formats, to capture richer and more comprehensive geographical and\ndemographic information. To address multi-source data fusion, we develop an\nattention-based graph neural network that effectively extracts and synthesizes\ninformation from GOMS maps while simultaneously capturing spatiotemporal\ntraffic dynamics from observed traffic data. A large-scale case study across 15\ncities in Europe and North America was conducted. The results demonstrate\nstable and satisfactory estimation accuracy across these cities, which suggests\nthat the trade-off challenge can be successfully addressed using our approach.",
        "Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.",
        "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
        "The flexible loads in power systems, such as interruptible and transferable\nloads, are critical flexibility resources for mitigating power imbalances.\nDespite their potential, accurate modeling of these loads is a challenging work\nand has not received enough attention, limiting their integration into\noperational frameworks. To bridge this gap, this paper develops a data-driven\nidentification theory and algorithm for price-responsive flexible loads\n(PRFLs). First, we introduce PRFL models that capture both static and dynamic\ndecision mechanisms governing their response to electricity price variations.\nSecond, We develop a data-driven identification framework that explicitly\nincorporates forecast and measurement errors. Particularly, we give a\ntheoretical analysis to quantify the statistical impact of such noise on\nparameter estimation. Third, leveraging the bilevel structure of the\nidentification problem, we propose a Bayesian optimization-based algorithm that\nfeatures the scalability to large sample sizes and the ability to offer\nposterior differentiability certificates as byproducts. Numerical tests\ndemonstrate the effectiveness and superiority of the proposed approach.",
        "Large Language Models (LLMs) have made significant progress in reasoning,\ndemonstrating their capability to generate human-like responses. This study\nanalyzes the problem-solving capabilities of LLMs in the domain of\nthermodynamics. A benchmark of 22 thermodynamic problems to evaluate LLMs is\npresented that contains both simple and advanced problems. Five different LLMs\nare assessed: GPT-3.5, GPT-4, and GPT-4o from OpenAI, Llama 3.1 from Meta, and\nle Chat from MistralAI. The answers of these LLMs were evaluated by trained\nhuman experts, following a methodology akin to the grading of academic exam\nresponses. The scores and the consistency of the answers are discussed,\ntogether with the analytical skills of the LLMs. Both strengths and weaknesses\nof the LLMs become evident. They generally yield good results for the simple\nproblems, but also limitations become clear: The LLMs do not provide consistent\nresults, they often fail to fully comprehend the context and make wrong\nassumptions. Given the complexity and domain-specific nature of the problems,\nthe statistical language modeling approach of the LLMs struggles with the\naccurate interpretation and the required reasoning. The present results\nhighlight the need for more systematic integration of thermodynamic knowledge\nwith LLMs, for example, by using knowledge-based methods.",
        "Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.",
        "Neural code models (NCMs) have demonstrated extraordinary capabilities in\ncode intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems\nhas garnered increasing attention. In particular, NCMs are often trained on\nlarge-scale data from potentially untrustworthy sources, providing attackers\nwith the opportunity to manipulate them by inserting crafted samples into the\ndata. This type of attack is called a code poisoning attack (also known as a\nbackdoor attack). It allows attackers to implant backdoors in NCMs and thus\ncontrol model behavior, which poses a significant security threat. However,\nthere is still a lack of effective techniques for detecting various complex\ncode poisoning attacks.\n  In this paper, we propose an innovative and lightweight technique for code\npoisoning detection named KillBadCode. KillBadCode is designed based on our\ninsight that code poisoning disrupts the naturalness of code. Specifically,\nKillBadCode first builds a code language model (CodeLM) on a lightweight\n$n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM\nto identify those tokens in (poisoned) code snippets that will make the code\nsnippets more natural after being deleted as trigger tokens. Considering that\nthe removal of some normal tokens in a single sample might also enhance code\nnaturalness, leading to a high false positive rate (FPR), we aggregate the\ncumulative improvement of each token across all samples. Finally, KillBadCode\npurifies the poisoned data by removing all poisoned samples containing the\nidentified trigger tokens. The experimental results on two code poisoning\nattacks and four code intelligence tasks demonstrate that KillBadCode\nsignificantly outperforms four baselines. More importantly, KillBadCode is very\nefficient, with a minimum time consumption of only 5 minutes, and is 25 times\nfaster than the best baseline on average.",
        "Speech-comprehension difficulties are common among older people. Standard\nspeech tests do not fully capture such difficulties because the tests poorly\nresemble the context-rich, story-like nature of ongoing conversation and are\ntypically available only in a country's dominant\/official language (e.g.,\nEnglish), leading to inaccurate scores for native speakers of other languages.\nAssessments for naturalistic, story speech in multiple languages require\naccurate, time-efficient scoring. The current research leverages modern large\nlanguage models (LLMs) in native English speakers and native speakers of 10\nother languages to automate the generation of high-quality, spoken stories and\nscoring of speech recall in different languages. Participants listened to and\nfreely recalled short stories (in quiet\/clear and in babble noise) in their\nnative language. LLM text-embeddings and LLM prompt engineering with semantic\nsimilarity analyses to score speech recall revealed sensitivity to known\neffects of temporal order, primacy\/recency, and background noise, and high\nsimilarity of recall scores across languages. The work overcomes limitations\nassociated with simple speech materials and testing of closed native-speaker\ngroups because recall data of varying length and details can be mapped across\nlanguages with high accuracy. The full automation of speech generation and\nrecall scoring provides an important step towards comprehension assessments of\nnaturalistic speech with clinical applicability.",
        "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, in which event pairs are considered individually,\nleading to computational inefficiency and a lack of global consistency in the\nresulting temporal graph. In this work, we propose a novel zero-shot method for\nTRE that generates a document's complete temporal graph at once, then applies\ntransitive constraints optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\nsignificantly outperforms existing zero-shot approaches while achieving\ncompetitive performance with supervised models.",
        "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.",
        "Let $ \\mu $ be a self-affine measure associated with a diagonal affine\niterated function system (IFS) $ \\Phi = \\{ (x_{1}, \\ldots, x_{d}) \\mapsto (\nr_{i, 1}x_{1} + t_{i,1}, \\ldots, r_{i,d}x_{d} + t_{i,d}) \\}_{i\\in\\Lambda} $ on\n$ \\mathbb{R}^{d} $ and a probability vector $ p = (p_{i})_{i\\in\\Lambda}$. For $\n1 \\leq j \\leq d $, denote the $ j $-th the Lyapunov exponent by $ \\chi_{j} :=\n\\sum_{i\\in\\Lambda} - p_{i} \\log | r_{i,j} |$, and define the IFS induced by $\n\\Phi $ on the $j$-th coordinate as $ \\Phi_{j} := \\{ x \\mapsto r_{i,j}x +\nt_{i,j}\\}_{i\\in\\Lambda}$. We prove that if $ \\chi_{j_{1}} \\neq \\chi_{j_{2}} $\nfor $ 1 \\leq j_{1} < j_{2} \\leq d $, and $ \\Phi_{j}$ is exponentially separated\nfor $ 1 \\leq j \\leq d $, then the dimension of $ \\mu $ is the minimum of $ d $\nand its Lyapunov dimension. This confirms a conjecture of Rapaport by removing\nthe additional assumption that the linear parts of the maps in $ \\Phi $ are\ncontained in a 1-dimensional subgroup. One of the main ingredients of the proof\ninvolves disintegrating $ \\mu $ into random measures with convolution\nstructure. In the course of the proof, we establish new results on dimension\nand entropy increase for these random measures.",
        "We give a proof of an extension of the Hartman-Grobman theorem to\nnonhyperbolic but asymptotically stable equilibria of vector fields. Moreover,\nthe linearizing topological conjugacy is (i) defined on the entire basin of\nattraction if the vector field is complete, and (ii) a $C^{k\\geq 1}$\ndiffeomorphism on the complement of the equilibrium if the vector field is\n$C^k$ and the underlying space is not $5$-dimensional. We also show that the\n$C^k$ statement in the $5$-dimensional case is equivalent to the\n$4$-dimensional smooth Poincar\\'{e} conjecture.",
        "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp:\/\/github.com\/ddgoodgood\/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
        "In the design of clinical trials, it is essential to assess the design\noperating characteristics (i.e., the probabilities of making correct\ndecisions). Common practice for the evaluation of operating characteristics in\nBayesian clinical trials relies on estimating the sampling distribution of\nposterior summaries via Monte Carlo simulation. It is computationally intensive\nto repeat this estimation process for each design configuration considered,\nparticularly for clustered data that are analyzed using complex,\nhigh-dimensional models. In this paper, we propose an efficient method to\nassess operating characteristics and determine sample sizes for Bayesian trials\nwith clustered data and multiple endpoints. We prove theoretical results that\nenable posterior probabilities to be modelled as a function of the sample size.\nUsing these functions, we assess operating characteristics at a range of sample\nsizes given simulations conducted at only two sample sizes. These theoretical\nresults are also leveraged to quantify the impact of simulation variability on\nour sample size recommendations. The applicability of our methodology is\nillustrated using a current clinical trial with clustered data.",
        "We consider the problem setting in which multiple autonomous agents must\ncooperatively navigate and perform tasks in an unknown,\ncommunication-constrained environment. Traditional multi-agent reinforcement\nlearning (MARL) approaches assume synchronous communications and perform poorly\nin such environments. We propose AsynCoMARL, an asynchronous MARL approach that\nuses graph transformers to learn communication protocols from dynamic graphs.\nAsynCoMARL can accommodate infrequent and asynchronous communications between\nagents, with edges of the graph only forming when agents communicate with each\nother. We show that AsynCoMARL achieves similar success and collision rates as\nleading baselines, despite 26\\% fewer messages being passed between agents."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
    "start_abstract":"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven"
      ],
      "abstract":[
        "The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Systematic calculation on alpha decay and cluster radioactivity of\n  superheavy nuclei",
        "Description of nucleon transfer reactions at intermediate energies\n  within the impulse picture",
        "Nuclear Schiff Moments and CP Violation",
        "Chiral symmetry and peripheral neutron-$\\alpha$ scattering",
        "New Skyrme parametrizations to describe finite nuclei and neutron star\n  matter with realistic effective masses. II. Adjusting the spin-dependent\n  terms",
        "$\\alpha+\\alpha+{}^{3}$He cluster structure in ${}^{11}$C",
        "Patterns of spin and pseudo-spin symmetries in nuclear relativistic\n  mean-field approaches",
        "Electromagnetic form factors of ${}^6$Li, ${}^7$Li, and ${}^7$Be in\n  cluster effective field theory",
        "Towards shell model interactions with credible uncertainties",
        "$\\Lambda$ and $\\Sigma$ potentials in neutron stars, hypernuclei, and\n  heavy-ion collisions",
        "$A=2,3$ nuclear contact coefficients in the Generalized Contact\n  Formalism",
        "$\\gamma W$-exchange contributions in neutron $\\beta$ decay in the\n  forward-angle limit",
        "Electric dipole excitations near the neutron separation energies in\n  $^{96}$Mo",
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "Nonlinear Temperature Sensitivity of Residential Electricity Demand:\n  Evidence from a Distributional Regression Approach",
        "Conformal Prediction and Human Decision Making",
        "On monotone alternating inverse monoids",
        "Oblique rotational axis detection using elliptical optical vortex based\n  on rotational Doppler effect",
        "Sublinear Variational Optimization of Gaussian Mixture Models with\n  Millions to Billions of Parameters",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "Revisiting Frank-Wolfe for Structured Nonconvex Optimization",
        "Cayley unitary elements in group algebras under oriented involutions",
        "Tractable General Equilibrium",
        "Mesoscopic Collective Dynamics in Liquids and the Dual Model",
        "Comprehensive landscape and simple rules for transition-metal Heusler\n  semiconductors",
        "Efficient Truncations of SU($N_c$) Lattice Gauge Theory for Quantum\n  Simulation",
        "Attractors for Singular-Degenerate Porous Medium Type Equations Arising\n  in Models for Biofilm Growth",
        "Topology-preserving discretization for the magneto-frictional equations\n  arising in the Parker conjecture"
      ],
      "abstract":[
        "In the Coulomb and Proximity Potential Model (CPPM) framework, we have\ninvestigated the cluster radioactivity and alpha decay half-lives of superheavy\nnuclei. We study 22 different versions of proximity potential forms that have\nbeen proposed to describe proton radioactivity, two-proton radioactivity,\nheavy-ion radioactivity, quasi-elastic scattering, fusion reactions, and other\napplications. The half-lives of cluster radioactivity and alpha decay of 41\natomic nuclei ranging from 221Fr to 244Cm were calculated, and the results\nindicate that the refined nuclear potential named BW91 is the most suitable\nproximity potential form for the cluster radioactivity and alpha decay of\nsuperheavy nuclei since the root-mean-square (RMS) deviation between the\nexperimental data and the relevant theoretical calculation results is the\nsmallest ({\\sigma}= 0.841). By using CPPM, we predicted the half-lives of 20\npotential cluster radioactivity and alpha decay candidates. These cluster\nradioactivities and alpha decays are energetically allowed or observable but\nnot yet quantified in NUBASE2020.",
        "Background:\n  At intermediate energies, transfer reactions hardly occur because the\nmomentum-matching condition is difficult to satisfy. In the standard\ndistorted-wave Born approximation, a particle to be transferred must have a\nmomentum being similar to the momentum transfer.\n  Purpose:\n  We propose a new reaction framework based on the distorted-wave impulse\napproximation for transfer reactions at intermediate energies, aiming to ease\nthe momentum-matching condition.\n  Methods:\n  The $(p,d)$ reaction is described as a $p$-$d$ elastic scattering for\nbackward-angle scattering in the target nucleus and the proton that formed a\n$pn$ pair is left and bound in the residual nucleus in the final channel. The\nmomentum transfer is shared by the deuteron in the target nucleus and the\nproton in the residual nucleus.\n  Results:\n  The new framework is applied to the $^{16}$O($p,d$)$^{15}$O reaction at\n200~MeV and compared with experimental data. The angular distribution is\nsatisfactorily well described, whereas an anomalously large scaling factor is\nneeded to reproduce the absolute value. The transition matrix is analyzed in\ndetail and the mechanism of the momentum sharing is clarified.\n  Conclusions:\n  The new reaction framework for transfer reactions at intermediate energies\nseems to be promising for describing the reaction mechanism but fails to\nexplain the absolute value of the cross section. The use of the $p$-$d$\ntransition amplitude instead of its cross section will be necessary to draw a\nconclusion on the applicability of the present framework.",
        "This paper reviews the calculation of nuclear Schiff moments, which one must\nknow in order to interpret experiments that search for time-reversal-violating\nelectric dipole moments in certain atoms and molecules. After briefly reviewing\nthe connection between dipole moments and CP violation in and beyond the\nStandard Model of particle physics, Schiff's theorem, which concerns the\nscreening of nuclear electric dipole moments by electrons, Schiff moments, and\nexperiments to measure dipole moments in atoms and molecules, the paper\nexamines attempts to compute Schiff moments in nuclei such as $^{199}$Hg and\noctupole-deformed isotopes such as $^{225}$Ra, which are particularly useful in\nexperiments. It then turns to ab initio nuclear-structure theory, describing\nways in which both the In-Medium Similarity Renormalization Group and\ncoupled-cluster theory can be used to compute important Schiff moments more\naccurately than the less controlled methods that have been applied so far.",
        "We propose and demonstrate that peripheral neutron-$\\alpha$ scattering at low\nenergies can serve as a sensitive and clean probe of the long-range\nthree-nucleon forces. To this aim, we perform {\\it ab initio} quantum Monte\nCarlo calculations using two- and three-nucleon interactions derived in chiral\neffective field theory up to third expansion order. We show that the\nlongest-range three-nucleon force stemming from the two-pion exchange plays a\ncrucial role in the proper description of the neutron-$\\alpha$ $D$-wave phase\nshifts. Our study reveals the predictive power of chiral symmetry in the\nfew-body sector and opens a new direction for probing and constraining\nthree-nucleon forces.",
        "Many common Skyrme functionals present ferromagnetic instabilities or\nunrealistic density dependence of the spin-1 Landau parameters. To solve these\nproblems, we consider the Skyrme interaction as a density-functional rather\nthan a density-dependent two-body force. This allows us to adjust the\nspin-dependent terms of the new extended Skyrme functionals of our previous\npaper [M. Duan and M. Urban, Phys. Rev. C 110, 065806 (2024)] independently\nwithout altering the properties of spin saturated matter. The parameters of the\nspin-dependent terms are determined by fitting the Landau parameters $G_0$ and\n$G_0^\\prime$ in neutron matter and symmetric nuclear matter and the\neffective-mass splitting of up and down particles in spin polarized matter to\nthe results of microscopic calculations. Using the new parametrizations, called\nSky3s and Sky4s, the spin-related properties of nuclear matter are in good\nagreement with the microscopic results. As an application, we compute response\nfunctions and neutrino scattering rates of neutron-star matter with the new\nfunctionals having realistic effective masses and Landau parameters.",
        "We study the $\\alpha + \\alpha + {}^{3}$He cluster structure of ${}^{11}$C\nwithin the microscopic cluster model. The calculations essentially reproduce\nthe energy spectra for both negative and positive parity states, particularly\nthe $3\/2_3^-$ state near the $\\alpha+\\alpha$+${}^{3}$He threshold. We also\ncalculate the isoscalar monopole, electric quadrupole transition strengths, and\nroot-mean-square radii for the low-lying states. These results suggest that the\n$3\/2_3^-$, $1\/2_2^-$, and $5\/2_3^-$ states have a well-developed $\\alpha +\n\\alpha$ + ${}^{3}$He cluster structure. The analysis of the generator\ncoordinate method wave functions indicates the dilute gaslike nature for the\n$3\/2_3^-$, $1\/2_2^-$, and $5\/2_3^-$ states, suggesting that they could be\ncandidates for the Hoyle-analog state. Furthermore, it is found that the\n$5\/2_2^+$ and $5\/2_3^+$ states may possess a linear chain structure.",
        "The behavior of spin doublets is known to play a major role in nuclear\nstructure and shell effects. Pseudo-spin doublets are also known to impact the\nsingle-particle spectrum. The covariant framework, having these two effects\nencoded in its approach, is an excellent tool to understand the main mechanism\ndriving theses spin and pseudo-spin symmetries and their breaking. A\nperturbative expansion of the degeneracy raising related to spin and\npseudo-spin effects is proposed, up to second order. It allows to understand\nthe main behavior of spin and pseudo-spin energy doublets, such as their A\ndependence, as well as their common footing and differences. In the case of the\nspin symmetry, only the lower component of the Dirac bi-spinor is involved,\nwhereas in the case of the pseudo-spin one, both the upper and lower components\nare involved. Their interplay with the covariant potentials is also analyzed.",
        "Effective field theory (EFT) provides a powerful model-independent\ntheoretical framework for illuminating complicated interactions across a wide\nrange of physics areas and subfields. In this work, we consider the low-energy\ndeuteron-Helium-4, triton-Helium-4, and helion-Helium-4 systems at low energies\nin cluster EFT. In particular, we focus on the deuteron + Helium-4 cluster\nconfiguration of the Lithium-6 nucleus, the triton + Helium-4 cluster\nconfiguration of the Lithium-7 nucleus, and the Helium-3-Helium-4 configuration\nof the Beryllium-7 nucleus, respectively. We illustrate how to directly extract\nthe asymptotic normalization coefficient and several observables using\nexperimental measurement of the electromagnetic form factors of these nuclei.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction.",
        "With an appropriate $YNN$ force, the $\\Lambda$ single-particle potential\n($\\Lambda$ potential) can be made strongly repulsive at high density, and one\ncan solve the hyperon puzzle of neutron stars. We investigate the consistency\nof such a $\\Lambda$ potential, evaluated recently from $YN$ and $YNN$ forces\nbased on chiral effective field theory, with hypernuclear data and heavy-ion\ncollision data. It is found that model calculations with such a $\\Lambda$\npotential can reproduce the data of the $\\Lambda$ hypernuclear spectroscopy and\nthe $\\Lambda$ directed flow in heavy-ion collisions. Also, we evaluate the\n$\\Sigma$ potential, which can be calculated by using the same hyperon forces as\nfor the $\\Lambda$ potential. Specifically, we show that the low-energy\nconstants characterizing the strength of the $YNN$ force can be chosen to\nsuppress the appearance of the $\\Lambda$'s in neutron stars while at the same\ntime the empirical value of the $\\Sigma$ potential is reproduced.",
        "This work focuses on extracting nuclear contact coefficients for \\( A = 2 \\),\n\\( A = 3 \\) and \\( A = 4 \\) nuclei within the Generalized Contact Formalism\nframework. We investigate the universality of these coefficients across\ndifferent nuclear systems and interaction models, using both local (in \\( r\n\\)-space) and non-local (in \\( k \\)-space) chiral potentials. The\nHyperspherical Harmonics method is employed to calculate the nuclear wave\nfunctions from which we obtain the two-body momentum distributions and the\ntwo-body density functions, which are essential for extracting the contact\ncoefficients. The adopted method is a rigorous ab-initio approach that can be\napplied to virtually any potential.\n  We present ratios of contact coefficients across various spin and isospin\nchannels, highlighting their independence from the used nuclear potential. This\nstudy extends previous work where only local interaction models were employed.\nFurthermore, we verify whether the contact coefficient ratio between different\nnuclei remains consistent even when non-local potentials are considered.\n  Future work will extend this analysis to heavier nuclei, such as \\( A = 4 \\)\nand \\( A = 6 \\) nuclei.",
        "In this work, the contributions from $\\gamma W$-exchange in neutron $\\beta$\ndecay are estimated at the amplitude level. Using a general form for the\nelectromagnetic (EM) form factors (FFs) of the proton, the EM FFs of the\nneutron, and the weak FFs of the $Wnp$ interaction as inputs, we present\nanalytical expressions for the inner part of the $\\gamma W$-exchange amplitude\nunder the forward angle limit. The differences and relations between our method\nand those used in the references are discussed. To compare our numerical\nresults with those provided in the references, we consider three types of FFs\nas examples. The numerical results show that when the favored EM FFs are used,\nour result for the contribution from the Fermi part is consistent with those\nreported in the references, while our results for the Gamow-Teller parts are\nabout 7\\% and 13\\% larger than those reported in Ref.~\\cite{Hayen-2021}.",
        "Electric dipole strength near the neutron separation energy significantly\nimpacts nuclear structure properties and astrophysical scenarios. These\nexcitations are complex in nature and may involve the so-called pygmy dipole\nresonance (PDR). Transition densities play a crucial role in understanding the\nnature of nuclear excited states, including collective excitations, as well as\nin constructing transition potentials in DWBA or coupled-channels equations. In\nthis work, we focus on electric dipole excitations in spherical molybdenum\nisotopes, particularly $^{96}$Mo, employing fully consistent\nHartree-Fock-Bogoliubov (HFB) and Quasiparticle Random Phase Approximation\n(QRPA) methods. We analyze the dipole strength near the neutron separation\nenergy, which represents the threshold for neutron capture processes, and\nexamine the isospin characteristics of PDR states through transition density\ncalculations. Examination of proton and neutron transition densities reveals\ndistinctive features of each dipole state, indicating their isoscalar and\nisovector nature. We observe that the primary component in the enhanced\nlow-energy region exhibits isovector character. The PDR displays a mixture of\nisoscalar and isovector nature, distinguishing it from the isovector giant\ndipole resonance (IVGDR). These findings lay the groundwork for future\ninvestigations into the role of transition densities in reaction models and for\ntheir application to inelastic scattering calculations.",
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "We estimate the temperature sensitivity of residential electricity demand\nduring extreme temperature events using the distribution-to-scalar regression\nmodel. Rather than relying on simple averages or individual quantile statistics\nof raw temperature data, we construct distributional summaries, such as\nprobability density, hazard rate, and quantile functions, to retain a more\ncomprehensive representation of temperature variation. This approach not only\nutilizes richer information from the underlying temperature distribution but\nalso enables the examination of extreme temperature effects that conventional\nmodels fail to capture. Additionally, recognizing that distribution functions\nare typically estimated from limited discrete observations and may be subject\nto measurement errors, our econometric framework explicitly addresses this\nissue. Empirical findings from the hazard-to-demand model indicate that\nresidential electricity demand exhibits a stronger nonlinear response to cold\nwaves than to heat waves, while heat wave shocks demonstrate a more pronounced\nincremental effect. Moreover, the temperature quantile-to-demand model produces\nlargely insignificant demand response estimates, attributed to the offsetting\ninfluence of two counteracting forces.",
        "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
        "In this paper, we consider the inverse submonoids $AM_n$ of monotone\ntransformations and $AO_n$ of order-preserving transformations of the\nalternating inverse monoid $AI_n$ on a chain with $n$ elements. We compute the\ncardinalities, describe the Green's structures and the congruences, and\ncalculate the ranks of these two submonoids of $AI_n$.",
        "The rotational Doppler effect (RDE) of structured light carrying orbital\nangular momentum (OAM) has attracted widespread attention for applications in\noptical sensors and OAM spectrum detection. These studies, however, based on\nRDE, are mostly focused on the motion parameters of rotating objects; other\nequally important attitude characteristics, e.g., the tilt angle of the axis of\nrotation, have rarely been considered. We observed an interesting phenomenon in\nthe experiments: the rotational Doppler spectral distribution varies with the\nellipticity of the elliptical optical vortex (EOV) and the tilt angle between\nthe rotational axis and optical axis, which inspired us to wonder if it is\npossible to detect oblique rotational axis or compensate the rotational Doppler\nbroadening effect induced by oblique incidence by utilizing the EOV. Here, we\nreveal the RDE quantitative relationship with tilt angle and ellipticity for\nthe first time and report a novel approach for tilt angle measurement. By\nemploying a series of EOV with periodically varying ellipticity to illuminate a\nrotating object and analyze the time-frequency spectral distribution of\nscattered light associated with ellipticity and tilt angle, the tilt angle can\nbe acquired accurately based on the specific relationship between the tilt\nangle and ellipticity of the EOV. Furthermore, the spectrum broadening effect\narising from oblique incidence in the actual scenario may be addressed through\nour scheme. The method may find applications in industrial manufacturing and\ntarget attitude measurement, and our results provide new insights for obtaining\nmore information about objects.",
        "Gaussian Mixture Models (GMMs) range among the most frequently used machine\nlearning models. However, training large, general GMMs becomes computationally\nprohibitive for datasets with many data points $N$ of high-dimensionality $D$.\nFor GMMs with arbitrary covariances, we here derive a highly efficient\nvariational approximation, which is integrated with mixtures of factor\nanalyzers (MFAs). For GMMs with $C$ components, our proposed algorithm\nsignificantly reduces runtime complexity per iteration from\n$\\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remaining\nconstant w.r.t. $C$. Numerical validation of this theoretical complexity\nreduction then shows the following: the distance evaluations required for the\nentire GMM optimization process scale sublinearly with $NC$. On large-scale\nbenchmarks, this sublinearity results in speed-ups of an order-of-magnitude\ncompared to the state-of-the-art. As a proof of concept, we train GMMs with\nover 10 billion parameters on about 100 million images, and observe training\ntimes of approximately nine hours on a single state-of-the-art CPU.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We introduce a new projection-free (Frank-Wolfe) method for optimizing\nstructured nonconvex functions that are expressed as a difference of two convex\nfunctions. This problem class subsumes smooth nonconvex minimization,\npositioning our method as a promising alternative to the classical Frank-Wolfe\nalgorithm. DC decompositions are not unique; by carefully selecting a\ndecomposition, we can better exploit the problem structure, improve\ncomputational efficiency, and adapt to the underlying problem geometry to find\nbetter local solutions. We prove that the proposed method achieves a\nfirst-order stationary point in $O(1\/\\epsilon^2)$ iterations, matching the\ncomplexity of the standard Frank-Wolfe algorithm for smooth nonconvex\nminimization in general. Specific decompositions can, for instance, yield a\ngradient-efficient variant that requires only $O(1\/\\epsilon)$ calls to the\ngradient oracle. Finally, we present numerical experiments demonstrating the\neffectiveness of the proposed method compared to the standard Frank-Wolfe\nalgorithm.",
        "Let $\\mathbf{F}$ be a real extension of $\\mathbb{Q}$, $G$ a finite group and\n$\\mathbf{F}G$ its group algebra. Given both a group homomorphism\n$\\sigma:G\\rightarrow \\{\\pm1\\}$ (called an orientation) and a group involution\n$^\\ast:G \\rightarrow G$ such that $gg^\\ast\\in N=ker(\\sigma)$, an oriented group\ninvolution $\\circledast$ of $\\mathbf{F}G$ is defined by $\\alpha=\\sum_{g\\in\nG}\\alpha_{g}g \\mapsto \\alpha^\\circledast=\\sum_{g\\in\nG}\\alpha_{g}\\sigma(g)g^{\\ast}$. In this paper, in case the involution on $G$ is\nthe classical one, $x\\mapsto x^{-1}$, $\\beta=x+x^{-1}$ is a skew-symmetric\nelement in $\\mathbf{F}G$ such that $1+\\beta$ is invertible, for $x\\in G$ with\n$\\sigma(x)=-1$, we consider Cayley unitary elements built out of $\\beta$. We\nprove that the coefficients of $(1+\\beta)^{-1}$ involve an interesting sequence\nwhich is a Fibonacci-like sequence.",
        "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
        "A microscopic vision is presented of a Dual Model of Liquids from a solid\npicture. Among the novelties of this model is that it provides quantitative\nexpressions of various extensive thermophysical properties. The introduction of\nthe statistical number of excited degrees of freedom (DoF) allows bypassing the\nproblem of other dual models which are sometimes unable to correctly reproduce\nthe expressions for those thermophysical quantities showing deviations due to\nthe activation or deactivation of internal DoF. The interpretation of the\nrelaxation times is given, their Order of Magnitude calculated and the way in\nwhich these times are involved in the different phases of the collective\ndynamics of liquids is discussed. A comparison is provided with results\nobtained in the frame of another phononic model of liquids, as well as with the\npredictions for the viscoelastic transition regions and with systems exhibiting\nkgap. In the last part of the paper, theoretical insights and experiments are\nsuggested as potential directions for future research and development.",
        "Heusler alloys, renowned for their multifunctionality and capacity for vast\nelemental customization, are primarily classified into half-Heusler (XYZ) and\nfull-Heusler (X2YZ) structural types. Typically, the 18-electron half-Heusler\nand the 24-electron full-Heusler alloys are recognized as semiconductors,\nfollowing the Slater-Pauling rule. Semiconductors are desired for many\napplications, but they represent a minor portion compared to the predominantly\nmetallic and half-metallic members of the Heusler family. To broaden the scope\nof Heusler semiconductors, advancements have been made in developing variants\nsuch as double-half Heuslers XX'Y2Z2 and quaternary full Heuslers XX'YZ, which\nincorporate four constituent elements. Recently, vacancy-filling\noff-stoichiometric Heuslers of ternary X1+bYZ (0 <= b <= 1) and quaternary\nXaX'bYZ (1 <= a + b <= 2) have emerged as a more versatile strategy. However,\nthe flexibility associated with off-stoichiometry inevitably leads to\ncomplications, including issues with fractional filling ratios and complex site\noccupations. This work presents a comprehensive landscape of\ntransition-metal-containing Heusler semiconductors, focusing on the\noff-stoichiometric Heuslers but seamlessly encompassing the\ninteger-stoichiometric systems. The structural and electronic properties can be\ntheoretically understood through a few simple rules. Many systems have been\nexperimentally validated, showcasing their potential for applications such as\nthermoelectric converters.",
        "Quantum simulations of lattice gauge theories offer the potential to directly\nstudy the non-perturbative dynamics of quantum chromodynamics, but naive\nanalyses suggest that they require large computational resources. Large $N_c$\nexpansions are performed to order 1\/$N_c$ to simplify the Hamiltonian of pure\nSU($N_c$) lattice gauge theories. A reformulation of the electric basis is\nintroduced with a truncation strategy based on the construction of local Krylov\nsubspaces with plaquette operators. Numerical simulations show that these\ntruncated Hamiltonians are consistent with traditional lattice calculations at\nrelatively small couplings. It is shown that the computational resources\nrequired for quantum simulation of time evolution generated by these\nHamiltonians is 17-19 orders of magnitude smaller than previous approaches.",
        "We investigate the long-time behaviour of solutions of a class of\nsingular-degenerate porous medium type equations in bounded Lipschitz domains\nwith mixed Dirichlet-Neumann boundary conditions. The existence of global\nattractors is shown under very general assumptions. Assuming, in addition, that\nsolutions are globally H\\\"older continuous and the reaction terms satisfy a\nsuitable sign condition in the vicinity of the degeneracy, we also prove the\nexistence of an exponential attractor, which, in turn, yields the finite\nfractal dimension of the global attractor. Moreover, we extend the results for\nscalar equations to systems where the degenerate equation is coupled to a\nsemilinear reaction-diffusion equation. The study of such systems is motivated\nby models for biofilm growth.",
        "The Parker conjecture, which explores whether magnetic fields in perfectly\nconducting plasmas can develop tangential discontinuities during magnetic\nrelaxation, remains an open question in astrophysics. Helicity conservation\nprovides a topological barrier during relaxation, preventing topologically\nnontrivial initial data relaxing to trivial solutions; preserving this\nmechanism discretely over long time periods is therefore crucial for numerical\nsimulation. This work presents an energy- and helicity-preserving finite\nelement discretization for the magneto-frictional system, for investigating the\nParker conjecture. The algorithm preserves a discrete version of the\ntopological barrier and a discrete Arnold inequality. We also discuss\nextensions to domains with nontrivial topology."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Multiscale modeling and simulation of brain blood flow",
    "start_abstract":"The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks"
      ],
      "abstract":[
        "Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance",
        "A Physics-Informed Blur Learning Framework for Imaging Systems",
        "Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis\n  from Japanese Haiku",
        "From Poses to Identity: Training-Free Person Re-Identification via\n  Feature Centralization",
        "Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation",
        "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks",
        "SFDLA: Source-Free Document Layout Analysis",
        "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing",
        "AdaFV: Rethinking of Visual-Language alignment for VLM acceleration",
        "Towards Interactive Deepfake Analysis",
        "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization",
        "A Label-Free High-Precision Residual Moveout Picking Method for Travel\n  Time Tomography based on Deep Learning",
        "USegMix: Unsupervised Segment Mix for Efficient Data Augmentation in\n  Pathology Images",
        "Behaviour of Newton Polygon over polynomial composition",
        "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers",
        "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine\n  Learning Techniques",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "Avoiding spurious sharpness minimization broadens applicability of SAM",
        "Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on\n  African American English",
        "MuJoCo Playground",
        "Adaptive Mesh Refinement for Variational Inequalities",
        "Efficient and Universal Neural-Network Decoder for Stabilizer-Based\n  Quantum Error Correction",
        "A View of the Certainty-Equivalence Method for PAC RL as an Application\n  of the Trajectory Tree Method",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Agentic AI Systems Applied to tasks in Financial Services: Modeling and\n  model risk management crews",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Balanced Rate-Distortion Optimization in Learned Image Compression",
        "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation"
      ],
      "abstract":[
        "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens.",
        "Accurate blur estimation is essential for high-performance imaging across\nvarious applications. Blur is typically represented by the point spread\nfunction (PSF). In this paper, we propose a physics-informed PSF learning\nframework for imaging systems, consisting of a simple calibration followed by a\nlearning process. Our framework could achieve both high accuracy and universal\napplicability. Inspired by the Seidel PSF model for representing spatially\nvarying PSF, we identify its limitations in optimization and introduce a novel\nwavefront-based PSF model accompanied by an optimization strategy, both\nreducing optimization complexity and improving estimation accuracy. Moreover,\nour wavefront-based PSF model is independent of lens parameters, eliminate the\nneed for prior knowledge of the lens. To validate our approach, we compare it\nwith recent PSF estimation methods (Degradation Transfer and Fast Two-step)\nthrough a deblurring task, where all the estimated PSFs are used to train\nstate-of-the-art deblurring algorithms. Our approach demonstrates improvements\nin image quality in simulation and also showcases noticeable visual quality\nimprovements on real captured images.",
        "In the era of the metaverse, where immersive technologies redefine human\nexperiences, translating abstract literary concepts into navigable 3D\nenvironments presents a fundamental challenge in preserving semantic and\nemotional fidelity. This research introduces HaikuVerse, a novel framework for\ntransforming poetic abstraction into spatial representation, with Japanese\nHaiku serving as an ideal test case due to its sophisticated encapsulation of\nprofound emotions and imagery within minimal text. While existing text-to-3D\nmethods struggle with nuanced interpretations, we present a literary-guided\napproach that synergizes traditional poetry analysis with advanced generative\ntechnologies. Our framework centers on two key innovations: (1) Hierarchical\nLiterary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both\nexplicit imagery and implicit emotional resonance through structured semantic\ndecomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage\npipeline that systematically transforms poetic elements into coherent 3D scenes\nthrough sequential diffusion processes, geometric optimization, and real-time\nenhancement. Extensive experiments demonstrate that HaikuVerse significantly\noutperforms conventional text-to-3D approaches in both literary fidelity and\nvisual quality, establishing a new paradigm for preserving cultural heritage in\nimmersive digital spaces. Project website at:\nhttps:\/\/syllables-to-scenes.github.io\/",
        "Person re-identification (ReID) aims to extract accurate identity\nrepresentation features. However, during feature extraction, individual samples\nare inevitably affected by noise (background, occlusions, and model\nlimitations). Considering that features from the same identity follow a normal\ndistribution around identity centers after training, we propose a Training-Free\nFeature Centralization ReID framework (Pose2ID) by aggregating the same\nidentity features to reduce individual noise and enhance the stability of\nidentity representation, which preserves the feature's original distribution\nfor following strategies such as re-ranking. Specifically, to obtain samples of\nthe same identity, we introduce two components: Identity-Guided Pedestrian\nGeneration: by leveraging identity features to guide the generation process, we\nobtain high-quality images with diverse poses, ensuring identity consistency\neven in complex scenarios such as infrared, and occlusion. Neighbor Feature\nCentralization: it explores each sample's potential positive samples from its\nneighborhood. Experiments demonstrate that our generative model exhibits strong\ngeneralization capabilities and maintains high identity consistency. With the\nFeature Centralization framework, we achieve impressive performance even with\nan ImageNet pre-trained model without ReID training, reaching mAP\/Rank-1 of\n52.81\/78.92 on Market1501. Moreover, our method sets new state-of-the-art\nresults across standard, cross-modality, and occluded ReID tasks, showcasing\nstrong adaptability.",
        "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.",
        "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps:\/\/github.com\/gosha20777\/cmKAN",
        "Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps:\/\/github.com\/s3setewe\/sfdla-DLAdapter.",
        "In this paper, we introduce CineBrain, the first large-scale dataset\nfeaturing simultaneous EEG and fMRI recordings during dynamic audiovisual\nstimulation. Recognizing the complementary strengths of EEG's high temporal\nresolution and fMRI's deep-brain spatial coverage, CineBrain provides\napproximately six hours of narrative-driven content from the popular television\nseries The Big Bang Theory for each of six participants. Building upon this\nunique dataset, we propose CineSync, an innovative multimodal decoding\nframework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural\nLatent Decoder. Our approach effectively fuses EEG and fMRI signals,\nsignificantly improving the reconstruction quality of complex audiovisual\nstimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a\ncomprehensive evaluation protocol that assesses reconstructions across semantic\nand perceptual dimensions. Experimental results demonstrate that CineSync\nachieves state-of-the-art video reconstruction performance and highlight our\ninitial success in combining fMRI and EEG for reconstructing both video and\naudio stimuli. Project Page: https:\/\/jianxgao.github.io\/CineBrain.",
        "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.",
        "Existing deepfake analysis methods are primarily based on discriminative\nmodels, which significantly limit their application scenarios. This paper aims\nto explore interactive deepfake analysis by performing instruction tuning on\nmulti-modal large language models (MLLMs). This will face challenges such as\nthe lack of datasets and benchmarks, and low training efficiency. To address\nthese issues, we introduce (1) a GPT-assisted data construction process\nresulting in an instruction-following dataset called DFA-Instruct, (2) a\nbenchmark named DFA-Bench, designed to comprehensively evaluate the\ncapabilities of MLLMs in deepfake detection, deepfake classification, and\nartifact description, and (3) construct an interactive deepfake analysis system\ncalled DFA-GPT, as a strong baseline for the community, with the Low-Rank\nAdaptation (LoRA) module. The dataset and code will be made available at\nhttps:\/\/github.com\/lxq1000\/DFA-Instruct to facilitate further research.",
        "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https:\/\/anonymous.4open.science\/r\/EHNet.",
        "Residual moveout (RMO) provides critical information for travel time\ntomography. The current industry-standard method for fitting RMO involves\nscanning high-order polynomial equations. However, this analytical approach\ndoes not accurately capture local saltation, leading to low iteration\nefficiency in tomographic inversion. Supervised learning-based image\nsegmentation methods for picking can effectively capture local variations;\nhowever, they encounter challenges such as a scarcity of reliable training\nsamples and the high complexity of post-processing. To address these issues,\nthis study proposes a deep learning-based cascade picking method. It\ndistinguishes accurate and robust RMOs using a segmentation network and a\npost-processing technique based on trend regression. Additionally, a data\nsynthesis method is introduced, enabling the segmentation network to be trained\non synthetic datasets for effective picking in field data. Furthermore, a set\nof metrics is proposed to quantify the quality of automatically picked RMOs.\nExperimental results based on both model and real data demonstrate that,\ncompared to semblance-based methods, our approach achieves greater picking\ndensity and accuracy.",
        "In computational pathology, researchers often face challenges due to the\nscarcity of labeled pathology datasets. Data augmentation emerges as a crucial\ntechnique to mitigate this limitation. In this study, we introduce an efficient\ndata augmentation method for pathology images, called USegMix. Given a set of\npathology images, the proposed method generates a new, synthetic image in two\nphases. In the first phase, USegMix constructs a pool of tissue segments in an\nautomated and unsupervised manner using superpixels and the Segment Anything\nModel (SAM). In the second phase, USegMix selects a candidate segment in a\ntarget image, replaces it with a similar segment from the segment pool, and\nblends them by using a pre-trained diffusion model. In this way, USegMix can\ngenerate diverse and realistic pathology images. We rigorously evaluate the\neffectiveness of USegMix on two pathology image datasets of colorectal and\nprostate cancers. The results demonstrate improvements in cancer classification\nperformance, underscoring the substantial potential of USegMix for pathology\nimage analysis.",
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.",
        "Fire safety practices are important to reduce the extent of destruction\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\nthe increasing number of false alarms. This paper presents a precise and\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\ndensity, computes weights according to the high and low-density regions,\nforwards the high region weights to KNN and low region weights to XGBoost and\ncombines the predictions. The proposed model is effective at reducing response\ntime, increasing fire safety, and minimizing the damage that fires cause. A\nspecifically designed dataset for smoke detection is utilized to test the\nproposed model. In addition, a variety of ML models, such as Logistic\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\npaper presents a concise summary of the comprehensive findings obtained by\ncomparing the outcomes of all models.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https:\/\/github.com\/Runtaozhou\/dialect_bias_eval.",
        "We introduce MuJoCo Playground, a fully open-source framework for robot\nlearning built with MJX, with the express goal of streamlining simulation,\ntraining, and sim-to-real transfer onto robots. With a simple \"pip install\nplayground\", researchers can train policies in minutes on a single GPU.\nPlayground supports diverse robotic platforms, including quadrupeds, humanoids,\ndexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from\nboth state and pixel inputs. This is achieved through an integrated stack\ncomprising a physics engine, batch renderer, and training environments. Along\nwith video results, the entire framework is freely available at\nplayground.mujoco.org",
        "Variational inequalities play a pivotal role in a wide array of scientific\nand engineering applications. This project presents two techniques for adaptive\nmesh refinement (AMR) in the context of variational inequalities, with a\nspecific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic\nSmoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal\nactive set indicator function as the initial iterate to a time-dependent heat\nequation problem. Solving a single step of this problem has the effect of\nsmoothing the indicator about the free boundary. We threshold this smoothed\nindicator function to identify elements near the free boundary. Key parameters\nsuch as timestep and threshold values significantly influence the efficacy of\nthis method.\n  The second strategy, UDO, focuses on the discrete identification of elements\nadjacent to the free boundary, employing a graph-based approach to mark\nneighboring elements for refinement. This technique resembles the dilation\nmorphological operation in image processing, but tailored for unstructured\nmeshes.\n  We also examine the theory of variational inequalities, the convergence\nbehavior of finite element solutions, and implementation in the Firedrake\nfinite element library. Convergence analysis reveals that accurate free\nboundary estimation is pivotal for solver performance. Numerical experiments\ndemonstrate the effectiveness of the proposed methods in dynamically enhancing\nmesh resolution around free boundaries, thereby improving the convergence rates\nand computational efficiency of variational inequality solvers. Our approach\nintegrates seamlessly with existing Firedrake numerical solvers, and it is\npromising for solving more complex free boundary problems.",
        "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
        "Reinforcement learning (RL) enables an agent interacting with an unknown MDP\n$M$ to optimise its behaviour by observing transitions sampled from $M$. A\nnatural entity that emerges in the agent's reasoning is $\\widehat{M}$, the\nmaximum likelihood estimate of $M$ based on the observed transitions. The\nwell-known \\textit{certainty-equivalence} method (CEM) dictates that the agent\nupdate its behaviour to $\\widehat{\\pi}$, which is an optimal policy for\n$\\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy\nminimax-optimal sample complexity in some regions of the parameter space for\nPAC RL with a generative model~\\citep{Agarwal2020GenModel}.\n  A seemingly unrelated algorithm is the ``trajectory tree method''\n(TTM)~\\citep{Kearns+MN:1999}, originally developed for efficient decision-time\nplanning in large POMDPs. This paper presents a theoretical investigation that\nstems from the surprising finding that CEM may indeed be viewed as an\napplication of TTM. The qualitative benefits of this view are (1) new and\nsimple proofs of sample complexity upper bounds for CEM, in fact under a (2)\nweaker assumption on the rewards than is prevalent in the current literature.\nOur analysis applies to both non-stationary and stationary MDPs.\nQuantitatively, we obtain (3) improvements in the sample-complexity upper\nbounds for CEM both for non-stationary and stationary MDPs, in the regime that\nthe ``mistake probability'' $\\delta$ is small. Additionally, we show (4) a\nlower bound on the sample complexity for finite-horizon MDPs, which establishes\nthe minimax-optimality of our upper bound for non-stationary MDPs in the\nsmall-$\\delta$ regime.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews that can effectively collaborate to perform complex\nmodeling and model risk management (MRM) tasks. The modeling crew consists of a\nmanager and multiple agents who perform specific tasks such as exploratory data\nanalysis, feature engineering, model selection, hyperparameter tuning, model\ntraining, model evaluation, and writing documentation. The MRM crew consists of\na manager along with specialized agents who perform tasks such as checking\ncompliance of modeling documentation, model replication, conceptual soundness,\nanalysis of outcomes, and writing documentation. We demonstrate the\neffectiveness and robustness of modeling and MRM crews by presenting a series\nof numerical examples applied to credit card fraud detection, credit card\napproval, and portfolio credit risk modeling datasets.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Learned image compression (LIC) using deep learning architectures has seen\nsignificant advancements, yet standard rate-distortion (R-D) optimization often\nencounters imbalanced updates due to diverse gradients of the rate and\ndistortion objectives. This imbalance can lead to suboptimal optimization,\nwhere one objective dominates, thereby reducing overall compression efficiency.\nTo address this challenge, we reformulate R-D optimization as a multi-objective\noptimization (MOO) problem and introduce two balanced R-D optimization\nstrategies that adaptively adjust gradient updates to achieve more equitable\nimprovements in both rate and distortion. The first proposed strategy utilizes\na coarse-to-fine gradient descent approach along standard R-D optimization\ntrajectories, making it particularly suitable for training LIC models from\nscratch. The second proposed strategy analytically addresses the reformulated\noptimization as a quadratic programming problem with an equality constraint,\nwhich is ideal for fine-tuning existing models. Experimental results\ndemonstrate that both proposed methods enhance the R-D performance of LIC\nmodels, achieving around a 2\\% BD-Rate reduction with acceptable additional\ntraining cost, leading to a more balanced and efficient optimization process.\nCode will be available at https:\/\/gitlab.com\/viper-purdue\/Balanced-RD.",
        "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https:\/\/github.com\/Zhenxuan-Zhang\/GEMA_score."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks",
    "start_abstract":"Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Multiscale modeling and simulation of brain blood flow"
      ],
      "abstract":[
        "The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Artificial Neural Networks for Magnetoencephalography: A review of an\n  emerging field",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Fundamental Trade-off Between Computation and Communication in Private\n  Coded Distributed Computing",
        "Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of\n  Market Information",
        "Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning",
        "Completely Integrable Foliations: Singular Locus, Invariant Curves and\n  Topological Counterparts",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Efficient Framework for Solving Plasma Waves with Arbitrary\n  Distributions",
        "Scalable skewed Bayesian inference for latent Gaussian models",
        "Signal-to-noise ratio aware minimax analysis of sparse linear regression",
        "Unveiling the Dynamics and Genesis of Small-scale Fine Structure Loops\n  in the Lower Solar Atmosphere",
        "Data mining the functional architecture of the brain's circuitry",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Single spin asymmetry in forward $pA$ collisions from Pomeron-Odderon\n  interference",
        "Keldysh field theory approach to direct electric and thermoelectric\n  currents in quantum dots coupled to superconducting leads",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Alpha-element abundance patterns in star-forming regions of the local\n  Universe"
      ],
      "abstract":[
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that\nmeasures the intricate brain dynamics underlying cognitive processes with an\nunparalleled combination of high temporal and spatial precision. MEG data\nanalytics has always relied on advanced signal processing and mathematical and\nstatistical tools for various tasks ranging from data cleaning to probing the\nsignals' rich dynamics and estimating the neural sources underlying the\nsurface-level recordings. Like in most domains, the surge in Artificial\nIntelligence (AI) has led to the increased use of Machine Learning (ML) methods\nfor MEG data classification. More recently, an emerging trend in this field is\nusing Artificial Neural Networks (ANNs) to address many MEG-related tasks. This\nreview provides a comprehensive overview of how ANNs are being used with MEG\ndata from three vantage points: First, we review work that employs ANNs for MEG\nsignal classification, i.e., for brain decoding. Second, we report on work that\nhas used ANNs as putative models of information processing in the human brain.\nFinally, we examine studies that use ANNs as techniques to tackle\nmethodological questions in MEG, including artifact correction and source\nestimation. Furthermore, we assess the current strengths and limitations of\nusing ANNs with MEG and discuss future challenges and opportunities in this\nfield. Finally, by establishing a detailed portrait of the field and providing\npractical recommendations for the future, this review seeks to provide a\nhelpful reference for both seasoned MEG researchers and newcomers to the field\nwho are interested in using ANNs to enhance the exploration of the complex\ndynamics of the human brain with MEG.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Distributed computing enables scalable machine learning by distributing tasks\nacross multiple nodes, but ensuring privacy in such systems remains a\nchallenge. This paper introduces a private coded distributed computing model\nthat integrates privacy constraints to keep task assignments hidden. By\nleveraging placement delivery arrays (PDAs), we design an extended PDA\nframework to characterize achievable computation and communication loads under\nprivacy constraints. By constructing two classes of extended PDAs, we explore\nthe trade-offs between computation and communication, showing that although\nprivacy increases communication overhead, it can be significantly alleviated\nthrough optimized PDA-based coded strategies.",
        "We develop a portfolio allocation framework that leverages deep learning\ntechniques to address challenges arising from high-dimensional, non-stationary,\nand low-signal-to-noise market information. Our approach includes a dynamic\nembedding method that reduces the non-stationary, high-dimensional state space\ninto a lower-dimensional representation. We design a reinforcement learning\n(RL) framework that integrates generative autoencoders and online meta-learning\nto dynamically embed market information, enabling the RL agent to focus on the\nmost impactful parts of the state space for portfolio allocation decisions.\nEmpirical analysis based on the top 500 U.S. stocks demonstrates that our\nframework outperforms common portfolio benchmarks and the predict-then-optimize\n(PTO) approach using machine learning, particularly during periods of market\nstress. Traditional factor models do not fully explain this superior\nperformance. The framework's ability to time volatility reduces its market\nexposure during turbulent times. Ablation studies confirm the robustness of\nthis performance across various reinforcement learning algorithms.\nAdditionally, the embedding and meta-learning techniques effectively manage the\ncomplexities of high-dimensional, noisy, and non-stationary financial data,\nenhancing both portfolio performance and risk management.",
        "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10\/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
        "We study codimension $q \\geq 2$ holomorphic foliations defined in a\nneighborhood of a point $P$ of a complex manifold that are completely\nintegrable, i.e. with $q$ independent meromorphic first integrals. We show that\neither $P$ is a regular point, a non-isolated singularity or there are\ninfinitely many invariant analytic varieties through $P$ of the same dimension\nas the foliation, the so called separatrices. Moreover, we see that this\nphenomenon is of topological nature.\n  Indeed, we introduce topological counterparts of completely integrable local\nholomorphic foliations and tools, specially the concept of total holonomy\ngroup, to build holomorphic first integrals if they have isolated separatrices.\nAs a result, we provide a topological characterization of completely integrable\nnon-degenerated elementary isolated singularities of vector fields with an\nisolated separatrix.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "Plasma, which constitutes 99\\% of the visible matter in the universe, is\ncharacterized by a wide range of waves and instabilities that play a pivotal\nrole in space physics, astrophysics, laser-plasma interactions, fusion\nresearch, and laboratory experiments. The linear physics of these phenomena is\ndescribed by kinetic dispersion relations (KDR). However, solving KDRs for\narbitrary velocity distributions remains a significant challenge, particularly\nfor non-Maxwellian distributions frequently observed in various plasma\nenvironments. This work introduces a novel, efficient, and unified numerical\nframework to address this challenge. The proposed method rapidly and accurately\nyields all significant solutions of KDRs for nearly arbitrary velocity\ndistributions, supporting both unstable and damped modes across all frequencies\nand wavevectors. The approach expands plasma species' velocity distribution\nfunctions using a series of carefully chosen orthogonal basis functions and\nemploys a highly accurate rational approximation to transform the problem into\nan equivalent matrix eigenvalue problem, eliminating the need for initial\nguesses. The efficiency and versatility of this framework are demonstrated,\nenabling simplified studies of plasma waves with arbitrary distributions. This\nadvancement paves the way for uncovering new physics in natural plasma\nenvironments, such as spacecraft observations in space plasmas, and\napplications like wave heating in fusion research.",
        "Approximate Bayesian inference for the class of latent Gaussian models can be\nachieved efficiently with integrated nested Laplace approximations (INLA).\nBased on recent reformulations in the INLA methodology, we propose a further\nextension that is necessary in some cases like heavy-tailed likelihoods or\nbinary regression with imbalanced data. This extension formulates a skewed\nversion of the Laplace method such that some marginals are skewed and some are\nkept Gaussian while the dependence is maintained with the Gaussian copula from\nthe Laplace method. Our approach is formulated to be scalable in model and data\nsize, using a variational inferential framework enveloped in INLA. We\nillustrate the necessity and performance using simulated cases, as well as a\ncase study of a rare disease where class imbalance is naturally present.",
        "We consider parameter estimation under sparse linear regression -- an\nextensively studied problem in high-dimensional statistics and compressed\nsensing. While the minimax framework has been one of the most fundamental\napproaches for studying statistical optimality in this problem, we identify two\nimportant issues that the existing minimax analyses face: (i) The\nsignal-to-noise ratio appears to have no effect on the minimax optimality,\nwhile it shows a major impact in numerical simulations. (ii) Estimators such as\nbest subset selection and Lasso are shown to be minimax optimal, yet they\nexhibit significantly different performances in simulations. In this paper, we\ntackle the two issues by employing a minimax framework that accounts for\nvariations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax\nframework. We adopt a delicate higher-order asymptotic analysis technique to\nobtain the SNR-aware minimax risk. Our theoretical findings determine three\ndistinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax\noptimal estimators exhibit markedly different behaviors. The new theory not\nonly offers much better elaborations for empirical results, but also brings new\ninsights to the estimation of sparse signals in noisy data.",
        "Recent high-resolution solar observations have unveiled the presence of\nsmall-scale loop-like structures in the lower solar atmosphere, often referred\nto as unresolved fine structures, low-lying loops, and miniature hot loops.\nThese structures undergo rapid changes within minutes, and their formation\nmechanism has remained elusive. In this study, we conducted a comprehensive\nanalysis of two small loops utilizing data from the Interface Region Imaging\nSpectrograph (IRIS), the Goode Solar Telescope (GST) at Big Bear Solar\nObservatory, and the Atmospheric Imaging Assembly (AIA) and the Helioseismic\nMagnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), aiming to\nelucidate the underlying process behind their formation. The GST observations\nrevealed that these loops, with lengths of $\\sim$3.5 Mm and heights of $\\sim$1\nMm, manifest as bright emission structures in H$\\alpha$ wing images,\nparticularly prominent in the red wing. IRIS observations showcased these loops\nin 1330 angstrom slit-jaw images, with TR and chromospheric line spectra\nexhibiting significant enhancement and broadening above the loops, indicative\nof plasmoid-mediated reconnection during their formation. Additionally, we\nobserved upward-erupting jets above these loops across various passbands.\nFurthermore, differential emission measurement analysis reveals an enhanced\nemission measure at the location of these loops, suggesting the presence of\nplasma exceeding 1 MK. Based on our observations, we propose that these loops\nand associated jets align with the minifilament eruption model. Our findings\nsuggest a unified mechanism governing the formation of small-scale loops and\njets akin to larger-scale X-ray jets.",
        "The brain is a highly complex organ consisting of a myriad of subsystems that\nflexibly interact and adapt over time and context to enable perception,\ncognition, and behavior. Understanding the multi-scale nature of the brain,\ni.e., how circuit- and moleclular-level interactions build up the fundamental\ncomponents of brain function, holds incredible potential for developing\ninterventions for neurodegenerative and psychiatric diseases, as well as open\nnew understanding into our very nature. Historically technological limitations\nhave forced systems neuroscience to be local in anatomy (localized, small\nneural populations in single brain areas), in behavior (studying single tasks),\nin time (focusing on specific stages of learning or development), and in\nmodality (focusing on imaging single biological quantities). New developments\nin neural recording technology and behavioral monitoring now provide the data\nneeded to break free of local neuroscience to global neuroscience: i.e.,\nunderstanding how the brain's many subsystem interact, adapt, and change across\nthe multitude of behaviors animals and humans must perform to thrive.\nSpecifically, while we have much knowledge of the anatomical architecture of\nthe brain (i.e., the hardware), we finally are approaching the data needed to\nfind the functional architecture and discover the fundamental properties of the\nsoftware that runs on the hardware. We must take this opportunity to bridge\nbetween the vast amounts of data to discover this functional architecture which\nwill face numerous challenges from low-level data alignment up to high level\nquestions of interpretable mathematical models of behavior that can synthesize\nthe myriad of datasets together.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Working in the hybrid framework of the high energy $pA$ collisions we\nidentify a new contribution to transverse single spin asymmetry (SSA). The\nphase necessary for the SSA is provided by the Pomeron-Odderon interference in\nthe dense nuclear target. The complete formula for the $pA \\to h X$ polarized\ncross section also contains the transversity distribution for the polarized\nprojectile as well as the real part of the twist-3 fragmentation function. We\nnumerically estimate the asymmetry $A_N$ and its nuclear dependence. Based on a\nmodel computation we find that $A_N$ can be a percent level in the forward and\nlow-$P_{h\\perp}$ region. For large nuclei we find significant suppression, with\n$A_N \\propto A^{-7\/6}$ parametrically. As a notable feature we find a node of\n$A_N$ as a function of the $P_{h\\perp}$ around the values of the initial\nsaturation scale that could be used to test this mechanism experimentally.",
        "We study the transport properties of a quantum dot contacted to two\nsuperconducting reservoirs by means of the Keldysh field theory approach. We\ndetermine the direct current occurring at equilibrium and the electric and\nthermoelectric currents triggered when the system is driven out of equilibrium\nby a voltage or a temperature bias, also for a normal-quantum\ndot-superconductor junction. In particular, we derive and present for the first\ntime the explicit expression of the thermoelectric current in a\nsuperconductor-quantum dot-superconductor junction for any values of the\ntemperature difference between the superconducting leads. We show that in the\nlinear response regime, in addition to the Josephson current, a weakly\nphase-dependent thermoelectric contribution occurs, providing that\nelectron-hole symmetry is broken. Far from linearity, instead, other\ncontributions arise which lead to thermoelectric effects, dominant at weak\ncoupling, also in the presence of particle-hole symmetry.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "(Abridged) We reassess the alpha-element abundance ratios (Ne\/O, S\/O, Ar\/O)\nwith respect to metallicity in ~1000 spectra of Galactic and extragalactic HII\nregions and star-forming galaxies (SFGs) of the local Universe. Using the DEep\nSpectra of Ionised REgions Database (DESIRED) Extended project (DESIRED-E),\nwhich includes spectra with direct electron temperature determinations, we\nhomogeneously derive physical conditions and chemical abundances for all\nobjects. Various ionisation correction factor (ICF) schemes are analyzed for\nNe, S, and Ar to identify the most reliable abundance estimates. Our findings\nindicate that the ICF scheme by Izotov et al. (2006) better reproduces the\nNe\/O, S\/O, and Ar\/O trends. Ne\/O ratios in HII regions display large dispersion\nand no clear dependence on O\/H, suggesting that current ICF(Ne) schemes fail\nfor these objects. However, SFGs show consistent linear relations with slightly\npositive slopes for log(Ne\/O) vs. 12+log(O\/H) or 12+log(Ne\/H), likely\ninfluenced by metallicity-dependent O dust depletion and ICF effects. The\nlog(S\/O) vs. 12+log(O\/H) distribution is largely constant, especially for HII\nregions or combined samples (SFGs + HII regions). Conversely, log(S\/O) vs.\n12+log(S\/H) shows a tight linear fit with a positive slope, flattening at\n12+log(S\/H) < 6.0, suggesting S contributions from SNe Ia. For log(Ar\/O) vs.\n12+log(O\/H), similar trends emerge for HII regions and SFGs, independent of\nionisation degree or ICF(Ar). A slight log(Ar\/O) decrease with increasing\n12+log(O\/H) contrasts with log(Ar\/O) vs. 12+log(Ar\/H), which shows a small\npositive slope, indicating a possible minor Ar contribution from SNe Ia."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Quantum machine learning in chemistry and materials",
    "start_abstract":"Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models.",
    "start_categories":[
      "cs.ET"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b26"
      ],
      "title":[
        "Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble"
      ],
      "abstract":[
        "Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Optimizing Lead-Free Chalcogenide Perovskites for High-Efficiency\n  Photovoltaics via Alloying Strategies",
        "Unraveling phase transformation with phononic hyperbolicity using\n  off-resonant terahertz light",
        "Galvanic molecular intercalation",
        "Quantum simulations of defects near the (0001) surface of\n  $\\alpha$-Al$_2$O$_3$",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys",
        "Modeling solute-grain boundary interactions in a bcc Ti-Mo alloy using\n  density functional theory",
        "Giant non-reciprocal band structure effect in a multiferroic material",
        "Computational Study of Magnetic Behaviour in Ni-Adsorbed Nb2C-OF MXene\n  using Density Functional Theory",
        "Uncertainty Quantification for Misspecified Machine Learned Interatomic\n  Potentials",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "Resolving the sodiation process in hard carbon anodes with nanostructure\n  specific X-ray imaging",
        "Universality for catalytic equations and fully parked trees",
        "A Production Routing Problem with Mobile Inventories",
        "Wikipedia Contributions in the Wake of ChatGPT",
        "Gaussian credible intervals in Bayesian nonparametric estimation of the\n  unseen",
        "Identifying rich clubs in spatiotemporal interaction networks",
        "First-principle based Floquet engineering of solids in the velocity\n  gauge",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Non-reciprocity and multibody interactions in acoustically levitated\n  particle systems: A three body problem",
        "On a reaction-diffusion virus model with general boundary conditions in\n  heterogeneous environments",
        "Schmid-Higgs Mode in the Presence of Pair-Breaking Interactions",
        "Thom polynomials for singularities of maps",
        "$\\texttt{PrecisionLauricella}$: package for numerical computation of\n  Lauricella functions depending on a parameter",
        "Unidentified Aerial Phenomena. Characterization of Dark UAPs",
        "Accumulation of Charge on an Extremal Black Hole's Event Horizon",
        "Expression of special stretched $9j$ coefficients in terms of $_5F_4$\n  hypergeometric series"
      ],
      "abstract":[
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Lead-free chalcogenide perovskites are emerging as game-changers in the race\nfor sustainable, high-performance photovoltaics. These materials offer a\nperfect trifecta: non-toxic elemental composition, exceptional phase stability,\nand outstanding optoelectronic properties. However, unlocking their full\npotential for solar cell applications requires advanced strategies to fine-tune\ntheir electronic and optical behavior. In this study, we take CaHfS$_{3}$-a\npromising but underexplored candidate-and revolutionize its performance by\nintroducing targeted substitutions: Ti at the cation site and Se at the anion\nsite. Using cutting-edge computational techniques, including density functional\ntheory, GW calculations, and the Bethe-Salpeter equation (BSE), we reveal how\nthese substitutions transform the material's properties. Our findings highlight\nthat alloyed compounds such as CaHfS$_{3-x}$Se$_{x}$ and\nCaHf$_{1-y}$Ti$_{y}$X$_{3}$ (X = S, Se) are not only phase-stable but also\nfeature adjustable direct G$_{0}$W$_{0}$@PBE bandgaps (1.29-2.67 eV), reduced\nexciton binding energies, and significantly improved polaron mobility. These\nmodifications enable better light absorption, reduced electron-hole\nrecombination, longer exciton lifetimes, and enhanced quantum yield.\nImpressively, the alloyed perovskites, specifically, for the Ti-rich Se-based\nperovskites, achieve a spectroscopic-limited maximum efficiency of up to\n28.06%, outperforming traditional lead-based halide perovskites. Our results\ndemonstrate that strategic alloying is a powerful tool to supercharge the\noptoelectronic properties of lead-free chalcogenide perovskites, positioning\nthem as strong contenders for next-generation photovoltaic technologies.",
        "Noncontacting and nondestructive control of geometric phase in conventional\nsemiconductors plays a pivotal role in various applications. In the current\nwork, we present a theoretical and computational investigation on terahertz\n(THz) light-induced phase transformation of conventional binary semiconducting\ncompounds among different structures including rock-salt, zinc-blende,\nwurtzite, and hexagonal phases. Using MgS and MgSe as prototypical examples, we\nperform anharmonic phonon mediated calculations and reveal large contrasting\nlattice contributed dielectric susceptibility in the THz regime. We then\nconstruct a THz-induced phase diagram under intermediate temperature and reveal\nrock-salt to hexagonal and then wurtzite structure transformations with\nincreasing light intensity. This does not require a high temperature\nenvironment as observed in traditional experiments. The low energy barrier\nsuggests that the phase transition kinetics can be fast, and the stable room\ntemperature phonon dispersions guarantee their non-volatile nature.\nFurthermore, we disclose the phononic hyperbolicity with strong anisotropic THz\nsusceptibility components, which serves as a natural hyperbolic material with\nnegative refractive index. Our work suggests the potential to realize\nmetastable hidden phases using noninvasive THz irradiation, which expands the\nconventional pressure-temperature ($P-T$) phase diagram by adding light as an\nadditional control factor.",
        "The intercalation of molecular species between the layers of van der Waals\n(vdW) materials has recently emerged as a powerful approach to combine the\nremarkable electronic and magnetic properties of vdW materials with the\nchemical flexibility of organic molecules. However, the full transformative\npotential of molecular intercalation remains underexplored, largely due to the\nlack of simple, broadly applicable methods that preserve high crystalline\nquality down to the few-layer limit. Here, we introduce a simple galvanic\napproach to intercalate different molecules into various vdW materials under\nambient conditions, leveraging the low reduction potential of selected metals\nto enable a spontaneous molecular insertion. We employ our method, which is\nparticularly well-suited for the in-situ intercalation of few-layer-thick\ncrystals, to intercalate nine vdW materials, including magnets and\nsuperconductors, with molecules ranging from conventional alkylammonium ions to\nmetallorganic and bio-inspired chiral cations. Notably, intercalation leads to\na molecule-dependent enhancement of the superconducting transition in 2H-TaS2,\nreaching a critical temperature of 4.7 K, higher than TaS2 monolayers.\nAdditionally, RuCl3 exhibits an unprecedented transition from antiferromagnetic\nto ferrimagnetic ordering upon intercalation with cobaltocenium. These results\nestablish our approach as a versatile technique for engineering atomically thin\nquantum materials and heterostructures, unlocking the transformative effects of\nmolecular intercalation.",
        "Defects in materials are ubiquitous and one of their adverse effects in\n$\\alpha$-Al$_2$O$_3$ is the initiation of corrosion. While this process starts\nnear the surface, the defects involved and their electronic structure need to\nbe elucidated with high accuracy. Since point defects are confined to a small\nspatial region, defect embedding theory allows the definition of an active\nspace, comprising of the defect electronic states, that is coupled to the\nenvironment of the host material. The active space Hamiltonian is of small\nrank, enabling access to its electronic properties using a high-level or even\nexact quantum theory. In this paper we use these techniques and\nfirst-principles simulations to compute the structural and electronic\nproperties of near-surface vacancies for the (0001) surface of\n$\\alpha$-Al$_2$O$_3$, and investigate the influence of defects and hydration on\nthe initiation and propagation of corrosion. We report the defect electronic\nstructure for strongly localized ground and excited states of the surface O\nvacancy and compare results obtained using full configuration interaction and a\nvariational quantum eigensolver on a quantum computer. Error mitigation\ntechniques are explored and shown to reduce the error due to the hardware noise\nto the point where the quantum result agrees with the exact solution within\nchemical accuracy.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors.",
        "Solute segregation in alloys is a key phenomenon which affects various\nmaterial characteristics such as embrittlement, grain growth and precipitation\nkinetics. In this work, the segregation energies of Y, Zr, and Nb to a\n\\textgreek{S}5 grain boundary in a bcc Ti-25 at \\% Mo alloy were determined\nusing density functional theory (DFT) calculations. A systematic approach was\nlaid out by computing the solution energy distributions in the bulk alloy using\nWarren-Cowley short-range order parameters to find a representative bulk-solute\nreference energy. Additionally, different scenarios were considered when a\nsolute atom replaces different sites in terms of their local Ti-Mo chemistry at\nthe GB plane to calculate the distribution of segregation energies. The solute\nsegregation to a Mo site at the GB plane is preferred rather than to a Ti site.\nFurther analysis shows that these segregation energy trends can be rationalized\nbased on a primarily elastic interaction. Thus the segregation energies scale\nwith the solute size such that Y has the largest segregation energies followed\nby Zr and Nb.",
        "Multiferroic materials, characterized by the coexistence of ferroelectricity\nand ferromagnetism, may unveil band structures suggestive of complex phenomena\nand new functionalities. In this Letter, we analyze the band structure of EuO\nin its multiferroic phase. Using density functional theory calculations and\ndetailed symmetry analysis, we reveal a previously overlooked non-reciprocal\nband structure effect, where the electronic energy bands exhibit asymmetry\nalong opposite directions with respect to the special points in the Brillouin\nzone. This effect, which is enabled by spin-orbit coupling, is giant for the\ntop valence Eu $4f$ bands, and can be switched by external electric or magnetic\nfields. Furthermore, this results in an enhanced bulk photovoltaic effect.\nSpecifically, our predictions indicate the emergence of a large injection\ncurrent response to linearly polarized light, resulting in a photoconductivity\nvalue several orders of magnitude higher than that reported in any other oxide\nmaterial. Ultimately, this non-reciprocal band structure effect and the\nassociated large bulk photovoltaic response may be general phenomena emerging\nnot just in EuO but also in other multiferroics or magnetoelectrics,\npotentially providing new cross-functionalities.",
        "Magnetic 2D materials have achieved significantly consideration owing to\ntheir encouraging applications. A variation of these 2D materials by occurrence\nof defects, by the transition-metal doping or adsorption or by the surface\nfunctionalization can initiate both the spin-polarization and magnetic\nproperties in these materials. Density functional theory (DFT) is used to\ndetermine the electric, magnetic properties along with the electronic\nstructures and stability of synthesized two-dimensional materials. This work\ndescribes the magnetic properties of Ni-ad-Nb2C-OF MXene. The study focuses on\nthe computational approach based first principal calculation providing insight\nonto the magnetic properties of adsorbed compound and comparing it with\npristine Nb2C-OF MXene. The pristine Nb2C-OF and Ni-ad-Nb2C-OF structures are\nsimulated and optimized using Wien2k software. Using exchange-correlational\nfunctionals; spin-GGA and spin-GGA+U (for Nickel U= 6eV), Ni-ad-Nb2C-OF\nelectronic band structure is found to be metallic having magnetic moment\ncalculated +1.01516{\\mu}_\\b{eta} showing its non-superconducting and\nferromagnetic behaviour. Owing to this magnetic nature, this 2D compound can be\nused for new upcoming applications such as spintronics and nano magnetic data\nstorage devices.",
        "The use of high-dimensional regression techniques from machine learning has\nsignificantly improved the quantitative accuracy of interatomic potentials.\nAtomic simulations can now plausibly target quantitative predictions in a\nvariety of settings, which has brought renewed interest in robust means to\nquantify uncertainties on simulation results. In many practical settings,\nencompassing both classical and a large class of machine learning potentials,\nthe dominant form of uncertainty is currently not due to lack of training data\nbut to misspecification, namely the inability of any one choice of model\nparameters to exactly match all ab initio training data. However, Bayesian\ninference, the most common formal tool used to quantify uncertainty, is known\nto ignore misspecification and thus significantly underestimates parameter\nuncertainties. Here, we employ a recent misspecification-aware regression\ntechnique to quantify parameter uncertainties, which is then propagated to a\nbroad range of phase and defect properties in tungsten via brute force\nresampling or implicit differentiation. The propagated misspecification\nuncertainties robustly envelope errors to direct \\textit{ab initio} calculation\nof material properties outside of the training dataset, an essential\nrequirement for any quantitative multi-scale modeling scheme. Finally, we\ndemonstrate application to recent foundational machine learning interatomic\npotentials, accurately predicting and bounding errors in MACE-MPA-0 energy\npredictions across the diverse materials project database. Perspectives for the\napproach in multiscale simulation workflows are discussed.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "Hard carbons show significant promise as anode materials for sodium-ion\nbatteries. However, monitoring the sodiation process in the hard carbon\nelectrode during cycling and understanding the sodiation mechanism remain\nchallenging. This article reports on operando 2D scanning small- and wide-angle\nX-ray scattering (SWAXS) and ex situ 3D SAXS tomography of hard carbon\nelectrodes during the sodiation process. Structural changes are monitored with\nspatial and temporal resolution during the electrochemical process and shows\nthat sodiation through micropore filling is the more dominating mechanism in\nthe later stages of sodiation, i.e. in the plateau region of the voltage\nprofile, while intercalation occurs continuously. Spatial inhomogeneities are\nresolved over the electrode and reveal an increased level of inhomogeneity at\nhigher degree of sodiation with regions of different degrees of micropore\nfilling. Resolving the processes spatially enables us to correlate plating,\nstarting from the interface between the electrode and the current collector, to\na higher degree of micropore filling. The work demonstrates how SWAXS imaging\ncan contribute to understanding the sodiation of hard carbon anodes, not only\nby spatially resolved analysis, but also as a method to decouple contributions\nfrom different components in a cell, enabling more accurate scattering analysis\nin in situ environments.",
        "We show that critical parking trees conditioned to be fully parked converge\nin the scaling limits towards the Brownian growth-fragmentation tree, a\nself-similar Markov tree different from Aldous' Brownian tree recently\nintroduced and studied by Bertoin, Curien and Riera. As a by-product of our\nstudy, we prove that positive non-linear polynomial equations involving a\ncatalytic variable display a universal polynomial exponent $5\/2$ at their\nsingularity, confirming a conjecture by Chapuy, Schaeffer and Drmota & Hainzl.\nCompared to previous analytical works on the subject, our approach is\nprobabilistic and exploits an underlying random walk hidden in the random tree\nmodel.",
        "Hydrogen is an energy vector, and one possible way to reduce CO 2 emissions.\nThis paper focuses on a hydrogen transport problem where mobile storage units\nare moved by trucks between sources to be refilled and destinations to meet\ndemands, involving swap operations upon arrival. This contrasts with existing\nliterature where inventories remain stationary. The objective is to optimize\ndaily routing and refilling schedules of the mobile storages. We model the\nproblem as a flow problem on a time-expanded graph, where each node of the\ngraph is indexed by a time-interval and a location and then, we give an\nequivalent Mixed Integer Linear Programming (MILP) formulation of the problem.\nFor small to medium-sized instances, this formulation can be efficiently solved\nusing standard MILP solvers. However, for larger instances, the computational\ncomplexity increases significantly due to the highly combinatorial nature of\nthe refilling process at the sources. To address this challenge, we propose a\ntwo-step heuristic that enhances.",
        "How has Wikipedia activity changed for articles with content similar to\nChatGPT following its introduction? We estimate the impact using\ndifferences-in-differences models, with dissimilar Wikipedia articles as a\nbaseline for comparison, to examine how changes in voluntary knowledge\ncontributions and information-seeking behavior differ by article content. Our\nanalysis reveals that newly created, popular articles whose content overlaps\nwith ChatGPT 3.5 saw a greater decline in editing and viewership after the\nNovember 2022 launch of ChatGPT than dissimilar articles did. These findings\nindicate heterogeneous substitution effects, where users selectively engage\nless with existing platforms when AI provides comparable content. This points\nto potential uneven impacts on the future of human-driven online knowledge\ncontributions.",
        "The unseen-species problem assumes $n\\geq1$ samples from a population of\nindividuals belonging to different species, possibly infinite, and calls for\nestimating the number $K_{n,m}$ of hitherto unseen species that would be\nobserved if $m\\geq1$ new samples were collected from the same population. This\nis a long-standing problem in statistics, which has gained renewed relevance in\nbiological and physical sciences, particularly in settings with large values of\n$n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the\nunseen-species problem under the Pitman-Yor prior, and propose a novel\nmethodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$,\nfor any $n\\geq1$. By leveraging a Gaussian central limit theorem for the\nposterior distribution of $K_{n,m}$, our method improves upon competitors in\ntwo key aspects: firstly, it enables the full parameterization of the\nPitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need\nof Monte Carlo sampling, enhancing computational efficiency. We validate the\nproposed method on synthetic and real data, demonstrating that it improves the\nempirical performance of competitors by significantly narrowing the gap between\nasymptotic and exact credible intervals for any $m\\geq1$.",
        "Spatial networks are widely used in various fields to represent and analyze\ninteractions or relationships between locations or spatially distributed\nentities.There is a network science concept known as the 'rich club'\nphenomenon, which describes the tendency of 'rich' nodes to form densely\ninterconnected sub-networks. Although there are established methods to quantify\ntopological, weighted, and temporal rich clubs individually, there is limited\nresearch on measuring the rich club effect in spatially-weighted temporal\nnetworks, which could be particularly useful for studying dynamic spatial\ninteraction networks. To address this gap, we introduce the spatially-weighted\ntemporal rich club (WTRC), a metric that quantifies the strength and\nconsistency of connections between rich nodes in a spatiotemporal network.\nAdditionally, we present a unified rich club framework that distinguishes the\nWTRC effect from other rich club effects, providing a way to measure\ntopological, weighted, and temporal rich club effects together. Through two\ncase studies of human mobility networks at different spatial scales, we\ndemonstrate how the WTRC is able to identify significant weighted temporal rich\nclub effects, whereas the unweighted equivalent in the same network either\nfails to detect a rich club effect or inaccurately estimates its significance.\nIn each case study, we explore the spatial layout and temporal variations\nrevealed by the WTRC analysis, showcasing its particular value in studying\nspatiotemporal interaction networks. This research offers new insights into the\nstudy of spatiotemporal networks, with critical implications for applications\nsuch as transportation, redistricting, and epidemiology.",
        "We introduce a practical and accurate strategy to capture light-matter\ninteractions using the Floquet formalism in the velocity gauge in combination\nwith realistic first-principle models of solids. The velocity gauge, defined by\nthe linear coupling to the vector potential, is a standard method to capture\nthe light-matter interaction in solids. However, its use with first-principle\nmodels has been limited by the challenging fact that it requires a large number\nof bands for convergence and its incompatibility with non-local pseudopotential\nplane wave methods. To improve its convergence properties, we explicitly take\ninto account the truncation of Hilbert space in the construction of the Floquet\nHamiltonian in the velocity gauge. To avoid the incompatibility with the\npseudopotentials, we base our computations on generalized tight-binding\nHamiltonians derived from first-principles through maximally-localized Wannier\nfunctions. We exemplify the approach by computing the optical absorption\nspectra of laser-dressed trans-polyacetylene chain using realistic electronic\nstructure. We show that, by proceeding in this way, Floquet consideration\ninvolving the truncated Hilbert spaces reproduces the full basis calculations\nwith only a few bands and with significantly reduced computation time. The\nstrategy has been implemented in FloqticS, a general code for the Floquet\nengineering of the optical properties of materials. Overall, this work\nintroduces a useful theoretical tool to realize Floquet engineering of\nrealistic solids in the velocity gauge.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "In active fluids and active solids the constituents individually generate\nmovement by each extracting energy from their environment or from their own\nsource. Non-reciprocal interactions among these active constituents then enable\nnovel collective behavior that often can be strikingly counterintuitive.\nHowever, non-reciprocity in these cases typically requires that the interacting\nbodies have different physical properties or it needs to be programmed\nexplicitly into all pairwise interactions. Here we show that collective\nactivity in a driven system can emerge spontaneously through multibody\nnonreciprocal forces, even if all bodies are individually non-active and have\nidentical properties. We demonstrate this with as few as three identical\nspheres, acoustically levitated in air, which exhibit collective activity as\nthey interact through non-pairwise forces: similar to the classic gravitational\nthree-body problem, the interaction between two spheres depends sensitively on\nthe relative position of the third sphere. Non-reciprocity arises naturally\nfrom both near-field sound scattering and microstreaming forces among the\nspheres. The underdamped dynamics in air furthermore make it possible to go\nbeyond collective center-of-mass propulsion or rotation and observe internal,\nengine-like reconfigurations that follow limit cycles. These findings open up\nnew possibilities for self-assembly, where now multibody interactions not only\ndetermine the resulting structure but also drive the spontaneously emerging\ndynamics.",
        "To describe the propagation of West Nile virus and\/or Zika virus, in this\npaper, we propose and study a time-periodic reaction-diffusion model with\ngeneral boundary conditions in heterogeneous environments and with four\nunknowns: susceptible host, infectious host, susceptible vector and infectious\nvector. We can prove that such problem has a positive time periodic solution if\nand only if host and vector persist and the basic reproduction ratio is greater\nthan one, and moreover the positive time periodic solution is unique and\nglobally asymptotically stable when it exists.",
        "Collective modes in superconductors provided the first realization of the\nHiggs mechanism. The transverse Goldstone mode acquires a gap (i.e. a mass)\nwhen it hybridizes with the electromagnetic gauge field. The longitudinal\nSchmid-Higgs mode, on the other hand, is always massive. In conventional BCS\ntheory, its gap is exactly $2\\Delta$, coinciding with the excitation threshold\nfor quasiparticles. Being situated right at the edge of the continuum spectrum\nit gives rise to peculiar dynamics for the Schmid-Higgs mode. For instance,\nwhen suddenly excited at $t=0$, it exhibits algebraically decaying oscillations\nof the form $\\sim \\sin(2\\Delta t)\/{t}^{1\/2}$. In this study, we explore the\nbehavior of Schmid-Higgs oscillations in the presence of pair-breaking\nmechanisms, such as magnetic impurities or in-plane magnetic fields. These\nprocesses suppress the quasiparticle excitation threshold down to\n$2\\varepsilon_g < 2\\Delta$, potentially placing the longitudinal mode within\nthe continuum spectrum. Despite this, we show that the algebraically decaying\noscillations persist, taking the form $\\sim \\sin(2\\varepsilon_g t)\/t^2$. The\nSchmid-Higgs mode becomes truly overdamped and exponentially decaying only in\nthe gapless superconductors with $\\varepsilon_g=0$.",
        "This is a gentle introduction to a general theory of universal polynomials\nassociated to classification of map-germs, called Thom polynomials. The theory\nwas originated by Ren\\'e Thom in the 1950s and has since been evolved in\nvarious aspects by many authors. In a nutshell, this is about intersection\ntheory on certain moduli spaces, say `classifying spaces of\nmono\/multi-singularities of maps', which provides consistent and deep insights\ninto both classical and modern enumerative geometry with many potential\napplications.",
        "We introduce the $\\texttt{PrecisionLauricella}$ package, a computational tool\ndeveloped in Wolfram Mathematica for high-precision numerical evaluations of\nLauricella functions with indices linearly dependent on a parameter,\n$\\varepsilon$. The package leverages a method based on analytical continuation\nvia Frobenius generalized power series, providing an efficient and accurate\nalternative to conventional approaches relying on multi-dimensional series\nexpansions or Mellin--Barnes representations. This one-dimensional approach is\nparticularly advantageous for high-precision calculations and facilitates\nfurther optimization through $\\varepsilon$-dependent reconstruction from\nevaluations at specific numerical values, enabling efficient parallelization.\nThe underlying mathematical framework for this method has been detailed in our\nprevious work, while the current paper focuses on the design, implementation,\nand practical applications of the $\\texttt{PrecisionLauricella}$ package.",
        "We use high-tech observations of Unidentified Aerial Phenomena (UAP) class\nobjects to evaluate their characteristics. We present data in three cases. (1)\nMulti-side daytime observations of UAPs over Kiev. (2) Night observations of a\ngroup of objects in the vicinity of the Moon. (3) UAP observations in the\ncombat zone in Ukraine. Dark UAPs in the visible wavelength range are observed\nonly during the day. At night they can only be seen in the infrared wavelength\nrange. We note large sizes of UAPs, from three to six kilometers.They exhibit\nlarge velocities, from 2.5 Mach and much larger. They have low albedo, from\nthree percent and below, that is, they actually exhibit features of a\ncompletely black body.",
        "We numerically analyze the behavior of a charged scalar field on a fixed\nextremal Reissner-Nordstr\\\"om background. We find an extension of the Aretakis\ninstability characterized by an accumulation of charge on the extremal event\nhorizon. In particular, when the charge coupling to the scalar field is\nsufficiently large, the charge density on the horizon asymptotes to a nonzero\nconstant at late times. By constructing monochromatic initial data at the onset\nof charged superradiance, we give evidence supporting the claim that this\ninstability is connected to the presence of a nearly zero-damped mode.\nThroughout this work, we employ a numerical integration scheme in compactified\ndouble-null coordinates, which allows us to capture the asymptotic behavior of\nthe matter at the boundaries of the spacetime.",
        "The Clebsch-Gordan coefficients or Wigner $3j$ symbols are known to be\nproportional to a $_3F_2(1)$ hypergeometric series, and Racah $6j$ coefficients\nto a $_4F_3(1)$. In general, however, non-trivial $9j$ symbols can not be\nexpressed as a $_5F_4$. In this letter, we show, using the Dougall-Ramanujan\nidentity, that special stretched $9j$ symbols can be reformulated as $_5F_4(1)$\nhypergeometric series."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b26",
    "start_title":"Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble",
    "start_abstract":"Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Quantum machine learning in chemistry and materials"
      ],
      "abstract":[
        "Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models."
      ],
      "categories":[
        "cs.ET"
      ]
    },
    "list":{
      "title":[
        "TAP-CAM: A Tunable Approximate Matching Engine based on Ferroelectric\n  Content Addressable Memory",
        "Limitations in Parallel Ising Machine Networks: Theory and Practice",
        "Statistical QoS Provisioning for Underwater Magnetic Induction\n  Communication",
        "Improving VANET Simulation Channel Model in an Urban Environment via\n  Calibration Using Real-World Communication Data",
        "Mapping Spiking Neural Networks to Heterogeneous Crossbar Architectures\n  using Integer Linear Programming",
        "Characterization and Mitigation of ADC Noise by Reference Tuning in\n  RRAM-Based Compute-In-Memory",
        "Monolithic 3D FPGAs Utilizing Back-End-of-Line Configuration Memories",
        "Blind Eye: Motion and Obstacle Detection Leveraging Wi-Fi",
        "Consensus ranking by quantum annealing",
        "Energy-Efficient Cryogenic Neuromorphic Network with Superconducting\n  Memristor",
        "Solving Boolean satisfiability problems with resistive content\n  addressable memories",
        "Modular Mechanism Design Optimization in Large-Scale Systems with\n  Manufacturing Cost Considerations",
        "Microdroplet-Based Communications with Frequency Shift Keying Modulation",
        "6KSFx Synth Dataset",
        "Optimized detection of cyber-attacks on IoT networks via hybrid deep\n  learning models",
        "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
        "Superficial Self-Improved Reasoners Benefit from Model Merging",
        "SpeHeatal: A Cluster-Enhanced Segmentation Method for Sperm Morphology\n  Analysis",
        "Global well-posedness of the defocusing nonlinear wave equation outside\n  of a ball with radial data for $3<p<5$",
        "Improving Discriminator Guidance in Diffusion Models",
        "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study",
        "HEATS: A Hierarchical Framework for Efficient Autonomous Target Search\n  with Mobile Manipulators",
        "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement\n  Learning: Design and Experiment",
        "Learning to reset in target search problems",
        "Analysis and Optimization of Robustness in Multiplex Flow Networks\n  Against Cascading Failures",
        "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect\n  Information Games",
        "New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration",
        "FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary\n  Unlearning"
      ],
      "abstract":[
        "Pattern search is crucial in numerous analytic applications for retrieving\ndata entries akin to the query. Content Addressable Memories (CAMs), an\nin-memory computing fabric, directly compare input queries with stored entries\nthrough embedded comparison logic, facilitating fast parallel pattern search in\nmemory. While conventional CAM designs offer exact match functionality, they\nare inadequate for meeting the approximate search needs of emerging\ndata-intensive applications. Some recent CAM designs propose approximate\nmatching functions, but they face limitations such as excessively large cell\narea or the inability to precisely control the degree of approximation. In this\npaper, we propose TAP-CAM, a novel ferroelectric field effect transistor\n(FeFET) based ternary CAM (TCAM) capable of both exact and tunable approximate\nmatching. TAP-CAM employs a compact 2FeFET-2R cell structure as the entry\nstorage unit, and similarities in Hamming distances between input queries and\nstored entries are measured using an evaluation transistor associated with the\nmatchline of CAM array. The operation, robustness and performance of the\nproposed design at array level have been discussed and evaluated, respectively.\nWe conduct a case study of K-nearest neighbor (KNN) search to benchmark the\nproposed TAP-CAM at application level. Results demonstrate that compared to 16T\nCMOS CAM with exact match functionality, TAP-CAM achieves a 16.95x energy\nimprovement, along with a 3.06% accuracy enhancement. Compared to 2FeFET TCAM\nwith approximate match functionality, TAP-CAM achieves a 6.78x energy\nimprovement.",
        "Analog Ising machines (IMs) occupy an increasingly prominent area of computer\narchitecture research, offering high-quality and low latency\/energy solutions\nto intractable computing tasks. However, IMs have a fixed capacity, with little\nto no utility in out-of-capacity problems. Previous works have proposed\nparallel, multi-IM architectures to circumvent this limitation. In this work we\ntheoretically and numerically investigate tradeoffs in parallel IM networks to\nguide researchers in this burgeoning field. We propose formal models of\nparallel IM excution models, then provide theoretical guarantees for\nprobabilistic convergence. Numerical experiments illustrate our findings and\nprovide empirical insight into high and low synchronization frequency regimes.\nWe also provide practical heuristics for parameter\/model selection, informed by\nour theoretical and numerical findings.",
        "Magnetic induction (MI) communication, with stable channel conditions and\nsmall antenna size, is considered as a promising solution for underwater\ncommunication network. However, the narrowband nature of the MI link can cause\nsignificant delays in the network. To comprehensively ensure the timeliness and\neffectiveness of the MI network, in this paper we introduce a statistical\nquality of service (QoS) framework for MI communication, aiming to maximize the\nachievable rate while provisioning delay and queue-length requirements.\nSpecifically, we employ effective capacity theory to model underwater MI\ncommunication. Based on convex optimization theory, we propose a current\ncontrol strategy that maximizes the effective capacity under the constraints of\nlimited channel capacity and limited power. Simulations demonstrate that the\ncurrent control strategy proposed for MI communication differs significantly\nfrom that in the conventional statistical QoS provisioning framework. In\naddition, compared to other current control strategies, the proposed strategy\nsubstantially improves the achievable rate under various delay QoS\nrequirements.",
        "Wireless communication channels in Vehicular Ad-hoc NETworks (VANETs) suffer\nfrom packet losses, which severely influences the performance of their\napplications. There are several reasons for this loss, including but not\nlimited to signal interference with itself after being reflected from the\nground and other objects, the doppler effect caused by the speed of the\nvehicle, and buildings and other vehicles blocking the signal. As a result,\nVANET simulators must be calibrated in order to mimic the behavior of\nreal-world vehicular communication channels effectively. In this paper, we\ncalibrated an OMNET++(Objective Modular Network Testbed in C++)\/Veins simulator\nfor VANET's dedicated short-range communications (DSRC) protocol using the\nfield data from the urban testbed in Downtown Chattanooga, TN. Channel\npropagation models, as well as physical layer parameters, were calibrated using\na Genetic Algorithm (GA). The performance of the calibrated simulator was\nimproved significantly in comparison with the default settings in Veins. The\nfinal results were compared to the real-world data collected from the testbed\nand performance shows that the final calibrated channel model performs better\nthan uncalibrated models in simulating the packet delivery pattern of DSRC\nchannels.",
        "Advances in novel hardware devices and architectures allow Spiking Neural\nNetwork evaluation using ultra-low power, mixed-signal, memristor crossbar\narrays. As individual network sizes quickly scale beyond the dimensional\ncapabilities of single crossbars, networks must be mapped onto multiple\ncrossbars. Crossbar sizes within modern Memristor Crossbar Architectures are\ndetermined predominately not by device technology but by network topology;\nmore, smaller crossbars consume less area thanks to the high structural\nsparsity found in larger, brain-inspired SNNs. Motivated by continuing\nincreases in SNN sparsity due to improvements in training methods, we propose\nutilizing heterogeneous crossbar sizes to further reduce area consumption. This\napproach was previously unachievable as prior compiler studies only explored\nsolutions targeting homogeneous MCAs. Our work improves on the state-of-the-art\nby providing Integer Linear Programming formulations supporting arbitrarily\nheterogeneous architectures. By modeling axonal interactions between neurons\nour methods produce better mappings while removing inhibitive a priori\nknowledge requirements. We first show a 16.7-27.6% reduction in area\nconsumption for square-crossbar homogeneous architectures. Then, we demonstrate\n66.9-72.7% further reduction when using a reasonable configuration of\nheterogeneous crossbar dimensions. Next, we present a new optimization\nformulation capable of minimizing the number of inter-crossbar routes. When\napplied to solutions already near-optimal in area an 11.9-26.4% routing\nreduction is observed without impacting area consumption. Finally, we present a\nprofile-guided optimization capable of minimizing the number of runtime spikes\nbetween crossbars. Compared to the best-area-then-route optimized solutions we\nobserve a further 0.5-14.8% inter-crossbar spike reduction while requiring 1-3\norders of magnitude less solver time.",
        "With the escalating demand for power-efficient neural network architectures,\nnon-volatile compute-in-memory designs have garnered significant attention.\nHowever, owing to the nature of analog computation, susceptibility to noise\nremains a critical concern. This study confronts this challenge by introducing\na detailed model that incorporates noise factors arising from both ADCs and\nRRAM devices. The experimental data is derived from a 40nm foundry RRAM\ntest-chip, wherein different reference voltage configurations are applied, each\ntailored to its respective module. The mean and standard deviation values of\nHRS and LRS cells are derived through a randomized vector, forming the\nfoundation for noise simulation within our analytical framework. Additionally,\nthe study examines the read-disturb effects, shedding light on the potential\nfor accuracy deterioration in neural networks due to extended exposure to\nhigh-voltage stress. This phenomenon is mitigated through the proposed\nlow-voltage read mode. Leveraging our derived comprehensive fault model from\nthe RRAM test-chip, we evaluate CIM noise impact on both supervised learning\n(time-independent) and reinforcement learning (time-dependent) tasks, and\ndemonstrate the effectiveness of reference tuning to mitigate noise impacts.",
        "This work presents a novel monolithic 3D (M3D) FPGA architecture that\nleverages stackable back-end-of-line (BEOL) transistors to implement\nconfiguration memory and pass gates, significantly improving area, latency, and\npower efficiency. By integrating n-type (W-doped In_2O_3) and p-type (SnO)\namorphous oxide semiconductor (AOS) transistors in the BEOL, Si SRAM\nconfiguration bits are substituted with a less leaky equivalent that can be\nprogrammed at logic-compatible voltages. BEOL-compatible AOS transistors are\ncurrently under extensive research and development in the device community,\nwith investment by leading foundries, from which reported data is used to\ndevelop robust physics-based models in TCAD that enable circuit design. The use\nof AOS pass gates reduces the overhead of reconfigurable circuits by mapping\nFPGA switch block (SB) and connection block (CB) matrices above configurable\nlogic blocks (CLBs), thereby increasing the proximity of logic elements and\nreducing latency. By interfacing with the latest Verilog-to-Routing (VTR)\nsuite, an AOS-based M3D FPGA design implemented in 7 nm technology is\ndemonstrated with 3.4x lower area-time squared product (AT^2), 27% lower\ncritical path latency, and 26% lower reconfigurable routing block power on\nbenchmarks including hyperdimensional computing and large language models\n(LLMs).",
        "Wireless Fidelity or Wi-Fi, has completely transfigured wireless networking\nby offering a smooth connection to the internet and networks, particularly when\ndealing with enclosed environments. As with the majority of wireless\ntechnology, it functions through radio communication. This makes it possible\nfor Wi-Fi to operate effectively close to an Access Point. However, a device's\nability to receive Wi-Fi signals can vary greatly. These discrepancies arise\nbecause of impediments or motions between the device and the access point. We\nhave creatively used these variances as unique opportunities for applications\nthat can be used to detect movement in confined areas. As this approach makes\nuse of the current wireless infrastructure, no additional hardware is required.\nThese applications could potentially be leveraged to enable sophisticated\nrobots or enhance security systems.",
        "Consensus ranking is a technique used to derive a single ranking that best\nrepresents the preferences of multiple individuals or systems. It aims to\naggregate different rankings into one that minimizes overall disagreement or\ndistance from each of the individual rankings. Kemeny ranking aggregation, in\nparticular, is a widely used method in decision-making and social choice, with\napplications ranging from search engines to music recommendation systems. It\nseeks to determine a consensus ranking of a set of candidates based on the\npreferences of a group of individuals. However, existing quantum annealing\nalgorithms face challenges in efficiently processing large datasets with many\ncandidates. In this paper, we propose a method to improve the performance of\nquantum annealing for Kemeny rank aggregation. Our approach identifies the\npairwise preference matrix that represents the solution list and subsequently\nreconstructs the ranking using classical methods. This method already yields\nbetter results than existing approaches. Furthermore, we present a range of\nenhancements that significantly improve the proposed method's performance,\nthereby increasing the number of candidates that can be effectively handled.\nFinally, we evaluate the efficiency of our approach by comparing its\nperformance and execution time with that of KwikSort, a well-known approximate\nalgorithm.",
        "Cryogenic neuromorphic systems, inspired by the brains unparalleled\nefficiency, present a promising paradigm for next generation computing\narchitectures.This work introduces a fully integrated neuromorphic framework\nthat combines superconducting memristor(SM) based spiking neurons and synapse\ntopologies to achieve a low power neuromorphic network with non volatile\nsynaptic strength.This neurosynaptic framework is validated by implementing the\ncart pole control task, a dynamic decision making problem requiring real time\ncomputation.Through detailed simulations, we demonstrate the network's ability\nto execute this task with an average fitness of 5965 timesteps across 1000\nrandomized test episodes, with 40 percent achieving the target fitness of\n15,000 timesteps (0.02s per timestep).The system achieves 23 distinct spiking\nrates across neurons, ensuring efficient information encoding.Our findings\nestablish the potential of SM based cryogenic neuromorphic systems to address\nthe energy and scalability limitations of traditional computing, paving the way\nfor biologically inspired, ultra low power computational frameworks.",
        "Solving optimization problems is a highly demanding workload requiring\nhigh-performance computing systems. Optimization solvers are usually difficult\nto parallelize in conventional digital architectures, particularly when\nstochastic decisions are involved. Recently, analog computing architectures for\naccelerating stochastic optimization solvers have been presented, but they were\nlimited to academic problems in quadratic polynomial format. Here we present\nKLIMA, a k-Local In-Memory Accelerator with resistive Content Addressable\nMemories (CAMs) and Dot-Product Engines (DPEs) to accelerate the solution of\nhigh-order industry-relevant optimization problems, in particular Boolean\nSatisfiability. By co-designing the optimization heuristics and circuit\narchitecture we improve the speed and energy to solution up to 182x compared to\nthe digital state of the art.",
        "Modular design maximizes utility by using standardized components in\nlarge-scale systems. From a manufacturing perspective, it supports green\ntechnology by reducing material waste and improving reusability. Industrially,\nit offers economic benefits through economies of scale, making it a practical\ndesign strategy. Typically, modularization selects a representative design from\npredefined candidates to meet all performance requirements. However, achieving\neffective modularization in mechanical mechanisms presents challenges. First,\nmechanisms depend on geometric relationships for functional motion, and varying\nloads lead to different optimal parameters, complicating representative design\nselection. Second, the chosen design often exceeds optimal parameters, causing\nover-specification and performance deviations, which worsen as scale increases.\nTo address this, we propose a modular mechanism design framework using\nsurrogate-based optimization. This approach finds optimal designs for\nlarge-scale systems and partitions them into groups, each assigned an optimized\ndesign. This multi-objective optimization (MOO) problem balances economies of\nscale and performance consistency. Unlike conventional methods based on\npredefined candidates and simple grouping, our framework optimizes design\nvariables flexibly for modularization. Additionally, we analyze manufacturing\ncost parameters to develop a decision support system for selecting optimal\nstrategies in diverse design scenarios. This enhances maintainability, improves\ninterchangeability, and fosters environmentally sustainable manufacturing.",
        "Droplet-based communications has been investigated as a more robust\nalternative to diffusion-based molecular communications (MC), yet most existing\ndemonstrations employ large \"plug-like\" droplets or simple T-junction designs\nfor droplet generation, restricting modulation strategies and achievable data\nrates. Here, we report a microfluidic communication system that encodes\ninformation via the generation rate of sub-100 $\\mu$m water-in-oil\nmicrodroplets using a microfabricated flow focusing architecture. By precisely\ntuning the flow rate of the dispersed-phase (water) via a pressure-regulated\nflow controller, we implement frequency shift keying modulation with four\nsymbols (4-FSK). A high-speed optical detection and video processing setup\nserves as the receiver, tracking system response in the microfluidic channel\nacross different symbol durations (20 s and 12 s) and quantifying error\nperformance. Despite the miniaturized device and channel architecture, our\nexperiments demonstrate programmable and reliable data transmission with\nminimal symbol errors. Beyond water-in-oil systems, the same encoding\nprinciples can be extended to other compartmentalized carriers (e.g., giant\nunilamellar vesicles, polymersomes) that can also be synthesized via flow\nfocusing techniques, paving the way for biocompatible, robust, and\nhigh-capacity communication in intrabody networks and the emerging Internet of\nBio-Nano Things.",
        "Procedural audio, often referred to as \"digital Foley\", generates sound from\nscratch using computational processes. It represents an innovative approach to\nsound-effects creation. However, the development and adoption of procedural\naudio has been constrained by a lack of publicly available datasets and models,\nwhich hinders evaluation and optimization. To address this important gap, this\npaper presents a dataset of 6000 synthetic audio samples specifically designed\nto advance research and development in sound synthesis within 30 sound\ncategories. By offering a description of the diverse synthesis methods used in\neach sound category and supporting the creation of robust evaluation\nframeworks, this dataset not only highlights the potential of procedural audio,\nbut also provides a resource for researchers, audio developers, and sound\ndesigners. This contribution can accelerate the progress of procedural audio,\nopening up new possibilities in digital sound design.",
        "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.",
        "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
        "As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.",
        "The accurate assessment of sperm morphology is crucial in andrological\ndiagnostics, where the segmentation of sperm images presents significant\nchallenges. Existing approaches frequently rely on large annotated datasets and\noften struggle with the segmentation of overlapping sperm and the presence of\ndye impurities. To address these challenges, this paper first analyzes the\nissue of overlapping sperm tails from a geometric perspective and introduces a\nnovel clustering algorithm, Con2Dis, which effectively segments overlapping\ntails by considering three essential factors: CONnectivity, CONformity, and\nDIStance. Building on this foundation, we propose an unsupervised method,\nSpeHeatal, designed for the comprehensive segmentation of the SPErm HEAd and\nTAiL. SpeHeatal employs the Segment Anything Model(SAM) to generate masks for\nsperm heads while filtering out dye impurities, utilizes Con2Dis to segment\ntails, and then applies a tailored mask splicing technique to produce complete\nsperm masks. Experimental results underscore the superior performance of\nSpeHeatal, particularly in handling images with overlapping sperm.",
        "We continue the study of the Dirichlet boundary value problem of nonlinear\nwave equation with radial data in the exterior $\\Omega = \\mathbb{R}^3\\backslash\n\\bar{B}(0,1)$. We combine the distorted Fourier truncation method in\n\\cite{Bourgain98:FTM}, the global-in-time (endpoint) Strichartz estimates in\n\\cite{XuYang:NLW} with the energy method in \\cite{GallPlan03:NLW} to prove the\nglobal well-posedness of the radial solution to the defocusing,\nenergy-subcriticial nonlinear wave equation outside of a ball in $\\left(\\dot\nH^{s}_{D}(\\Omega) \\cap L^{p+1}(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$\nwith $1-\\frac{(p+3)(1-s_c)}{4(2p-3)}<s<1$, $s_c=\\frac{3}{2}-\\frac{2}{p-1} $,\nwhich extends the result for the cubic nonlinearity in \\cite{XuYang:NLW} to the\ncase $3<p<5$. Except from the argument in \\cite{XuYang:NLW}, another new\ningredient is that we need make use of the radial Sobolev inequality to deal\nwith the super-conformal nonlinearity in addition to the Sobolev inequality.",
        "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.",
        "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement.",
        "Utilizing robots for autonomous target search in complex and unknown\nenvironments can greatly improve the efficiency of search and rescue missions.\nHowever, existing methods have shown inadequate performance due to hardware\nplatform limitations, inefficient viewpoint selection strategies, and\nconservative motion planning. In this work, we propose HEATS, which enhances\nthe search capability of mobile manipulators in complex and unknown\nenvironments. We design a target viewpoint planner tailored to the strengths of\nmobile manipulators, ensuring efficient and comprehensive viewpoint planning.\nSupported by this, a whole-body motion planner integrates global path search\nwith local IPC optimization, enabling the mobile manipulator to safely and\nagilely visit target viewpoints, significantly improving search performance. We\npresent extensive simulated and real-world tests, in which our method\ndemonstrates reduced search time, higher target search completeness, and lower\nmovement cost compared to classic and state-of-the-art approaches. Our method\nwill be open-sourced for community benefit.",
        "This paper addresses the multi-robot pursuit problem for an unknown target,\nencompassing both target state estimation and pursuit control. First, in state\nestimation, we focus on using only bearing information, as it is readily\navailable from vision sensors and effective for small, distant targets.\nChallenges such as instability due to the nonlinearity of bearing measurements\nand singularities in the two-angle representation are addressed through a\nproposed uniform bearing-only information filter. This filter integrates\nmultiple 3D bearing measurements, provides a concise formulation, and enhances\nstability and resilience to target loss caused by limited field of view (FoV).\nSecond, in target pursuit control within complex environments, where challenges\nsuch as heterogeneity and limited FoV arise, conventional methods like\ndifferential games or Voronoi partitioning often prove inadequate. To address\nthese limitations, we propose a novel multiagent reinforcement learning (MARL)\nframework, enabling multiple heterogeneous vehicles to search, localize, and\nfollow a target while effectively handling those challenges. Third, to bridge\nthe sim-to-real gap, we propose two key techniques: incorporating adjustable\nlow-level control gains in training to replicate the dynamics of real-world\nautonomous ground vehicles (AGVs), and proposing spectral-normalized RL\nalgorithms to enhance policy smoothness and robustness. Finally, we demonstrate\nthe successful zero-shot transfer of the MARL controllers to AGVs, validating\nthe effectiveness and practical feasibility of our approach. The accompanying\nvideo is available at https:\/\/youtu.be\/HO7FJyZiJ3E.",
        "Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.",
        "Networked systems are susceptible to cascading failures, where the failure of\nan initial set of nodes propagates through the network, often leading to\nsystem-wide failures. In this work, we propose a multiplex flow network model\nto study robustness against cascading failures triggered by random failures.\nThe model is inspired by systems where nodes carry or support multiple types of\nflows, and failures result in the redistribution of flows within the same layer\nrather than between layers. To represent different types of interdependencies\nbetween the layers of the multiplex network, we define two cases of failure\nconditions: layer-independent overload and layer-influenced overload. We\nprovide recursive equations and their solutions to calculate the steady-state\nfraction of surviving nodes, validate them through a set of simulation\nexperiments, and discuss optimal load-capacity allocation strategies. Our\nresults demonstrate that allocating the total excess capacity to each layer\nproportional to the mean effective load in the layer and distributing that\nexcess capacity equally among the nodes within the layer ensures maximum\nrobustness. The proposed framework for different failure conditions allows us\nto analyze the two overload conditions presented and can be extended to explore\nmore complex interdependent relationships.",
        "We study the problem of adapting to a known sub-rational opponent during\nonline play while remaining robust to rational opponents. We focus on large\nimperfect-information (zero-sum) games, which makes it impossible to inspect\nthe whole game tree at once and necessitates the use of depth-limited search.\nHowever, all existing methods assume rational play beyond the depth-limit,\nwhich only allows them to adapt a very limited portion of the opponent's\nbehaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses\na strategy-portfolio approach - which we refer to as matrix-valued states - for\ndepth-limited search. This allows the algorithm to fully utilise all\ninformation about the opponent model, making it the first robust-adaptation\nmethod to be able to do so in large imperfect-information games. As an\nadditional benefit, the use of matrix-valued states makes the algorithm simpler\nthan traditional methods based on optimal value functions. Our experimental\nresults in poker and battleship show that ABD yields more than a twofold\nincrease in utility when facing opponents who make mistakes beyond the depth\nlimit and also delivers significant improvements in utility and safety against\nrandomly generated opponents.",
        "Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. To advance this field, we introduce a new REC\ndataset with two key features. First, it is designed with controllable\ndifficulty levels, requiring fine-grained reasoning across object categories,\nattributes, and relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing, explicitly testing a model's ability to\nreject non-existent targets, an often-overlooked yet critical challenge in\nexisting datasets. To address fine-grained compositional REC, we propose novel\nmethods based on a Specialist-MLLM collaboration framework, leveraging the\ncomplementary strengths of them: Specialist Models handle simpler tasks\nefficiently, while MLLMs are better suited for complex reasoning. Based on this\nsynergy, we introduce two collaborative strategies. The first, Slow-Fast\nAdaptation (SFA), employs a routing mechanism to adaptively delegate simple\ntasks to Specialist Models and complex tasks to MLLMs. Additionally, common\nerror patterns in both models are mitigated through a target-refocus strategy.\nThe second, Candidate Region Selection (CRS), generates multiple bounding box\ncandidates based on Specialist Model and uses the advanced reasoning\ncapabilities of MLLMs to identify the correct target. Extensive experiments on\nour dataset and other challenging compositional benchmarks validate the\neffectiveness of our approaches. The SFA strategy achieves a trade-off between\nlocalization accuracy and efficiency, and the CRS strategy greatly boosts the\nperformance of both Specialist Models and MLLMs. We aim for this work to offer\nvaluable insights into solving complex real-world tasks by strategically\ncombining existing tools for maximum effectiveness, rather than reinventing\nthem.",
        "Machine unlearning is an emerging field that selectively removes specific\ndata samples from a trained model. This capability is crucial for addressing\nprivacy concerns, complying with data protection regulations, and correcting\nerrors or biases introduced by certain data. Unlike traditional machine\nlearning, where models are typically static once trained, machine unlearning\nfacilitates dynamic updates that enable the model to ``forget'' information\nwithout requiring complete retraining from scratch. There are various machine\nunlearning methods, some of which are more time-efficient when data removal\nrequests are fewer.\n  To decrease the execution time of such machine unlearning methods, we aim to\nreduce the size of data removal requests based on the fundamental assumption\nthat the removal of certain data would not result in a distinguishable\nretrained model. We first propose the concept of unnecessary unlearning, which\nindicates that the model would not alter noticeably after removing some data\npoints. Subsequently, we review existing solutions that can be used to solve\nour problem. We highlight their limitations in adaptability to different\nunlearning scenarios and their reliance on manually selected parameters. We\nconsequently put forward FUNU, a method to identify data points that lead to\nunnecessary unlearning. FUNU circumvents the limitations of existing solutions.\nThe idea is to discover data points within the removal requests that have\nsimilar neighbors in the remaining dataset. We utilize a reference model to set\nparameters for finding neighbors, inspired from the area of model memorization.\nWe provide a theoretical analysis of the privacy guarantee offered by FUNU and\nconduct extensive experiments to validate its efficacy."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Cerebral aneurysms. New Engl. J. Medicine",
    "start_abstract":"Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Data-driven science and engineering: machine learning, dynamical systems, and control"
      ],
      "abstract":[
        "\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "T\\'yr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via\n  Global Sparsity Distribution Optimization",
        "Artificial Intelligence in Reactor Physics: Current Status and Future\n  Prospects",
        "Fuzzy Information Entropy and Region Biased Matrix Factorization for Web\n  Service QoS Prediction",
        "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated\n  Learning",
        "Probabilistic neural operators for functional uncertainty quantification",
        "A Flexible Fairness Framework with Surrogate Loss Reweighting for\n  Addressing Sociodemographic Disparities",
        "Controlling Neural Collapse Enhances Out-of-Distribution Detection and\n  Transfer Learning",
        "Online-BLS: An Accurate and Efficient Online Broad Learning System for\n  Data Stream Classification",
        "GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time\n  Distribution Shifts",
        "Pairwise Elimination with Instance-Dependent Guarantees for Bandits with\n  Cost Subsidy",
        "AdaGC: Improving Training Stability for Large Language Model Pretraining",
        "HypeRL: Parameter-Informed Reinforcement Learning for Parametric PDEs",
        "Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method",
        "Effects of oxidation and impurities in lithium surfaces on the emitting\n  wall plasma sheath",
        "Conservative, pressure-equilibrium-preserving discontinuous Galerkin\n  method for compressible, multicomponent flows",
        "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "Infinite-temperature thermostats by energy localization in a\n  nonequilibrium setup",
        "A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data",
        "Grey system model on time scales",
        "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing\n  Attacks",
        "A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing\n  a Self-Organizing UAV Swarm",
        "Single-molecule phosphorescence and intersystem crossing in a coupled\n  exciton-plasmon system",
        "SEAlign: Alignment Training for Software Engineering Agent",
        "Discrete-time weak approximation of a Black-Scholes model with drift and\n  volatility Markov switching",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Modular and Integrated AI Control Framework across Fiber and Wireless\n  Networks for 6G",
        "Antiferromagnetic and bond-order-wave phases in the half-filled\n  two-dimensional optical Su-Schrieffer-Heeger-Hubbard model"
      ],
      "abstract":[
        "Structural pruning enhances hardware-agnostic inference efficiency for large\nlanguage models (LLMs) but often struggles to maintain performance. Local\npruning performs efficient layer-by-layer compression but ignores global\ntopology. Global pruning has the potential to find the optimal solution\nalthough resource-intensive. However, existing methods tend to rank structural\nsaliency uniformly, ignoring inter-structure dependencies and failing to\nachieve end-to-end optimization. To address these limitations, we propose\nT\\'yr-the-Pruner, an efficient end-to-end search-based global structural\npruning framework. This framework constructs a supernet by repeatedly applying\nlocal pruning across a range of sparsity ratios to each layer in an LLM, with\nthe core goal of determining the optimal sparsity distribution under a target\noverall sparsity ratio. Concretely, we introduce an effective local pruning and\nan expectation error accumulation approach to improve supernet construction.\nFurthermore, we employ an iterative prune-and-search strategy with\ncoarse-to-fine sparsity granularity to ensure efficient search convergence.\nExperimental results show that T\\'yr-the-Pruner achieves state-of-the-art\nstructural pruning, retaining 97% of the dense model's performance while\nremoving a challenging 50% of Llama-3.1-70B's parameters.",
        "Reactor physics is the study of neutron properties, focusing on using models\nto examine the interactions between neutrons and materials in nuclear reactors.\nArtificial intelligence (AI) has made significant contributions to reactor\nphysics, e.g., in operational simulations, safety design, real-time monitoring,\ncore management and maintenance. This paper presents a comprehensive review of\nAI approaches in reactor physics, especially considering the category of\nMachine Learning (ML), with the aim of describing the application scenarios,\nfrontier topics, unsolved challenges and future research directions. From\nequation solving and state parameter prediction to nuclear industry\napplications, this paper provides a step-by-step overview of ML methods applied\nto steady-state, transient and combustion problems. Most literature works\nachieve industry-demanded models by enhancing the efficiency of deterministic\nmethods or correcting uncertainty methods, which leads to successful\napplications. However, research on ML methods in reactor physics is somewhat\nfragmented, and the ability to generalize models needs to be strengthened.\nProgress is still possible, especially in addressing theoretical challenges and\nenhancing industrial applications such as building surrogate models and digital\ntwins.",
        "Nowadays, there are many similar services available on the internet, making\nQuality of Service (QoS) a key concern for users. Since collecting QoS values\nfor all services through user invocations is impractical, predicting QoS values\nis a more feasible approach. Matrix factorization is considered an effective\nprediction method. However, most existing matrix factorization algorithms focus\non capturing global similarities between users and services, overlooking the\nlocal similarities between users and their similar neighbors, as well as the\nnon-interactive effects between users and services. This paper proposes a\nmatrix factorization approach based on user information entropy and region\nbias, which utilizes a similarity measurement method based on fuzzy information\nentropy to identify similar neighbors of users. Simultaneously, it integrates\nthe region bias between each user and service linearly into matrix\nfactorization to capture the non-interactive features between users and\nservices. This method demonstrates improved predictive performance in more\nrealistic and complex network environments. Additionally, numerous experiments\nare conducted on real-world QoS datasets. The experimental results show that\nthe proposed method outperforms some of the state-of-the-art methods in the\nfield at matrix densities ranging from 5% to 20%.",
        "Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack",
        "Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.",
        "This paper presents a new algorithmic fairness framework called\n$\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ Fair Machine Learning\n($\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ FML), designed to optimize fairness\nlevels across sociodemographic attributes. Our framework employs a new family\nof surrogate loss functions, paired with loss reweighting techniques, allowing\nprecise control over fairness-accuracy trade-offs through tunable\nhyperparameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. To efficiently\nsolve the learning objective, we propose Parallel Stochastic Gradient Descent\nwith Surrogate Loss (P-SGD-S) and establish convergence guarantees for both\nconvex and nonconvex loss functions. Experimental results demonstrate that our\nframework improves overall accuracy while reducing fairness violations,\noffering a smooth trade-off between standard empirical risk minimization and\nstrict minimax fairness. Results across multiple datasets confirm its\nadaptability, ensuring fairness improvements without excessive performance\ndegradation.",
        "Out-of-distribution (OOD) detection and OOD generalization are widely studied\nin Deep Neural Networks (DNNs), yet their relationship remains poorly\nunderstood. We empirically show that the degree of Neural Collapse (NC) in a\nnetwork layer is inversely related with these objectives: stronger NC improves\nOOD detection but degrades generalization, while weaker NC enhances\ngeneralization at the cost of detection. This trade-off suggests that a single\nfeature space cannot simultaneously achieve both tasks. To address this, we\ndevelop a theoretical framework linking NC to OOD detection and generalization.\nWe show that entropy regularization mitigates NC to improve generalization,\nwhile a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for\nbetter detection. Based on these insights, we propose a method to control NC at\ndifferent DNN layers. In experiments, our method excels at both tasks across\nOOD datasets and DNN architectures.",
        "The state-of-the-art online learning models generally conduct a single online\ngradient descent when a new sample arrives and thus suffer from suboptimal\nmodel weights. To this end, we introduce an online broad learning system\nframework with closed-form solutions for each online update. Different from\nemploying existing incremental broad learning algorithms for online learning\ntasks, which tend to incur degraded accuracy and expensive online update\noverhead, we design an effective weight estimation algorithm and an efficient\nonline updating strategy to remedy the above two deficiencies, respectively.\nSpecifically, an effective weight estimation algorithm is first developed by\nreplacing notorious matrix inverse operations with Cholesky decomposition and\nforward-backward substitution to improve model accuracy. Second, we devise an\nefficient online updating strategy that dramatically reduces online update\ntime. Theoretical analysis exhibits the splendid error bound and low time\ncomplexity of our model. The most popular test-then-training evaluation\nexperiments on various real-world datasets prove its superiority and\nefficiency. Furthermore, our framework is naturally extended to data stream\nscenarios with concept drift and exceeds state-of-the-art baselines.",
        "We consider the problem of test-time domain generalization, where a model is\ntrained on several source domains and adjusted on target domains never seen\nduring training. Different from the common methods that fine-tune the model or\nadjust the classifier parameters online, we propose to generate multiple layer\nparameters on the fly during inference by a lightweight meta-learned\ntransformer, which we call \\textit{GeneralizeFormer}. The layer-wise parameters\nare generated per target batch without fine-tuning or online adjustment. By\ndoing so, our method is more effective in dynamic scenarios with multiple\ntarget distributions and also avoids forgetting valuable source distribution\ncharacteristics. Moreover, by considering layer-wise gradients, the proposed\nmethod adapts itself to various distribution shifts. To reduce the\ncomputational and time cost, we fix the convolutional parameters while only\ngenerating parameters of the Batch Normalization layers and the linear\nclassifier. Experiments on six widely used domain generalization datasets\ndemonstrate the benefits and abilities of the proposed method to efficiently\nhandle various distribution shifts, generalize in dynamic scenarios, and avoid\nforgetting.",
        "Multi-armed bandits (MAB) are commonly used in sequential online\ndecision-making when the reward of each decision is an unknown random variable.\nIn practice however, the typical goal of maximizing total reward may be less\nimportant than minimizing the total cost of the decisions taken, subject to a\nreward constraint. For example, we may seek to make decisions that have at\nleast the reward of a reference ``default'' decision, with as low a cost as\npossible. This problem was recently introduced in the Multi-Armed Bandits with\nCost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem\ndomains where a primary metric (cost) is constrained by a secondary metric\n(reward), and the rewards are unknown. In our work, we address variants of\nMAB-CS including ones with reward constrained by the reward of a known\nreference arm or by the subsidized best reward. We introduce the\nPairwise-Elimination (PE) algorithm for the known reference arm variant and\ngeneralize PE to PE-CS for the subsidized best reward variant. Our\ninstance-dependent analysis of PE and PE-CS reveals that both algorithms have\nan order-wise logarithmic upper bound on Cost and Quality Regret, making our\npolicies the first with such a guarantee. Moreover, by comparing our upper and\nlower bound results we establish that PE is order-optimal for all known\nreference arm problem instances. Finally, experiments are conducted using the\nMovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the\neffectiveness of PE and the superior balance between performance and\nreliability offered by PE-CS compared to baselines from the literature.",
        "Large Language Models (LLMs) face increasing loss spikes during scaling,\nundermining training stability and final performance. While gradient clipping\nmitigates this issue, traditional global approaches poorly handle\nparameter-specific gradient variations and decaying gradient norms. We propose\n**AdaGC**, an adaptive gradient clipping framework that automatically adjusts\nlocal thresholds per parameter through exponential moving average of gradient\nnorms. Theoretical analysis proves AdaGC's convergence under non-convex\nconditions. Extensive experiments demonstrate significant improvements: On\nLlama-2 7B\/13B, AdaGC completely eliminates loss spikes while reducing WikiText\nperplexity by 3.5% (+0.14pp LAMBADA accuracy) for 7B and achieving 0.65% lower\ntraining loss with 1.47% reduced validation perplexity for 13B compared to\nglobal clipping. For CLIP ViT-Base, AdaGC converges 25% faster than StableAdamW\nwith full spike elimination. The method shows universal effectiveness across\narchitectures (Llama-2 7B\/13B) and modalities (CLIP), with successful\nintegration into diverse optimizers like AdamW and Lion. Source code will be\nreleased on GitHub.",
        "In this work, we devise a new, general-purpose reinforcement learning\nstrategy for the optimal control of parametric partial differential equations\n(PDEs). Such problems frequently arise in applied sciences and engineering and\nentail a significant complexity when control and\/or state variables are\ndistributed in high-dimensional space or depend on varying parameters.\nTraditional numerical methods, relying on either iterative minimization\nalgorithms or dynamic programming, while reliable, often become computationally\ninfeasible. Indeed, in either way, the optimal control problem must be solved\nfor each instance of the parameters, and this is out of reach when dealing with\nhigh-dimensional time-dependent and parametric PDEs. In this paper, we propose\nHypeRL, a deep reinforcement learning (DRL) framework to overcome the\nlimitations shown by traditional methods. HypeRL aims at approximating the\noptimal control policy directly. Specifically, we employ an actor-critic DRL\napproach to learn an optimal feedback control strategy that can generalize\nacross the range of variation of the parameters. To effectively learn such\noptimal control laws, encoding the parameter information into the DRL policy\nand value function neural networks (NNs) is essential. To do so, HypeRL uses\ntwo additional NNs, often called hypernetworks, to learn the weights and biases\nof the value function and the policy NNs. We validate the proposed approach on\ntwo PDE-constrained optimal control benchmarks, namely a 1D\nKuramoto-Sivashinsky equation and a 2D Navier-Stokes equations, by showing that\nthe knowledge of the PDE parameters and how this information is encoded, i.e.,\nvia a hypernetwork, is an essential ingredient for learning parameter-dependent\ncontrol policies that can generalize effectively to unseen scenarios and for\nimproving the sample efficiency of such policies.",
        "The rapid advancements in data-driven methodologies have underscored the\ncritical importance of ensuring data quality. Consequently, detecting\nout-of-distribution (OOD) data has emerged as an essential task to maintain the\nreliability and robustness of data-driven models, in general, and machine and\ndeep learning models, in particular. In this study, we leveraged the convex\nhull property of a dataset and the fact that anomalies highly contribute to the\nincrease of the CH's volume to propose a novel anomaly detection algorithm. Our\nalgorithm computes the CH's volume as an increasing number of data points are\nremoved from the dataset to define a decision line between OOD and\nin-distribution data points. We compared the proposed algorithm to seven widely\nused anomaly detection algorithms over ten datasets, showing comparable results\nfor state-of-the-art (SOTA) algorithms. Moreover, we show that with a\ncomputationally cheap and simple check, one can detect datasets that are\nwell-suited for the proposed algorithm which outperforms the SOTA anomaly\ndetection algorithms.",
        "Use of lithium as a surface coating in fusion devices improves plasma\nperformance, but the change in wall properties affects the secondary electron\nemission properties of the material. Lithium oxidizes easily, which drives the\nemission yield well above unity. We present here simulations demonstrating the\nchange in sheath structure from monotonic to the nonmonotonic space-charge\nlimited sheath using an energy-dependent data-driven emission model which\nself-consistently captures both secondary emission and backscattering\npopulations. Increased secondary electron emission from the material has\nramifications for the degradation and erosion of the wall. Results shows that\nthe oxidation leads to an increased electron flux into the wall, and a reduced\nion flux. The net transfer of energy to the surface is significantly greater\nfor the oxidized case than for the pure lithium case. High reflection rates of\nlow-energy backscattered particles leads to a high re-emission rate at the\nwall.",
        "This paper concerns preservation of velocity and pressure equilibria in\nsmooth, compressible, multicomponent flows in the inviscid limit. First, we\nderive the velocity-equilibrium and pressure-equilibrium conditions of a\nstandard discontinuous Galerkin method that discretizes the conservative form\nof the compressible, multicomponent Euler equations. We show that under certain\nconstraints on the numerical flux, the scheme is\nvelocity-equilibrium-preserving. However, standard discontinuous Galerkin\nschemes are not pressure-equilibrium-preserving. Therefore, we introduce a\ndiscontinuous Galerkin method that discretizes the pressure-evolution equation\nin place of the total-energy conservation equation. Semidiscrete conservation\nof total energy, which would otherwise be lost, is restored via the correction\nterms of [Abgrall, J. Comput. Phys., 372, 2018, pp. 640-666] and [Abgrall et\nal., J. Comput. Phys., 453, 2022, 110955]. Since the addition of the correction\nterms prevents exact preservation of pressure and velocity equilibria, we\npropose modifications that then lead to a velocity-equilibrium-preserving,\npressure-equilibrium-preserving, and (semidiscretely) energy-conservative\ndiscontinuous Galerkin scheme, although there are certain tradeoffs. Additional\nextensions are also introduced. We apply the developed scheme to smooth,\ninterfacial flows involving mixtures of thermally perfect gases initially in\npressure and velocity equilibria to demonstrate its performance in one, two,\nand three spatial dimensions.",
        "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe\/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Some lattice models having two conservation laws may display an equilibrium\nphase transition from a homogeneous (positive temperature - PT) to a condensed\n(negative temperature) phase, where a finite fraction of the energy is\nlocalized in a few sites. We study one such stochastic model in an\nout-of-equilibrium setup, where the ends of the lattice chain are attached to\ntwo PT baths. We show that localized peaks may spontaneously emerge, acting as\ninfinite-temperature heat baths. The number $N_b$ of peaks is expected to grow\nin time $t$ as $N_b \\sim \\sqrt{\\ln t}$, as a consequence of an effective\nfreezing of the dynamics. Asymptotically, the chain spontaneously subdivides\ninto three intervals: the two external ones lying inside the PT region; the\nmiddle one characterized by peaks superposed to a background lying along the\ninfinite-temperature line. In the thermodynamic limit, the Onsager formalism\nallows determining the shape of the whole profile.",
        "The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.",
        "The Grey System Theory (GST) is a powerful mathematical framework employed\nfor modeling systems with uncertain or incomplete information. This paper\nproposes an integration of the GST with time scales, a generalized approach\nthat encompasses both discrete and continuous time models. The proposed model,\ncalled the Grey System Model on Time Scales (GST-T), offers a robust solution\nfor analyzing hybrid systems where events occur on varying time domains.",
        "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI.",
        "The smart metering infrastructure may become one of the key elements in\nefficiently managing energy in smart cities. At the same time, traditional\nmeasurement record collection is performed by manual methods, which raises\ncost, safety, and accuracy issues. This paper proposes an innovative SMI\narchitecture based on an unmanned aerial vehicle swarm organizing itself for\nthe autonomous data collection in smart metering infrastructure with\nscalability and cost-effectiveness while minimizing risks. We design an\narchitecture-based comprehensive system with various phases of operation,\ncommunication protocols, and robust failure-handling mechanisms to ensure\nreliable operations. We further perform extensive simulations in maintenance of\nprecise formations during flight, efficient data collection from smart meters,\nand adaptation to various failure scenarios. Importantly, we analyze the energy\nconsumption of the proposed system in both drone flight operations and network\ncommunication. We now propose a battery sizing strategy and provide an estimate\nof the operational lifetime of the swarm, underlining the feasibility and\npracticality of our approach. Our results show that UAV swarms have great\npotential to revolutionize smart metering and to bring a further brick to\ngreener and more resilient smart cities.",
        "Scanning the sharp metal tip of a scanning tunneling microscope (STM) over a\nmolecule allows tuning the coupling between the tip plasmon and a molecular\nfluorescence emitter. This allows access to local variations of fluorescence\nfield enhancement and wavelength shifts, which are central parameters for\ncharacterizing the plasmon-exciton coupling. Performing the same for\nphosphorescence with molecular scale resolution remains a significant\nchallenge. In this study, we present the first investigation of phosphorescence\nfrom isolated Pt-Phthalocyanine molecules by analyzing tip-enhanced emission\nspectra in both current-induced and laser-induced phosphorescence. The latter\ndirectly monitors singlet-to-triplet state intersystem crossing of a molecule\nbelow the tip. The study paves the way to a detailed understanding of triplet\nexcitation pathways and their potential control at sub-molecular length scales.\nAdditionally, the coupling of organic phosphors to plasmonic structures is a\npromising route for the improving light-emitting diodes.",
        "Recent advances in code generation models have demonstrated impressive\ncapabilities in automating software development tasks, yet these models still\nstruggle in real-world software engineering scenarios. Although current\ntraining methods, particularly post-training, excel at solving competitive\nprogramming problems, they fail to adequately prepare models for the\ncomplexities of practical software development. This misalignment raises the\ncritical question: Are existing alignment training methods well suited for\nreal-world software engineering tasks? In this study, we identify this issue\nand propose SEAlign, a novel alignment framework designed to bridge the gap\nbetween code generation models and real-world software development tasks.\nSEAlign leverages the unique characteristics of software engineering processes,\nincluding high-quality workflow steps, to enhance model capabilities. Our\nframework further employs Monte Carlo Tree Search for fine-grained alignment in\nmulti-step decision processes, followed by preference optimization on critical\nactions to ensure models meet real-world requirements. We evaluate SEAlign on\nthree standard agentic benchmarks for real-world software engineering,\nincluding HumanEvalFix, SWE-Bench-Lite, and SWE-Bench-Verified. Experimental\nresults demonstrate state-of-the-art performance with minimal training\noverhead. In addition, we develop an agent-based software development platform\nusing SEAlign, which successfully automates the creation of several small\napplications. Human evaluations of these applications highlight significant\nimprovements in both task performance and user experience. Our findings\nunderscore the potential of SEAlign to accelerate the adoption of large code\nmodels in real-world software development. We believe that this research makes\na meaningful step towards fully automated software engineering.",
        "We consider a continuous-time financial market with an asset whose price is\nmodeled by a linear stochastic differential equation with drift and volatility\nswitching driven by a uniformly ergodic jump Markov process with a countable\nstate space (in fact, this is a Black-Scholes model with Markov switching). We\nconstruct a multiplicative scheme of series of discrete-time markets with\ndiscrete-time Markov switching. First, we establish that the discrete-time\nswitching Markov chains weakly converge to the limit continuous-time Markov\nprocess. Second, having this in hand, we apply conditioning on Markov chains\nand prove that the discrete-time market models themselves weakly converge to\nthe Black-Scholes model with Markov switching. The convergence is proved under\nvery general assumptions both on the discrete-time net profits and on a\ngenerator of a continuous-time Markov switching process.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "The rapid evolution of communication networks towards 6G increasingly\nincorporates advanced AI-driven controls across various network segments to\nachieve intelligent, zero-touch operation. This paper proposes a comprehensive\nand modular framework for AI controllers, designed to be highly flexible and\nadaptable for use across both fiber optical and radio networks. Building on the\nprinciples established by the O-RAN Alliance for near-Real-Time RAN Intelligent\nControllers (near-RT RICs), our framework extends this AI-driven control into\nthe optical domain. Our approach addresses the critical need for a unified AI\ncontrol framework across diverse network transport technologies and domains,\nenabling the development of intelligent, automated, and scalable 6G networks.",
        "Electron-phonon ($e$-ph) interactions arise in many strongly correlated\nquantum materials from the modulation of the nearest-neighbor hopping\nintegrals, as in the celebrated Su-Schrieffer-Heeger (SSH) model. Nevertheless,\nrelatively few non-perturbative studies of correlated SSH models have been\nconducted in dimensions greater than one, and those that have been done have\nprimarily focused on bond models, where generalized displacements independently\nmodulate each hopping integral. We conducted a sign-problem free determinant\nquantum Monte Carlo study of the optical SSH-Hubbard model on a two-dimensional\nsquare lattice, where site-centered phonon modes simultaneously modulate pairs\nof nearest-neighbor hopping integrals. We report the model's low-temperature\nphase diagram in the challenging adiabatic regime ($\\Omega\/E_\\mathrm{F} \\sim\n1\/8$). It exhibits insulating antiferromagnetic Mott and bond-order-wave (BOW)\nphases with a narrow region of coexistence between them. We also find that a\ncritical $e$-ph coupling is required to stabilize the BOW phase in the small\n$U$ limit. Lastly, in stark contrast to recent findings for the model's bond\nvariant, we find no evidence for a long-range antiferromagnetism in the pure\n$(U\/t=0)$ optical SSH model."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Data-driven science and engineering: machine learning, dynamical systems, and control",
    "start_abstract":"\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Cerebral aneurysms. New Engl. J. Medicine"
      ],
      "abstract":[
        "Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Evolvable Soma Theory of Ageing: Insights from Computer Simulations",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "An LLM-Powered Clinical Calculator Chatbot Backed by Verifiable Clinical\n  Calculators and their Metadata",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "On the hierarchy of plate models for a singularly perturbed multi-well\n  nonlinear elastic energy",
        "Some topological genera and Jacobi forms",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions",
        "Probing topological matter and fermion dynamics on a neutral-atom\n  quantum computer",
        "Irregularities of distribution and Fourier transforms of\n  multi-dimensional convex bodies",
        "Photonic heat amplifiers based on Anderson insulators",
        "Frequency-noise-insensitive universal control of Kerr-cat qubits",
        "Bilateral Bailey pairs and Rogers-Ramanujan type identities",
        "Entropy Inequalities Constrain Holographic Erasure Correction",
        "From Data to Combinatorial Multivector field Through an\n  Optimization-Based Framework",
        "Model Predictive and Reinforcement Learning Methods for Active Flow\n  Control of an Airfoil with Dual-point Excitation of Plasma Actuators",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "Test fields and naked singularities: is the second law the cosmic\n  censor?",
        "Lonely passenger problem: the more buses there are, the more lonely\n  passengers there will be",
        "Universal self-gravitating skyrmions"
      ],
      "abstract":[
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Biological evolution continuously refines the design of species, resulting in\nhighly optimised organisms over hundreds of millennia. Intuitively, we expect\nthat random changes-evolution's primary mechanism-are more likely to be harmful\nthan beneficial, leading to widespread detrimental effects in evolving species.\nThe Evolvable Soma Theory of Ageing (ESTA) suggests that ageing is the\ncumulative result of these harmful effects, which predominantly cause bodily\ndamage, while a few may lead to beneficial adaptations that evolution can\nexploit. While the disposable soma theory views ageing as a consequence of\nlimited evolutionary pressure, ESTA posits that ageing is essentially evolution\nin action. In this study, we gather evidence supporting this theory through\ncomputer simulations. We conduct experiments using a platform where genes are\nlinked to onset values that determine when they are expressed. Three scenarios\nare tested: one with single-point fitness evaluation, constant mutation rate\nand fixed gene onsets; one with single-point fitness evaluation,\nonset-dependent mutation rate and fixed gene onsets; and one with spread\nfitness evaluation, onset-dependent mutation rate and evolvable gene onsets.\nThe last scenario, which embodies the evolvable soma hypothesis, demonstrates\nsuperior performance in both algorithmic efficiency and biological plausibility\ncompared to the others.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Clinical calculators are widely used, and large language models (LLMs) make\nit possible to engage them using natural language. We demonstrate a\npurpose-built chatbot that leverages software implementations of verifiable\nclinical calculators via LLM tools and metadata about these calculators via\nretrieval augmented generation (RAG). We compare the chatbot's response\naccuracy to an unassisted off-the-shelf LLM on four natural language\nconversation workloads. Our chatbot achieves 100% accuracy on queries\ninterrogating calculator metadata content and shows a significant increase in\nclinical calculation accuracy vs. the off-the-shelf LLM when prompted with\ncomplete sentences (86.4% vs. 61.8%) or with medical shorthand (79.2% vs.\n62.0%). It eliminates calculation errors when prompted with complete sentences\n(0% vs. 16.8%) and greatly reduces them when prompted with medical shorthand\n(2.4% vs. 18%). While our chatbot is not ready for clinical use, these results\nshow progress in minimizing incorrect calculation results.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "In the celebrated work of Friesecke, James and M\\\"uller '06 the authors\nderive a hierarchy of models for plates by carefully analyzing the\n$\\Gamma$-convergence of the rescaled nonlinear elastic energy. The key\ningredient of their proofs is the rigidity estimate proved in an earlier work\nof theirs. Here we consider the case in which the elastic energy has a\nmulti-well structure: this type of functional arises, for example, in the study\nof solid-solid phase transitions. Since the rigidity estimate fails in the case\nof compatible wells, we follow Alicandro, Dal Maso, Lazzaroni and Palombaro '18\nand add a regularization term to the energy that penalizes jumps from one well\nto another, leading to good compactness properties. In this setting we recover\nthe full hierarchy of plate models with an explicit dependence on the wells.\nFinally, we study the convergence of energy minimizers with suitable external\nforces and full Neumann boundary conditions. To do so, we adapt the definition\nof optimal rotations introduced by Maor, Mora '21.",
        "We revisit and elucidate the $\\widehat{A}$-genus, Hirzebruch's $L$-genus and\nWitten's $W$-genus, cobordism invariants of special classes of manifolds. After\nslight modification, involving Hecke's trick, we find that the\n$\\widehat{A}$-genus and $L$-genus arise directly from Jacobi's theta function.\nIn this way, for every $k\\geq 0,$ we obtain exact formulas for the quasimodular\nexpressions of $\\widehat{A}_k$ and $L_k$ as ``traces'' of partition Eisenstein\nseries \\[\\widehat{\\mathcal{A}}_k(\\tau)=\n\\operatorname{Tr}_k(\\phi_{\\widehat{A}};\\tau)\\ \\ \\ \\ \\ \\ {\\text {\\rm and}}\\ \\ \\\n\\ \\ \\ \\mathcal{L}_k(\\tau)= \\operatorname{Tr}_k(\\phi_L;\\tau). \\] Surprisingly,\nRamanujan defined twists of the $\\widehat{\\mathcal{A}}_k(\\tau)$ in his ``lost\nnotebook'' in his study of derivatives of theta functions, decades before Borel\nand Hirzebruch rediscovered them in the context of spin manifolds. In addition,\nwe show that the nonholomorphic $G_2^{\\star}$-completion of the characteristic\nseries of the Witten genus is the Jacobi theta function avatar of the\n$\\widehat{A}$-genus.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions.",
        "Quantum simulations of many-body systems are among the most promising\napplications of quantum computers. In particular, models based on\nstrongly-correlated fermions are central to our understanding of quantum\nchemistry and materials problems, and can lead to exotic, topological phases of\nmatter. However, due to the non-local nature of fermions, such models are\nchallenging to simulate with qubit devices. Here we realize a digital quantum\nsimulation architecture for two-dimensional fermionic systems based on\nreconfigurable atom arrays. We utilize a fermion-to-qubit mapping based on\nKitaev's model on a honeycomb lattice, in which fermionic statistics are\nencoded using long-range entangled states. We prepare these states efficiently\nusing measurement and feedforward, realize subsequent fermionic evolution\nthrough Floquet engineering with tunable entangling gates interspersed with\natom rearrangement, and improve results with built-in error detection.\nLeveraging this fermion description of the Kitaev spin model, we efficiently\nprepare topological states across its complex phase diagram and verify the\nnon-Abelian spin liquid phase by evaluating an odd Chern number. We further\nexplore this two-dimensional fermion system by realizing tunable dynamics and\ndirectly probing fermion exchange statistics. Finally, we simulate strong\ninteractions and study dynamics of the Fermi-Hubbard model on a square lattice.\nThese results pave the way for digital quantum simulations of complex fermionic\nsystems for materials science, chemistry, and high-energy physics.",
        "W. Schmidt, H. Montgomery, and J. Beck proved a result on irregularities of\ndistribution with respect to $d$-dimensional balls. In this paper, we extend\ntheir result to any $d$-dimensional convex body with a smooth boundary and\nfinite order of contact. As an intermediate step, we prove a geometric\ninequality for the Fourier transform of the characteristic function of a convex\nbody.",
        "A photonic heat amplifier (PHA) designed for cryogenic operations is\nintroduced and analyzed. This device comprises two Anderson insulator\nreservoirs connected by lossless lines, allowing them to exchange heat through\nphotonic modes. This configuration enables negative differential thermal\nconductance (NDTC), which can be harnessed to amplify thermal signals. To\nachieve this, we maintain one reservoir at a high temperature, serving as the\nsource terminal of a thermal transistor. Concurrently, in the other one, we\nestablish tunnel contacts to metallic reservoirs, which function as the gate\nand drain terminals. With this arrangement, it is possible to control the heat\nflux exchange between the source and drain by adjusting the gate temperature.\nWe present two distinct parameter choices that yield different performances:\nthe first emphasizes modulating the source-drain heat current, while the second\nfocuses on the temperature modulation of the colder Anderson insulator. Lastly,\nwe present a potential design variation in which all electronic reservoirs are\nthermally connected through only photonic modes, allowing interactions between\ndistant elements. The proposal of the PHA addresses the lack of thermal\ntransistors and amplifiers in the mK range while being compatible with the rich\ntoolbox of circuit quantum electrodynamics. It can be adapted to various\napplications, including sensing and developing thermal circuits and control\ndevices at sub-Kelvin temperatures, which are relevant to quantum technologies.",
        "We theoretically study the influence of frequency uncertainties on the\noperation of a Kerr-cat qubit. As the mean photon number increases, Kerr-cat\nqubits provide an increasing level of protection against phase errors induced\nby unknown frequency shifts during idling and X rotations. However, realizing\nrotations about the other principal axes (e.g., Y and Z axes) while preserving\nrobustness is nontrivial. To address this challenge, we propose a universal set\nof gate schemes which circumvents the tradeoff between protection and\ncontrollability in Kerr-cat qubits and retains robustness to unknown frequency\nshifts to at least first order. Assuming an effective Kerr oscillator model, we\ntheoretically and numerically analyze the robustness of elementary gates on\nKerr-cat qubits, with special focus on gates along nontrivial rotation axes. An\nappealing application of this qubit design would include tunable\nsuperconducting platforms, where the induced protection against frequency noise\nwould allow for a more flexible choice of operating point and thus the\npotential mitigation of the impact of spurious two-level systems.",
        "Rogers-Ramanujan type identities occur in various branches of mathematics and\nphysics. As a classic and powerful tool to deal with Rogers-Ramanujan type\nidentities, the theory of Bailey's lemma has been extensively studied and\ngeneralized. In this paper, we found a bilateral Bailey pair that naturally\narises from the q-binomial theorem. By applying the bilateral versions of\nBailey lemmas, Bailey chains and Bailey lattices, we derive a number of\nRogers-Ramanujan type identities, which unify many known identities as special\ncases. Further combined with the bilateral Bailey chains due to Berkovich,\nMcCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we\nalso obtain identities on Appell-Lerch series and identities of Andrews-Gordon\ntype. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we\nderive identities on Hecke-type series.",
        "We interpret holographic entropy inequalities in terms of erasure correction.\nThe non-saturation of an inequality is a necessary condition for certain\nschemes of holographic erasure correction, manifested in the bulk as non-empty\noverlaps of corresponding entanglement wedges.",
        "This paper extends and generalizes previous works on constructing\ncombinatorial multivector fields from continuous systems (see [10]) and the\nconstruction of combinatorial vector fields from data (see [2]) by introducing\nan optimization based framework for the construction of combinatorial\nmultivector fields from finite vector field data. We address key challenges in\nconvexity, computational complexity and resolution, providing theoretical\nguarantees and practical methodologies for generating combinatorial\nrepresentation of the dynamics of our data.",
        "This paper presents an analysis of Model Predictive Control (MPC) and\nReinforcement Learning (RL) approaches for active flow control over a NACA\n(National Advisory Committee for Aeronautics) 4412 airfoil around the static\nstall condition at a Reynolds number of 4*10^5. The Reynolds Averaged\nNavier-Stokes (RANS) equations with the Scale-Adaptive Simulation (SAS)\nturbulence model are utilized for the numerical simulations. The dielectric\nbarrier discharge (DBD) plasma actuators were employed in dual excitation mode\nfor flow separation control. The study systematically evaluates adaptive MPC,\ntemporal difference reinforcement learning (TDRL), and deep Q-learning (DQL)\nbased on optimizing the excitation frequency and expediting the time to\nidentify stable conditions. Moreover, an integrated approach that combines\nsignal processing with DQL is examined. The results demonstrate that while MPC\nand RL significantly improve flow control, RL approaches offer superior\nadaptability and performance. In optimal conditions, a lift coefficient of\naround 1.619 was achieved within less than 2.5 seconds with an excitation\nfrequency of 100 or 200 Hz. This research highlights that RL-based approaches\ncould perform better in flow control applications than MPC.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "It has been claimed that a Kerr-Newman black hole can generically be overspun\nby neutral test fields, and it has been argued that even when backreactions are\ntaken into account, the black hole can still be destroyed. In this paper, we\nrevisit the weak cosmic censorship conjecture for a Kerr-Newman black hole with\na test scalar field and a neutrino field, and point out that the assumption in\nprevious work regarding the energy and angular momentum of the test fields\nabsorbed by the black hole violates the second law of black hole\nthermodynamics. By solving the test scalar field and neutrino field near the\nevent horizon explicitly, we demonstrate that an extremal Kerr-Newman black\nhole cannot be overspun by a test scalar field but can be destroyed by a\nneutrino field. Our results indicate that the condition required to overspin an\nextremal Kerr-Newman black hole coincides with the condition needed to violate\nthe second law of black hole thermodynamics. Furthermore, we observe that such\na violation of the second law might inevitably result in a breakdown of the\nweak cosmic censorship conjecture.",
        "Empty buses are standing at a bus station. $n$ passengers arrive, and they\neach board a bus completely at random (meaning that they choose uniformly and\nindependently). Then all buses depart. We show that the more buses there are,\nthe more likely it is that someone (i.e. at least one passenger) travels alone\n(while $n$ is fixed). More generally, we show that the number of lonely\npassengers increases with the number of buses, in the sense of stochastic\ndominance. This problem turned out to be surprisingly difficult, with no short\nsolution known to the author so far, despite the efforts of many experts. Some\nof the results can also be formulated as properties of Stirling numbers of the\nsecond kind.",
        "The self-gravitating skyrmion is an exact solution of the Einstein\n$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,\nliving in a $4$-dimensional space-time in the presence of a cosmological\nconstant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into\n$SU(N)$ in the Euler angles parametrization, this solution can be generalized\nto include arbitrary values of the flavor number and, consequently, allowing\nhigher values of the topological charge. Also, we show that higher-order\ncorrections in the 't Hooft expansion can be considered while still preserving\nthe analytical nature of the solutions. Finally we will show that from the\ngravitational solutions it is possible to construct skyrmions in flat\nspace-time at a finite volume."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Addressing disparities in the global epidemiology of stroke",
    "start_abstract":"Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Artificial intelligence applications in stroke"
      ],
      "abstract":[
        "Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Agency Is Frame-Dependent",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning",
        "Why Do Multi-Agent LLM Systems Fail?",
        "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
        "Psychometric-Based Evaluation for Theorem Proving with Large Language\n  Models",
        "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "Intention Recognition in Real-Time Interactive Navigation Maps",
        "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
        "Complex Brillouin Zone for Localised Modes in Hermitian and\n  Non-Hermitian Problems",
        "Explicit adaptive time stepping for the Cahn-Hilliard equation by\n  exponential Krylov subspace and Chebyshev polynomial methods",
        "On reflected isotropic stable processes",
        "Quantum Quandaries: Unraveling Encoding Vulnerabilities in Quantum\n  Neural Networks",
        "Evaluating Compression and Nanoindentation in FCC Nickel: A Methodology\n  for Interatomic Potential Selection",
        "Integrated Sensing and Communication System Based on Radio Frequency\n  Resonance Beam",
        "Activity of low mass stars in the light of spot signature in the Fourier\n  domain",
        "Impact of structural distortions on the correlated electronic structure\n  of orbital-selective Mott insulating Na$_3$Co$_2$SbO$_6$ under strains",
        "Influence of nanoparticulates and microgrooves on the secondary electron\n  yield and electrical resistance of laser-treated copper surfaces",
        "Accelerated DC loadflow solver for topology optimization",
        "Predicting Spin-Dependent Coulomb Interaction Based on the Yang-Mills\n  Equations",
        "Experimental realization of a quantum heat engine based on\n  dissipation-engineered superconducting circuits",
        "Range-Only Dynamic Output Feedback Controller for Safe and Secure Target\n  Circumnavigation",
        "Quantum Diffie-Hellman key exchange",
        "Criteria for ion acceleration in laboratory magnetized\n  quasi-perpendicular collisionless shocks: when are 2D simulations enough?"
      ],
      "abstract":[
        "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems.",
        "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
        "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1\/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https:\/\/github.com\/zzli2022\/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
        "Large language models (LLMs) for formal theorem proving have become a\nprominent research focus. At present, the proving ability of these LLMs is\nmainly evaluated through proof pass rates on datasets such as miniF2F. However,\nthis evaluation method overlooks the varying importance of theorems. As a\nresult, it fails to highlight the real performance disparities between LLMs and\nleads to high evaluation costs. This study proposes a psychometric-based\nevaluation method for theorem proving with LLMs, comprising two main\ncomponents: Dataset Annotation and Adaptive Evaluation. First, we propose a\nmetric calculation method to annotate the dataset with difficulty and\ndiscrimination metrics. Specifically, we annotate each theorem in the miniF2F\ndataset and grade them into varying difficulty levels according to the\nperformance of LLMs, resulting in an enhanced dataset: miniF2F-Graded.\nExperimental results show that the difficulty grading in miniF2F-Graded better\nreflects the theorem difficulty perceived by LLMs. Secondly, we design an\nadaptive evaluation method to dynamically select the most suitable theorems for\ntesting based on the annotated metrics and the real-time performance of LLMs.\nWe apply this method to evaluate 10 LLMs. The results show that our method\nfinely highlights the performance disparities between LLMs. It also reduces\nevaluation costs by using only 23% of the theorems in the dataset.",
        "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "In this demonstration, we develop IntentRec4Maps, a system to recognise\nusers' intentions in interactive maps for real-world navigation. IntentRec4Maps\nuses the Google Maps Platform as the real-world interactive map, and a very\neffective approach for recognising users' intentions in real-time. We showcase\nthe recognition process of IntentRec4Maps using two different Path-Planners and\na Large Language Model (LLM).\n  GitHub: https:\/\/github.com\/PeijieZ\/IntentRec4Maps",
        "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
        "We develop a mathematical and numerical framework for studying evanescent\nwaves in subwavelength band gap materials. By establishing a link between the\ncomplex Brillouin zone and various Hermitian and non-Hermitian phenomena,\nincluding defect localisation in band gap materials and the non-Hermitian skin\neffect, we provide a unified perspective on these systems. In two-dimensional\nstructures, we develop analytical techniques and numerical methods to study\nsingularities of the complex band structure. This way, we demonstrate that gap\nfunctions effectively predict the decay rates of defect states. Furthermore, we\npresent an analysis of the Floquet transform with respect to complex\nquasimomenta. Based on this, we show that evanescent waves may undergo a phase\ntransition, where local oscillations drastically depend on the location of\ncorresponding frequency inside the band gap.",
        "The Cahn-Hilliard equation has been widely employed within various\nmathematical models in physics, chemistry and engineering. Explicit stabilized\ntime stepping methods can be attractive for time integration of the\nCahn-Hilliard equation, especially on parallel and hybrid supercomputers. In\nthis paper, we propose an exponential time integration method for the\nCahn-Hilliard equation and describe its efficient Krylov subspace based\nimplementation. We compare the method to a Chebyshev polynomial local iteration\nmodified (LIM) time stepping scheme. Both methods are explicit (i.e., do not\ninvolve linear system solution) and tested with both constant and adaptively\nchosen time steps.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Quantum computing (QC) has the potential to revolutionize fields like machine\nlearning, security, and healthcare. Quantum machine learning (QML) has emerged\nas a promising area, enhancing learning algorithms using quantum computers.\nHowever, QML models are lucrative targets due to their high training costs and\nextensive training times. The scarcity of quantum resources and long wait times\nfurther exacerbate the challenge. Additionally, QML providers may rely on third\nparty quantum clouds for hosting models, exposing them and their training data\nto potential threats. As QML as a Service (QMLaaS) becomes more prevalent,\nreliance on third party quantum clouds poses a significant security risk. This\nwork demonstrates that adversaries in quantum cloud environments can exploit\nwhite box access to QML models to infer the users encoding scheme by analyzing\ncircuit transpilation artifacts. The extracted data can be reused for training\nclone models or sold for profit. We validate the proposed attack through\nsimulations, achieving high accuracy in distinguishing between encoding\nschemes. We report that 95% of the time, the encoding can be predicted\ncorrectly. To mitigate this threat, we propose a transient obfuscation layer\nthat masks encoding fingerprints using randomized rotations and entanglement,\nreducing adversarial detection to near random chance 42% , with a depth\noverhead of 8.5% for a 5 layer QNN design.",
        "We performed molecular dynamics simulations to investigate the mechanical\nresponse of face-centered cubic (FCC) nickel under uniaxial compression and\nnanoindentation using traditional interatomic potentials, including the\nEmbedded Atom Method (EAM) and Modified Embedded Atom Method (MEAM). By\ncalculating the generalized stacking fault energy (GSFE), we analyzed the\ndissociated slip paths responsible for stacking fault formation and partial\nShockley dislocations during mechanical loading. Our findings highlight the\ncritical importance of selecting appropriate interatomic potentials to model\ncompression and nanoindentation tests accurately, aligning simulations with\nexperimental observations. We propose a practical methodology for identifying\nempirical interatomic potentials suitable for mechanical testing of\nsingle-element materials. This approach establishes a benchmark for FCC nickel\nsimulations and provides a basis for extending these methods to more complex\nNi-based alloys, facilitating comparisons with experimental results such as\nthose from electron microscopy.",
        "To address the challenge of complex beam control in traditional\nmultiple-input multiple-output (MIMO) systems, research has proposed\nestablishing adaptive beam alignment by utilizing retro-directive antenna (RDA)\narrays to create echo resonance between the base station (BS) and user\nequipment (UE), thereby reducing system computational load. However, in\nconventional resonant beam systems (RBS), the use of the same frequency for\nuplink and downlink inevitably leads to echo interference issues. Therefore,\nthis paper proposes an innovative design for an resonance beam-based integrated\nsensing and communication (RB-ISAC) system to achieve efficient passive sensing\nand bidirectional communication. In this system, the UE does not actively\ntransmit signals but instead relies on a passive phase conjugation and\nfrequency conversion structure to separate the uplink and downlink carrier\nfrequencies. Additionally, through effective compensation for signal\npropagation loss, resonance is achieved after multiple iterations, at which\npoint the beam's field distribution becomes a low-diffraction-loss,\nhigh-focusing pattern, automatically aligning the transmitter with the\nreceiver. This enables high-precision passive positioning while facilitating\nuplink and downlink communication. Simulation results demonstrate that the\nproposed system can achieve resonance after multiple iterations, and can\nsupport uplink and downlink communication within a range of 5 meters while\nachieving passive direction of arrival (DOA) estimation with an error of less\nthan 2 degrees.",
        "Context. Magnetic fields exhibit a wide variety of behaviours in low mass\nstars and further characterization is required to understand these\nobservations. Stellar photometry from space missions such as MOST, CoRoT,\nKepler, and, in future PLATO, provide thousands of highly precise light curves\n(LC) that can shed new light upon stellar activity, in particular through the\nsignature of transiting spots. Aims. We study the impact of star spots on light\ncurves in the Fourier domain, reducing the degeneracies encountered by direct\nspot modelling in the temporal domain, and use this new formulation to explore\nthe spot properties from the available data. Methods. We propose a model of LC\npower spectra at low frequency based on a description of spot transits that\nallows us to retrieve information about the amplitude of their photometric\nimpact $\\mathcal{H}$, and about the spot mean lifetime over the observation\n$\\tau_{\\rm life}$ when the power spectrum exibits rotation peaks. We first\nvalidate this method with simulated LCs and then apply it to the Kepler data to\nextract global trends over a set of more than 37 755 stars. Results. This\nanalysis leads to a classification of the sample into \"peakless\" or \"with\npeaks\" spectra, and enables the identification of different activity regimes\nbased on $\\mathcal{H}$ and $\\tau_{\\rm life}$ for different ranges of Rossby\nnumber. More specifically, we observe an intense regime of activity between Ro\n= 0.7 and Ro = 1, for stars with masses under 1$M_\\odot$. Conclusions. This new\nsystematic method can be used to provide new observational constraints on\nstellar activity (and possibly a link with stellar magnetism) when applied to\nlarge photometric datasets, such as those from the future PLATO mission.",
        "Na$_{3}$Co$_{2}$SbO$_6$ is a promising candidate to realize the Kitaev spin\nliquid phase since the large Kitaev spin exchange interaction is tunable via\nthe change in electronic structure, such as the trigonal crystal field\nsplitting ($\\Delta_{TCF}$). Here, we show that the uncorrelated electronic\nstructure of Na$_{3}$Co$_{2}$SbO$_6$ is rather insensitive to the strain effect\ndue to the low crystal symmetry accompanied by oxygen displacements and the\npresence of Sb $s$ orbitals. This suggests that the Kitaev spin-exchange\ninteraction obtained from perturbation theory also does not depend much on the\nstrain effect. Using density functional theory plus dynamical mean field\ntheory, we find that the correlated electronic structure of\nNa$_{3}$Co$_{2}$SbO$_6$ is an orbital selective Mott insulating state where the\ntrigonal $a_{1g}$ orbital is insulating due to correlation-assisted\nhybridization, while other $d$ orbitals behave as typical Mott insulators,\nresulting in tunability of $\\Delta_{TCF}$ under the strain effect effectively.\nOur results show that the local Co-site symmetry and dynamical correlation\neffects will play an important role in engineering the novel magnetic phase in\nthis and related materials.",
        "Laser surface structuring has proven to be an effective technique for\nachieving a copper surface with secondary electron yield (SEY) values close to\nor below unity. However, the attributes that minimize SEY, such as moderately\ndeep grooves and redeposited nanoparticles, may lead to undesirable\nconsequences, including increased radio frequency surface resistance. This\ninvestigation systematically examined data about different cleaning procedures\ndesigned to eliminate redeposited adsorbed particulates. Various analysis\ntechniques were used iteratively after each consecutive cleaning step,\nproviding insights into the evolving surface characteristics. The collected\nexperimental results identified distinct impacts of microgrooves, groove\norientation, and associated particulates on secondary electron yield and\nsurface resistance. Exposing the crests while retaining high particulate\ncoverage in the grooves leads to reduced SEY values and surface resistance,\nsuggesting that the tips of the grooves exert a more significant influence on\nsurface current density than the groove depth. At the same time, nanoparticles\nin the grooves have a more significant impact on SEY values than the exposed\ntips at the surface.",
        "We present a massively parallel solver that accelerates DC loadflow\ncomputations for power grid topology optimization tasks. Our approach leverages\nlow-rank updates of the Power Transfer Distribution Factors (PTDFs) to\nrepresent substation splits, line outages, and reconfigurations without ever\nrefactorizing the system. Furthermore, we implement the core routines on\nGraphics Processing Units (GPUs), thereby exploiting their high-throughput\narchitecture for linear algebra. A two-level decomposition separates changes in\nbranch topology from changes in nodal injections, enabling additional speed-ups\nby an in-the-loop brute force search over injection variations at minimal\nadditional cost. We demonstrate billion-loadflow-per-second performance on\npower grids of varying sizes in workload settings which are typical for\ngradient-free topology optimization such as Reinforcement Learning or Quality\nDiversity methods. While adopting the DC approximation sacrifices some accuracy\nand prohibits the computation of voltage magnitudes, we show that this\nsacrifice unlocks new scales of computational feasibility, offering a powerful\ntool for large-scale grid planning and operational topology optimization.",
        "The standard Coulomb interaction is one of four fundamental interactions in\nNature. It is interesting to know how will the standard Coulomb interaction be\nmodified when it meets spin. Since the standard Coulomb potential is a simple\nbut fundamental solution of Maxwell's equations, hence Maxwell's equations can\npredict the existence of the standard Coulomb potential. The Yang-Mills\nequations are the natural generalizations of Maxwell's equations from the\nAbelian potentials to the non-Abelian ones, thus based on the Yang-Mills\nequations, one can predict the reasonable form of spin-dependent Coulomb\npotential, which naturally reduces the standard Coulomb potential if without\nconsidering the spin. Our work sheds a new light to how to couple spin with\nfundamental interactions.",
        "Quantum heat engines (QHEs) have attracted long-standing scientific interest,\nespecially inspired by considerations of the interplay between heat and work\nwith the quantization of energy levels, quantum superposition, and\nentanglement. Operating QHEs calls for effective control of the thermal\nreservoirs and the eigenenergies of the quantum working medium of the engine.\nAlthough superconducting circuits enable accurate engineering of controlled\nquantum systems, beneficial in quantum computing, this framework has not yet\nbeen employed to experimentally realize a cyclic QHE. Here, we experimentally\ndemonstrate a quantum heat engine based on superconducting circuits, using a\nsingle-junction quantum-circuit refrigerator (QCR) as a two-way tunable heat\nreservoir coupled to a flux-tunable transmon qubit acting as the working medium\nof the engine. We implement a quantum Otto cycle by a tailored drive on the QCR\nto sequentially induce cooling and heating, interleaved with flux ramps that\ncontrol the qubit frequency. Utilizing single-shot qubit readout, we monitor\nthe evolution of the qubit state during several cycles of the heat engine and\nmeasure positive output powers and efficiencies that agree with our simulations\nof the quantum evolution. Our results verify theoretical models on the\nthermodynamics of quantum heat engines and advance the control of\ndissipation-engineered thermal environments. These proof-of-concept results\npave the way for explorations on possible advantages of QHEs with respect to\nclassical heat engines.",
        "The safety and security of robotic systems are paramount when navigating\naround a hostile target. This paper addresses the problem of circumnavigating\nan unknown target by a unicycle robot while ensuring it maintains a desired\nsafe distance and remains within the sensing region around the target\nthroughout its motion. The proposed control design methodology is based on the\nconstruction of a joint Lyapunov function that incorporates: (i) a quadratic\npotential function characterizing the desired target-circumnavigation\nobjective, and (ii) a barrier Lyapunov function-based potential term to enforce\nsafety and sensing constraints on the robot's motion. A notable feature of the\nproposed control design is its reliance exclusively on local range measurements\nbetween the robot and the target, realized using a dynamic output feedback\ncontroller that treats the range as the only observable output for feedback.\nUsing the Lyapunov stability theory, we show that the desired equilibrium of\nthe closed-loop system is asymptotically stable, and the prescribed safety and\nsecurity constraints are met under the proposed controllers. We also obtain\nrestrictive bounds on the post-design signals and provide both simulation and\nexperimental results to validate the theoretical contributions.",
        "The Diffie-Hellman key exchange plays a crucial role in conventional\ncryptography, as it allows two legitimate users to establish a common, usually\nephemeral, secret key. Its security relies on the discrete-logarithm problem,\nwhich is considered to be a mathematical one-way function, while the final key\nis formed by random independent actions of the two users. In the present work\nwe investigate the extension of Diffie-Hellman key exchange to the quantum\nsetting, where the two legitimate users exchange independent random quantum\nstates. The proposed protocol relies on the bijective mapping of integers onto\na set of symmetric coherent states, and we investigate the regime of parameters\nfor which the map behaves as a quantum one-way function. Its security is\nanalyzed in the framework of minimum-error-discrimination and\nphoton-number-splitting attacks, while its performance and the challenges in a\npossible realization are also discussed.",
        "The study of collisionless shocks and their role in cosmic ray acceleration\nhas gained importance through observations and simulations, driving interest in\nreproducing these conditions in laboratory experiments using high-power lasers.\nIn this work, we examine the role of three-dimensional (3D) effects in ion\nacceleration in quasi-perpendicular shocks under laboratory-relevant\nconditions. Using hybrid particle-in-cell simulations (kinetic ions and fluid\nelectrons), we explore how the Alfv\\'enic and sonic Mach numbers, along with\nplasma beta, influence ion energization, unlocked only in 3D, and establish\nscaling criteria for when conducting 3D simulations is necessary. Our results\nshow that efficient ion acceleration requires Alfv\\'enic Mach numbers $\\geq 25$\nand sonic Mach numbers $\\geq 13$, with plasma-$\\beta \\leq 5$. We theoretically\nfound that, while 2D simulations suffice for current laboratory-accessible\nshock conditions, 3D effects become crucial for shock velocities exceeding 1000\nkm\/s and experiments sustaining the shock for at least 10 ns. We surveyed\nprevious laboratory experiments on collisionless shocks and found that 3D\neffects are unimportant under those conditions, implying that 1D and 2D\nsimulations should be enough to model the accelerated ion spectra. However, we\ndo find that the same experiments are realistically close to accessing the\nregime relevant to 3D effects, an exciting prospect for future laboratory\nefforts. We propose modifications to past experimental configurations to\noptimize and control 3D effects on ion acceleration. These proposed experiments\ncould be used to benchmark plasma astrophysics kinetic codes and\/or employed as\ncontrollable sources of energetic particles."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Artificial intelligence applications in stroke",
    "start_abstract":"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Addressing disparities in the global epidemiology of stroke"
      ],
      "abstract":[
        "Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "The Amplitude Modulation Structure of Japanese Infant- and\n  Child-Directed Speech: Longitudinal Data Reveal Universal Acoustic Physical\n  Structures Underpinning Moraic Timing",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Neuro-oscillatory models of cortical speech processing",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Reasoning within and between collective action problems",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "Automated generation of epilepsy surgery resection masks; The RAMPS\n  pipeline",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Multidimensional moment problem and Stieltjes transform",
        "Algebraic solution of the Jacobi inverse problem and explicit addition\n  laws",
        "Yang-Mills-Utiyama Theory and Graviweak Correspondence",
        "Measuring Top Yukawa Coupling through $2\\rightarrow 3$ VBS at Muon\n  Collider",
        "Phonons in Electron Crystals with Berry Curvature",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing\n  Recovery Rate Predictions",
        "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE)\n  XVII. Statistical properties of individual HII regions in unperturbed systems",
        "Birth of magnetized low-mass protostars and circumstellar disks",
        "The soccer model, stochastic ordering and martingale transport",
        "MNE: overparametrized neural evolution with applications to diffusion\n  processes and sampling",
        "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks",
        "On generalized Tur{\\'a}n problems with bounded matching number and\n  circumference",
        "The sliding tile puzzle, roots to polynomials, and $\\textbf{P}$ vs.\n  $\\textbf{NP}$ complexity",
        "Multi-constraint Graph Partitioning Problems Via Recursive Bipartition\n  Algorithm Based on Subspace Minimization Conjugate Gradient Method"
      ],
      "abstract":[
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "Infant-directed speech (IDS) is highly rhythmic, and in European languages\nIDS is dominated by patterns of amplitude modulation (AM) at ~2Hz (reflecting\nprosody) and ~5Hz (reflecting individual syllables). The rhythm structure of\nspoken Japanese is thought to differ from European stress-timed and\nsyllable-timed languages, depending on moraic units. Morae comprise any onset\nphoneme and vowel phonemes within a syllable, PA-N-DA. Arguably, initial speech\nencoding via cortical tracking by infants is likely to utilize\nlanguage-universal physical acoustic structures in speech rather than\nlanguage-specific structures like morae, since the infant brain must be\nprepared to acquire any human language. Here a language-blind computational\nmodel of linguistic rhythm based on features of the amplitude envelope is used\nto compute these physical acoustic stimulus characteristics for Japanese. Using\n~18,000 samples of natural IDS and child-directed speech (CDS) recorded\nlongitudinally from 6 parents while speaking to their children over the ages\n0-5 years, we find that the temporal modulation patterns that characterise the\namplitude envelope of Japanese are highly similar to those found for\nstress-timed and syllable-timed European languages. However, the AM band\ncorresponding to the syllabic level in CDS\/IDS in European languages (~2-12Hz,\ntheta-rate cortical tracking) was elongated in Japanese (2.5-17Hz). Further,\nthe phase synchronization ratios between the two slowest AM bands were as\nlikely to be 1:3 as 1:2, which differs from European languages where 1:2 ratios\nare dominant. Accordingly, the language-universal amplitude-driven physical\nacoustic structures important for cortical speech tracking flexibly accommodate\nlanguage-specific differences in core rhythmic units.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Understanding cooperation in social systems is challenging because the\never-changing rules that govern societies interact with individual actions,\nresulting in intricate collective outcomes. In virtual-world experiments, we\nallowed people to make changes in the systems that they are making decisions\nwithin and investigated how they weigh the influence of different rules in\ndecision-making. When choosing between worlds differing in more than one rule,\na naive heuristics model predicted participants decisions as well, and in some\ncases better, than game earnings (utility) or by the subjective quality of\nsingle rules. In contrast, when a subset of engaged participants made\ninstantaneous (within-world) decisions, their behavior aligned very closely\nwith objective utility and not with the heuristics model. Findings suggest\nthat, whereas choices between rules may deviate from rational benchmarks, the\nfrequency of real time cooperation decisions to provide feedback can be a\nreliable indicator of the objective utility of these rules.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "MRI-based delineation of brain tissue removed by epilepsy surgery can be\nchallenging due to post-operative brain shift. In consequence, most studies use\nmanual approaches which are prohibitively time-consuming for large sample\nsizes, require expertise, and can be prone to errors.\n  We propose RAMPS (Resections And Masks in Preoperative Space), an automated\npipeline to generate a 3D resection mask of pre-operative tissue. Our pipeline\nleverages existing software including FreeSurfer, SynthStrip, Sythnseg and ANTS\nto generate a mask in the same space as the patient's pre-operative T1 weighted\nMRI. We compare our automated masks against manually drawn masks and two other\nexisting pipelines (Epic-CHOP and ResectVol).\n  Comparing to manual masks (N=87), RAMPS achieved a median(IQR) dice\nsimilarity of 0.86(0.078) in temporal lobe resections, and 0.72(0.32) in\nextratemporal resections. In comparison to other pipelines, RAMPS had higher\ndice similarities (N=62) (RAMPS:0.86, Epic-CHOP: 0.72, ResectVol: 0.72).\n  We release a user-friendly, easy to use pipeline, RAMPS, open source for\naccurate delineation of resected tissue.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "The truncated multidimensional moment problem is studied in terms of the\nStieltjes transform as the interpolation problem. A step-by-step algorithm is\nconstructed for the multidimensional moment problem and the set of solutions is\nfound in terms of continued fractions.",
        "We formulate a solution to the Algebraic version of the Inverse Jacobi\nproblem. Using this solution we produce explicit addition laws on any algebraic\ncurve generalizing the law suggested by Leykin [2] in the case of (n, s)\ncurves. This gives a positive answer to a question asked by T. Shaska whether\naddition laws appearing in [2] can be produced in a coordinate free manner.",
        "This report provides a geometrical Yang-Mills theory, including gravity. The\ntheory treats the space-time symmetry of the local Lorentz group in the same\nmanner as the internal gauge symmetry. We extend this general relativistic\nYang-Mills theory in the Minkowski space-time to a broader space, including\nEuclidean and Minkowskian spaces. In the extended space, we can transport\ntopological achievements from the Euclidean Yang-Mills theory into the\nLorentzian Yang-Mills theory. This extension also provides a relation between\nspace-time and internal symmetry. The perspective provided by the extended\ntheory suggests the novel relation between gravity and the weak force, leading\nus to the graviweak correspondence.",
        "We study the measurement of top Yukawa coupling through $2\\rightarrow 3$ VBS\nat future muon colliders, focusing on the lepton and semi-lepton channels of\n$\\nu\\nu tth\/z$. First, analyzing the partonic amplitudes of $W_LW_L\\rightarrow\nt\\bar t h\/Z_L$ and simulating the full processes of $\\nu\\nu tth\/z$ without\ndecaying, we find they are highly sensitive to the anomalous top Yukawa $\\delta\ny_t$. This sensitivity is enhanced by selecting helicities of the final $t\\bar\nt$ and $Z$ to be $t_L\\bar t_L+t_R\\bar t_R$ and $Z_L$, which serves as the\ndefault and core setting of our analysis. We then obtain the limits on $\\delta\ny_t$ with this setting, giving $[-1.0\\%, 1.1\\%]$ for $\\nu\\nu tth$ only and\n$[-0.36\\%, 0.92\\%]$ for $\\nu\\nu tth$ and $\\nu\\nu ttz$ combined at $30$ TeV and\n$1\\sigma$. Second, we proceed to analyze the processes after decaying and with\nbackground processes. To enhance the sensitivity to $\\delta y_t$, our settings\ninclude selecting the helicities of the final particles, as well as applying\nsuitable cuts. However, we don't do bin-by-bin analysis. We obtain the limits\non $\\delta y_t$ for those channels at $10\/30$ TeV and $1\\sigma\/2 \\sigma$. The\nbest limit is from the semi-lepton channel of $\\nu\\nu tth$. With spin tagging\nefficiency at $\\epsilon_s=0.9$, it gives $[-1.6\\% , 1.8\\%]$ at $1\\sigma$ and $\n[-2.4\\%, 2.7\\% ]$ at $2\\sigma$ at $30$ TeV; $[-7.0\\%, 6.7\\%]$ at $1\\sigma$ and\n$[-9.8\\%, 9.8\\%]$ at $2\\sigma$ at $10$ TeV.",
        "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
        "The Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE) is a\nblind narrow-band Halpha+[NII] imaging survey of the Virgo cluster carried out\nwith MegaCam at the CFHT telescope. The survey provides deep narrow-band images\nfor 385 galaxies hosting star forming HII regions. We identify individual HII\nregions and measure their main physical properties such as Halpha luminosity,\nequivalent diameter, and electron density with the purpose of deriving standard\nrelations as reference for future local and high-z studies of HII regions in\nstar forming systems in different environments. For this purpose we use a\ncomplete sample of ~ 13.000 HII regions of luminosity L(Halpha)>= 10^37 erg\ns^-1 to derive the main statistical properties of HII regions in unperturbed\nsystems, identified as those galaxies with a normal HI gas content (64\nobjects). These are the composite Halpha luminosity function, equivalent\ndiameter and electron density distribution, and luminosity-size relation. We\nalso derive the main scaling relations between several parameters\nrepresentative of the HII regions properties (total number, luminosity of the\nfirst ranked regions, fraction of the diffuse component, best fit parameters of\nthe Schechter luminosity function measured for individual galaxies) and those\ncharacterising the properties of the host galaxies (stellar mass, star\nformation rate and specific star formation rate, stellar mass and star\nformation rate surface density, metallicity, molecular-to-atomic gas ratio,\ntotal gas-to-dust mass ratio). We briefly discuss the results of this analysis\nand their implications in the study of the star formation process in galaxy\ndiscs.",
        "Although protostars and disks are often studied separately owing to numerical\nand observational challenges, breakthroughs in recent years have highlighted\nthe need to study both objects in concert. The role of magnetic fields in this\nregard must be investigated. We aim to describe the birth of the protostar and\nthat of its disk, as well as their early joint evolution following the second\ncollapse. We wish to study the structure of the nascent star-disk system, while\nfocusing on the innermost sub-AU region. We carry out high resolution 3D RMHD\nsimulations, describing the collapse of dense cloud cores to stellar densities.\nThe calculations reach $\\approx 2.3$ yr after protostellar birth. Our\nsimulations are also compared to their hydro counterpart to better isolate the\nrole of magnetic fields. When accounting for ambipolar diffusion, the\nefficiency of magnetic braking is drastically reduced and the nascent protostar\nreaches breakup velocity, thus forming a rotationally supported disk. The\ndiffusion of the magnetic field also allows for the implantation of a $\\sim\n\\mathrm{kG}$ field in the protostar, which is thereafter maintained. The\nmagnetic field is mainly toroidal in the star-disk system, although a notable\nvertical component threads it. We also show that the nascent disk is prone to\nthe MRI, although our resolution is inadequate to capture the mechanism. We\nnote a sensitivity of the disk's properties with regards to the angular\nmomentum inherited prior to the second collapse, as well as the magnetic field\nstrength. These calculations carry multiple implications on several issues in\nstellar formation theory, and offer perspectives for future modeling of the\nsystem. Should the fossil field hypothesis to explain the origins of magnetic\nfields in young stellar objects hold, we show that a $\\sim \\mathrm{kG}$ field\nstrength may be implanted and maintained in the protostar at birth.",
        "Tournaments are competitions between a number of teams, the outcome of which\ndetermines the relative strength or rank of each team. In many cases, the\nstrength of a team in the tournament is given by a score. Perhaps, the most\nstriking mathematical result on the tournament is Moon's theorem, which\nprovides a necessary and sufficient condition for a feasible score sequence via\nmajorization. To give a probabilistic interpretation of Moon's result, Aldous\nand Kolesnik introduced the soccer model,the existence of which gives a short\nproof of Moon's theorem. However, the existence proof of Aldous and Kolesnik is\nnonconstructive, leading to the question of a ``canonical'' construction of the\nsoccer model. The purpose of this paper is to provide explicit constructions of\nthe soccer model with an additional stochastic ordering constraint, which can\nbe formulated by martingale transport. Two solutions are given: one is by\nsolving an entropy optimization problem via Sinkhorn's algorithm, and the other\nrelies on the idea of shadow couplings. It turns out that both constructions\nyield the property of strong stochastic transitivity. The nontransitive\nsituations of the soccer model are also considered.",
        "We propose a framework for solving evolution equations within parametric\nfunction classes, especially ones that are specified by neural networks. We\ncall this framework the minimal neural evolution (MNE) because it is motivated\nby the goal of seeking the smallest instantaneous change in the neural network\nparameters that is compatible with exact solution of the evolution equation at\na set of evolving collocation points. Formally, the MNE is quite similar to the\nrecently introduced Neural Galerkin framework, but a difference in perspective\nmotivates an alternative sketching procedure that effectively reduces the\nlinear systems solved within the integrator to a size that is interpretable as\nan effective rank of the evolving neural tangent kernel, while maintaining a\nsmooth evolution equation for the neural network parameters. We focus\nspecifically on the application of this framework to diffusion processes, where\nthe score function allows us to define intuitive dynamics for the collocation\npoints. These can in turn be propagated jointly with the neural network\nparameters using a high-order adaptive integrator. In particular, we\ndemonstrate how the Ornstein-Uhlenbeck diffusion process can be used for the\ntask of sampling from a probability distribution given a formula for the\ndensity but no training data. This framework extends naturally to allow for\nconditional sampling and marginalization, and we show how to systematically\nremove the sampling bias due to parametric approximation error. We validate the\nefficiency, systematic improvability, and scalability of our approach on\nillustrative examples in low and high spatial dimensions.",
        "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.",
        "Let \\( \\mathcal{F} \\) be a family of graphs. The generalized Tur\\'an number\n\\( \\operatorname{ex}(n, K_r, \\mathcal{F}) \\) is the maximum number of $K_r$ in\nan \\( n \\)-vertex graph that does not contain any member of \\( \\mathcal{F} \\)\nas a subgraph. Recently, Alon and Frankl initiated the study of Tur\\'an\nproblems with bounded matching number. In this paper, we determine the\ngeneralized Tur\\'an number of \\( C_{\\geq k} \\) with bounded matching number.",
        "This work explores the relationship between solution space and time\ncomplexity in the context of the $\\textbf{P}$ vs. $\\textbf{NP}$ problem,\nparticularly through the lens of the sliding tile puzzle and root finding\nalgorithms. We focus on the trade-off between finding a solution and verifying\nit, highlighting how understanding the structure of the solution space can\ninform the complexity of these problems. By examining the relationship between\nthe number of possible configurations and the time complexity required to\ntraverse this space we demonstrate that the minimal time to verify a solution\nis often smaller than the time required to discover it. Our results suggest\nthat the efficiency of solving $\\textbf{NP}$-complete problems is not only\ndetermined by the ability to find solutions but also by how effectively we can\nnavigate and characterize the solution space. This study contributes to the\nongoing discourse on computational complexity, particularly in understanding\nthe interplay between solution space size, algorithm design, and the inherent\nchallenges of finding versus verifying solutions.",
        "The graph partitioning problem is a well-known NP-hard problem. In this\npaper, we formulate a 0-1 quadratic integer programming model for the graph\npartitioning problem with vertex weight constraints and fixed vertex\nconstraints, and propose a recursive bipartition algorithm based on the\nsubspace minimization conjugate gradient method. To alleviate the difficulty of\nsolving the model, the constrained problem is transformed into an unconstrained\noptimization problem using equilibrium terms, elimination methods, and\ntrigonometric properties, and solved via an accelerated subspace minimization\nconjugate gradient algorithm. Initial feasible partitions are generated using a\nhyperplane rounding algorithm, followed by heuristic refinement strategies,\nincluding one-neighborhood and two-interchange adjustments, to iteratively\nimprove the results. Numerical experiments on knapsack-constrained graph\npartitioning and industrial examples demonstrate the effectiveness and\nfeasibility of the proposed algorithm."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study",
    "start_abstract":"Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study"
      ],
      "abstract":[
        "Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neuro-oscillatory models of cortical speech processing",
        "How constraints on editing affects cultural evolution",
        "The Role of Affective States in Computational Psychiatry",
        "What Drives Cluster Cool-Core Transformations? A Population Level\n  Analysis of TNG-Cluster",
        "A First Look at \"Continuous Spin\" Gravity -- Time Delay Signatures",
        "Unified model of the Hall effect from insulator to overdoped compounds\n  in cuprate superconductors",
        "Every group is the automorphism group of a graph with arbitrarily large\n  genus",
        "Equivalence principles in Weyl transverse gravity",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Topological Data Analysis of Abelian Magnetic Monopoles in Gauge\n  Theories",
        "Two-sided bounds on the point-wise spatial decay of ground states in the\n  renormalized Nelson model with confining potentials",
        "Elusive properties of countably infinite graphs",
        "Quantum stick-slip motion in nanoscaled friction",
        "Constructing Fundamentals for the Theory of Proportions and Symbolic\n  Allusions Applied Interdisciplinarily",
        "Observation of the $W$-annihilation process $D_s^+ \\to \\omega\\rho^+$ and\n  measurement of $D_s^+ \\to \\phi\\rho^+$ in $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$\n  decays",
        "PAH Feature Ratios Around Stellar Clusters and Associations in 19 Nearby\n  Galaxies"
      ],
      "abstract":[
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "In this study, we examine the frequency and physical drivers of\ntransformations from cool-core (CC) to non-cool-core (NCC) clusters, and vice\nversa, in a sample of 352 massive galaxy clusters (M_vir = 10^14-15.3 M_sun)\nfrom the TNG-Cluster magnetohydrodynamical cosmological simulation of galaxies.\nBy identifying transformations based on the evolution of central entropy and\nfocusing on z<2.5, we find that clusters frequently undergo such events,\ndepending on their assembly and supermassive black hole histories. On average,\nclusters experience 2 to 3 transformations. Transformations can occur in both\ndirections and can be temporary, but those to higher entropy cores, i.e. in the\ndirection from CC to NCC states, are the vast majority. CC phases are shorter\nthan NCC phases, and thus overall the TNG-Cluster population forms with\nlow-entropy cores and moves towards NCC states with time. We study the role\nthat mergers play in driving transformations, and find that mergers within\n~1Gyr prior to a transformation toward higher (but not lower) entropy cores\noccur statistically more often than in a random control sample. Most\nimportantly, we find examples of mergers associated with CC disruption\nregardless of their mass ratio or angular momentum. However, past merger\nactivity is not a good predictor for z=0 CC status, at least based on core\nentropy, even though clusters undergoing more mergers eventually have the\nhighest core entropy values at z=0. We consider the interplay between AGN\nfeedback and evolving cluster core thermodynamics. We find that core\ntransformations are accompanied by an increase in AGN activity, whereby\nfrequent and repeated (kinetic) energy injections from the central SMBHs can\nproduce a collective, long-term impact on central entropy, ultimately heating\ncluster cores. Whether such fast-paced periods of AGN activity are triggered by\nmergers is plausible, but not necessary.",
        "We consider the possibility that gravity is mediated by \"continuous spin\"\nparticles, i.e., massless particles whose invariant spin scale $\\rho_g$ is\nnon-zero. In this case, the primary helicity-2 modes of gravitational radiation\non a Minkowski background mix with a tower of integer-helicity partner modes\nunder boosts, with $\\rho_g$ controlling the degree of mixing. We develop a\nformalism for coupling spinless matter to continuous spin gravity at linearized\nlevel. Using this formalism, we calculate the time delay signatures induced by\ngravitational waves in an idealized laser interferometer detector. The\nfractional deviation from general relativity predictions is $O(\\rho_g\/\\omega)$\nfor gravitational wave frequencies $\\omega >\\rho_g$, and the effects of waves\nwith $\\omega \\lesssim \\rho_g$ are damped. The precision and low frequency\nranges of gravitational wave detectors suggest potential sensitivity to spin\nscales at or below $\\sim 10^{-14}$ eV at ground-based laser interferometers and\n$\\sim 10^{-24}$ eV at pulsar timing arrays, motivating further analysis of\nobservable signatures.",
        "Measurements of the Hall coefficient in La$_{2-x}$Sr$_x$CuO$_4$, ranging from\nthe undoped ($x = p = 0$) Mott insulator to overdoped compounds, exhibit a\ntemperature dependence that offers insights into their electronic structure. We\ninterpret these results using a model based on the theory of phase-separation\n(PS) dynamics, which begins at half-filled ($n = 1$) and at a temperature\n$T_{\\rm PS}(p)$, near the pseudogap temperature $T^*(p)$. The $n = 1$ holes\nhave low mobility and provide the modulations of the charge density waves\n(CDW). As doping increases from $p = 0$, these modulations guide the additional\np holes to occupy alternating CDW domains. This charge inhomogeneity may\nfacilitate the formation of localized superconducting amplitudes below the\ncritical onset temperature $T_{\\rm c}^{\\rm max}(p)$. Using thermal activation\nexpressions, along with quantum tunnelling between the charge domains, we\nsuccessfully reproduce all Hall coefficient measurements $R_{\\rm H}(p,T)$ and\nhighlight the relevant energies of cuprates. The calculations confirm three\nsignificant electronic features: the phase-separating role of the pseudogap\ntemperature, the superconducting state achieved through phase coherence,\n  and the two types of charge carriers whose energies and mobilities become\ncomparable at $p \\approx 0.19$, where\n  $T^*(p) \\approx T_{\\rm c}^{\\rm max}(p)$. This results in a crossover from $n\n= p$ to $n = 1 + p$. These findings, along with the\n  $R_{\\rm H}(p,T)$ calculations from insulating to overdoped compounds,\nunderscore the critical role of the electronic\n  phase separation in the properties of cuprates.",
        "We prove that, to every abstract group $G$, we can associate a sequence of\ngraphs $\\Gamma_n$ such that the automorphism group of $\\Gamma_n$ is isomorphic\nto $G$ and the genus of $\\Gamma_n$ is an unbounded function of $n$.",
        "There exist two consistent theories of massless, self-interacting gravitons,\nwhich differ by their local symmetries: general relativity and Weyl transverse\ngravity. We show that these two theories are also the only two metric\ndescriptions of gravity in 4 spacetime dimensions which obey the equivalence\nprinciple for test gravitational physics. We further analyse how the weaker\nformulations of the equivalence principle are realised in Weyl transverse\ngravity (and its generalisations). The analysis sheds light on the behaviour of\nmatter fields in this theory.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "Motivated by recent literature on the possible existence of a second\nhigher-temperature phase transition in Quantum Chromodynamics, we revisit the\nproposal that colour confinement is related to the dynamics of magnetic\nmonopoles using methods of Topological Data Analysis, which provides a\nmathematically rigorous characterisation of topological properties of\nquantities defined on a lattice. After introducing persistent homology, one of\nthe main tools in Topological Data Analysis, we shall discuss how this concept\ncan be used to quantitatively analyse the behaviour of monopoles across the\ndeconfinement phase transition. Our approach is first demonstrated for Compact\n$U(1)$ Lattice Gauge Theory, which is known to have a zero-temperature\ndeconfinement phase transition driven by the restoration of the symmetry\nassociated with the conservation of the magnetic charge. For this system, we\nperform a finite-size scaling analysis of observables capturing the homology of\nmagnetic current loops, showing that the expected value of the deconfinement\ncritical coupling is reproduced by our analysis. We then extend our method to\n$SU(3)$ gauge theory, in which Abelian magnetic monopoles are identified after\nprojection in the Maximal Abelian Gauge. A finite-size scaling of our\nhomological observables of Abelian magnetic current loops at temporal size $N_t\n= 4$ provides the expected value of the critical coupling with an accuracy that\nis generally higher than that obtained with conventional thermodynamic\napproaches at comparable statistics, hinting towards the relevance of\ntopological properties of monopole currents for confinement.",
        "We study the renormalized Nelson model for a scalar matter particle in a\ncontinuous confining potential interacting with a possibly massless quantized\nradiation field. When the radiation field is massless we impose a mild infrared\nregularization ensuring that the Nelson Hamiltonian has a non-degenerate ground\nstate in all considered cases. Employing Feynman-Kac representations, we derive\nlower bounds on the point-wise spatial decay of the partial Fock space norms of\nground state eigenvectors. Here the exponential rate function governing the\ndecay is given by the Agmon distance familiar from the analysis of\nSchr\\\"{o}dinger operators. For a large class of confining potentials, our lower\nbounds on the decay of ground state eigenvectors match asymptotically with the\nupper bounds implied by previous work of the present authors.",
        "A graph property is elusive (or evasive) if any algorithm testing it by\nasking questions of the form ''Is there an edge between vertices x and y?''\nmust, in the worst case, examine all pairs of vertices. Elusiveness for\ninfinite vertex sets has been first studied by Csern\\'ak and Soukup, who proved\nthat the long-standing Aanderaa-Karp-Rosenberg Conjecture -- which states that\nevery nontrivial monotone graph property is elusive -- fails for infinite\nvertex sets. We extend their work by giving a closer look to the case when the\nvertex set is countably infinite and the ''algorithm'' terminates after\ninfinitely many steps. Among others, we prove that connectedness is elusive,\nwhich strengthens a result of Csern\\'ak and Soukup. We give counterexamples to\nthe infinite version of the Aanderaa-Karp-Rosenberg Conjecture even if the\n''algorithm'' is required to terminate after infinitely many steps, which\nstrengthens results of Csern\\'ak and Soukup.",
        "Friction in atomistic systems is usually described by the classical\nPrandtl-Tomlinson model suitable for capturing the dragging force of a\nnanoparticle in a periodic potential. Here we consider the quantum mechanical\nversion of this model in which the dissipation is facilitated by releasing heat\nto an external bath reservoir. The time evolution of the system is captured\nwith the Liouville-von Neumann equation through the density matrix of the\nsystem in the Markov approximation. We examine several kinetic and dissipative\nproperties of the nanoparticle by delineating classical vs quantum mechanical\neffects. We find that that the Landau-Zener tunneling is a key factor in the\noverall reduction of the frictional dissipation when compared to the classical\nmotion in which such tunneling is absent. This in-depth study analyzes the\ninterplay between velocity, strength of interaction, and temperature to control\nthe frictional process and provide useful guidelines for experimental data\ninterpretation.",
        "The Theory of Proportions and Symbolic Allusions applied Interdisciplinary\n(TPASAI) is a framework that integrates mathematics, linguistics, psychology,\nand game theory to uncover hidden patterns and proportions in reality. Its\ncentral idea is that numerical encoding of symbols, dates, and language can\nreveal recurring structures and connections that reflect universal principles.\nBy applying fractal analysis, the theory identifies patterns across different\nscales, offering a unifying perspective on the structure of the world. One key\naspect of TPASAI is symbolic analysis, which allows for the reinterpretation of\ntraumatic experiences in psychotherapy. For example, assigning numerical values\nto elements like fingers, dates, or words can help individuals uncover\nmeaningful associations between personal experiences and collective symbols.\nThis approach encourages cognitive flexibility and provides a therapeutic\navenue for recontextualizing emotions. The theory also incorporates principles\nof game theory, which frame reality as a system of symbolic \"codes\" governed by\nrules that can be understood and strategically used. This perspective is\nespecially useful for psychological conditions like obsessive-compulsive\ndisorder (OCD), enabling patients to approach their obsessions as decipherable\npatterns rather than rigid constraints. TPASAI has practical applications in\npsychology, education, and technology. In education, it aids in teaching\nmathematical and linguistic concepts by exploring connections between symbolic\nrepresentations and real-world events. In technology, the methodology can be\nemployed in ciphering and natural language processing. The innovation of TPASAI\nlies in its ability to merge the structured rigor of mathematics with the\ninterpretative flexibility of symbolic analysis, offering a deeper\nunderstanding of events and relationships.",
        "We present the first amplitude analysis and branching fraction measurement of\nthe decay $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$, using $e^+e^-$ collision data\ncollected with the BESIII detector at center-of-mass energies between 4.128 and\n4.226 GeV corresponding to an integrated luminosity of 7.33 fb$^{-1}$, and\nreport the first observation of the pure $W$-annihilation decay $D_s^+ \\to\n\\omega\\rho^+$ with a branching fraction of $(0.99\\pm0.08_{\\rm stat}\\pm0.07_{\\rm\nsyst})\\%$. In comparison to the low significance of the $\\mathcal{D}$ wave in\nthe decay $D_s^+ \\to \\phi\\rho^+$, the dominance of the $\\mathcal{D}$ wave over\nthe $\\mathcal{S}$ and $\\mathcal{P}$ waves, with a fraction of\n$(51.85\\pm7.28_{\\rm stat}\\pm7.90_{\\rm syst})\\%$ observed in the decay, provides\ncrucial information for the``polarization puzzle\", as well as for the\nunderstanding of charm meson decays. The branching fraction of $D^+_s\\to\n\\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$ is measured to be $(4.41\\pm0.15_{\\rm\nstat}\\pm0.13_{\\rm syst})\\%$. Moreover, the branching fraction of $D_s^+ \\to\n\\phi\\rho^+$ is measured to be $(3.98\\pm0.33_{\\rm stat}\\pm0.21_{\\rm syst})\\%$,\nand the $R_{\\phi}= {\\mathcal{B}(\\phi\\to\\pi^+\\pi^-\\pi^0)}\/{\\mathcal{B}(\\phi\\to\nK^+K^-)}$ is determined to be $(0.222\\pm0.019_{\\rm stat}\\pm0.016_{\\rm syst}$),\nwhich is consistent with the previous measurement based on charm meson decays,\nbut deviates from the results from $e^+e^-$ annihilation and $K$-$N$ scattering\nexperiments by more than 3$\\sigma$.",
        "We present a comparison of observed polycyclic aromatic hydrocarbon (PAH)\nfeature ratios in 19 nearby galaxies with a grid of theoretical expectations\nfor near- and mid-infrared dust emission. The PAH feature ratios are drawn from\nCycle 1 JWST observations and are measured for 7224 stellar clusters and 29176\nstellar associations for which we have robust ages and mass estimates from HST\nfive-band photometry. Though there are galaxy-to-galaxy variations, the\nobserved PAH feature ratios largely agree with the theoretical models,\nparticularly those that are skewed toward more ionized and larger PAH size\ndistributions. For each galaxy we also extract PAH feature ratios for 200\npc-wide circular regions in the diffuse interstellar medium, which serve as a\nnon-cluster\/association control sample. Compared to what we find for stellar\nclusters and associations, the 3.3um\/7.7um and 3.3um\/11.3um ratios from the\ndiffuse interstellar medium are $\\sim 0.10-0.15$ dex smaller. When the observed\nPAH feature ratios are compared to the radiation field hardness as probed by\nthe [OIII]\/H$\\beta$ ratio, we find anti-correlations for nearly all galaxies in\nthe sample. These results together suggest that the PAH feature ratios are\ndriven by the shape intensity of the radiation field, and that the smallest\nPAHs -- observed via JWST F335M imaging -- are increasingly 'processed' or\ndestroyed in regions with the most intense and hard radiation fields."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study",
    "start_abstract":"Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study"
      ],
      "abstract":[
        "Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "QGAIC: Quantum Inspired Genetic Algorithm for Image Classification",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "A Weight Adaptation Trigger Mechanism in Decomposition-based\n  Evolutionary Multi-Objective Optimisation",
        "Was Tournament Selection All We Ever Needed? A Critical Reflection on\n  Lexicase Selection",
        "A Runtime Analysis of the Multi-Valued Compact Genetic Algorithm on\n  Generalized LeadingOnes",
        "Cancermorphic Computing Toward Multilevel Machine Intelligence",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Quantum Simplicial Neural Networks",
        "Performance of Practical Quantum Oblivious Key Distribution",
        "SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model\n  Generation More Reliable",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
        "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
        "Logistic regression models: practical induced prior specification",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "How to compute the volume in low dimension?",
        "Deeply Optimizing the SAT Solver for the IC3 Algorithm",
        "Remarks on log pluricanonical representations",
        "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation",
        "Run-and-tumble particles with 1D Coulomb interaction: the active jellium\n  model and the non-reciprocal self-gravitating gas",
        "Revealing the Implicit Noise-based Imprint of Generative Models",
        "Rationality and categorical properties of the moduli of instanton\n  bundles on the projective 3-space",
        "A Spiral Bicycle Track that Can Be Traced by a Unicycle"
      ],
      "abstract":[
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "This study uses two meta-heuristics methodologies to introduce two novel\nquantum-inspired meta heuristic approaches: quantum-inspired genetic algorithm\n(QIGA1) and quantum-inspired genetic algorithm with dynamic approach (QIGA2).\nThe two suggested methods combine a classical and quantum genetic algorithm\napproach. Both approaches use The correlation coefficient as an assessment\nfunction to identify the best (optimal) values for binary image. In quantum\ncomputing, they use simple ideas like qubits and state superposition. Due to\nthese characteristics, parallelism which uses the time discreteness of quantum\nmechanical systems, is exhibited. For five distinct MNIST datasets, the\nperformance of all participating algorithms has been assessed by comparing the\nsuggested approaches first with their traditional approach counterparts and\nthen with the proposed methods QIGA1 and QIGA2. Each method's ideal threshold\nvalue, associated fitness value (best and average), loss, and accuracy for each\nMNIST dataset have all been published. The outcomes demonstrate the superior\nefficiency of the suggested approaches over their traditional equivalents.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "Decomposition-based multi-objective evolutionary algorithms (MOEAs) are\nwidely used for solving multi-objective optimisation problems. However, their\neffectiveness depends on the consistency between the problems Pareto front\nshape and the weight distribution. Decomposition-based MOEAs, with uniformly\ndistributed weights (in a simplex), perform well on problems with a regular\n(simplex-like) Pareto front, but not on those with an irregular Pareto front.\nPrevious studies have focused on adapting the weights to approximate the\nirregular Pareto front during the evolutionary process. However, these\nadaptations can actually harm the performance on the regular Pareto front via\nchanging the weights during the search process that are eventually the best fit\nfor the Pareto front. In this paper, we propose an algorithm called the weight\nadaptation trigger mechanism for decomposition-based MOEAs (ATM-MOEA\/D) to\ntackle this issue. ATM-MOEA\/D uses an archive to gradually approximate the\nshape of the Pareto front during the search. When the algorithm detects\nevolution stagnation (meaning the population no longer improves significantly),\nit compares the distribution of the population with that of the archive to\ndistinguish between regular and irregular Pareto fronts. Only when an irregular\nPareto front is identified, the weights are adapted. Our experimental results\nshow that the proposed algorithm not only performs generally better than seven\nstate-of-the-art weight-adapting methods on irregular Pareto fronts but also is\nable to achieve the same results as fixed-weight methods like MOEA\/D on regular\nPareto fronts.",
        "The success of lexicase selection has led to various extensions, including\nits combination with down-sampling, which further increased performance.\nHowever, recent work found that down-sampling also leads to significant\nimprovements in the performance of tournament selection. This raises the\nquestion of whether tournament selection combined with down-sampling is the\nbetter choice, given its faster running times. To address this question, we run\na set of experiments comparing epsilon-lexicase and tournament selection with\ndifferent down-sampling techniques on synthetic problems of varying noise\nlevels and problem sizes as well as real-world symbolic regression problems.\nOverall, we find that down-sampling improves generalization and performance\neven when compared over the same number of generations. This means that\ndown-sampling is beneficial even with way fewer fitness evaluations.\nAdditionally, down-sampling successfully reduces code growth. We observe that\npopulation diversity increases for tournament selection when combined with\ndown-sampling. Further, we find that tournament selection and epsilon-lexicase\nselection with down-sampling perform similar, while tournament selection is\nsignificantly faster. We conclude that tournament selection should be further\nanalyzed and improved in future work instead of only focusing on the\nimprovement of lexicase variants.",
        "In the literature on runtime analyses of estimation of distribution\nalgorithms (EDAs), researchers have recently explored univariate EDAs for\nmulti-valued decision variables. Particularly, Jedidia et al. gave the first\nruntime analysis of the multi-valued UMDA on the r-valued LeadingOnes\n(r-LeadingOnes) functions and Adak et al. gave the first runtime analysis of\nthe multi-valued cGA (r-cGA) on the r-valued OneMax function. We utilize their\nframework to conduct an analysis of the multi-valued cGA on the r-valued\nLeadingOnes function. Even for the binary case, a runtime analysis of the\nclassical cGA on LeadingOnes was not yet available. In this work, we show that\nthe runtime of the r-cGA on r-LeadingOnes is O(n^2r^2 log^3 n log^2 r) with\nhigh probability.",
        "Despite their potential to address crucial bottlenecks in computing\narchitectures and contribute to the pool of biological inspiration for\nengineering, pathological biological mechanisms remain absent from\ncomputational theory. We hereby introduce the concept of cancer-inspired\ncomputing as a paradigm drawing from the adaptive, resilient, and evolutionary\nstrategies of cancer, for designing computational systems capable of thriving\nin dynamic, adversarial or resource-constrained environments. Unlike known\nbioinspired approaches (e.g., evolutionary and neuromorphic architectures),\ncancer-inspired computing looks at emulating the uniqueness of cancer cells\nsurvival tactics, such as somatic mutation, metastasis, angiogenesis and immune\nevasion, as parallels to desirable features in computing architectures, for\nexample decentralized propagation and resource optimization, to impact areas\nlike fault tolerance and cybersecurity. While the chaotic growth of cancer is\ncurrently viewed as uncontrollable in biology, randomness-based algorithms are\nalready being successfully demonstrated in enhancing the capabilities of other\ncomputing architectures, for example chaos computing integration. This vision\nfocuses on the concepts of multilevel intelligence and context-driven mutation,\nand their potential to simultaneously overcome plasticity-limited neuromorphic\napproaches and the randomness of chaotic approaches. The introduction of this\nconcept aims to generate interdisciplinary discussion to explore the potential\nof cancer-inspired mechanisms toward powerful and resilient artificial systems.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Large language models (LLMs) have demonstrated remarkable performance, yet\ntheir diverse strengths and weaknesses prevent any single LLM from achieving\ndominance across all tasks. Ensembling multiple LLMs is a promising approach to\ngenerate reliable responses but conventional ensembling frameworks suffer from\nhigh computational overheads. This work introduces Scalable Consistency\nEnsemble (SCE), an efficient framework for ensembling LLMs by prompting\nconsistent outputs. The SCE framework systematically evaluates and integrates\noutputs to produce a cohesive result through two core components: SCE-CHECK, a\nmechanism that gauges the consistency between response pairs via semantic\nequivalence; and SCE-FUSION, which adeptly merges the highest-ranked consistent\nresponses from SCE-CHECK, to optimize collective strengths and mitigating\npotential weaknesses. To improve the scalability with multiple inference\nqueries, we further propose ``{You Only Prompt Once}'' (YOPO), a novel\ntechnique that reduces the inference complexity of pairwise comparison from\nquadratic to constant time. We perform extensive empirical evaluations on\ndiverse benchmark datasets to demonstrate \\methodName's effectiveness. Notably,\nthe \\saccheckcomponent outperforms conventional baselines with enhanced\nperformance and a significant reduction in computational overhead.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
        "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
        "Bayesian inference for statistical models with a hierarchical structure is\noften characterized by specification of priors for parameters at different\nlevels of the hierarchy. When higher level parameters are functions of the\nlower level parameters, specifying a prior on the lower level parameters leads\nto induced priors on the higher level parameters. However, what are deemed\nuninformative priors for lower level parameters can induce strikingly non-vague\npriors for higher level parameters. Depending on the sample size and specific\nmodel parameterization, these priors can then have unintended effects on the\nposterior distribution of the higher level parameters.\n  Here we focus on Bayesian inference for the Bernoulli distribution parameter\n$\\theta$ which is modeled as a function of covariates via a logistic\nregression, where the coefficients are the lower level parameters for which\npriors are specified. A specific area of interest and application is the\nmodeling of survival probabilities in capture-recapture data and occupancy and\ndetection probabilities in presence-absence data. In particular we propose\nalternative priors for the coefficients that yield specific induced priors for\n$\\theta$. We address three induced prior cases. The simplest is when the\ninduced prior for $\\theta$ is Uniform(0,1). The second case is when the induced\nprior for $\\theta$ is an arbitrary Beta($\\alpha$, $\\beta$) distribution. The\nthird case is one where the intercept in the logistic model is to be treated\ndistinct from the partial slope coefficients; e.g., $E[\\theta]$ equals a\nspecified value on (0,1) when all covariates equal 0. Simulation studies were\ncarried out to evaluate performance of these priors and the methods were\napplied to a real presence\/absence data set and occupancy modelling.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Estimating the volume of a convex body is a canonical problem in theoretical\ncomputer science. Its study has led to major advances in randomized algorithms,\nMarkov chain theory, and computational geometry. In particular, determining the\nquery complexity of volume estimation to a membership oracle has been a\nlongstanding open question. Most of the previous work focuses on the\nhigh-dimensional limit. In this work, we tightly characterize the\ndeterministic, randomized and quantum query complexity of this problem in the\nhigh-precision limit, i.e., when the dimension is constant.",
        "The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm\nthat has significantly influenced the field in recent years due to its\nefficiency, scalability, and completeness. It utilizes SAT solvers to solve a\nseries of SAT queries associated with relative induction. In this paper, we\nintroduce several optimizations for the SAT solver in IC3 based on our\nobservations of the unique characteristics of these SAT queries. By observing\nthat SAT queries do not necessarily require decisions on all variables, we\ncompute a subset of variables that need to be decided before each solving\nprocess while ensuring that the result remains unaffected. Additionally, noting\nthat the overhead of binary heap operations in VSIDS is non-negligible, we\nreplace the binary heap with buckets to achieve constant-time operations.\nFurthermore, we support temporary clauses without the need to allocate a new\nactivation variable for each solving process, thereby eliminating the need to\nreset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which\nintegrates these optimizations. A comprehensive evaluation highlights the\nperformance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3\ndemonstrates an average speedup of 3.61 times in solving time compared to the\nIC3 implementation based on MiniSat.",
        "We show the finiteness of log pluricanonical representations under the\nassumption of the existence of a good minimal model.",
        "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT.",
        "Recently we studied $N$ run-and-tumble particles in one dimension - which\nswitch with rate $\\gamma$ between driving velocities $\\pm v_0$ - interacting\nvia the long range 1D Coulomb potential (also called rank interaction), both in\nthe attractive and in the repulsive case, with and without a confining\npotential. We extend this study in two directions. First we consider the same\nsystem, but inside a harmonic confining potential, which we call \"active\njellium\". We obtain a parametric representation of the particle density in the\nstationary state at large $N$, which we analyze in detail. Contrary to the\nlinear potential, there is always a steady-state where the density has a\nbounded support. However, we find that the model still exhibits transitions\nbetween phases with different behaviors of the density at the edges, ranging\nfrom a continuous decay to a jump, or even a shock (i.e. a cluster of\nparticles, which manifests as a delta peak in the density). Notably, the\ninteractions forbid a divergent density at the edges, which may occur in the\nnon-interacting case. In the second part, we consider a non-reciprocal version\nof the rank interaction: the $+$ particles (of velocity $+v_0$) are attracted\ntowards the $-$ particles (of velocity $-v_0$) with a constant force $b\/N$,\nwhile the $-$ particles are repelled by the $+$ particles with a force of same\namplitude. In order for a stationary state to exist we add a linear confining\npotential. We derive an explicit expression for the stationary density at large\n$N$, which exhibits an explicit breaking of the mirror symmetry with respect to\n$x=0$. This again shows the existence of several phases, which differ by the\npresence or absence of a shock at $x=0$, with one phase even exhibiting a\nvanishing density on the whole region $x>0$. Our analytical results are\ncomplemented by numerical simulations for finite $N$.",
        "With the rapid advancement of vision generation models, the potential\nsecurity risks stemming from synthetic visual content have garnered increasing\nattention, posing significant challenges for AI-generated image detection.\nExisting methods suffer from inadequate generalization capabilities, resulting\nin unsatisfactory performance on emerging generative models. To address this\nissue, this paper presents a novel framework that leverages noise-based\nmodel-specific imprint for the detection task. Specifically, we propose a novel\nnoise-based imprint simulator to capture intrinsic patterns imprinted in images\ngenerated by different models. By aggregating imprints from various generative\nmodels, imprints of future models can be extrapolated to expand training data,\nthereby enhancing generalization and robustness. Furthermore, we design a new\npipeline that pioneers the use of noise patterns, derived from a noise-based\nimprint extractor, alongside other visual features for AI-generated image\ndetection, resulting in a significant improvement in performance. Our approach\nachieves state-of-the-art performance across three public benchmarks including\nGenImage, Synthbuster and Chameleon.",
        "We prove the rationality and irreducibility of the moduli space of\nmathematical instanton vector bundles of arbitrary rank and charge on $\\mathbb\nP^3$. In particular, the result applies to the rank-2 case. This problem was\nfirst studied by Barth, Hartshorne, Hirschowitz-Narasimhan in the late 1970s.\nWe also show that the mathematical instantons of variable rank and charge form\na monoidal category. The proof is based on an in-depth analysis of the\nBarth-Hulek monad-construction and on a detailed description of the moduli\nspace of (framed and unframed) stable bundles on Hirzebruch surfaces.",
        "A unibike curve is a track that can be made by either a bicycle or a\nunicycle. More precisely, the end of a unit tangent vector at any point on a\nunibike curve lies on the curve (so the bike's front wheel always lies on the\ntrack made by the rear wheel). David Finn found such a curve in 2002, but it\nloops around itself in an extremely complicated way with many twists and\nself-intersections. Starting with the polar square root curve r = sqrt[t\/(2\npi)] and iterating a simple construction involving a differential equation\napparently leads in the limit to a unibike curve having a spiral shape. The\niteration gets each curve as a rear track of its predecessor. Solving hundreds\nof differential equations numerically, where each depends on the preceding one,\nleads to error buildup, but with some care one can get a curve having unibike\nerror less than 10^-7. The evidence is strong for the conjecture that the limit\nof the iteration exists and is a unibike curve."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Convolutional recurrent neural networks for music classification",
    "start_abstract":"We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A deep representation for invariance and music classification"
      ],
      "abstract":[
        "Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification."
      ],
      "categories":[
        "physics.class-ph"
      ]
    },
    "list":{
      "title":[
        "Sliding with Friction and The Brachistochrone Problem",
        "Concepts of Electric Circuit Analysis Revised: Analysis without Current\n  Direction",
        "Canonical Energy-Momentum Tensor of Abelian Fields",
        "Inflation and instabilities of a spherical magnetoelastic balloon",
        "On minimizing cyclists' ascent times: Part II",
        "A geometric derivation of Noether's theorem",
        "The magnetic scalar potential and demagnetization vector for a prism",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Magnetic levitation at low rotation frequencies using an on-axis\n  magnetic field",
        "A lower bound for the energy decay rate in piezoelectricity",
        "The shooting methods to solve 3D nonlinear strings assemblies",
        "About the Keplerization of motion in any central force field",
        "Driven damped harmonic oscillator revisited: energy resonance",
        "NeuroPMD: Neural Fields for Density Estimation on Product Manifolds",
        "Superstructure reflexions in tilted perovskites Part 1",
        "Qubit operations using a modular optical system engineered with\n  PyOpticL: a code-to-CAD optical layout tool",
        "Low-energy neutron cross-talk between organic scintillator detectors",
        "Serrin's overdetermined problems on epigraphs",
        "Symbolic Computations of the Two-Colored Diagrams for Central\n  Configurations of the Planar N-vortex Problem",
        "Local perfect chirality at reflection-zeros away from exceptional points\n  in optical whispering gallery microcavity",
        "The Lehmer complex of a Bruhat interval",
        "Theoretical determination of Gilbert damping in reduced dimensions",
        "Pricing American Parisian Options under General Time-Inhomogeneous\n  Markov Models",
        "Algorithms for 2-Solvable Difference Equations",
        "On a formula of the $q$-series $_{2k+4}\\phi_{2k+3}$ and its applications",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment",
        "Orbital Angular Momentum Experimental Bound on the Maximum Predictive\n  Power of Physical Theories in Multi-Dimensional Systems"
      ],
      "abstract":[
        "We analyze the motion of a particle in the gravity field along a family of\ndifferentiable curves taking into account the Coulomb friction forces. A\nparametric equation of the optimal curves is given that generalizes the cycloid\none in this case. The results of numerical calculations in the Mathcad program\nshow that the found curve minimize the descent time for a given friction\ncoefficient and can claim to be a brachistochrone with Coulomb friction.",
        "The electric current as the flux of current density -- a signed scalar, not a\nvector -- is inconsistent with the concept of the current direction, commonly\ninvoked in the electric circuit analyses within, for example, Kirchhoff's\ncurrent law, the resistance rule, and Lenz's law. This paper presents an\napproach to the circuit analysis that is not predicated upon the ``current\ndirection,'' and thus avoids such inconsistencies. The approach is summarized\nin five rules and the application of these rules is demonstrated by analyzing\nan RLC series circuit.",
        "In this tutorial, we provide the natural derivation of symmetrical,\ngauge-invariant canonical energy-momentum tensor for the abelian gauge field,\ni.e., the electromagnetic field.",
        "This study explores the instabilities during the axisymmetric inflation of an\ninitially spherical magnetoelastic balloon, modeled as a magnetizable Ogden\nmaterial, under combined internal pressure and a non-uniform magnetic field\ngenerated by current-carrying coils. The nonlinear interplay of geometric and\nmaterial effects leads to governing equations sensitive to bifurcations and\ninstabilities. A coordinate singularity at the poles of the balloon is\nidentified within the system of governing differential equations, which is\nresolved through an appropriate choice of field variables and L'H\\^opital's\nrule. Stability analysis reveals that as inflation progresses, axisymmetry is\nbroken through a supercritical pitchfork bifurcation, resulting in a\npear-shaped equilibrium. This symmetry is later restored through a reverse\nsubcritical pitchfork bifurcation, forming an isolated loop of pear-shaped\nsolutions containing stable and unstable branches in the case of a\nsix-parameter Ogden material model (SPOM). The onset of symmetry-breaking\nbifurcations is influenced by material parameters and magnetic field intensity,\nwith critical values beyond which such bifurcations are suppressed. Both\nsymmetry-preserving and pear-shaped configurations are stable under small\nasymmetric perturbations in both magnetic and non-magnetic cases. Snap-through\ntransitions between pear-shaped and axisymmetric configurations are also\nobserved.",
        "We formulate an optimization of a bicycle ascent time under the constraints\nof the average, maximum, and minimum powers. In contrast to the first part of\nthis study, we do not restrict the departure to flying starts with an initial\nspeed determined by the model and its optimization. We allow for various\ninitial speeds, from a standstill to a launched start. We accomplish this by\ngeneralizing the discontinuous piecewise constant speed model to a continuous\npiecewise linear speed model. Regardless of the initial speed, steepness or\nprofile of the ascent the optimal strategy tends to a constant ground speed, in\nagreement with the conclusion of the previous, more restricted, formulation.\nThis new formulation allows us to compare various initial-speed strategies and,\nhence, has a direct application to competitive cycling. Notably, in timetrials\ncomposed of flat and steep sections, it helps one decide whether or not to\nchange bicycle, which requires stopping and restarting, from one that is more\nappropriate for flats to one that is more appropriate for uphills.",
        "Noether's theorem is a cornerstone of analytical mechanics, making the link\nbetween symmetries and conserved quantities. In this article, I propose a\nsimple, geometric derivation of this theorem that circumvents the usual\ndifficulties that a student of this field usually encounters. The derivation is\nbased on the direct use of the differential form $pdq -Hdt$, where $p$ is the\nmomentum and $H$ the Hamiltonian, integrated over a simple curve.",
        "We analytically solve Poisson's equation for the magnetic scalar potential\ngenerated by a magnetized prism and determine a closed-form solution for the\nmagnetic scalar potential given only in terms of arctan and natural logarithmic\nfunctions. We furthermore show that this can be written as a demagnetization\nvector, containing all the geometric information of the prism, multiplied with\nthe magnetization, analogous to the way demagnetization tensors are written. We\nvalidate the derived analytical expression for the magnetic scalar potential by\ncomparing with a finite element simulation and show that these agree perfectly.\nWe finally extend the concept of the demagnetization tensor, which contains the\ngeometric information for the source generating the potential, to gravitational\nobjects.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "Magnetic levitation by rotation is a simply yet astonishing phenomenon where\na permanent magnet can be levitated by placing it in the vicinity of another\npermanent magnet that rotates sufficiently fast. The few previous works on this\nnovel type of magnetic levitation all required magnets rotating on the order of\n200 Hz. Here we investigate the influence of applying an on-axis, static\nmagnetic field and show that this can lower the needed rotation frequency to\nbelow 50 Hz. We explain this by a detailed analysis of the force producing\nlevitation, which is a superposition of a repelling force caused by the\noff-axis (rotating) magnetic field and an attractive force due to the on-axis\nfield. We study this force and resulting levitation experimentally,\nanalytically and numerically for three different rotor magnet configurations,\nshowing that the levitation distance and frequency range can be accurately\npredicted from both the numerical and analytical models.",
        "This paper establishes a lower bound for the energy decay rate in\npiezoelectric cylinders. The bound incorporates material properties and\ngeometric factors, including the cross-section's Poincar\\'e-Wirtinger and Korn\nconstants. A detailed analysis of a circular cross-section cylinder yields a\nprecise numerical lower bound, illustrating the practical application of this\nresult.",
        "This article presents an alternative approach to finite elements for modeling\nand analyzing 3D static mooring lines using string theory and the shooting\nmethod (SM) to solve two-point boundary value problems (TPBVPs) for 3D\nnonlinear static string equations with various boundary condition (BC) types\nrelevant to offshore slender system assemblies.The two-point boundary value\nproblem for nonlinear extensible elastic strings was formulated by\nincorporating arbitrary 3D external distributed loads that are not restricted\nto gravity alone. The TPBVP was reformulated based on the formalism of the\nshooting method. A multi-body\/multi-shooting approach is proposed to handle\nmulti-material segments and line assemblies. A formulation of the boundary\nconditions that allows the modeling of Dirichlet, Robin, and mixed boundary\nconditions representing the displacement, force, and combined\nforce\/displacement constraints is presented.Four validation cases are\npresented, comparing the results to analytical solutions: (1) a single catenary\nsegment with ball-prismatic joint boundary conditions under several imposed\nforces, (2) a review of all possible boundary conditions for strings, including\nspring-based BCs, (3) the Velaria problem with nonlinear radial distributed\nload, and (4) a three-segment hanging configuration with different material\nproperties connected by a buoy.The results demonstrate an accuracy under 10-9\nin terms of absolute errors for both positions and tensions along the entire\nlength of the mooring lines. The proposed method also provides error control\nthrough adaptive step integration. It demonstrates high accuracy in modeling\ncomplex 3D kinematics and configurations for mooring lines, while limiting the\niterative problem size.The proposed method provides an efficient alternative to\ndiscretization-based techniques for analyzing static configurations of string\nkinematics slender systems with various end constraints, such as mooring lines\nand hawsers assemblies in offshore engineering, while maintaining simplicity in\napproach and implementation.",
        "The method of keplerization of one-body motion in any central force field,\nintroduced by Martinusi and Gurfil in 2012, is reviewed and reformulated into a\ngeneral homogenization method which applies to any kind of bounded motion. It\nis also shown how this extended method provides a proof of the existence of a\ndynamical symmetry group and how it can be used to extend that group to a\nglobal symmetry group, for any such system",
        "We derive the exact expression for the resonant frequency of the\ntime-averaged steady-state energy and show that this frequency is excellently\napproximated by the arithmetic mean of the amplitude and velocity resonant\nfrequencies. In addition, we argue that the frequency of the amplitude\nresonance can be regarded as the energy resonance frequency, since it provides\nthe maximal peak values of instantaneous energy.",
        "We propose a novel deep neural network methodology for density estimation on\nproduct Riemannian manifold domains. In our approach, the network directly\nparameterizes the unknown density function and is trained using a penalized\nmaximum likelihood framework, with a penalty term formed using manifold\ndifferential operators. The network architecture and estimation algorithm are\ncarefully designed to handle the challenges of high-dimensional product\nmanifold domains, effectively mitigating the curse of dimensionality that\nlimits traditional kernel and basis expansion estimators, as well as overcoming\nthe convergence issues encountered by non-specialized neural network methods.\nExtensive simulations and a real-world application to brain structural\nconnectivity data highlight the clear advantages of our method over the\ncompeting alternatives.",
        "The superstructure spots that appear in diffraction patterns of tilted\nperovskites are well documented and easily calculated using crystallographic\nsoftware. Here, by considering a distortion mode as a perturbation of the\nprototype perovskite structure, we show how the structure factor equation\nyields Boolean conditions for the presence of first order superstructure\nreflexions. A subsequent article describes conditions for second order\nreflexions, which appear only in structures with mixed in-phase and anti-phase\noxygen octahedral tilting. This approach may have some advantages for the\nanalysis of electron diffraction patterns of perovskites.",
        "Complex optical design is hindered by conventional piecewise setup, which\nprevents modularization and therefore abstraction of subsystems at the circuit\nlevel. This limits multiple fields that require complex optics systems,\nincluding quantum computing with atoms and trapped ions, because their optical\nsystems are not scalable. We present an open-source Python library for optical\nlayout (PyOpticL) which uses beam-path simulation and dynamic beam-path routing\nfor quick and easy optical layout by placing optical elements along the beam\npath without a priori specification, enabling dynamic layouts with automatic\nrouting and connectivity. We use PyOpticL to create modular drop-in optical\nbaseplates for common optical subsystems used in atomic and molecular optics\n(AMO) experiments including laser sources, frequency and intensity modulation,\nand locking to an atomic reference for stabilization. We demonstrate this\nminimal working example of a dynamic full laser system for strontium trapped\nions by using it for laser cooling, qubit state detection, and 99.9% fidelity\nsingle-qubit gates with 3D printed baseplates. This enables a new paradigm of\ndesign abstraction layers for engineering optical systems leveraging modular\nbaseplates, as they can be used for any wavelength in the system and enables\nscaling up the underlying optical systems for quantum computers. This new\nopen-source hardware and software code-to-CAD library seeks to foster\nopen-source collaborative hardware and systems design across numerous fields of\nresearch including AMO physics and quantum computing with neutral atoms and\ntrapped ions.",
        "A series of measurements have been performed with low-energy monoenergetic\nneutrons to characterise cross-talk between two organic scintillator detectors.\nCross-talk time-of-flight spectra and probabilities were determined for neutron\nenergies from 1.4 to 15.5 MeV and effective scattering angles ranging from\n$\\sim$50{\\deg} to $\\sim$100{\\deg}. Monte-Carlo simulations incorporating both\nthe active and inactive materials making up the detectors showed reasonable\nagreement with the measurements. Whilst the time-of-flight spectra were very\nwell reproduced, the cross-talk probabilities were only in approximate\nagreement with the measurements, with the most significant discrepancies\n($\\sim$40 %) occurring at the lowest energies. The neutron interaction\nprocesses producing cross-talk at the energies explored here are discussed in\nthe light of these results.",
        "In this work we establish some rigidity results for Serrin's overdetermined\nproblem \\begin{equation*}\n  \\left\\{\n  \\begin{array}{cll}\n  - \\Delta u=f(u) & \\text{in}& \\Omega,\\newline\n  u > 0& \\text{in} & \\Omega,\\newline\n  u=0 & \\text{on} & \\partial \\Omega,\\newline\n  \\dfrac{\\partial u}{\\partial \\eta} = \\mathfrak{c} = const. & \\text{on} &\n\\partial \\Omega,\n  \\end{array}\n  \\right. \\end{equation*}\n  when $\\Omega \\subset \\mathbb{R}^N$ is an epigraph (not necessarily globally\nLipschitz-continuous) and $u$ is a classical solution, possibly unbounded. In\nbroad terms, our main results prove that $\\Omega$ must be an affine half-space\nand $u$ must be one-dimensional, provided the epigraph is bounded from below.\nThese results hold when $f$ is of Allen-Cahn type and $ N \\geq 2$ or,\nalternatively, when $f$ is locally Lipschitz-continuous (with no restriction on\nthe sign of $f(0)$) and $ N \\leq 3$. These results partially answer a question\nraised by Berestycki, Caffarelli and Nirenberg in [1]. Finally, when $f(0) <0$,\nwe also prove a new monotonicity result, valid in any dimension $ N \\geq 2$.",
        "We apply the singular sequence method to investigate the finiteness problem\nfor stationary configurations of the planar N-vortex problem. The initial step\nof the singular sequence method involves identifying all two-colored diagrams.\nThese diagrams represent potential scenarios where finiteness may fail. We\ndevelop a symbolic computation algorithm to determine all two-colored diagrams\nfor central configurations of the planar N-vortex problem.",
        "Recently, a local and imperfect chirality of the resonant eigenmode at the\nexceptional point (EP) has been reported in the optical whispering gallery\nmicrocavity system perturbed by two strong nanoscatterers [Phys. Rev. A 108,\nL041501 (2023)]. Here, we discover a local perfect chirality of the resonant\neigenmode away from the EP in the parameter space of the strongly perturbed\nmicrocavity system. By considering the multiple scattering process of the\nazimuthally propagating modes (APMs) at the nanoscatterers with a\nfirst-principles-based model, the local perfect chirality is predicted to\nresult from the unidirectional reflectionlessness, i.e., the reflection-zero\n(R-zero) of the APMs at the two nanoscatterers. Numerical results and model\npredictions consistently show that the structural parameters of the R-zero\ntypically deviate from those of the EP, which means that the pair of split\nresonant eigenmodes at the R-zero have different complex resonance frequencies\nand electromagnetic fields. In general, only one of the pair of split\neigenmodes exhibits a local perfect chirality within the local azimuthal range\ndivided by the two nanoscatterers. With the decrease of the two nanoscatterers'\nsizes or their relative azimuthal angle, the R-zero tends to coincide with the\nEP.",
        "We introduce Lehmer codes, with immersions in the Bruhat order, for several\nfinite Coxeter groups, including all the classical Weyl groups. This allows to\nassociate to each lower Bruhat interval of these groups a multicomplex whose\nf-polynomial is the Poincar\\'e polynomial of the interval. Via a general\nconstruction, we prove that these polynomials are h-polynomials of\nvertex-decomposable simplicial complexes. Moreover we provide a classification,\nin terms of unimodal permutations, of Poincar\\'e polynomials of smooth Schubert\nvarieties in flag manifolds.",
        "An ab initio scheme based on the linear response theory of exchange torque\ncorrelation is presented to calculate intrinsic Gilbert damping parameters in\nmagnets of reduced dimensions. The method implemented into the real-space\nKorringa-Kohn-Rostoker (RS-KKR) Greens' function framework enables to obtain\ndiagonal elements of the atomic-site-dependent on-site and non-local Gilbert\ndamping tensor. Going from the 3D bulk and surfaces of iron and cobalt\nferromagnets addressed in our previous work [Phys. Rev. B 109, 094417 (2024)],\nin the present paper monolayers of Fe and Co on (001)- and (111)-oriented Cu,\nAg, and Au substrates are studied, and particularly the substrate-dependent\ntrends are compared. Furthermore, the Gilbert damping parameters are calculated\nfor Fe and Co adatoms and dimers on (001)-oriented substrates. It is\ninvestigated how the damping parameter of single adatoms depends on their\nvertical position. This dependence is quantified in relation to the adatoms'\ndensity of states at the Fermi energy showing a non-monotonic behavior. By\nrotating the spin moment of the adatoms and collinear magnetic dimers, an\nanisotropic behavior of the damping is revealed. Finally, a significant, three-\nto ten-times increase of the on-site Gilbert damping is found in\nantiferromagnetic dimers in comparison to the ferromagnetic ones, whilst the\ninter-site damping is even more enhanced.",
        "This paper develops general approaches for pricing various types of\nAmerican-style Parisian options (down-in\/-out, perpetual\/finite-maturity) with\ngeneral payoff functions based on continuous-time Markov chain (CTMC)\napproximation under general 1D time-inhomogeneous Markov models. For the\ndown-in types, by conditioning on the Parisian stopping time, we reduce the\npricing problem to that of a series of vanilla American options with different\nmaturities and their prices integrated with the distribution function of the\nParisian stopping time yield the American Parisian down-in option price. This\nfacilitates an efficient application of CTMC approximation to obtain the\napproximate option price by calculating the required quantities. For the\nperpetual down-in cases under time-homogeneous models, significant\ncomputational cost can be reduced. The down-out cases are more complicated, for\nwhich we use the state augmentation approach to record the excursion duration\nand then the approximate option price is obtained by solving a series of\nvariational inequalities recursively with the Lemke's pivoting method. We show\nthe convergence of CTMC approximation for all the types of American Parisian\noptions under general time-inhomogeneous Markov models, and the accuracy and\nefficiency of our algorithms are confirmed with extensive numerical\nexperiments.",
        "Our paper \"Solving Third Order Linear Difference Equations in Terms of Second\nOrder Equations\" gave two algorithms for solving difference equations in terms\nof lower order equations: an algorithm for absolute factorization, and an\nalgorithm for solving third order equations in terms of second order. Here we\nimprove the efficiency for absolute factorization, and extend the other\nalgorithm to order four.",
        "In this paper we apply a formula of the very-well poised $_{2k+4}\\phi_{2k+3}$\nto write a $k$-tuple sum of $q$-series as a linear combination of terms wherein\neach term is a product of expressions of the form $\\frac{1}{(qy,\nqy^{-1};q)_\\infty}$. As an application, we shall express a variety of sums and\ndouble sums of $q$-series as linear combinations of infinite products. Our\nformulas are motivated by their connection to overpartition pairs.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size.",
        "The completeness of quantum mechanics in predictive power is a central\nquestion in its foundational study. While most investigations focus on\ntwo-dimensional systems, high-dimensional systems are more general and widely\napplicable. Building on the non-extensibility theorem by Colbeck and Renner\n[Phys. Rev. Lett. 101, 050403 (2008)], which established that no higher theory\ncan enhance the predictive power of quantum mechanics for two-dimensional\nsystems, we extend this result to arbitrarily dimensional systems. We connect\nmaximum potential predictive power achievable by any alternative theory to\nexperimentally observable correlations, and establish optimal experimental\nbounds across varying dimensions by exploiting two-photon orbital angular\nmomentum entangled states with entanglement concentration. These bounds falsify\na broader class of alternative theories, including Bell's and Leggett's models,\nand those that remain theoretically ambiguous or experimentally unverified. Our\nfindings not only deepen the foundational understanding of quantum mechanics\nbut also hold significant potential for high-dimensional quantum cryptography."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A deep representation for invariance and music classification",
    "start_abstract":"Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification.",
    "start_categories":[
      "physics.class-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Convolutional recurrent neural networks for music classification"
      ],
      "abstract":[
        "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Spiking Neural Networks for Temporal Processing: Status Quo and Future\n  Prospects",
        "Toward Relative Positional Encoding in Spiking Transformers",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement\n  Learning",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
        "Hydrated Cable Bacteria Exhibit Protonic Conductivity Over Long\n  Distances",
        "Torsion models for tensor-triangulated categories",
        "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks",
        "Localization of critical points in annular conical sets via the method\n  of Nehari manifold",
        "Temporal Preference Optimization for Long-Form Video Understanding",
        "Detecting APT Malware Command and Control over HTTP(S) Using Contextual\n  Summaries",
        "PCSI -- The Platform for Content-Structure Inference",
        "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays",
        "Scalable intensity-based photonic matrix-vector multiplication processor\n  using single-wavelength time-division-multiplexed signals",
        "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an\n  AI-Driven Era",
        "Algorithms and Hardness Results for the $(k,\\ell)$-Cover Problem",
        "A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization",
        "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "Uncertainty Expression for Human-Robot Task Communication"
      ],
      "abstract":[
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Temporal processing is fundamental for both biological and artificial\nintelligence systems, as it enables the comprehension of dynamic environments\nand facilitates timely responses. Spiking Neural Networks (SNNs) excel in\nhandling such data with high efficiency, owing to their rich neuronal dynamics\nand sparse activity patterns. Given the recent surge in the development of\nSNNs, there is an urgent need for a comprehensive evaluation of their temporal\nprocessing capabilities. In this paper, we first conduct an in-depth assessment\nof commonly used neuromorphic benchmarks, revealing critical limitations in\ntheir ability to evaluate the temporal processing capabilities of SNNs. To\nbridge this gap, we further introduce a benchmark suite consisting of three\ntemporal processing tasks characterized by rich temporal dynamics across\nmultiple timescales. Utilizing this benchmark suite, we perform a thorough\nevaluation of recently introduced SNN approaches to elucidate the current\nstatus of SNNs in temporal processing. Our findings indicate significant\nadvancements in recently developed spiking neuron models and neural\narchitectures regarding their temporal processing capabilities, while also\nhighlighting a performance gap in handling long-range dependencies when\ncompared to state-of-the-art non-spiking models. Finally, we discuss the key\nchallenges and outline potential avenues for future research.",
        "Spiking neural networks (SNNs) are bio-inspired networks that model how\nneurons in the brain communicate through discrete spikes, which have great\npotential in various tasks due to their energy efficiency and temporal\nprocessing capabilities. SNNs with self-attention mechanisms (Spiking\nTransformers) have recently shown great advancements in various tasks such as\nsequential modeling and image classifications. However, integrating positional\ninformation, which is essential for capturing sequential relationships in data,\nremains a challenge in Spiking Transformers. In this paper, we introduce an\napproximate method for relative positional encoding (RPE) in Spiking\nTransformers, leveraging Gray Code as the foundation for our approach. We\nprovide comprehensive proof of the method's effectiveness in partially\ncapturing relative positional information for sequential tasks. Additionally,\nwe extend our RPE approach by adapting it to a two-dimensional form suitable\nfor image patch processing. We evaluate the proposed RPE methods on several\ntasks, including time series forecasting, text classification, and patch-based\nimage classification. Our experimental results demonstrate that the\nincorporation of RPE significantly enhances performance by effectively\ncapturing relative positional information.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps:\/\/github.com\/EMI-Group\/evorl.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
        "This study presents the direct measurement of proton transport along\nfilamentous Desulfobulbaceae, or cable bacteria. Cable bacteria are filamentous\nmulticellular microorganisms that have garnered much interest due to their\nability to serve as electrical conduits, transferring electrons over several\nmillimeters. Our results indicate that cable bacteria can also function as\nprotonic conduits because they contain proton wires that transport protons at\ndistances greater than 100 um. We find that protonic conductivity ({\\sigma}P)\nalong cable bacteria varies between samples and is measured as high as 114 +\/-\n28 uS cm^-1 at 25-degrees C and 70-percent relative humidity (RH). For cable\nbacteria, the protonic conductance (GP) and {\\sigma}P are dependent upon the\nRH, increasing by as much as 26-fold between 60-percent and 80-percent RH. This\nobservation implies that proton transport occurs via the Grotthuss mechanism\nalong water associated with cable bacteria, forming proton wires. In order to\ndetermine {\\sigma}P and GP along cable bacteria, we implemented a protocol\nusing a modified transfer-printing technique to deposit either palladium\ninterdigitated protodes (IDP), palladium transfer length method (TLM) protodes,\nor gold interdigitated electrodes(IDE) on top of cable bacteria. Due to the\nrelatively mild nature of the transfer-printing technique, this method should\nbe applicable to a broad array of biological samples and curved materials. The\nobservation of protonic conductivity in cable bacteria presents possibilities\nfor investigating the importance of long-distance proton transport in microbial\necosystems and to potentially build biotic or biomimetic scaffolds to interface\nwith materials via proton-mediated gateways or channels.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization.",
        "Using the Nehari manifold method, we establish sufficient conditions such\nthat a smooth functional attains a ground state within an annular domain of a\nclosed cone. The localization we obtain immediately allows for multiplicity\nwhen applied to disjoint conical sets. To illustrate our results, we consider a\ntwo-point boundary value problem and obtain a solution within a shell of a\nclosed cone, defined in terms of a Harnack inequality with respect to the\nenergy norm. The conditions imposed on the nonlinear term naturally extend\nthose from classical examples in the literature which were derived using the\nmethod of Nehari manifold on the entire domain.",
        "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps:\/\/ruili33.github.io\/tpo_website.",
        "Advanced Persistent Threats (APTs) are among the most sophisticated threats\nfacing critical organizations worldwide. APTs employ specific tactics,\ntechniques, and procedures (TTPs) which make them difficult to detect in\ncomparison to frequent and aggressive attacks. In fact, current network\nintrusion detection systems struggle to detect APTs communications, allowing\nsuch threats to persist unnoticed on victims' machines for months or even\nyears. In this paper, we present EarlyCrow, an approach to detect APT malware\ncommand and control over HTTP(S) using contextual summaries.\n  The design of EarlyCrow is informed by a novel threat model focused on TTPs\npresent in traffic generated by tools recently used as part of APT campaigns.\nThe threat model highlights the importance of the context around the malicious\nconnections, and suggests traffic attributes which help APT detection.\nEarlyCrow defines a novel multipurpose network flow format called PairFlow,\nwhich is leveraged to build the contextual summary of a PCAP capture,\nrepresenting key behavioral, statistical and protocol information relevant to\nAPT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a\nheadline macro average F1-score of 93.02% with FPR of $0.74%.",
        "The Platform for Content-Structure Inference (PCSI, pronounced \"pixie\")\nfacilitates the sharing of information about the process of converting Web\nresources into structured content objects that conform to a predefined format.\nPCSI records encode methods for deriving structured content from classes of\nURLs, and report the results of applying particular methods to particular URLs.\nThe methods are scripts written in Hex, a variant of Awk with facilities for\ntraversing the HTML DOM.",
        "Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.",
        "Photonic integrated circuits provide a compact platform for ultrafast and\nenergy-efficient matrix-vector multiplications (MVMs) in the optical domain.\nRecently, schemes based on time-division multiplexing (TDM) have been proposed\nas scalable approaches for realizing large-scale photonic MVM processors.\nHowever, existing demonstrations rely on coherent detection or multiple\nwavelengths, both of which complicate their operations. In this work, we\ndemonstrate a scalable TDM-based photonic MVM processor that uses only\nsingle-wavelength intensity-modulated optical signals, thereby avoiding\ncoherent detection and enabling simplified operations. A 32-channel processor\nis fabricated on a Si-on-insulator (SOI) platform and used to experimentally\nperform convolution operations in a convolutional neural network (CNN) for\nhandwritten digit recognition, achieving a classification accuracy of 93.47%\nfor 1500 images.",
        "Software developers balance a variety of different tasks in a workweek, yet\nthe allocation of time often differs from what they consider ideal. Identifying\nand addressing these deviations is crucial for organizations aiming to enhance\nthe productivity and well-being of the developers. In this paper, we present\nthe findings from a survey of 484 software developers at Microsoft, which aims\nto identify the key differences between how developers would like to allocate\ntheir time during an ideal workweek versus their actual workweek. Our analysis\nreveals significant deviations between a developer's ideal workweek and their\nactual workweek, with a clear correlation: as the gap between these two\nworkweeks widens, we observe a decline in both productivity and satisfaction.\nBy examining these deviations in specific activities, we assess their direct\nimpact on the developers' satisfaction and productivity. Additionally, given\nthe growing adoption of AI tools in software engineering, both in the industry\nand academia, we identify specific tasks and areas that could be strong\ncandidates for automation. In this paper, we make three key contributions: 1)\nWe quantify the impact of workweek deviations on developer productivity and\nsatisfaction 2) We identify individual tasks that disproportionately affect\nsatisfaction and productivity 3) We provide actual data-driven insights to\nguide future AI automation efforts in software engineering, aligning them with\nthe developers' requirements and ideal workflows for maximizing their\nproductivity and satisfaction.",
        "A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in\nat least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal\ncombinatorics and the literature on edge modification problems, we study the\nalgorithmic version of the $(k,\\ell)$-cover problem. Given a connected graph\n$G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of\nnon-edges of $G$ such that their addition to $G$ results in a graph with a $(k,\n\\ell)$-cover. For every constant $k\\geq3$, we show that the $(k,1)$-cover\nproblem is $\\mathbb{NP}$-complete for general graphs. Moreover, we show that\nfor every constant $k\\geq 3$, the $(k,1)$-cover problem admits no\npolynomial-time constant-factor approximation algorithm unless\n$\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can\nbe solved in polynomial time when the input graph is chordal. For the class of\ntrees and general values of $k$, we show that the $(k,1)$-cover problem is\n$\\mathbb{NP}$-hard even for spiders. However, we show that for every $k\\geq4$,\nthe $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor\napproximable when the input graph is a tree.",
        "The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.",
        "Generative models that can produce realistic images have improved\nsignificantly in recent years. The quality of the generated content has\nincreased drastically, so sometimes it is very difficult to distinguish between\nthe real images and the generated ones. Such an improvement comes at a price of\nethical concerns about the usage of the generative models: the users of\ngenerative models can improperly claim ownership of the generated content\nprotected by a license. In this paper, we propose an approach to embed\nwatermarks into the generated content to allow future detection of the\ngenerated content and identification of the user who generated it. The\nwatermark is embedded during the inference of the model, so the proposed\napproach does not require the retraining of the latter. We prove that\nwatermarks embedded are guaranteed to be robust against additive perturbations\nof a bounded magnitude. We apply our method to watermark diffusion models and\nshow that it matches state-of-the-art watermarking schemes in terms of\nrobustness to different types of synthetic watermark removal attacks.",
        "An underlying assumption of many existing approaches to human-robot task\ncommunication is that the robot possesses a sufficient amount of environmental\ndomain knowledge, including the locations of task-critical objects. This\nassumption is unrealistic if the locations of known objects change or have not\nyet been discovered by the robot. In this work, our key insight is that in many\nscenarios, robot end users possess more scene insight than the robot and need\nways to express it. Presently, there is a lack of research on how solutions for\ncollecting end-user scene insight should be designed. We thereby created an\nUncertainty Expression System (UES) to investigate how best to elicit end-user\nscene insight. The UES allows end users to convey their knowledge of object\nuncertainty using either: (1) a precision interface that allows meticulous\nexpression of scene insight; (2) a painting interface by which users create a\nheat map of possible object locations; and (3) a ranking interface by which end\nusers express object locations via an ordered list. We then conducted a user\nstudy to compare the effectiveness of these approaches based on the accuracy of\nscene insight conveyed to the robot, the efficiency at which end users are able\nto express this scene insight, and both usability and task load. Results\nindicate that the rank interface is more user friendly and efficient than the\nprecision interface, and that the paint interface is the least accurate."
      ]
    }
  }
]