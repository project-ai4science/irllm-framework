[
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal",
    "start_abstract":"Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots"
      ],
      "abstract":[
        "Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods."
      ],
      "categories":[
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Age of Information in Multi-Relay Networks with Maximum Age Scheduling",
        "Identification of High Impedance Faults Utilizing Recurrence Plots",
        "Performance Analysis of Spatial and Temporal Learning Networks in the\n  Presence of DVL Noise",
        "RIS-enabled Multi-user M-QAM Uplink NOMA Systems: Design, Analysis, and\n  Optimization",
        "Training Channel Selection for Learning-based 1-bit Precoding in Massive\n  MU-MIMO",
        "Bayesian Beamforming for Integrated Sensing and Communication Systems",
        "Efficient Sampling Allocation Strategies for General Graph-Filter-Based\n  Signal Recovery",
        "RIS-based Physical Layer Security for Integrated Sensing and\n  Communication: A Comprehensive Survey",
        "Joint Bistatic Positioning and Monostatic Sensing: Optimized Beamforming\n  and Performance Tradeoff",
        "Integrated Long-range Sensing and Communications in Multi Target\n  Scenarios using CP-OFDM",
        "Characterisation of exposure to non-ionising electromagnetic fields in\n  the Spanish INMA birth cohort: Study protocol",
        "Maximum-Entropy-Rate Selection of Features for Classifying Changes in\n  Knee and Ankle Dynamics During Running",
        "Bistatic Micro-Doppler Analysis of a Vertical Takeoff and Landing (VTOL)\n  Drone in ICAS Framework",
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities"
      ],
      "abstract":[
        "We propose and evaluate age of information (AoI)-aware multiple access\nmechanisms for the Internet of Things (IoT) in multi-relay two-hop networks.\nThe network considered comprises end devices (EDs) communicating with a set of\nrelays in ALOHA fashion, with new information packets to be potentially\ntransmitted every time slot. The relays, in turn, forward the collected packets\nto an access point (AP), the final destination of the information generated by\nthe EDs. More specifically, in this work we investigate the performance of four\nage-aware algorithms that prioritize older packets to be transmitted, namely\nmax-age matching (MAM), iterative max-age scheduling (IMAS), age-based delayed\nrequest (ABDR), and buffered ABDR (B-ABDR). The former two algorithms are\nadapted into the multi-relay setup from previous research, and achieve\nsatisfactory average AoI and average peak AoI performance, at the expense of a\nsignificant amount of information exchange between the relays and the AP. The\nlatter two algorithms are newly proposed to let relays decide which one(s) will\ntransmit in a given time slot, requiring less signaling than the former\nalgorithms. We provide an analytical formulation for the AoI lower bound\nperformance, compare the performance of all algorithms in this set-up, and show\nthat they approach the lower bound. The latter holds especially true for\nB-ABDR, which approaches the lower bound the most closely, tilting the scale in\nits favor, as it also requires far less signaling than MAM and IMAS.",
        "This paper presents a systematic approach to detecting High Impedance Faults\n(HIFs) in medium voltage distribution networks using recurrence plots and\nmachine learning. We first simulate 1150 internal faults, including 300 HIFs,\n1000 external faults, and 40 normal conditions using the PSCAD\/EMTDC software.\nKey features are extracted from the 3-phase differential currents using wavelet\ncoefficients, which are then converted into recurrence matrices. A multi-stage\nclassification framework is employed, where the first classification stage\nidentifies internal faults, and the second stage distinguishes HIFs from other\ninternal faults. The framework is evaluated using accuracy, precision, recall,\nand F1 score. Tree-based classifiers, particularly Random Forest and Decision\nTree, achieve superior performance, with 99.24% accuracy in the first stage and\n98.26% in the second. The results demonstrate the effectiveness of integrating\nrecurrence analysis with machine learning for fault detection in power\ndistribution networks.",
        "Navigation is a critical aspect of autonomous underwater vehicles (AUVs)\noperating in complex underwater environments. Since global navigation satellite\nsystem (GNSS) signals are unavailable underwater, navigation relies on inertial\nsensing, which tends to accumulate errors over time. To mitigate this, the\nDoppler velocity log (DVL) plays a crucial role in determining navigation\naccuracy. In this paper, we compare two neural network models: an adapted\nversion of BeamsNet, based on a one-dimensional convolutional neural network,\nand a Spectrally Normalized Memory Neural Network (SNMNN). The former focuses\non extracting spatial features, while the latter leverages memory and temporal\nfeatures to provide more accurate velocity estimates while handling biased and\nnoisy DVL data. The proposed approaches were trained and tested on real AUV\ndata collected in the Mediterranean Sea. Both models are evaluated in terms of\naccuracy and estimation certainty and are benchmarked against the least squares\n(LS) method, the current model-based approach. The results show that the neural\nnetwork models achieve over a 50% improvement in RMSE for the estimation of the\nAUV velocity, with a smaller standard deviation.",
        "Non-orthogonal multiple access (NOMA) is widely recognized for enhancing the\nenergy and spectral efficiency through effective radio resource sharing.\nHowever, uplink NOMA systems face greater challenges than their downlink\ncounterparts, as their bit error rate (BER) performance is hindered by an\ninherent error floor due to error propagation caused by imperfect successive\ninterference cancellation (SIC). This paper investigates BER performance\nimprovements enabled by reconfigurable intelligent surfaces (RISs) in\nmulti-user uplink NOMA transmission. Specifically, we propose a novel\nRIS-assisted uplink NOMA design, where the RIS phase shifts are optimized to\nenhance the received signal amplitudes while mitigating the phase rotations\ninduced by the channel. To achieve this, we first develop an accurate channel\nmodel for the effective user channels, which facilitates our BER analysis. We\nthen introduce a channel alignment scheme for a two-user scenario, enabling\nefficient SIC-based detection and deriving closed-form BER expressions. We\nfurther extend the analysis to a generalized setup with an arbitrary number of\nusers and modulation orders for quadrature amplitude modulation signaling.\nUsing the derived BER expressions, we develop an optimized uplink NOMA power\nallocation (PA) scheme that minimizes the average BER while satisfying the user\ntransmit power constraints. It will be shown that the proposed NOMA detection\nscheme, in conjunction with the optimized PA strategy, eliminate SIC error\nfloors at the base station. The theoretical BER expressions are validated using\nsimulations, which confirms the effectiveness of the proposed design in\neliminating BER floors.",
        "Learning-based algorithms have gained great popularity in communications\nsince they often outperform even carefully engineered solutions by learning\nfrom training samples. In this paper, we show that the selection of appropriate\ntraining examples can be important for the performance of such learning-based\nalgorithms. In particular, we consider non-linear 1-bit precoding for massive\nmulti-user MIMO systems using the C2PO algorithm. While previous works have\nalready shown the advantages of learning critical coefficients of this\nalgorithm, we demonstrate that straightforward selection of training samples\nthat follow the channel model distribution does not necessarily lead to the\nbest result. Instead, we provide a strategy to generate training data based on\nthe specific properties of the algorithm, which significantly improves its\nerror floor performance.",
        "The uncertainty of the sensing target brings great challenge to the\nbeamforming design of the integrated sensing and communication (ISAC) system.\nTo address this issue, we model the scattering coefficient and azimuth angle of\nthe target as random variables and introduce a novel metric, expected detection\nprobability (EPd), to quantify the average detection performance from a\nBayesian perspective. Furthermore, we design a Bayesian beamforming scheme to\noptimize the expected detection probability under the limited power budget and\ncommunication performance constraints. A successive convex approximation and\nsemidefinite relaxation-based (SCA-SDR) algorithm is developed for the\ncomplicated non-convex optimization problem corresponding to the beamforming\nscheme. Simulation results show that the proposed scheme outperforms other\nbenchmarks and exhibits robust detection performance when parameters of the\ntarget are unknown and random.",
        "Sensor placement plays a crucial role in graph signal recovery in\nunderdetermined systems. In this paper, we present the graph-filtered\nregularized maximum likelihood (GFR-ML) estimator of graph signals, which\nintegrates general graph filtering with regularization to enhance signal\nrecovery performance under a limited number of sensors. Then, we investigate\ntask-based sampling allocation aimed at minimizing the mean squared error (MSE)\nof the GFR-ML estimator by wisely choosing sensor placement. Since this MSE\ndepends on the unknown graph signals to be estimated, we propose four cost\nfunctions for the optimization of the sampling allocation: the biased\nCram$\\acute{\\text{e}}$r-Rao bound (bCRB), the worst-case MSE (WC-MSE), the\nBayesian MSE (BMSE), and the worst-case BMSE (WC-BMSE), where the last two\nassume a Gaussian prior. We investigate the properties of these cost functions\nand develop two algorithms for their practical implementation: 1) the\nstraightforward greedy algorithm; and 2) the alternating projection gradient\ndescent (PGD) algorithm that reduces the computational complexity. Simulation\nresults on synthetic and real-world datasets of the IEEE 118-bus power system\nand the Minnesota road network demonstrate that the proposed sampling\nallocation methods reduce the MSE by up to $50\\%$ compared to the common\nsampling methods A-design, E-design, and LR-design in the tested scenarios.\nThus, the proposed methods improve the estimation performance and reduce the\nrequired number of measurements in graph signal processing (GSP)-based signal\nrecovery in the case of underdetermined systems.",
        "Integrated Sensing and Communication (ISAC) is a crucial component of future\nwireless networks, enabling seamless integration of Communication and Sensing\n(C\\&S) functionalities. However, ensuring security in ISAC systems remains a\nsignificant challenge, as both C\\&S data are susceptible to adversarial\nthreats. Physical Layer Security (PLS) has emerged as a key framework for\nmitigating these risks at the transmission level. Reconfigurable Intelligent\nSurfaces (RIS) further enhance PLS by dynamically shaping the radio environment\nto improve both secrecy along with C\\&S performance. This survey begins with an\noverview of RIS, PLS, and ISAC fundamentals, establishing a foundation for\nunderstanding their integration. The state-of-the-art RIS-assisted PLS\napproaches in ISAC systems are then categorized into passive RIS and Active RIS\n(ARIS) paradigms. Passive RIS-based techniques focus on optimizing system\nthroughput, covert communication, and Secrecy Rates (SRs), alongside improving\nsensing Signal-to-Noise Ratio (SNR) and Weighted Sum Rate (WSR) under various\nconstraints. ARIS-based strategies extend these capabilities by actively\noptimizing beamforming to enhance secrecy and covert rates while ensuring\nrobust sensing under communication and security constraints. By reviewing both\npassive and ARIS-based security frameworks, this survey highlights the\ntransformative role of RIS in strengthening ISAC security. Furthermore, it\nexplores key optimization methodologies, technical challenges, and future\nresearch directions for integrating RIS with PLS to ensure secure and efficient\nISAC in next-generation 6G wireless networks.",
        "We investigate joint bistatic positioning (BP) and monostatic sensing (MS)\nwithin a multi-input multi-output orthogonal frequency-division system. Based\non the derived Cram\\'er-Rao Bounds (CRBs), we propose novel beamforming\noptimization strategies that enable flexible performance trade-offs between BP\nand MS. Two distinct objectives are considered in this multi-objective\noptimization problem, namely, enabling user equipment to estimate its own\nposition while accounting for unknown clock bias and orientation, and allowing\nthe base station to locate passive targets. We first analyze digital schemes,\nproposing both weighted-sum CRB and weighted-sum mismatch (of beamformers and\ncovariance matrices) minimization approaches. These are examined under\nfull-dimension beamforming (FDB) and low-complexity codebook-based power\nallocation (CPA). To adapt to low-cost hardwares, we develop unit-amplitude\nanalog FDB and CPA schemes based on the weighted-sum mismatch of the covariance\nmatrices paradigm, solved using distinct methods. Numerical results confirm the\neffectiveness of our designs, highlighting the superiority of minimizing the\nweighted-sum mismatch of covariance matrices, and the advantages of mutual\ninformation fusion between BP and MS.",
        "6G communication systems promise to deliver sensing capabilities by utilizing\nthe orthogonal frequency division multiplexing (OFDM) communication signal for\nsensing. However, the cyclic prefix inherent in OFDM systems limits the sensing\nrange, necessitating compensation techniques to detect small, distant targets\nlike drones. In this paper, we show that state-of-the-art coherent compensation\nmethods fail in scenarios involving multiple targets, resulting in an increased\nnoise floor in the radar image. Our contributions include a novel multi target\ncoherent compensation algorithm and a generalized\nsignal-to-interference-and-noise ratio for multiple targets to evaluate the\nperformance. Our algorithm achieves the same detection performance at long\ndistances requiring only 3.6% of the radio resources compared to classical OFDM\nradar processing. This enables resource efficient sensing at long distances in\nmulti target scenarios with legacy communications-only networks.",
        "Analysis of the association between exposure to electromagnetic fields of\nnon-ionising radiation (EMF-NIR) and health in children and adolescents is\nhindered by the limited availability of data, mainly due to the difficulties on\nthe exposure assessment. This study protocol describes the methodologies used\nfor characterising exposure of children to EMF-NIR in the INMA (INfancia y\nMedio Ambiente- Environment and Childhood) Project, a prospective cohort study.\nIndirect (proximity to emission sources, questionnaires on sources use and\ngeospatial propagation models) and direct methods (spot and fixed longer-term\nmeasurements and personal measurements) were conducted in order to assess\nexposure levels of study participants aged between 7 and 18 years old. The\nmethodology used varies depending on the frequency of the EMF-NIR and the\nenvironment (homes, schools and parks). Questionnaires assessed the use of\nsources contributing both to Extremely Low Frequency (ELF) and Radiofrequency\n(RF) exposure levels. Geospatial propagation models (NISMap) are implemented\nand validated for environmental outdoor sources of RFs using spot measurements.\nSpot and fixed longer-term ELF and RF measurements were done in the\nenvironments where children spend most of the time. Moreover, personal\nmeasurements were taken in order to assess individual exposure to RF. The\nexposure data are used to explore their relationships with proximity and\/or use\nof EMF-NIR sources.",
        "This paper investigates deteriorations in knee and ankle dynamics during\nrunning. Changes in lower limb accelerations are analyzed by a wearable\nmusculo-skeletal monitoring system. The system employs a machine learning\ntechnique to classify joint stiffness. A maximum-entropyrate method is\ndeveloped to select the most relevant features. Experimental results\ndemonstrate that distance travelled and energy expended can be estimated from\nobserved changes in knee and ankle motions during 5 km runs.",
        "Integrated Communication and Sensing (ICAS) is a key technology that enables\nsensing functionalities within the next-generation mobile communication (6G).\nJoint design and optimization of both functionalities could allow coexistence,\ntherefore it advances toward joint signal processing and using the same\nhardware platform and common spectrum. Contributing to ICAS sensing, this paper\npresents the measurement and analysis of the micro-Doppler signature of\nVertical Takeoff and Landing (VTOL) drones. Measurement is performed with an\nOFDM-like communication signal and bistatic constellation, which is a typical\ncase in ICAS scenarios. This work shows that micro-Doppler signatures can be\nused to precisely distinguish flight modes, such as take-off, landing,\nhovering, transition, and cruising.",
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Sparse MRI: The application of compressed sensing for rapid MR imaging",
    "start_abstract":"Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
    "start_categories":[
      "cs.LG",
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Design of an Automated Ethanol Vapor Generating System for Alcohol Use\n  Disorder(AUD) Animal Studies",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "Solar radiation and atmospheric CO$_2$ predict young leaf production in\n  a moist evergreen tropical forest: Insights from 23 years",
        "Reconstructing Noisy Gene Regulation Dynamics Using\n  Extrinsic-Noise-Driven Neural Stochastic Differential Equations",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors"
      ],
      "abstract":[
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "Alcohol Use Disorder (AUD) is a prevalent addictive disorder affecting an\nestimated 29.5 million Americans. It is characterized by impaired control over\nalcohol consumption despite negative consequences. The number of diagnostic\ncriteria met by an individual typically determines the severity of AUD.\nResearch into AUD focuses on understanding individual susceptibility\ndifferences and developing preventive strategies. Alcohol vapor inhalation has\nemerged as a promising method for pathophysiological investigations in animals,\nallowing researchers to control the dose and duration of alcohol exposure. This\napproach is crucial for studying the escalation of voluntary alcohol-drinking\nbehavior. Current commercial systems for alcohol vapor generation have\nlimitations, including combustion risks and the need to adjust multiple\nparameters. Other methods, like bubbling or blow-over evaporation, face\nchallenges in maintaining equilibrium and avoiding aerosolization. To address\nthese issues, a new type of ethanol vapor generating system is proposed that\nrelies solely on temperature control, creating a vacuum into which ethanol\nevaporates under thermodynamic control. This approach eliminates the need to\nadjust multiple parameters and offers improved accuracy and precision in vapor\ndose delivery. We validated the system as anticipated, achieving stable ethanol\nvapor after a few priming cycles. Using a 1.2 L cylinder, we obtained\napproximately 3.6 L of saturated vapor\/air mix in 1 minute. Gravimetric results\nshowed that each cycle produced about 100 mg\/L or ~10,000 ppm vapor-to-air\nmixture. The intended use of the ethanol vapor generator is to provide a\nconcentrated ethanol vapor \/ air mixture to be further diluted before\ndelivering to the animals.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Climate change impacts ecosystems worldwide, affecting animal behaviour and\nsurvival both directly and indirectly through changes such as the availability\nof food. For animals reliant on leaves as a primary food source, understanding\nhow climate change influences leaf production of trees is crucial, yet this is\nunderstudied, especially in moist evergreen tropical forests. We analyzed a\n23-year dataset of young leaf phenology from a moist tropical forest in Kibale\nNational Park, Uganda, to examine seasonal and long-term patterns of 12 key\ntree species consumed by folivorous primates. We described phenological\npatterns and explored relationships between young leaf production of different\ntree species and climate variables. We also assessed the suitability of the\nEnhanced Vegetation Index (EVI) as a proxy for young leaf production in moist\nevergreen tropical forests. Our results showed that tree species exhibited\ndistinct phenological patterns, with most species producing young leaves during\ntwo seasonal peaks aligned with the rainy seasons. Rainfall, cloud cover, and\nmaximum temperature were the most informative predictors of seasonal variation\nin young leaf production. However, solar radiation and atmospheric CO$_2$ were\nmost informative regarding long-term trends. EVI was strongly correlated with\nyoung leaf production within years but less effective for capturing\ninter-annual trends. These findings highlight the complex relationship between\nclimate and young leaf phenology in moist evergreen tropical forests, and helps\nus understand the changes in food availability for tropical folivores.",
        "Proper regulation of cell signaling and gene expression is crucial for\nmaintaining cellular function, development, and adaptation to environmental\nchanges. Reaction dynamics in cell populations is often noisy because of (i)\ninherent stochasticity of intracellular biochemical reactions (``intrinsic\nnoise'') and (ii) heterogeneity of cellular states across different cells that\nare influenced by external factors (``extrinsic noise''). In this work, we\nintroduce an extrinsic-noise-driven neural stochastic differential equation\n(END-nSDE) framework that utilizes the Wasserstein distance to accurately\nreconstruct SDEs from trajectory data from a heterogeneous population of cells\n(extrinsic noise). We demonstrate the effectiveness of our approach using both\nsimulated and experimental data from three different systems in cell biology:\n(i) circadian rhythms, (ii) RPA-DNA binding dynamics, and (iii) NF$\\kappa$B\nsignaling process. Our END-nSDE reconstruction method can model how cellular\nheterogeneity (extrinsic noise) modulates reaction dynamics in the presence of\nintrinsic noise. It also outperforms existing time-series analysis methods such\nas recurrent neural networks (RNNs) and long short-term memory networks\n(LSTMs). By inferring cellular heterogeneities from data, our END-nSDE\nreconstruction method can reproduce noisy dynamics observed in experiments. In\nsummary, the reconstruction method we propose offers a useful surrogate\nmodeling approach for complex biophysical processes, where high-fidelity\nmechanistic models may be impractical.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MoDL: Model-Based Deep Learning Architecture for Inverse Problems",
    "start_abstract":"We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations.",
    "start_categories":[
      "cs.LG",
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Design of an Automated Ethanol Vapor Generating System for Alcohol Use\n  Disorder(AUD) Animal Studies",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "Solar radiation and atmospheric CO$_2$ predict young leaf production in\n  a moist evergreen tropical forest: Insights from 23 years",
        "Reconstructing Noisy Gene Regulation Dynamics Using\n  Extrinsic-Noise-Driven Neural Stochastic Differential Equations",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors"
      ],
      "abstract":[
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "Alcohol Use Disorder (AUD) is a prevalent addictive disorder affecting an\nestimated 29.5 million Americans. It is characterized by impaired control over\nalcohol consumption despite negative consequences. The number of diagnostic\ncriteria met by an individual typically determines the severity of AUD.\nResearch into AUD focuses on understanding individual susceptibility\ndifferences and developing preventive strategies. Alcohol vapor inhalation has\nemerged as a promising method for pathophysiological investigations in animals,\nallowing researchers to control the dose and duration of alcohol exposure. This\napproach is crucial for studying the escalation of voluntary alcohol-drinking\nbehavior. Current commercial systems for alcohol vapor generation have\nlimitations, including combustion risks and the need to adjust multiple\nparameters. Other methods, like bubbling or blow-over evaporation, face\nchallenges in maintaining equilibrium and avoiding aerosolization. To address\nthese issues, a new type of ethanol vapor generating system is proposed that\nrelies solely on temperature control, creating a vacuum into which ethanol\nevaporates under thermodynamic control. This approach eliminates the need to\nadjust multiple parameters and offers improved accuracy and precision in vapor\ndose delivery. We validated the system as anticipated, achieving stable ethanol\nvapor after a few priming cycles. Using a 1.2 L cylinder, we obtained\napproximately 3.6 L of saturated vapor\/air mix in 1 minute. Gravimetric results\nshowed that each cycle produced about 100 mg\/L or ~10,000 ppm vapor-to-air\nmixture. The intended use of the ethanol vapor generator is to provide a\nconcentrated ethanol vapor \/ air mixture to be further diluted before\ndelivering to the animals.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Climate change impacts ecosystems worldwide, affecting animal behaviour and\nsurvival both directly and indirectly through changes such as the availability\nof food. For animals reliant on leaves as a primary food source, understanding\nhow climate change influences leaf production of trees is crucial, yet this is\nunderstudied, especially in moist evergreen tropical forests. We analyzed a\n23-year dataset of young leaf phenology from a moist tropical forest in Kibale\nNational Park, Uganda, to examine seasonal and long-term patterns of 12 key\ntree species consumed by folivorous primates. We described phenological\npatterns and explored relationships between young leaf production of different\ntree species and climate variables. We also assessed the suitability of the\nEnhanced Vegetation Index (EVI) as a proxy for young leaf production in moist\nevergreen tropical forests. Our results showed that tree species exhibited\ndistinct phenological patterns, with most species producing young leaves during\ntwo seasonal peaks aligned with the rainy seasons. Rainfall, cloud cover, and\nmaximum temperature were the most informative predictors of seasonal variation\nin young leaf production. However, solar radiation and atmospheric CO$_2$ were\nmost informative regarding long-term trends. EVI was strongly correlated with\nyoung leaf production within years but less effective for capturing\ninter-annual trends. These findings highlight the complex relationship between\nclimate and young leaf phenology in moist evergreen tropical forests, and helps\nus understand the changes in food availability for tropical folivores.",
        "Proper regulation of cell signaling and gene expression is crucial for\nmaintaining cellular function, development, and adaptation to environmental\nchanges. Reaction dynamics in cell populations is often noisy because of (i)\ninherent stochasticity of intracellular biochemical reactions (``intrinsic\nnoise'') and (ii) heterogeneity of cellular states across different cells that\nare influenced by external factors (``extrinsic noise''). In this work, we\nintroduce an extrinsic-noise-driven neural stochastic differential equation\n(END-nSDE) framework that utilizes the Wasserstein distance to accurately\nreconstruct SDEs from trajectory data from a heterogeneous population of cells\n(extrinsic noise). We demonstrate the effectiveness of our approach using both\nsimulated and experimental data from three different systems in cell biology:\n(i) circadian rhythms, (ii) RPA-DNA binding dynamics, and (iii) NF$\\kappa$B\nsignaling process. Our END-nSDE reconstruction method can model how cellular\nheterogeneity (extrinsic noise) modulates reaction dynamics in the presence of\nintrinsic noise. It also outperforms existing time-series analysis methods such\nas recurrent neural networks (RNNs) and long short-term memory networks\n(LSTMs). By inferring cellular heterogeneities from data, our END-nSDE\nreconstruction method can reproduce noisy dynamics observed in experiments. In\nsummary, the reconstruction method we propose offers a useful surrogate\nmodeling approach for complex biophysical processes, where high-fidelity\nmechanistic models may be impractical.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation",
    "start_abstract":"Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b14",
        "b1"
      ],
      "title":[
        "Sparse MRI: The application of compressed sensing for rapid MR imaging",
        "MoDL: Model-Based Deep Learning Architecture for Inverse Problems"
      ],
      "abstract":[
        "Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
        "We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations."
      ],
      "categories":[
        "cs.LG",
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "Visualizing Machine Learning Models for Enhanced Financial\n  Decision-Making and Risk Management",
        "Geodesic Variational Bayes for Multiway Covariances",
        "Describing Nonstationary Data Streams in Frequency Domain",
        "End-to-End triplet loss based fine-tuning for network embedding in\n  effective PII detection",
        "SMPR: A structure-enhanced multimodal drug-disease prediction model for\n  drug repositioning and cold start",
        "Globality Strikes Back: Rethinking the Global Knowledge of CLIP in\n  Training-Free Open-Vocabulary Semantic Segmentation",
        "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment",
        "Unveiling the Power of Noise Priors: Enhancing Diffusion Models for\n  Mobile Traffic Prediction",
        "Sparse Autoencoders Can Interpret Randomly Initialized Transformers",
        "HMCGeo: IP Region Prediction Based on Hierarchical Multi-label\n  Classification",
        "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
        "LLM-TabFlow: Synthetic Tabular Data Generation with Inter-column Logical\n  Relationship Preservation",
        "CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation",
        "Pressure-Induced Structural and Dielectric Changes in Liquid Water at\n  Room Temperature",
        "AugGen: Synthetic Augmentation Can Improve Discriminative Models",
        "CAPOS: The bulge Cluster APOGEE Survey VII: First detailed chemical\n  analysis of NGC 6316",
        "Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low\n  Earth Orbit",
        "The Impact of Building-Induced Visibility Restrictions on Intersection\n  Accidents",
        "Scientific literature cited in patents: A Technology Transfer indicator\n  in Portuguese universities",
        "An atomistic approach for modeling of polarizability and Raman\n  scattering of water clusters and liquid water",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Sobol-CPI: a Doubly Robust Conditional Permutation Importance Statistic",
        "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR",
        "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for\n  Anomaly Detection in CAN Bus",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Joint Optimization of Resource Allocation and Radar Receiver Selection\n  in Integrated Communication-Radar Systems",
        "Personalize Your LLM: Fake it then Align it"
      ],
      "abstract":[
        "This study emphasizes how crucial it is to visualize machine learning models,\nespecially for the banking industry, in order to improve interpretability and\nsupport predictions in high stakes financial settings. Visual tools enable\nperformance improvements and support the creation of innovative financial\nmodels by offering crucial insights into the algorithmic decision-making\nprocesses. Within a financial machine learning framework, the research uses\nvisually guided experiments to make important concepts, such risk assessment\nand portfolio allocation, more understandable. The study also examines\nvariations in trading tactics and how they relate to risk appetite, coming to\nthe conclusion that the frequency of portfolio rebalancing is negatively\ncorrelated with risk tolerance. Finding these ideas is made possible in large\npart by visualization. The study concludes by presenting a novel method of\nlocally stochastic asset weighing, where visualization facilitates data\nextraction and validation. This highlights the usefulness of these methods in\nfurthering the field of financial machine learning research.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "Concept drift is among the primary challenges faced by the data stream\nprocessing methods. The drift detection strategies, designed to counteract the\nnegative consequences of such changes, often rely on analyzing the problem\nmetafeatures. This work presents the Frequency Filtering Metadescriptor -- a\ntool for characterizing the data stream that searches for the informative\nfrequency components visible in the sample's feature vector. The frequencies\nare filtered according to their variance across all available data batches. The\npresented solution is capable of generating a metadescription of the data\nstream, separating chunks into groups describing specific concepts on its\nbasis, and visualizing the frequencies in the original spatial domain. The\nexperimental analysis compared the proposed solution with two state-of-the-art\nstrategies and with the PCA baseline in the post-hoc concept identification\ntask. The research is followed by the identification of concepts in the\nreal-world data streams. The generalization in the frequency domain adapted in\nthe proposed solution allows to capture the complex feature dependencies as a\nreduced number of frequency components, while maintaining the semantic meaning\nof data.",
        "There are many approaches in mobile data ecosystem that inspect network\ntraffic generated by applications running on user's device to detect personal\ndata exfiltration from the user's device. State-of-the-art methods rely on\nfeatures extracted from HTTP requests and in this context, machine learning\ninvolves training classifiers on these features and making predictions using\nlabelled packet traces. However, most of these methods include external feature\nselection before model training. Deep learning, on the other hand, typically\ndoes not require such techniques, as it can autonomously learn and identify\npatterns in the data without external feature extraction or selection\nalgorithms. In this article, we propose a novel deep learning based end-to-end\nlearning framework for prediction of exposure of personally identifiable\ninformation (PII) in mobile packets. The framework employs a pre-trained large\nlanguage model (LLM) and an autoencoder to generate embedding of network\npackets and then uses a triplet-loss based fine-tuning method to train the\nmodel, increasing detection effectiveness using two real-world datasets. We\ncompare our proposed detection framework with other state-of-the-art works in\ndetecting PII leaks from user's device.",
        "Repositioning drug-disease relationships has always been a hot field of\nresearch. However, actual cases of biologically validated drug relocation\nremain very limited, and existing models have not yet fully utilized the\nstructural information of the drug. Furthermore, most repositioning models are\nonly used to complete the relationship matrix, and their practicality is poor\nwhen dealing with drug cold start problems. This paper proposes a\nstructure-enhanced multimodal relationship prediction model (SMRP). SMPR is\nbased on the SMILE structure of the drug, using the Mol2VEC method to generate\ndrug embedded representations, and learn disease embedded representations\nthrough heterogeneous network graph neural networks. Ultimately, a drug-disease\nrelationship matrix is constructed. In addition, to reduce the difficulty of\nusers' use, SMPR also provides a cold start interface based on structural\nsimilarity based on reposition results to simply and quickly predict\ndrug-related diseases. The repositioning ability and cold start capability of\nthe model are verified from multiple perspectives. While the AUC and ACUPR\nscores of repositioning reach 99% and 61% respectively, the AUC of cold start\nachieve 80%. In particular, the cold start Recall indicator can reach more than\n70%, which means that SMPR is more sensitive to positive samples. Finally, case\nanalysis is used to verify the practical value of the model and visual analysis\ndirectly demonstrates the improvement of the structure to the model. For quick\nuse, we also provide local deployment of the model and package it into an\nexecutable program.",
        "Recent works modify CLIP to perform open-vocabulary semantic segmentation in\na training-free manner (TF-OVSS). In CLIP, patch-wise image representations\nmainly encode the homogeneous image-level properties and thus are not\ndiscriminative enough, hindering its application to the dense prediction task.\nPrevious works make image features more distinct across patches, through making\neach patch mainly attend to itself or the neighboring patches within a narrow\nlocal window. However, with their modifications, the ability of CLIP to\naggregate global context information, which is known to be useful for\ndistinguishing confusing categories, is largely weakened. In this paper, we\npropose a new method named GCLIP, which mines the beneficial global knowledge\nof CLIP to facilitate the TF-OVSS task. Firstly, we aim to equip the last-block\nattention with image-level properties while not introducing homogeneous\nattention patterns across patches. In GCLIP, we merge the attention from the\nglobal token emerging blocks with the Query-Query attention to realize this\ngoal. Secondly, we aim to make the Value embeddings of the last-block attention\nmodule more distinct and semantically correlated. To realize this, we design a\nnovel channel suppression strategy. As the representation of each patch is\nfinally determined by the attention weights and the Value embeddings, our\nmethod can generate more discriminative patch-level image features while\nabsorbing global context information. Extensive experiments on five standard\nbenchmarks demonstrate that our method consistently outperforms previous\nstate-of-the-arts.",
        "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https:\/\/github.com\/LAMDASZ-ML\/TabFSBench.",
        "Accurate prediction of mobile traffic, \\textit{i.e.,} network traffic from\ncellular base stations, is crucial for optimizing network performance and\nsupporting urban development. However, the non-stationary nature of mobile\ntraffic, driven by human activity and environmental changes, leads to both\nregular patterns and abrupt variations. Diffusion models excel in capturing\nsuch complex temporal dynamics due to their ability to capture the inherent\nuncertainties. Most existing approaches prioritize designing novel denoising\nnetworks but often neglect the critical role of noise itself, potentially\nleading to sub-optimal performance. In this paper, we introduce a novel\nperspective by emphasizing the role of noise in the denoising process. Our\nanalysis reveals that noise fundamentally shapes mobile traffic predictions,\nexhibiting distinct and consistent patterns. We propose NPDiff, a framework\nthat decomposes noise into \\textit{prior} and \\textit{residual} components,\nwith the \\textit{prior} derived from data dynamics, enhancing the model's\nability to capture both regular and abrupt variations. NPDiff can seamlessly\nintegrate with various diffusion-based prediction models, delivering\npredictions that are effective, efficient, and robust. Extensive experiments\ndemonstrate that it achieves superior performance with an improvement over\n30\\%, offering a new perspective on leveraging diffusion models in this domain.",
        "Sparse autoencoders (SAEs) are an increasingly popular technique for\ninterpreting the internal representations of transformers. In this paper, we\napply SAEs to 'interpret' random transformers, i.e., transformers where the\nparameters are sampled IID from a Gaussian rather than trained on text data. We\nfind that random and trained transformers produce similarly interpretable SAE\nlatents, and we confirm this finding quantitatively using an open-source\nauto-interpretability pipeline. Further, we find that SAE quality metrics are\nbroadly similar for random and trained transformers. We find that these results\nhold across model sizes and layers. We discuss a number of number interesting\nquestions that this work raises for the use of SAEs and auto-interpretability\nin the context of mechanistic interpretability.",
        "Fine-grained IP geolocation plays a critical role in applications such as\nlocation-based services and cybersecurity. Most existing fine-grained IP\ngeolocation methods are regression-based; however, due to noise in the input\ndata, these methods typically encounter kilometer-level prediction errors and\nprovide incorrect region information for users. To address this issue, this\npaper proposes a novel hierarchical multi-label classification framework for IP\nregion prediction, named HMCGeo. This framework treats IP geolocation as a\nhierarchical multi-label classification problem and employs residual\nconnection-based feature extraction and attention prediction units to predict\nthe target host region across multiple geographical granularities. Furthermore,\nwe introduce probabilistic classification loss during training, combining it\nwith hierarchical cross-entropy loss to form a composite loss function. This\napproach optimizes predictions by utilizing hierarchical constraints between\nregions at different granularities. IP region prediction experiments on the New\nYork, Los Angeles, and Shanghai datasets demonstrate that HMCGeo achieves\nsuperior performance across all geographical granularities, significantly\noutperforming existing IP geolocation methods.",
        "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
        "Synthetic tabular data have widespread applications in industrial domains\nsuch as healthcare, finance, and supply chains, owing to their potential to\nprotect privacy and mitigate data scarcity. However, generating realistic\nsynthetic tabular data while preserving inter-column logical relationships\nremains a significant challenge for the existing generative models. To address\nthese challenges, we propose LLM-TabFlow, a novel approach that leverages Large\nLanguage Model (LLM) reasoning to capture complex inter-column relationships\nand compress tabular data, while using Score-based Diffusion to model the\ndistribution of the compressed data in latent space. Additionally, we introduce\nan evaluation framework, which is absent in literature, to fairly assess the\nperformance of synthetic tabular data generation methods in real-world\ncontexts. Using this framework, we conduct extensive experiments on two\nreal-world industrial datasets, evaluating LLM-TabFlow against other five\nbaseline methods, including SMOTE (an interpolation-based approach) and other\nstate-of-the-art generative models. Our results show that LLM-TabFlow\noutperforms all baselines, fully preserving inter-column relationships while\nachieving the best balance between data fidelity, utility, and privacy. This\nstudy is the first to explicitly address inter-column relationship preservation\nin synthetic tabular data generation, offering new insights for developing more\nrealistic and reliable tabular data generation methods.",
        "The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.",
        "Understanding the pressure-dependent dielectric properties of water is\ncrucial for a wide range of scientific and practical applications. In this\nstudy, we employ a deep neural network trained on density functional theory\ndata to investigate the dielectric properties of liquid water at room\ntemperature across a pressure range of 0.1 MPa to 1000 MPa. We observe a\nnonlinear increase in the static dielectric constant $\\epsilon_0$ with\nincreasing pressure, a trend that is qualitatively consistent with experimental\nobservations. This increase in $\\epsilon_0$ is primarily attributed to the\nincrease in water density under compression, which enhances collective dipole\nfluctuations within the hydrogen-bonding network as well as the dielectric\nresponse. Despite the increase in $\\epsilon_0$, our results reveal a decrease\nin the Kirkwood correlation factor $G_K$ with increasing pressure. This\ndecrease in $G_K$ is attributed to pressure-induced structural distortions in\nthe hydrogen-bonding network, which weaken dipolar correlations by disrupting\nthe ideal tetrahedral arrangement of water molecules.",
        "The increasing dependence on large-scale datasets in machine learning\nintroduces significant privacy and ethical challenges. Synthetic data\ngeneration offers a promising solution; however, most current methods rely on\nexternal datasets or pre-trained models, which add complexity and escalate\nresource demands. In this work, we introduce a novel self-contained synthetic\naugmentation technique that strategically samples from a conditional generative\nmodel trained exclusively on the target dataset. This approach eliminates the\nneed for auxiliary data sources. Applied to face recognition datasets, our\nmethod achieves 1--12\\% performance improvements on the IJB-C and IJB-B\nbenchmarks. It outperforms models trained solely on real data and exceeds the\nperformance of state-of-the-art synthetic data generation baselines. Notably,\nthese enhancements often surpass those achieved through architectural\nimprovements, underscoring the significant impact of synthetic augmentation in\ndata-scarce environments. These findings demonstrate that carefully integrated\nsynthetic data not only addresses privacy and resource constraints but also\nsubstantially boosts model performance. Project page\nhttps:\/\/parsa-ra.github.io\/auggen",
        "As part of the bulge Cluster APOgee Survey (CAPOS), high-resolution, high\nSignal-to-Noise Ratio Near-Infrared spectroscopy, we aim to conduct the most\nrobust chemical study to date for NGC 6316, deriving abundances for a number of\nelements with a variety of nucleosynthetic origins, most of which have never\nbeen studied before in this cluster. We use the Brussels Automatic Code for\nCharacterizing High accuracy Spectra (BACCHUS) with atmospheric parameters\nphotometrically obtained in order to determine, for the first time, abundances\nfor C, N, O, Mg, Al, Si, P, K, Ca, Ti, V, Cr, Mn, Fe, Ni and Ce for this\ncluster. We obtained a mean metallicity [Fe\/H] = -0.87 +- 0.02, finding no\nindication of an intrinsic metallicity spread. Our metallicity agrees with the\nmost recent values from other studies, revising earlier values that were ~0.5\ndex metal-richer. With this new value, this cluster, long believed to be a\nmember of the classical metal-rich group of bulge GCs around -0.5, now falls in\nthe dominant bulge globular cluster peak around [Fe\/H] = -1. The cluster\npresents a clear C-N anticorrelation. We also found a [{\\alpha}\/Fe] = 0.3 +-\n0.02. Our abundances show similar behaviour to other in situ globular clusters\nwith comparable metallicity. We obtained E(B-V) = 0.71 and (M-m)_0 = 15.32 +-\n0.05 by isochrone fitting, in good agreement with the recent determinations\nfrom other works. We derive an overall metallicity [M\/H] = -0.6 +- 0.05 by\nisochrone fitting, in agreement with our abundance determination. According to\nthe mean [Mg\/Fe] and [Al\/Fe] abundances from first population stars, NGC 6316\nis an in-situ globular cluster, in accordance with various dynamical\nclassifications.",
        "This paper presents an analysis and experimental demonstration of\nsingle-satellite single-pass geolocation of a terrestrial broadcast Global\nNavigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The\nproliferation of LEO-based GNSS receivers offers the prospect of unprecedented\nspectrum awareness, enabling persistent GNSS interference detection and\ngeolocation. Accurate LEO-based single-receiver emitter geolocation is possible\nwhen a range-rate time history can be extracted for the emitter. This paper\npresents a technique crafted specifically for indiscriminate broadcast-type\nGNSS spoofing signals. Furthermore, it explores how unmodeled oscillator\ninstability and worst-case spoofer-introduced signal variations degrade the\ngeolocation estimate. The proposed geolocation technique is validated by a\ncontrolled experiment, in partnership with Spire Global, in which a LEO-based\nreceiver captures broadcast GNSS spoofing signals transmitted from a known\nground station on a non-GNSS frequency band.",
        "Traffic accidents, especially at intersections, are a major road safety\nconcern. Previous research has extensively studied intersection-related\naccidents, but the effect of building-induced visibility restrictions at\nintersections on accident rates has been under-explored, particularly in urban\ncontexts. Using OpenStreetMap data, the UK's geographic and accident datasets,\nand the UK Traffic Count Dataset, we formulated a novel approach to estimate\naccident risk at intersections. This method factors in the area visible to\ndrivers, accounting for views blocked by buildings - a distinctive aspect in\ntraffic accident analysis. Our findings reveal a notable correlation between\nthe road visible percentage and accident frequency. In the model, the\ncoefficient for \"road visible percentage\" is 1.7450, implying a strong positive\nrelationship. Incorporating this visibility factor enhances the model's\nexplanatory power, with increased R-square values and reduced AIC and BIC,\nindicating a better data fit. This study underscores the essential role of\narchitectural layouts in road safety and suggests that urban planning\nstrategies should consider building-induced visibility restrictions. Such\nconsideration could be an effective approach to mitigate accident rates at\nintersections. This research opens up new avenues for innovative, data-driven\nurban planning and traffic management strategies, highlighting the importance\nof visibility enhancements for safer roads.",
        "The study aims to identify the process of transfer from science to technology\nthat occurs in the main Portuguese public universities. The methodology was\nbased on the analysis of the scientific literature cited in patents. Data was\nobtained from the Lens patent database. 10,514 scientific articles cited in\npatents were retrieved. A descriptive analysis of the data was performed.\nScience maps were created to visualize the main research trends. The results\nshowed a valuable impact of academic research in certain scientific\ndisciplines, such as Chemistry, Biology, Materials Sciences and Medicine. The\nmain research fronts were cancer, nanoparticles, biomaterials, tissue\nengineering or molecular biology. In conclusion, the research produced by\nPortuguese universities has generated relevant knowledge for patented\ninventions and the science-technology flow within specific areas.",
        "In this work, we develop a framework for atomistic modeling of electronic\npolarizability to predict the Raman spectra of hydrogen-bonded clusters and\nliquids from molecular dynamics (MD) simulations. The total polarizability of\nthe system is assumed to arise from contributions of both the monomer unit and\nintermolecular interactions. The generalized bond-polarizability model (GBPM),\ninspired by the classic bond-polarizability model, effectively describes the\nelectronic polarizability of a monomer. To account for the electronic\npolarizability arising from intermolecular interactions, we use a basis set of\nrapidly decaying functions of interatomic distances. We apply this model to\ncalculate the electronic polarizability and Raman spectra of water clusters\n((H2O)r, r = 2, 3, 4, 5, 6) and liquid water. The computational results are\ncompared with the results of quantum-mechanical calculations for clusters and\nto experimental data for the liquid. It is demonstrated that this simple and\nphysically motivated model, which relies on a small number of parameters,\nperforms well for clusters at both low and high temperatures, capturing strong\nanharmonic effects. Moreover, its high transferability suggests its\napplicability to other water clusters. These results suggest that a\nhierarchical approach based on the Jacob's ladder of increasingly sophisticated\nand accurate atomistic polarizability models incorporating additional effects\ncan be used for efficient modeling of Raman spectra from MD simulations of\nclusters, liquids and solids.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Conditional Permutation Importance (CPI) has been recently introduced for\nVariable Importance analysis with good empirical results. In this work, we\nfirst provide theoretical guarantees for CPI. We establish a double robustness\nproperty to detect null covariates, making it a suitable model for variable\nselection. We then present a modified and still computationally efficient\nversion, Sobol-CPI, that aims to estimate a well-known variable importance\nmeasure, the Total Sobol Index (TSI). We prove that it is nonparametrically\nefficient, and we provide a procedure to control the type-I error. Through\nnumerical experiments, we show that Sobol-CPI preserves the double robustness\nproperty in practice.",
        "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.",
        "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "In this paper, we investigate a distributed multi-input multi-output and\northogonal frequency division multiplexing (MIMO-OFDM) dual-function\nradar-communication (DFRC) system, which enables simultaneous communication and\nsensing in different subcarrier sets. To obtain the best tradeoff between\ncommunication and sensing performance, we first derive Cramer-Rao Bound (CRB)\nof targets in the detection area, and then maximize the transmission rate by\njointly optimizing the power\/subcarriers allocation and the selection of radar\nreceivers under the constraints of detection performance and total transmit\npower. To tackle the non-convex mixed integer programming problem, we decompose\nthe original problem into a semidefinite programming (SDP) problem and a convex\nquadratic integer problem and solve them iteratively. The numerical results\ndemonstrate the effectiveness of our proposed algorithm, as well as the\nperformance improvement brought by optimizing radar receivers selection.",
        "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The Great Barrier Reef: an environmental history",
    "start_abstract":"Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature"
      ],
      "abstract":[
        "Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer\n  Depression Detection",
        "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM\n  Red Teaming",
        "Sundial: A Family of Highly Capable Time Series Foundation Models",
        "Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA",
        "Evaluating the Systematic Reasoning Abilities of Large Language Models\n  through Graph Coloring",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "AdditiveLLM: Large Language Models Predict Defects in Additive\n  Manufacturing",
        "Policy Teaching via Data Poisoning in Learning from Human Preferences",
        "Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks",
        "E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction",
        "GreenAuto: An Automated Platform for Sustainable AI Model Design on Edge\n  Devices",
        "DPFAGA-Dynamic Power Flow Analysis and Fault Characteristics: A Graph\n  Attention Neural Network",
        "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
        "Resolution Invariant Autoencoder",
        "Extending Dense Passage Retrieval with Temporal Information",
        "Exploring constraints on the core radius and density jumps inside Earth\n  using atmospheric neutrino oscillations",
        "The Kodaira dimension of Hilbert modular threefolds",
        "Some NP Complete Problems Based on Algebra and Algebraic Geometry",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Simpliciality of vector-valued function spaces",
        "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating\n  Report from Raw Data",
        "Block Flow: Learning Straight Flow on Data Blocks",
        "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives",
        "Training Consistency Models with Variational Noise Coupling",
        "Formulas as Processes, Deadlock-Freedom as Choreographies (Extended\n  Version)",
        "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from\n  Motion-Blurred Images",
        "Optimal Control of Fluid Restless Multi-armed Bandits: A Machine\n  Learning Approach",
        "Central-moment-based discrete Boltzmann modeling of compressible flows"
      ],
      "abstract":[
        "Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.",
        "With LLM usage rapidly increasing, their vulnerability to jailbreaks that\ncreate harmful outputs are a major security risk. As new jailbreaking\nstrategies emerge and models are changed by fine-tuning, continuous testing for\nsecurity vulnerabilities is necessary. Existing Red Teaming methods fall short\nin cost efficiency, attack success rate, attack diversity, or extensibility as\nnew attack types emerge. We address these challenges with Modular And Diverse\nMalicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses\nautomatic assignment of attack strategies into relevant attack clusters,\nchooses the most relevant clusters for a malicious goal, and then combines\nstrategies from the selected clusters to achieve diverse novel attacks with\nhigh attack success rates. MAD-MAX further merges promising attacks together at\neach iteration of Red Teaming to boost performance and introduces a similarity\nfilter to prune out similar attacks for increased cost efficiency. The MAD-MAX\napproach is designed to be easily extensible with newly discovered attack\nstrategies and outperforms the prominent Red Teaming method Tree of Attacks\nwith Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and\nqueries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals\nin our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX\ndoes so with only 10.9 average queries to the target LLM compared to TAP with\n23.3.\n  WARNING: This paper contains contents which are offensive in nature.",
        "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on time series without discrete tokenization. Conditioned on\narbitrary-length time series, our model is pre-trained without specifying any\nprior distribution and can generate multiple probable predictions, achieving\nflexibility in representation learning beyond using parametric densities.\nTowards time series foundation models, we leverage minimal but crucial\nadaptations of Transformers and curate TimeBench with 1 trillion time points,\ncomprising mostly real-world datasets and synthetic data. By mitigating mode\ncollapse through TimeFlow Loss, we pre-train a family of Sundial models on\nTimeBench, which exhibit unprecedented model capacity and generalization\nperformance on zero-shot forecasting. In addition to presenting good scaling\nbehavior, Sundial achieves new state-of-the-art on both point forecasting and\nprobabilistic forecasting benchmarks. We believe that Sundial's pioneering\ngenerative paradigm will facilitate a wide variety of forecasting scenarios.",
        "Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence.\n  In this work, we propose a novel parameter-efficient Bayesian LoRA,\ndemonstrating that effective uncertainty quantification can be achieved in very\nlow-dimensional parameter spaces. The proposed method achieves strong\nperformance with improved calibration and generalization while maintaining\ncomputational efficiency. Our empirical findings show that, with the\nappropriate projection of the weight space: (1) uncertainty can be effectively\nmodeled in a low-dimensional space, and (2) weight covariances exhibit low\nranks.",
        "Contemporary large language models are powerful problem-solving tools, but\nthey exhibit weaknesses in their reasoning abilities which ongoing research\nseeks to mitigate. We investigate graph coloring as a means of evaluating an\nLLM's capacities for systematic step-by-step reasoning and possibility space\nexploration, as well as effects of semantic problem framing. We test Claude 3.5\nSonnet, Llama 3.1 405B, Gemini 1.5 Pro, GPT-4o, o1-mini, and DeepSeek-R1 on a\ndataset of $k$-coloring problems with $2 \\leq k \\leq 4$ and vertex count $4\n\\leq n \\leq 8$, using partial algorithmic solvers to further categorize\nproblems by difficulty. In addition to substantial but varying framing effects,\nwe find that all models except o1-mini and R1 exhibit $>60\\%$ error rates on\ndifficult problem types in all frames ($>15\\%$ for o1-mini and $>10\\%$ for R1),\nand no model achieves perfect accuracy even in the simple domain of 2-coloring\n4-vertex graphs. Our results highlight both the considerable recent progress in\nLLM systematic reasoning and the limits of its reliability, especially in\nrelation to increasing computational costs. We expect that more complex graph\ncoloring problems, and procedural generation of arbitrary-complexity reasoning\nproblems more broadly, offer further untapped potential for LLM benchmarking.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "In this work we investigate the ability of large language models to predict\nadditive manufacturing defect regimes given a set of process parameter inputs.\nFor this task we utilize a process parameter defect dataset to fine-tune a\ncollection of models, titled AdditiveLLM, for the purpose of predicting\npotential defect regimes including Keyholing, Lack of Fusion, and Balling. We\ncompare different methods of input formatting in order to gauge the model's\nperformance to correctly predict defect regimes on our sparse Baseline dataset\nand our natural language Prompt dataset. The model displays robust predictive\ncapability, achieving an accuracy of 93\\% when asked to provide the defect\nregimes associated with a set of process parameters. The incorporation of\nnatural language input further simplifies the task of process parameters\nselection, enabling users to identify optimal settings specific to their build.",
        "We study data poisoning attacks in learning from human preferences. More\nspecifically, we consider the problem of teaching\/enforcing a target policy\n$\\pi^\\dagger$ by synthesizing preference data. We seek to understand the\nsusceptibility of different preference-based learning paradigms to poisoned\npreference data by analyzing the number of samples required by the attacker to\nenforce $\\pi^\\dagger$. We first propose a general data poisoning formulation in\nlearning from human preferences and then study it for two popular paradigms,\nnamely: (a) reinforcement learning from human feedback (RLHF) that operates by\nlearning a reward model using preferences; (b) direct preference optimization\n(DPO) that directly optimizes policy using preferences. We conduct a\ntheoretical analysis of the effectiveness of data poisoning in a setting where\nthe attacker is allowed to augment a pre-existing dataset and also study its\nspecial case where the attacker can synthesize the entire preference dataset\nfrom scratch. As our main results, we provide lower\/upper bounds on the number\nof samples required to enforce $\\pi^\\dagger$. Finally, we discuss the\nimplications of our results in terms of the susceptibility of these learning\nparadigms under such data poisoning attacks.",
        "Coreset Selection (CS) identifies a subset of training data that achieves\nmodel performance comparable to using the entire dataset. Many state-of-the-art\nCS methods, select coresets using scores whose computation requires training\nthe downstream model on the entire dataset and recording changes in its\nbehavior on samples as it trains (training dynamics). These scores are\ninefficient to compute and hard to interpret as they do not indicate whether a\nsample is difficult to learn in general or only for a specific model. Our work\naddresses these challenges by proposing an interpretable score that gauges a\nsample's difficulty using human-understandable textual attributes (concepts)\nindependent of any downstream model. Specifically, we measure the alignment\nbetween a sample's visual features and concept bottlenecks, derived via large\nlanguage models, by training a linear concept bottleneck layer and compute the\nsample's difficulty score using it. We then use this score and a stratified\nsampling strategy to identify the coreset. Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10,\nCIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets,\neven at high pruning rates, and achieve model performance comparable to or\nbetter than coresets found by training dynamics-based methods.",
        "Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS\/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS\/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS\/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.",
        "We present GreenAuto, an end-to-end automated platform designed for\nsustainable AI model exploration, generation, deployment, and evaluation.\nGreenAuto employs a Pareto front-based search method within an expanded neural\narchitecture search (NAS) space, guided by gradient descent to optimize model\nexploration. Pre-trained kernel-level energy predictors estimate energy\nconsumption across all models, providing a global view that directs the search\ntoward more sustainable solutions. By automating performance measurements and\niteratively refining the search process, GreenAuto demonstrates the efficient\nidentification of sustainable AI models without the need for human\nintervention.",
        "We propose the joint graph attention neural network (GAT), clustering with\nadaptive neighbors (CAN) and probabilistic graphical model for dynamic power\nflow analysis and fault characteristics. In fact, computational efficiency is\nthe main focus to enhance, whilst we ensure the performance accuracy at the\naccepted level. Note that Machine Learning (ML) based schemes have a\nrequirement of sufficient labeled data during training, which is not easily\nsatisfied in practical applications. Also, there are unknown data due to new\narrived measurements or incompatible smart devices in complex smart grid\nsystems. These problems would be resolved by our proposed GAT based framework,\nwhich models the label dependency between the network data and learns object\nrepresentations such that it could achieve the semi-supervised fault diagnosis.\nTo create the joint label dependency, we develop the graph construction from\nthe raw acquired signals by using CAN. Next, we develop the probabilistic\ngraphical model of Markov random field for graph representation, which supports\nfor the GAT based framework. We then evaluate the proposed framework in the\nuse-case application in smart grid and make a fair comparison to the existing\nmethods.",
        "While the capabilities of generative foundational models have advanced\nrapidly in recent years, methods to prevent harmful and unsafe behaviors remain\nunderdeveloped. Among the pressing challenges in AI safety, machine unlearning\n(MU) has become increasingly critical to meet upcoming safety regulations. Most\nexisting MU approaches focus on altering the most significant parameters of the\nmodel. However, these methods often require fine-tuning substantial portions of\nthe model, resulting in high computational costs and training instabilities,\nwhich are typically mitigated by access to the original training dataset.\n  In this work, we address these limitations by leveraging Singular Value\nDecomposition (SVD) to create a compact, low-dimensional projection that\nenables the selective forgetting of specific data points. We propose Singular\nValue Decomposition for Efficient Machine Unlearning (SEMU), a novel approach\ndesigned to optimize MU in two key aspects. First, SEMU minimizes the number of\nmodel parameters that need to be modified, effectively removing unwanted\nknowledge while making only minimal changes to the model's weights. Second,\nSEMU eliminates the dependency on the original training dataset, preserving the\nmodel's previously acquired knowledge without additional data requirements.\n  Extensive experiments demonstrate that SEMU achieves competitive performance\nwhile significantly improving efficiency in terms of both data usage and the\nnumber of modified parameters.",
        "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down\/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
        "Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.",
        "Atmospheric neutrinos, through their weak interactions, can serve as an\nindependent tool for exploring the internal structure of Earth. The information\nobtained would be complementary to that provided by seismic and gravitational\nmeasurements. The Earth matter effects in neutrino oscillations depend upon the\nenergy of neutrinos and the electron density distribution that they encounter\nduring their journey through Earth, and hence, can be used to probe the inner\nstructure of Earth. In this contribution, we demonstrate how well an\natmospheric neutrino experiment, such as an iron calorimeter detector (ICAL),\nwould simultaneously constrain the density jumps inside Earth and determine the\nlocation of the core-mantle boundary. In this work, we employ a five-layered\ndensity model of Earth, where the layer densities and core radius are modified\nto explore the parameter space, ensuring that the mass and moment of inertia of\nEarth remain constant while satisfying the hydrostatic equilibrium condition.\nWe further demonstrate that the charge identification capability of an\nICAL-like detector would play a crucial role in obtaining these correlated\nconstraints.",
        "Following a method introduced by Thomas-Vasquez and developed by Grundman, we\nprove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are\nof general type, and that some are of nonnegative Kodaira dimension. The new\ningredient is a detailed study of the geometry and combinatorics of totally\npositive integral elements $x$ of a fractional ideal $I$ in a totally real\nnumber field $K$ with the property that $\\mathop{\\mathrm{tr}} xy <\n\\mathop{\\mathrm{min}} I \\mathop{\\mathrm{tr}} y$ for some $y \\gg 0 \\in K$.",
        "This paper describes several new problems and ideas concerning algebraic\ngeometry and complexity theory. It first uses the idea of coloring graphs with\nelements of finite fields. This procedure then shows that graph coloring\nproblems can be converted into membership problems for a new family of\nalgebraic varieties, coloring varieties, which are closely related to\ndeterminantal varieties. This in turn shows that the problem of NP vs P can be\nconverted into questions of if certain polynomials of large degree over finite\nfields have low multiplicative complexity.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in tasks such as Retrieval-Augmented Generation (RAG) and\nautonomous AI agent workflows. Yet, when faced with large sets of unstructured\ndocuments requiring progressive exploration, analysis, and synthesis, such as\nconducting literature survey, existing approaches often fall short. We address\nthis challenge -- termed Progressive Document Investigation -- by introducing\nGraphy, an end-to-end platform that automates data modeling, exploration and\nhigh-quality report generation in a user-friendly manner. Graphy comprises an\noffline Scrapper that transforms raw documents into a structured graph of Fact\nand Dimension nodes, and an online Surveyor that enables iterative exploration\nand LLM-driven report generation. We showcase a pre-scrapped graph of over\n50,000 papers -- complete with their references -- demonstrating how Graphy\nfacilitates the literature-survey scenario. The demonstration video can be\nfound at https:\/\/youtu.be\/uM4nzkAdGlM.",
        "Flow-matching models provide a powerful framework for various applications,\noffering efficient sampling and flexible probability path modeling. These\nmodels are characterized by flows with low curvature in learned generative\ntrajectories, which results in reduced truncation error at each sampling step.\nTo further reduce curvature, we propose block matching. This novel approach\nleverages label information to partition the data distribution into blocks and\nmatch them with a prior distribution parameterized using the same label\ninformation, thereby learning straighter flows. We demonstrate that the\nvariance of the prior distribution can control the curvature upper bound of\nforward trajectories in flow-matching models. By designing flexible\nregularization strategies to adjust this variance, we achieve optimal\ngeneration performance, effectively balancing the trade-off between maintaining\ndiversity in generated samples and minimizing numerical solver errors. Our\nresults demonstrate competitive performance with models of the same parameter\nscale.Code is available at \\url{https:\/\/github.com\/wpp13749\/block_flow}.",
        "LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field.",
        "Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at $64 \\times 64$ resolution in\n2-step generation. Our code is available at https:\/\/github.com\/sony\/vct .",
        "We introduce a novel approach to studying properties of processes in the\n{\\pi}-calculus based on a processes-as-formulas interpretation, by establishing\na correspondence between specific sequent calculus derivations and computation\ntrees in the reduction semantics of the recursion-free {\\pi}-calculus. Our\nmethod provides a simple logical characterisation of deadlock-freedom for the\nrecursion- and race-free fragment of the {\\pi}-calculus, supporting key\nfeatures such as cyclic dependencies and an independence of the name\nrestriction and parallel operators. Based on this technique, we establish a\nstrong completeness result for a nontrivial choreographic language: all\ndeadlock-free and race-free finite {\\pi}-calculus processes composed in\nparallel at the top level can be faithfully represented by a choreography. With\nthese results, we show how the paradigm of computation-as-derivation extends\nthe reach of logical methods for the study of concurrency, by bridging\nimportant gaps between logic, the expressiveness of the {\\pi}-calculus, and the\nexpressiveness of choreographic languages.",
        "3D Gaussian Splatting (3DGS) has gained significant attention for their\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.",
        "We propose a machine learning approach to the optimal control of fluid\nrestless multi-armed bandits (FRMABs) with state equations that are either\naffine or quadratic in the state variables. By deriving fundamental properties\nof FRMAB problems, we design an efficient machine learning based algorithm.\nUsing this algorithm, we solve multiple instances with varying initial states\nto generate a comprehensive training set. We then learn a state feedback policy\nusing Optimal Classification Trees with hyperplane splits (OCT-H). We test our\napproach on machine maintenance, epidemic control and fisheries control\nproblems. Our method yields high-quality state feedback policies and achieves a\nspeed-up of up to 26 million times compared to a direct numerical algorithm for\nfluid problems.",
        "In this work, a central-moment-based discrete Boltzmann method (CDBM) is\nconstructed for fluid flows with variable specific heat ratios. The central\nkinetic moments are employed to calculate the equilibrium discrete velocity\ndistribution function in the CDBM. In comparison to previous incompressible\ncentral-moment-based lattice Boltzmann method, the CDBM possesses the\ncapability of investigating compressible flows with thermodynamic\nnonequilibrium effects beyond conventional hydrodynamic models. Unlike all\nexisting DBMs which are constructed in raw-moment space, the CDBM stands out by\ndirectly providing the nonequilibrium effects related to the thermal\nfluctuation. The proposed method has been rigorously validated using benchmarks\nof the Sod shock tube, Lax shock tube, shock wave phenomena, two-dimensional\nsound wave, and the Taylor-Green vortex flow. The numerical results exhibit an\nexceptional agreement with theoretical predictions."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature",
    "start_abstract":"Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The Great Barrier Reef: an environmental history"
      ],
      "abstract":[
        "Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "Foliar Uptake of Biocides: Statistical Assessment of Compartmental and\n  Diffusion-Based Models",
        "Predicting novel pharmacological activities of compounds using PubChem\n  IDs and machine learning (CID-SID ML model)",
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Bowel Incision Closure with a Semi-Automated Robot-Assisted Laser Tissue\n  Soldering System",
        "AI-Driven Hybrid Ecological Model for Predicting Oncolytic Viral Therapy\n  Dynamics",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "An affordable, wearable, fiber-free pulsed-mode diffuse speckle contrast\n  flowmetry (PM-DSCF) sensor for noninvasive measurements of deep cerebral\n  blood flow",
        "Existence of Viscosity Solutions to Abstract Cauchy Problems via\n  Nonlinear Semigroups",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Abundance of spin liquids in the $S=1$ bilinear-biquadratic model on the\n  pyrochlore lattice, and its application to $\\mathrm{NaCaNi}_2\\mathrm{F}_7$",
        "Coherence DeepClean: Toward autonomous denoising of gravitational-wave\n  detector data",
        "On Stein spaces with finite homotopy rank-sum",
        "Distribution amplitudes of heavy-light pseudo-scalar and vector mesons\n  from Dyson-Schwinger equations framework",
        "Effects of particle angularity on granular self-organization",
        "Random matrices acting on sets: Independent columns",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "COMPLETED CYCLES LEAKY HURWITZ NUMBERS",
        "Oriented diameter of the complete tripartite graph (III)",
        "Trends and Reversion in Financial Markets on Time Scales from Minutes to\n  Decades",
        "Thermostats without conjugate points",
        "Rethinking Approximate Gaussian Inference in Classification",
        "Dynamically Learning to Integrate in Recurrent Neural Networks"
      ],
      "abstract":[
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "The global population increase leads to a high food demand, and to reach this\ntarget products such as pesticides are needed to protect the crops. Research is\nfocusing on the development of new products that can be less harmful to the\nenvironment, and mathematical models are tools that can help to understand the\nmechanism of uptake of pesticides and then guide in the product development\nphase. This paper applies a systematic methodology to model the foliar uptake\nof pesticides, to take into account the uncertainties in the experimental data\nand in the model structure. A comparison between different models is conducted,\nfocusing on the identifiability of model parameters through dynamic sensitivity\nprofiles and correlation analysis. Lastly, data augmentation studies are\nconducted to exploit the model for the design of experiments and to provide a\npractical support to future experimental campaigns, paving the way for further\napplication of model-based design of experiments techniques in the context of\nfoliar uptake.",
        "Significance and Object: The proposed methodology aims to provide time- and\ncost-effective approach for the early stage in drug discovery. The machine\nlearning models developed in this study used only the identification numbers\nprovided by PubChem. Thus, a drug development researcher who has obtained a\nPubChem CID and SID can easily identify new functionality of their compound.\nThe approach was demonstrated, using four bioassay which were on (i) the\nantagonists of human D3 dopamine receptors; (ii) the promoter Rab9 activators;\n(iii) small molecule inhibitors of CHOP to regulate the unfolded protein\nresponse to ER stress; (iv) antagonists of the human M1 muscarinic receptor.\nSolution: The four bioassays used for demonstration of the approach were\nprovided by PubChem. For each bioassay, the generated by PubChem CIDs, SIDs\nwere extracted together with the corresponding activity. The resulting dataset\nwas sifted with the dataset on a water solubility bioassay, remaining only the\ncompounds common for both bioassays. In this way, the inactive compounds were\nreduced. Then, all active compounds were added, and the resulted dataset was\nlater used for machine learning based on scikit learn algorithms. Results: The\naverage values of the ML models` metrics for the four bioassays were: 83.82%\nAccuracy with 5.35 standard deviation; 87.9% Precision with 5.04 standard\ndeviation; 77.1% Recall with 7.65 standard deviation; 82.1% F1 with 6.44\nstandard deviation; 83.4% ROC with 5.09 standard deviation.",
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "Traditional methods for closing gastrointestinal (GI) surgery incisions, like\nsuturing and stapling, present significant challenges, including potentially\nlife-threatening leaks. These techniques, especially in robot-assisted\nminimally invasive surgery (RAMIS), require advanced manual skills. While their\nrepetitive and time-consuming nature makes them suitable candidates for\nautomation, the automation process is complicated by the need for extensive\ncontact with the tissue. Addressing this, we demonstrate a semi-autonomous\ncontactless surgical procedure using our novel Robot-assisted Laser Tissue\nSoldering (RLTS) system on a live porcine bowel. Towards this in-vivo\ndemonstration, we optimized soldering protocols and system parameters in\nex-vivo experiments on porcine bowels and a porcine cadaver. To assess the RLTS\nsystem performance, we compared the pressure at which the anastomosis leaked\nbetween our robotic soldering and manual suturing. With the best setup, we\nadvanced to an in-vivo Heineke Mikulicz closure on small bowel incision in live\npigs and evaluated their healing for two weeks. All pigs successfully\ncompleting the procedure (N=5) survived without leaks and the histology\nindicated mucosal regeneration and fibrous tissue adhesion. This marks the\nfirst in-vivo semi-automated contactless incision closure, paving the way for\nautomating GI surgery incision closure which has the potential to become an\nalternative to traditional methods.",
        "Oncolytic viral therapy (OVT) is an emerging precision therapy for aggressive\nand recurrent cancers. However, its clinical efficacy is hindered by the\ncomplexity of tumor-virus-immune interactions and the lack of predictive models\nfor personalized treatment. This study develops a data-driven, AI-powered\ncomputational model combining time-delayed Generalized Lotka-Volterra equations\nwith advanced optimization algorithms, including Genetic Algorithms,\nDifferential Evolution, and Reinforcement Learning, to optimize OVT\noscillations' growth and damping. We hypothesize that the model can provide\naccurate, real-time predictions of OVT responses while identifying key\nbiomarkers to enhance therapeutic efficacy. The model demonstrates strong\npredictive accuracy, achieving mean squared error (MSE) < 0.02 and R-squared >\n0.82. It also identifies experimentally validated biomarkers such as TNF, NFkB,\nCD81, TRAF2, IL18, and BID, among other inflammatory cytokines and\nextracellular matrix reconstruction factors, despite being causally agnostic\nand unaware of specific experimental conditions or therapeutic combinations.\nGene set enrichment analysis confirmed these biosignatures as critical\npredictors of tumor progression and indicated that photodynamic therapy\nactivates immune responses similar to those elicited by combined OVT and immune\ncheckpoint inhibitors. This hybrid model represents a significant step toward\nprecision oncology and computational medicine, enabling longitudinal, adaptive\ntreatment regimens and developing targeted immunotherapies based on molecular\nsignatures, potentially improving patient outcomes.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Significance: Measuring cerebral blood flow (CBF) is crucial for diagnosing\nvarious cerebral diseases. An affordable, wearable, and fiber-free\ncontinuous-wave speckle contrast flowmetry (CW-DSCF) technique has been\ndeveloped for continuous monitoring of CBF variations. However, its application\nin adult humans is limited by shallow tissue penetration. Aim: To develop an\ninnovative pulse-mode DSCF (PM-DSCF) system for continuous monitoring of CBF\nvariations in adult humans. Approach: The PM-DSCF utilizes an 808 nm laser\ndiode and a small NanEye camera to capture diffuse laser speckle fluctuations\ncaused by red blood cell movement in the brain (i.e., CBF). Operating in\nshort-pulse mode (duty cycle < 5%), the system maximizes peak pulse light power\nfor deeper tissue penetration, while ensuring that the average power density\nremains within ANSI safety standards for skin exposure. The PM-DSCF was\nevaluated on tissue-simulating phantoms and in adult humans. Results: The\nmaximum effective source-detector distance increased from 15 mm (CW-DSCF) to 35\nmm (PM-DSCF). The PM-DSCF successfully detected CBF variations in adult brains\nduring head-up-tilting experiments, consistent with physiological expectations.\nConclusions: Switching from CW mode to PM mode significantly increases the\nmaximum tissue penetration depth from ~7.5 mm (CW-DSCF) to ~17.5 mm (PM-DSCF),\nenabling successful CBF measurements in adult humans.",
        "In this work, we provide conditions for nonlinear monotone semigroups on\nlocally convex vector lattices to give rise to a generalized notion of\nviscosity solutions to a related nonlinear partial differential equation. The\nsemigroup needs to satisfy a convexity estimate, so called $K$-convexity,\nw.r.t. another family of operators, defined on a potentially larger locally\nconvex vector lattice. We then show that, under mild continuity requirements on\nthe bounding family of operators, the semigroup yields viscosity solutions to\nthe abstract Cauchy problem given in terms of its generator in the larger\nlocally convex vector lattice. We apply our results to drift control problems\nfor infinite-dimensional L\\'evy processes and robust optimal control problems\nfor infinite-dimensional Ornstein-Uhlenbeck processes.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "Long considered the ''poor cousins'' of spin-1\/2 systems, magnets built of\nspin-1 moments have recently come to fore as a rich source of novel phases of\nmatter. Here we explore the phases which arise in a spin-1 magnet on the\npyrochlore lattice, once biquadratic interactions are taken into account. Using\na combination of variational and Monte Carlo techniques, built around the exact\ntreatment of spin-1 at the level of a single site, we uncover seven distinct\nspin liquid phases. Dynamical calculations for one of these spin liquids are\nshown to be in good agreement with inelastic neutron scattering on the spin-1\npyrochlore $\\mathrm{NaCaNi}_2\\mathrm{F}_7$. These results suggest that the\nrange of spin liquid phases found in spin-1 pyrochlores may be even richer than\nin materials with (pseudo-)spin-1\/2 moments.",
        "Technical and environmental noise in ground-based laser interferometers\ndesigned for gravitational-wave observations like Advanced LIGO, Advanced Virgo\nand KAGRA, can manifest as narrow (<1Hz) or broadband ($10'$s or even $100'$s\nof Hz) spectral lines and features in the instruments' strain amplitude\nspectral density. When the sources of this noise cannot be identified or\nremoved, in cases where there are witness sensors sensitive to this noise\nsource, denoising of the gravitational-wave strain channel can be performed in\nsoftware, enabling recovery of instrument sensitivity over affected frequency\nbands. This noise hunting and removal process can be particularly challenging\ndue to the wealth of auxiliary channels monitoring the interferometry and the\nenvironment and the non-linear couplings that may be present. In this work, we\npresent a comprehensive analysis approach and corresponding cyberinfrastructure\nto promptly identify and remove noise in software using machine learning\ntechniques. The approach builds on earlier work (referred to as DeepClean) in\nusing machine learning methods for linear and non-linear regression of noise.\nWe demonstrate how this procedure can be operated and optimized in a tandem\nfashion close to online data taking; it starts off with a coherence monitoring\nanalysis that first singles out and prioritizes witness channels that can then\nbe used by DeepClean. The resulting denoised strain by DeepClean reflects a\n1.4\\% improvement in the binary neutron star range, which can translate into a\n4.3\\% increase in the sensitive volume. This cyber infrastructure we refer to\nas Coherence DeepClean, or CDC, is a significant step toward autonomous\noperations of noise subtraction for ground-based interferometers.",
        "A topological space (not necessarily simply connected) is said to have finite\nhomotopy rank-sum if the sum of the ranks of all higher homotopy groups (from\nthe second homotopy group onward) is finite. In this article, we consider Stein\nspaces of arbitrary dimension satisfying the above rational homotopy theoretic\nproperty, although most of this article focuses on Stein surfaces only. We\ncharacterize all Stein surfaces satisfying the finite homotopy rank-sum\nproperty. In particular, if such a Stein surface is affine and every element of\nits fundamental group is finite, it is either simply connected or has a\nfundamental group of order $2$. A detailed classification of the smooth complex\naffine surfaces of the non-general type satisfying the finite homotopy rank-sum\nproperty is obtained. It turns out that these affine surfaces are\nEilenberg--MacLane spaces whenever the fundamental group is infinite.",
        "We systematically investigate leading-twist distribution amplitudes of ground\nstate heavy-light pseudo-scalar and vector mesons, the results of $B^*$,\n$B^*_s$, $B_c^*$ mesons are reported for the first time within the\nDyson-Schwinger equations framework. A novel numerical method for calculating\nMellin moments is proposed, which can avoid extrapolation or fitting in\nprevious similar studies. Based on it, we calculate the first eight Mellin\nmoments of mesons and reconstruct their distribution amplitudes. It is found\nthat, in flavor-asymmetric systems, distribution amplitude $\\phi(x)$ is skewed\nto one side, with the position of the maximum $\\sim M^f_E\/(M^f_E+M^g_E)$, where\n$M_E$ is Euclidean constituent quark mass and $f\/g$ denote the flavor of\nheavier\/lighter quark in the meson, respectively. For systems with the same\nvalence quark structure, the first Mellin moments follow the relation $\\langle\n\\xi \\rangle_{0^-} < \\langle \\xi \\rangle^{\\|}_{1^-} < \\langle \\xi\n\\rangle^{\\perp}_{1^-}$, where $\\xi = 2x - 1$ and $x$ is the momentum fraction\ncarried by the heavier quark. Our predictions can be compared with experimental\ndata and further theoretical calculations in the future, and the results of\nlight mesons such as $\\pi$, $K$, $\\rho$ are consistent with recent lattice\ndata.",
        "Recent studies of two-dimensional poly-disperse disc systems revealed a\ncoordinated self-organisation of cell stresses and shapes, with certain\ndistributions collapsing onto a master form for many processes, size\ndistributions, friction coefficients, and cell orders. Here we examine the\neffects of grain angularity on the indicators of self-organisation, using\nsimulations of bi-disperse regular $N$-polygons and varying $N$ systematically.\nWe find that: the strong correlation between local cell stresses and\norientations, as well as the collapses of the conditional distributions of\nscaled cell stress ratios to a master Weibull form for all cell orders $k$, are\nindependent of angularity and friction coefficient. In contrast, increasing\nangularity makes the collapses of the conditional distributions sensitive to\nchanges in the friction coefficient.",
        "We study random matrices with independent subgaussian columns. Assuming each\ncolumn has a fixed Euclidean norm, we establish conditions under which such\nmatrices act as near-isometries when restricted to a given subset of their\ndomain. We show that, with high probability, the maximum distortion caused by\nsuch a matrix is proportional to the Gaussian complexity of the subset, scaled\nby the subgaussian norm of the matrix columns. This linear dependence on the\nsubgaussian norm is a new phenomenon, as random matrices with independent rows\nor independent entries typically exhibit superlinear dependence. As a\nconsequence, normalizing the columns of random sparse matrices leads to\nstronger embedding guarantees.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We introduce $(r+1)$-completed cycles $k$-leaky Hurwitz numbers and prove\npiecewise polynomiality as well as establishing their chamber polynomiality\nstructure and their wall crossing formulae. For $k=0$ the results recover\nprevious results of Shadrin-Spitz-Zvonkine. The specialization for $r=1$\nrecovers Hurwitz numbers that are close to the ones studied by\nCavalieri-Markwig-Ranganathan and Cavalieri-Markwig-Schmitt. The ramifications\ndiffer by a lower order torus correction, natural from the Fock space\nperspective, not affecting the genus zero enumeration, nor the enumeration for\nleaky parameter values $k = \\pm 1$ in all genera.",
        "Given a bridgeless graph $G$, let $\\mathbb{D}(G)$ be the set of all strong\norientations of $G$, and define the oriented diameter $f(G)$ of $G$ to be the\nminimum of diameters $diam(D)$ among all the strong orientations $D\\in\n\\mathbb{D}(G)$, i.e., $f(G)=\\min\\{diam(D)\\mid D\\in \\mathbb{D}(G)\\}$. In this\npaper, we determine the oriented diameter of complete tripartite graph\n$K(3,p,q)$ for $p\\geqslant 5$. Combining with the previous results, the\noriented diameter of complete tripartite graph $K(3,p,q)$ are known.",
        "We empirically analyze the reversion of financial market trends with time\nhorizons ranging from minutes to decades. The analysis covers equities,\ninterest rates, currencies and commodities and combines 14 years of futures\ntick data, 30 years of daily futures prices, 330 years of monthly asset prices,\nand yearly financial data since medieval times.\n  Across asset classes, we find that markets are in a trending regime on time\nscales that range from a few hours to a few years, while they are in a\nreversion regime on shorter and longer time scales. In the trending regime,\nweak trends tend to persist, which can be explained by herding behavior of\ninvestors. However, in this regime trends tend to revert before they become\nstrong enough to be statistically significant, which can be interpreted as a\nreturn of asset prices to their intrinsic value. In the reversion regime, we\nfind the opposite pattern: weak trends tend to revert, while those trends that\nbecome statistically significant tend to persist.\n  Our results provide a set of empirical tests of theoretical models of\nfinancial markets. We interpret them in the light of a recently proposed\nlattice gas model, where the lattice represents the social network of traders,\nthe gas molecules represent the shares of financial assets, and efficient\nmarkets correspond to the critical point. If this model is accurate, the\nlattice gas must be near this critical point on time scales from 1 hour to a\nfew days, with a correlation time of a few years.",
        "We generalize Hopf's theorem to thermostats: the total thermostat curvature\nof a thermostat without conjugate points is non-positive, and vanishes only if\nthe thermostat curvature is identically zero. We further show that, if the\nthermostat curvature is zero, then the flow has no conjugate points, and the\nGreen bundles collapse almost everywhere. Given a thermostat without conjugate\npoints, we prove that the Green bundles are transversal everywhere if and only\nif it admits a dominated splitting. Finally, we provide an example showing that\nHopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is\nalso the first example of a thermostat with a dominated splitting which is not\nAnosov.",
        "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https:\/\/github.com\/bmucsanyi\/probit.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Learning Polynomials with Neural Networks",
    "start_abstract":"We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "physics.chem-ph"
      ]
    },
    "list":{
      "title":[
        "The Conundrum of Diffuse Basis Sets: A Blessing for Accuracy yet a Curse\n  for Sparsity",
        "Equation-of-motion Coupled-cluster singles, doubles and(full) triples\n  for doubly ionized and two-electron-attached states: A Computational\n  implementation",
        "Efficient Machine Learning Approach for Yield Prediction in Chemical\n  Reactions",
        "Photokinetics of Photothermal Reactions",
        "Imaging the Photochemistry of Cyclobutanone using Ultrafast Electron\n  Diffraction: Experimental Results",
        "Stochastic resolution of identity to CC2 for large systems: Oscillator\n  strength and ground state gradient calculations",
        "Monolayer-Defined Flat Colloidal PbSe Quantum Dots in Extreme\n  Confinement",
        "Endofullerenes and Dispersion-Corrected Density Functional\n  Approximations: A Cautionary Tale",
        "MetaWFN: A Platform for Unified Implementation of Many-Electron\n  Wavefunctions",
        "Two-dimensional fluorescence spectroscopy with quantum entangled photons\n  and time- and frequency-resolved two-photon coincidence detection",
        "Complex potential energy surfaces: gradients with projected CAP\n  technique",
        "Investigating ferromagnetic response in monolayer CVD grown MoS$_{2}$\n  flakes using quantum weak measurement",
        "Taming the Virtual Space for Incremental Full Configuration Interaction",
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Optimizing compilation of error correction codes for 2xN quantum dot\n  arrays and its NP-hardness",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "Leveraging the Bias-Variance Tradeoff in Quantum Chemistry for Accurate\n  Negative Singlet-Triplet Gap Predictions: A Case for Double-Hybrid DFT",
        "Galaxy mass profiles with convolutional neural networks",
        "Auto-Balancer: Harnessing idle network resources for enhanced market\n  stability",
        "On a planetary forcing of global seismicity",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Chiral supersolid and dissipative time crystal in Rydberg-dressed\n  Bose-Einstein condensates with Raman-induced spin-orbit coupling",
        "Sub-MHz Radio Background from Ultralight Dark Photon Dark Matter",
        "Kolyvagin's conjecture for modular forms at non-ordinary primes",
        "Global $C^{1,\\alpha}$ regularity for Monge-Amp\\`ere equations on planar\n  convex domains",
        "Investigation of Inverse Velocity Dispersion in a Solar Energetic\n  Particle Event Observed by Solar Orbiter",
        "Averaging over the circles the gaussian free field in the Poincar{\\'e}\n  disk"
      ],
      "abstract":[
        "Diffuse atomic orbital basis sets have proven to be essential to obtain\naccurate interaction energies, especially in regard to non-covalent\ninteractions. However, they also have a detrimental impact on the sparsity of\nthe one-particle density matrix (1-PDM), to a degree stronger than the spatial\nextent of the basis functions alone could explain. This is despite the fact\nthat the matrix elements of the 1-PDM of insulators (systems with significant\nHOMO-LUMO gaps) are expected to decay exponentially with increasing real-space\ndistance from the diagonal and the asymptotic decay rate is expected to have a\nwell-defined basis set limit. The observed low sparsity of the 1-PDM appears to\nbe independent of representation and even persists after projecting the 1-PDM\nonto a real-space grid, leading to the conclusion that this \"curse of sparsity\"\nis solely a basis set artifact, which, counterintuitively, becomes worse for\nlarger basis sets, seemingly contradicting the notion of a well-defined basis\nset limit. We show that this is a consequence of the low locality of the\ncontra-variant basis functions as quantified by the inverse overlap matrix\n$\\mathbf{S}^{-1}$ being significantly less sparse than its covariant dual.\nIntroducing the model system of an infinite non-interacting chain of helium\natoms, we are able to quantify the exponential decay rate to be proportional to\nthe diffuseness as well as local incompleteness of the basis set, meaning small\nand diffuse basis sets are affected the most. Finally, we propose one solution\nto the conundrum in the form of the complementary auxiliary basis set (CABS)\nsingles correction in combination with compact, low l-quantum-number basis\nsets, showing promising results for non-covalent interactions.",
        "We present our computational implementation of the equation-of-motion (EOM)\ncoupled-cluster (CC) singles, doubles, and triples (SDT) method for computing\ndoubly ionized (DIP) and two-electron attached (DEA) states within Q-CHEM.\nThese variants have been implemented within both the (conventional) double\nprecision (DP) and the single precision (SP) algorithms and will be available\nin the upcoming major release of {\\sl Q-CHEM}. We present here the programmable\nexpressions and some pilot application of $CH_2$ for DIP and DEA EOM-CCSDT.",
        "Developing machine learning (ML) models for yield prediction of chemical\nreactions has emerged as an important use case scenario in very recent years.\nIn this space, reaction datasets present a range of challenges mostly stemming\nfrom imbalance and sparsity. Herein, we consider chemical language\nrepresentations for reactions to tap into the potential of natural language\nprocessing models such as the ULMFiT (Universal Language Model Fine Tuning) for\nyield prediction, which is customized to work across such distribution\nsettings. We contribute a new reaction dataset with more than 860 manually\ncurated reactions collected from literature spanning over a decade, belonging\nto a family of catalytic meta-C(sp2)-H bond activation reactions of high\ncontemporary importance. Taking cognizance of the dataset size, skewness toward\nthe higher yields, and the sparse distribution characteristics, we developed a\nnew (i) time- and resource-efficient pre-training strategy for downstream\ntransfer learning, and (ii) the CFR (classification followed by regression)\nmodel that offers state-of-the-art yield predictions, surpassing conventional\ndirect regression (DR) approaches. Instead of the prevailing pre-training\npractice of using a large number of unlabeled molecules (1.4 million) from the\nChEMBL dataset, we first created a pre-training dataset SSP1 (0.11 million), by\nusing a substructure-based mining from the PubChem database, which is found to\nbe equally effective and more time-efficient in offering enhanced performance.\nThe CFR model with the ULMFiT-SSP1 regressor achieved an impressive RMSE of\n8.40 for the CFR-major and 6.48 for the CFR-minor class in yield prediction on\nthe title reaction, with a class boundary of yield at 53 %. Furthermore, the\nCFR model is highly generalizable as evidenced by the significant improvement\nover the previous benchmark reaction datasets.",
        "Photothermal reactions, involving both photochemical and thermal\nreaction-steps, are the most abundant sequences in photochemistry. The\nderivation of their rate-laws is standardized, but the integration of these\nrate-laws has not yet been achieved. Indeed, the field still lacks integrated\nrate-laws for the description of these reactions behavior, and\/or\nidentification of their reaction-order. This made a comprehensive account of\nthe photo-kinetics of photothermal reactions to be a gap in the knowledge. This\ngap is addressed in the present paper by introducing an unprecedented general\nmodel equation capable to mapping out the kinetic traces of such reactions when\nexposed to light or in the dark. The integrated rate-law model equation also\napplies when the reactive medium is exposed to either monochromatic or\npolychromatic light irradiation. The validity of the model equation was\nestablished against simulated data obtained by a fourth-order Runge-Kutta\nmethod. It was then used to describe and quantify several situations of\nphotothermal reactions, such as the effects of initial concentration, spectator\nmolecules, and incident radiation intensity, and the impact of the latter on\nthe photonic yield. The model equation facilitated a general elucidation method\nto determine the intrinsic reaction parameters (quantum yields and\nabsorptivities of the reactive species) for any photothermal mechanism whose\nnumber of species are known. This paper contributes to rationalizing\nphoto-kinetics along the same general guidelines adopted in chemical kinetics.",
        "We investigated the ultrafast structural dynamics of cyclobutanone following\nphotoexcitation at $\\lambda=200$ nm using gas-phase megaelectronvolt ultrafast\nelectron diffraction. Our investigation complements the simulation studies of\nthe same process within this special issue. It provides information about both\nelectronic state population and structural dynamics through well-separable\ninelastic and elastic electron scattering signatures. We observe the\ndepopulation of the photoexcited S$_2$ state of cyclobutanone with n3s Rydberg\ncharacter through its inelastic electron scattering signature with a time\nconstant of $(0.29 \\pm 0.2)$ ps towards the S$_1$ state. The S$_1$ state\npopulation undergoes ring-opening via a Norrish Type-I reaction, likely while\npassing through a conical intersection with S$_0$. The corresponding structural\nchanges can be tracked by elastic electron scattering signatures. These changes\nappear with a delay of $(0.14 \\pm 0.05)$ ps with respect the initial\nphotoexcitation, which is less than the S$_2$ depopulation time constant. This\nbehavior provides evidence for the ballistic nature of the ring-opening once\nthe S$_1$ state is reached. The resulting biradical species react further\nwithin $(1.2 \\pm 0.2)$ ps via two rival fragmentation channels yielding ketene\nand ethylene, or propene and carbon monoxide. Our study showcases both the\nvalue of gas-phase ultrafast diffraction studies as an experimental benchmark\nfor nonadiabatic dynamics simulation methods and the limits in the\ninterpretation of such experimental data without comparison to such\nsimulations.",
        "An implementation of stochastic resolution of identity (sRI) approximation to\nCC2 oscillator strengths as well as ground state analytical gradients is\npresented. The essential 4-index electron repulsion integrals (ERIs) are\ncontracted with a set of stochastic orbitals on the basis of the RI technique\nand the orbital energy differences in the denominators are decoupled with the\nLaplace transform. These lead to a significant scaling reduction from O(N^5) to\nO(N^3) for oscillator strengths and gradients with the size of the basis set,\nN. The gradients need a large number of stochastic orbitals with O(N^3), so we\nprovide an additional O(N^4) version with better accuracy and smaller prefactor\nby adopting sRI partially. Such steep computational acceleration of nearly two\nor one order of magnitude is very attractive for large systems. This work is an\nextension to our previous implementations of sRI-CC2 ground and excited state\nenergies and shows the feasibility of introducing sRI to CC2 properties beyond\nenergies.",
        "Colloidal two-dimensional lead chalcogenide nanocrystals represent an\nintriguing new class of materials that push the boundaries of quantum\nconfinement by combining a crystal thickness down to the monolayer with\nconfinement in the lateral dimension. In particular flat PbSe quantum dots\nexhibit efficient telecommunication band-friendly photoluminescence (1.43 -\n0.83 eV with up to 61% quantum yield) that is highly interesting for\nfiber-optics information processing. By using cryogenic scanning tunneling\nmicroscopy and spectroscopy, we probe distinct single layer-defined PbSe\nquantum dot populations down to a monolayer with in-gap state free quantum\ndot-like density of states, in agreement with theoretical tight binding\ncalculations. Cryogenic ensemble photoluminescence spectra reveal mono-, bi-,\nand trilayer contribution, confirming the structural, electronic and\ntheoretical results. From larger timescale shifts and ratio changes in the\noptical spectra we infer Ostwald ripening in solution and fusing in deposited\nsamples of thinner flat PbSe quantum dots, which can be slowed down by surface\npassivation with PbI2. By uncovering the interplay between thickness, lateral\nsize and density of states, as well as the synthetic conditions and\npost-synthetic handling, our findings enable the target-oriented synthesis of\ntwo-dimensional PbSe quantum dots with precisely tailored optical properties at\ntelecom wavelengths.",
        "A recent study by Panchagnula et al. [J. Chem. Phys. 161, 054308 (2024)]\nillustrated the non-concordance of a variety of electronic structure methods at\ndescribing the symmetric double-well potential expected along the anisotropic\ndirection of the endofullerene Ne@C$_{70}$. In this article we delve deeper\ninto the difficulties of accurately capturing the dispersion interaction within\nthis system, scrutinising a variety of state-of-the-art density-functional\napproximations (DFAs) and dispersion corrections (DCs). We identify rigorous\ncriteria for the double-well potential and compare the shapes, barrier heights,\nand minima positions obtained with the DFAs and DCs to the correlated\nwavefunction data in the previous study, alongside new coupled-cluster\ncalculations. We show that many of the DFAs are extremely sensitive to the\nnumerical integration grid used, and note that the choice of DC is not\nindependent of the DFA. Functionals with many empirical parameters tuned for\nmain-group thermochemistry do not necessarily result in a reasonable PES, while\nimproved performance can be obtained using nearly dispersionless DFAs with very\nfew empirical parameters and allowing the DC to compensate. We pose the\nNe@C$_{70}$ system as a challenge to functional developers and as a diagnostic\nsystem for testing dispersion corrections, and reiterate the need for more\nexperimental data for comparison.",
        "\\texttt{MetaWFN} is a C++ template-based architecture designed for flexible\nand rapid development of wavefunction-based quantum chemical methods. It is\nhighly modular, extendable, and efficient. This is achieved by decoupling the\nthree distinct aspects of quantum chemical methods\n  (i.e., nature of Hamiltonian, structure of wavefunction, and strategy of\nparallelization ), thereby allowing for separate treatment of them through\ntheir internal type-trait and tagging systems furnished by C++ metaprogramming.\nOnce the second-quantized Hamiltonians, whether nonrelativistic (spin-free) or\nrelativistic (spin-dependent), are decomposed into topologically equivalent\ndiagrams for a unified evaluation of the basic coupling coefficients between\n(randomly selected) spin-free or spin-dependent configuration state functions\nor Slater determinants incorporating full molecular symmetry (including single\nor double point group and spin or time reversal symmetry), the many-electron\nwavefunctions, whether built up with scalar or spinor orbitals, can be\nassembled with the same templates. As for parallelization, \\texttt{MetaWFN}\nsupports both OpenMP and MPI, with the majority of the latter being translated\nautomatically from its OpenMP counterparts. The whole structure of\n\\texttt{MetaWFN} is reviewed here, with some showcases for illustrating its\nperformance.",
        "Recent theoretical studies in quantum spectroscopy have emphasized the\npotential of non-classical correlations in entangled photon pairs for\nselectively targeting specific nonlinear optical processes in nonlinear optical\nresponses. However, because of the extremely low intensity of the nonlinear\noptical signal generated by irradiating molecules with entangled photon pairs,\ntime-resolved spectroscopic measurements using entangled photons have yet to be\nexperimentally implemented. In this paper, we theoretically propose a quantum\nspectroscopy measurement employing a time-resolved fluorescence approach that\naligns with the capabilities of current photon detection technologies. The\nproposed quantum spectroscopy affords two remarkable advantages over\nconventional two-dimensional electronic spectroscopy. First, it enables the\nacquisition of two-dimensional spectra without requiring control over multiple\npulsed lasers. Second, it reduces the complexity of the spectra because the\nspectroscopic signal is contingent upon the nonlinear optical process of\nspontaneous emission. These advantages are similar to those achieved in a\nprevious study [Fujihashi et al., J. Chem. Phys. 160, 104201 (2024)]. However,\nour approach achieves sufficient signal intensities that can be readily\ndetected using existing photon detection technologies, thereby rendering it a\npracticable. Our findings will potentially facilitate the first experimental\nreal-time observation of dynamic processes in molecular systems using quantum\nentangled photon pairs.",
        "The complex absorbing potential (CAP) technique is one of the commonly used\nNon-Hermitian quantum mechanics approaches for characterizing electronic\nresonances. CAP combined with various electronic structure methods has shown\npromising results in quantifying the energies and widths of electronic\nresonances in molecular systems. While CAP-based methods can be used to map\ncomplex potential energy surfaces for resonance states, efficient exploration\nof these surfaces, e.g. geometry optimization or dynamical simulations, require\ninformation on the nuclear gradient. Currently, the only nuclear gradients\navailable for CAP-based methods are for Hartree-Fock and Equation-of-Motion\nCoupled-Cluster method with single and double excitations (J. Chem. Phys. 146,\n031101 (2017)). Here we provide a general approach that relies on projected CAP\nformulation and extends gradients and non-adiabatic couplings formulations\ndeveloped for bound-state electronic structure methods to resonances. The\napproach is not limited to a specific electron structure method and is\ngenerally applicable to any electronic structure methods, provided the\ninformation on the gradients and non-adiabatic couplings is available for bound\nstates. Here, we focus on the State-Averaged Complete Active Space\nSelf-Consistent Field (SA-CASSCF) and Multi-Reference Configurational\nInteraction with Single excitation (MR-CIS) as our methods of choice. We\nestablish the accuracy of the developed gradients and report equilibrium\ngeometries for several representative temporary anion species\n($\\mathrm{N_2^-}$, $\\mathrm{H_2CO^-}$, $\\mathrm{H_2CO_2^-}$ and\n$\\mathrm{C_2H_4^-}$).",
        "We synthesize MoS$_{2}$ atomic layer flakes at different growth conditions to\ntailor S-terminated and Mo-terminated edge defect states that are investigated\nfor their ferromagnetic response. We leverage quantum weak measurement\nprinciples to construct a spin Hall effect of light-based magneto-optic Kerr\neffect (SHEL-MOKE) setup to sense the ultra-small magnetic response from the\nsynthesized atomic layers. Our findings demonstrate that Mo-terminated edge\nstates are the primary source of ferromagnetic response from MoS$_{2}$ flakes,\nwhich is consistent with X-ray photoelectron, Raman and photoluminescence\nspectroscopic results. In the process, we demonstrate SHEL-MOKE to be a robust\ntechnique to investigate ultra weak properties in novel atomic-scale materials.",
        "Incremental full configuration interaction (iFCI) closely approximates the\nFCI limit with polynomial cost through a many-body expansion of the correlation\nenergy, providing highly accurate total energies within a given basis set. To\nextend iFCI beyond previous basis set limitations, this work introduces a novel\nnatural orbital screening approach, iNO-FCI. By consideration of the importance\nof virtual orbital selection in the convergence of iFCI, iNO-FCI maximizes the\nconsistency between orbitals selected for each correlated body. iNO-FCI employs\na principle of cancellation of errors and ensures that the same set of virtual\nNOs are used for interdependent terms. This strategy significantly reduces\ncomputational cost without compromises in precision. Computational savings of\nup to 95 percent are demonstrated, allowing access to larger basis sets that\nwere previously computationally prohibitive. iNO-FCI is herein introduced and\nbenchmarked for several difficult test cases involving double-bond\ndissociation, biradical systems, conjugated $\\pi$ systems, and the spin gap of\na Cu-based transition metal complex.",
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "The ability to physically move qubits within a register allows the design of\nhardware-specific error correction codes which can achieve fault-tolerance\nwhile respecting other constraints. In particular, recent advancements have\ndemonstrated the shuttling of electron and hole spin qubits through a quantum\ndot array with high fidelity. Exploiting this, we design an error correction\narchitecture, consisting merely of two parallel quantum dot arrays, an\nexperimentally validated architecture compatible with classical wiring and\ncontrol constraints. We develop a suite of heuristic methods for compiling any\nstabilizer error-correcting code's syndrome-extraction circuit to run with a\nminimal number of shuttling operations. In simulation, these heuristics show\nthat fault tolerance can be achieved on several contemporary quantum\nerror-correcting codes requiring only modestly-optimistic noise parameters.\nFurthermore, we demonstrate how constant column-weight qLDPC codes can be\ncompiled in a provably minimal number of shuttles that scales constantly with\ncode size using Shor-style syndrome extraction. In addition, we provide a proof\nof the NP hardness of minimizing the number of shuttle operations for codes not\nin that class.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Molecules that violate Hund's rule -- having first excited singlet state\n(S$_1$) below the triplet state (T$_1$) -- are rare yet promising as efficient\nlight emitters. Their high-throughput identification demands exceptionally\naccurate excited-state modeling to minimize false positives and negatives.\nBenchmarking twelve S$_1$-T$_1$ energy gaps, we find that local variants of\nADC(2) and CC2 deliver excellent accuracy and speed for screening medium-sized\nmolecules. Notably, while double-hybrid DFT approximations (e.g., B2GP-PLYP and\nPBE-QIDH) exhibit high mean errors ($>100$ meV) despite very low standard\ndeviations ($\\approx10$ meV), exploring their parameter space reveals that a\nconfiguration with 75% exchange and 55% correlation reduces the mean error to\nbelow $5$ meV -- albeit with increased variance. Using this low-bias\nparameterization as an internal reference, we correct the systematic error\nwhile maintaining low variance, effectively combining the strengths of both\nlow-bias and low-variance DFT parameterizations to enhance overall accuracy.\nOur findings suggest that low-variance DFT methods -- often overlooked due to\nhigh bias -- can serve as reliable tools for predictive modeling in\nfirst-principles molecular design.",
        "Determining the dynamical mass profiles of dispersion-supported galaxies is\nparticularly challenging due to projection effects and the unknown shape of\ntheir velocity anisotropy profile. Our goal is to develop a machine learning\nalgorithm capable of recovering dynamical mass profiles of dispersion-supported\ngalaxies from line-of-sight stellar data. Traditionally, this task relies on\ntime-consuming methods that require profile parameterization and assume\ndynamical equilibrium and spherical symmetry. We train a convolutional neural\nnetwork model using various sets of cosmological hydrodynamical simulations of\ngalaxies. By extracting projected stellar data from the simulated galaxies and\nfeeding it into the model, we obtain the posterior distribution of the\ndynamical mass profile at ten different radii. Additionally, we evaluate the\nperformance of existing literature mass estimators on our dataset. Our model\nachieves more accurate results than any literature mass estimator while also\nproviding enclosed mass estimates at radii where no previous estimators exist.\nWe confirm that the posterior distributions produced by the model are\nwell-calibrated, ensuring they provide meaningful uncertainties. However,\nissues remain, as the method loses performance when trained on one set of\nsimulations and applied to another, highlighting the importance of improving\nthe generalization of ML methods trained on specific galaxy simulations.",
        "We propose a mechanism embedded into the foundational infrastructure of a\nblockchain network, designed to improve the utility of idle network resources,\nwhilst enhancing market microstructure efficiency during block production by\nleveraging both network-owned and external capital. By systematically seeking\nto use idle network resources for internally capture arbitrageable\ninefficiencies, the mechanism mitigates extractable value leakage, reduces\nexecution frictions, and improves price formation across venues. This framework\noptimises resource allocation by incentivising an ordered set of transactions\nto be identified and automatically executed at the end of each block,\nredirecting any realised arbitrage income - to marketplaces operating on the\nhost blockchain network (and other stakeholders), which may have otherwise been\nextracted as rent by external actors. Crucially, this process operates without\nintroducing additional inventory risk, ensuring that the network remains a\nneutral facilitator of price discovery. While the systematic framework\ngoverning the distribution of these internally captured returns is beyond the\nscope of this work, reinvesting them to support the ecosystem deployed on the\nhost blockchain network is envisioned to endogenously enhance liquidity,\nstrengthen transactional efficiency, and promote the organic adoption of the\nblockchain for end users. This mechanism is designed specifically for Supra's\nblockchain and seeks to maximally utilise its highly efficient automation\nframework to enhance the blockchain network's efficiency.",
        "We have explored the temporal variability of the seismicity at global scale\nover the last 124 years, as well as its potential drivers. To achieve this, we\nconstructed and analyzed an averaged global seismicity curve for earthquakes of\nmagnitude equal or greater than 6 since 1900. Using Singular Spectrum Analysis,\nwe decomposed this curve and compared the extracted pseudo-cycles with two\nglobal geophysical parameters associated with Earth's tides: length-of-day\nvariations and sea-level changes. Our results reveal that these three\ngeophysical phenomena can be be explained with 90% accuracy, as the sum of up\nto seven periodic components, largely aligned with planetary ephemerides: 1\nyear, 3.4 years (Quasi-Biennial Oscillation, QBO), $\\sim$11 years, $\\sim$14\nyears, $\\sim$18.6 years (lunar nodal cycle), $\\sim$33 years, and $\\sim$60\nyears. We discuss these results in the framework of Laplace's theory, with a\nparticular focus on the phase relationships between seismicity, length-of-day\nvariations, and sea-level changes to further elucidate the underlying physical\nmechanisms. Finally,integrating observations from seismogenic regions, we\npropose a trigger mechanism based on solid Earth-hydrosphere interactions,\nemphasizing the key role of water-rock interactions in modulating earthquake\noccurrence.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "Spin-orbit coupling (SOC) is one of the key factors that affect the chiral\nsymmetry of matter by causing the spatial symmetry breaking of the system. We\nfind that Raman-induced SOC can induce a chiral supersolid phase with a helical\nantiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein\ncondensates (BECs) in a harmonic trap by modulating the Raman coupling\nstrength, strong contrast with the mirror symmetric supersolid phase containing\nskyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state\nphase diagrams are presented as a function of the Rydberg interaction strength\nand the SOC strength, as well as that of the Rydberg interaction strength and\nthe Raman coupling strength, respectively. It is shown that the interplay among\nRaman-induced SOC, soft-core long-range Rydberg interactions, and contact\ninteractions favors rich ground-state structures including half-quantum vortex\nphase, stripe supersolid phase, toroidal stripe phase with a central\nAnderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror\nsymmetric supersolid phase, chiral supersolid phase and standing-wave\nsupersolid phase. In addition, the effects of rotation and in-plane quadrupole\nmagnetic field on the ground state of the system are analyzed. In these two\ncases, the chiral supersolid phase is broken and the ground state tends to form\na miscible phase. Furthermore, the stability and superfluid properties of the\ntwo-component BECs with Raman-induced SOC and Rydberg interactions in free\nspace are revealed by solving the Bogoliubov-de Gennes equation. Finally, we\ndemonstrate that when the initial state is a chiral supersolid phase the\nrotating harmonic trapped system sustains dissipative continuous time crystal\nby studying the rotational dynamic behaviors of the system.",
        "Dark photons are a well-motivated candidate for dark matter, but their\ndetection becomes challenging for ultralight masses with both experimental and\nastrophysical probes. In this work, we propose a new approach to explore this\nregime through the dark inverse Compton scattering of ultralight dark photons\nwith cosmic ray electrons and positrons. We show this process generates a\npotentially observable background radiation that is most prominent at\nfrequencies below MHz. We compute this effect using the latest cosmic ray\nmodels and radio absorption maps. Comparing it to observations of the Milky\nWay's radio spectrum from Explorer 43, Radio Astronomy Explorer 2, and the\nParker Solar Probe, we place leading constraints on the kinetic mixing of dark\nphoton dark matter for masses $\\lesssim 2 \\times 10^{-17} \\ \\rm eV$.",
        "In this article we prove a version of Kolyvagin's conjecture for modular\nforms at non-ordinary primes. In particular, we generalize the work of Wang on\na converse to a higher weight Gross-Zagier-Kolyvagin theorem in order to prove\nthe conjecture under the hypothesis that some Selmer group has rank one. The\nmain ingredients that we use in non-ordinary setting are the signed Selmer\ngroups introduced by Lei, Loeffler and Zerbes. We will also use a result of\nWan, i.e., the $p$-part of the Tamagawa number conjecture for non-ordinary\nmodular forms with analytic rank zero. Starting from the rank one case we will\nshow how to prove the full version of the conjecture.",
        "In this paper, we establish the global H\\\"older gradient estimate for\nsolutions to the Dirichlet problem of the Monge-Amp\\`ere equation $\\det D^2u =\nf$ on strictly convex but not uniformly convex domain $\\Omega$.",
        "Inverse velocity dispersion (IVD) events, characterized by higher-energy\nparticles arriving later than lower-energy particles, challenge the classical\nunderstanding of SEP events and are increasingly observed by spacecraft, such\nas Parker Solar Probe (PSP) and Solar Orbiter (SolO). However, the mechanisms\nunderlying IVD events remain poorly understood. This study aims to investigate\nthe physical processes responsible for long-duration IVD events by analyzing\nthe SEP event observed by SolO on 2022 June 7. We explore the role of evolving\nshock connectivity, particle acceleration at interplanetary (IP) shocks, and\ncross-field transport in shaping the observed particle profiles.We utilize data\nfrom Energetic Particle Detector (EPD) suite onboard SolO to analyze the\ncharacteristics of the IVD, and model the event using the Heliospheric\nEnergetic Particle Acceleration and Transport (HEPAT) model. The IVD event\nexhibited a distinct and long-duration IVD signature, across proton energies\nfrom 1 to 20 MeV and lasting for approximately 10 hours. Simulations suggest\nthat evolving shock connectivity and the evolution of shock play a primary role\nin the IVD signature, with SolO transitioning from shock flank to nose over\ntime, resulting in a gradual increase in maximum particle energy along the\nfield line. Furthermore, model results show that limited cross-field diffusion\ncan influence both the nose energy and the duration of the IVD event. This\nstudy demonstrates that long-duration IVD events are primarily driven by\nevolving magnetic connectivity along a non-uniform shock that evolves over\ntime, where the connection moves to more efficient acceleration sites as the\nshock propagates farther from the Sun. Other mechanisms, such as acceleration\ntime at the shock, may also contribute to the observed IVD features.",
        "The gaussian free field on the unit disk $D$ can be seen as a two-dimensional\nversion of the Brownian bridge on the interval [0,1]. It is intrinsically\nassociated with the Sobolev space $H_0^1 (D)$. To define the latter, we can\nchoose any metric conformally equivalent to the Euclidean metric on $D$. This\nnote is an introduction to the gaussian free field on the unit disk whose aim\nis to highlight some of the conveniences offered by hyperbolic geometryon $D$\nto describe the first properties of this probabilistic object."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "physics.chem-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Learning Polynomials with Neural Networks"
      ],
      "abstract":[
        "We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Memorize or Generalize? Evaluating LLM Code Generation with Evolved\n  Questions",
        "Generative AI in Education: From Foundational Insights to the Socratic\n  Playground for Learning",
        "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph\n  Retrieval for LLM Reasoning",
        "Make Full Use of Testing Information: An Integrated Accelerated Testing\n  and Evaluation Method for Autonomous Driving Systems",
        "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
        "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Parallel Belief Contraction via Order Aggregation",
        "Palatable Conceptions of Disembodied Being: Terra Incognita in the Space\n  of Possible Minds",
        "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion\n  Probabilistic Model",
        "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Governing AI Agents",
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression",
        "Activation Steering in Neural Theorem Provers",
        "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement\n  Learning",
        "Spectral Unmixing Comparison with Sparse, Iterative and Mixed Integer\n  Programming Models",
        "Convergence analysis for a variant of manifold proximal point algorithm\n  based on Kurdyka-{\\L}ojasiewicz property",
        "Structure-Aware Correspondence Learning for Relative Pose Estimation",
        "Fixed point results for single and multi-valued three-points\n  contractions",
        "Data-driven Control of T-Product-based Dynamical Systems",
        "Exploring the Effects of Level of Control in the Initialization of\n  Shared Whiteboarding Sessions in Collaborative Augmented Reality",
        "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player\n  Evaluation System",
        "Incremental Approximate Single-Source Shortest Paths with Predictions",
        "How does the restriction of representations change under translations? A\n  story for the general linear groups and the unitary groups",
        "Understanding SGD with Exponential Moving Average: A Case Study in\n  Linear Regression",
        "Modeling of stochastic processes in $L_p(T)$ using orthogonal\n  polynomials"
      ],
      "abstract":[
        "Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.",
        "This paper explores the synergy between human cognition and Large Language\nModels (LLMs), highlighting how generative AI can drive personalized learning\nat scale. We discuss parallels between LLMs and human cognition, emphasizing\nboth the promise and new perspectives on integrating AI systems into education.\nAfter examining challenges in aligning technology with pedagogy, we review\nAutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its\nsuccesses, limitations, and unfulfilled aspirations. We then introduce the\nSocratic Playground, a next-generation ITS that uses advanced transformer-based\nmodels to overcome AutoTutor's constraints and provide personalized, adaptive\ntutoring. To illustrate its evolving capabilities, we present a JSON-based\ntutoring prompt that systematically guides learner reflection while tracking\nmisconceptions. Throughout, we underscore the importance of placing pedagogy at\nthe forefront, ensuring that technology's power is harnessed to enhance\nteaching and learning rather than overshadow it.",
        "Recent large language model (LLM) reasoning, despite its success, suffers\nfrom limited domain knowledge, susceptibility to hallucinations, and\nconstrained reasoning depth, particularly in small-scale models deployed in\nresource-constrained environments. This paper presents the first investigation\ninto integrating step-wise knowledge graph retrieval with step-wise reasoning\nto address these challenges, introducing a novel paradigm termed as\ngraph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to\nretrieve and process relevant mathematical knowledge in a step-wise manner,\nenhancing their problem-solving abilities without additional training. To this\nend, we propose KG-RAR, a framework centered on process-oriented knowledge\ngraph construction, a hierarchical retrieval strategy, and a universal\npost-retrieval processing and reward model (PRP-RM) that refines retrieved\ninformation and evaluates each reasoning step. Experiments on the Math500 and\nGSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging\nresults, achieving a 20.73\\% relative improvement with Llama-3B on Math500.",
        "Testing and evaluation is an important step before the large-scale\napplication of the autonomous driving systems (ADSs). Based on the three level\nof scenario abstraction theory, a testing can be performed within a logical\nscenario, followed by an evaluation stage which is inputted with the testing\nresults of each concrete scenario generated from the logical parameter space.\nDuring the above process, abundant testing information is produced which is\nbeneficial for comprehensive and accurate evaluations. To make full use of\ntesting information, this paper proposes an Integrated accelerated Testing and\nEvaluation Method (ITEM). Based on a Monte Carlo Tree Search (MCTS) paradigm\nand a dual surrogates testing framework proposed in our previous work, this\npaper applies the intermediate information (i.e., the tree structure, including\nthe affiliation of each historical sampled point with the subspaces and the\nparent-child relationship between subspaces) generated during the testing stage\ninto the evaluation stage to achieve accurate hazardous domain identification.\nMoreover, to better serve this purpose, the UCB calculation method is improved\nto allow the search algorithm to focus more on the hazardous domain boundaries.\nFurther, a stopping condition is constructed based on the convergence of the\nsearch algorithm. Ablation and comparative experiments are then conducted to\nverify the effectiveness of the improvements and the superiority of the\nproposed method. The experimental results show that ITEM could well identify\nthe hazardous domains in both low- and high-dimensional cases, regardless of\nthe shape of the hazardous domains, indicating its generality and potential for\nthe safety evaluation of ADSs.",
        "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
        "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps:\/\/github.com\/ADaM-BJTU\/AutoCoA",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "The standard ``serial'' (aka ``singleton'') model of belief contraction\nmodels the manner in which an agent's corpus of beliefs responds to the removal\nof a single item of information. One salient extension of this model introduces\nthe idea of ``parallel'' (aka ``package'' or ``multiple'') change, in which an\nentire set of items of information are simultaneously removed. Existing\nresearch on the latter has largely focussed on single-step parallel\ncontraction: understanding the behaviour of beliefs after a single parallel\ncontraction. It has also focussed on generalisations to the parallel case of\nserial contraction operations whose characteristic properties are extremely\nweak. Here we consider how to extend serial contraction operations that obey\nstronger properties. Potentially more importantly, we also consider the\niterated case: the behaviour of beliefs after a sequence of parallel\ncontractions. We propose a general method for extending serial iterated belief\nchange operators to handle parallel change based on an n-ary generalisation of\nBooth & Chandler's TeamQueue binary order aggregators.",
        "Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.",
        "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach.",
        "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https:\/\/github.com\/linjiemu\/MMXU}{https:\/\/github.com\/linjiemu\/MMXU}.",
        "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
        "With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod.",
        "Hyperspectral unmixing is the analytical process of determining the pure\nmaterials and estimating the proportions of such materials composed within an\nobserved mixed pixel spectrum. We can unmix mixed pixel spectra using linear\nand nonlinear mixture models. Ordinary least squares (OLS) regression serves as\nthe foundation for many linear mixture models employed in Hyperspectral Image\nanalysis. Though variations of OLS are implemented, studies rarely address the\nunderlying assumptions that affect results. This paper provides an in depth\ndiscussion on the assumptions inherently endorsed by the application of OLS\nregression. We also examine variations of OLS models stemming from highly\neffective approaches in spectral unmixing -- sparse regression, iterative\nfeature search strategies and Mathematical programming. These variations are\ncompared to a novel unmixing approach called HySUDeB. We evaluated each\napproach's performance by computing the average error and precision of each\nmodel. Additionally, we provide a taxonomy of the molecular structure of each\nmineral to derive further understanding into the detection of the target\nmaterials.",
        "We incorporate an iteratively reweighted strategy in the manifold proximal\npoint algorithm (ManPPA) in [12] to solve an enhanced sparsity inducing model\nfor identifying sparse yet nonzero vectors in a given subspace. We establish\nthe global convergence of the whole sequence generated by our algorithm by\nassuming the Kurdyka-Lojasiewicz (KL) properties of suitable potential\nfunctions. We also study how the KL exponents of the different potential\nfunctions are related. More importantly, when our enhanced model and algorithm\nreduce, respectively, to the model and ManPPA with constant stepsize considered\nin [12], we show that the sequence generated converges linearly as long as the\noptimal value of the model is positive, and converges finitely when the limit\nof the sequence lies in a set of weak sharp minima. Our results improve [13,\nTheorem 2.4], which asserts local quadratic convergence in the presence of weak\nsharp minima when the constant stepsize is small.",
        "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.",
        "In this paper, we are concerned with the study of the existence of fixed\npoints for single and multi-valued three-points contractions. Namely, we first\nintroduce a new class of single-valued mappings defined on a metric space\nequipped with three metrics. A fixed point theorem is established for such\nmappings. The obtained result recovers that established recently by the second\nauthor [J. Fixed Point Theory Appl. 25 (2023) 74] for the class of\nsingle-valued mappings contracting perimeters of triangles. We next extend our\nstudy by introducing the class of multivalued three points contractions. A\nfixed point theorem, which is a multi-valued version of that obtained in the\nabove reference, is established. Some examples showing the validity of our\nobtained results are provided.",
        "Data-driven control is a powerful tool that enables the design and\nimplementation of control strategies directly from data without explicitly\nidentifying the underlying system dynamics. While various data-driven control\ntechniques, such as stabilization, linear quadratic regulation, and model\npredictive control, have been extensively developed, these methods are not\ninherently suited for multi-linear dynamical systems, where the states are\nrepresented as higher-order tensors. In this article, we propose a novel\nframework for data-driven control of T-product-based dynamical systems (TPDSs),\nwhere the system evolution is governed by the T-product between a third-order\ndynamic tensor and a third-order state tensor. In particular, we offer\nnecessary and sufficient conditions to determine the data informativity for\nsystem identification, stabilization by state feedback, and T-product quadratic\nregulation of TPDSs with detailed complexity analyses. Finally, we validate our\nframework through numerical examples.",
        "Augmented Reality (AR) collaboration can benefit from a shared 2D surface,\nsuch as a whiteboard. However, many features of each collaborators physical\nenvironment must be considered in order to determine the best placement and\nshape of the shared surface. We explored the effects of three methods for\nbeginning a collaborative whiteboarding session with varying levels of user\ncontrol: MANUAL, DISCRETE CHOICE, and AUTOMATIC by conducting a simulated AR\nstudy within Virtual Reality (VR). In the MANUAL method, users draw their own\nsurfaces directly in the environment until they agree on the placement; in the\nDISCRETE CHOICE method, the system provides three options for whiteboard size\nand location; and in the AUTOMATIC method, the system automatically creates a\nwhiteboard that fits within each collaborators environment. We evaluate these\nthree conditions in a study in which two collaborators used each method to\nbegin collaboration sessions. After establishing a session, the users worked\ntogether to complete an affinity diagramming task using the shared whiteboard.\nWe found that the majority of participants preferred to have direct control\nduring the initialization of a new collaboration session, despite the\nadditional workload induced by the Manual method.",
        "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
        "The algorithms-with-predictions framework has been used extensively to\ndevelop online algorithms with improved beyond-worst-case competitive ratios.\nRecently, there is growing interest in leveraging predictions for designing\ndata structures with improved beyond-worst-case running times. In this paper,\nwe study the fundamental data structure problem of maintaining approximate\nshortest paths in incremental graphs in the algorithms-with-predictions model.\nGiven a sequence $\\sigma$ of edges that are inserted one at a time, the goal is\nto maintain approximate shortest paths from the source to each vertex in the\ngraph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm\nstart'' its state.\n  As our main result, we design a learned algorithm that maintains\n$(1+\\epsilon)$-approximate single-source shortest paths, which runs in\n$\\tilde{O}(m \\eta \\log W\/\\epsilon)$ time, where $W$ is the weight of the\nheaviest edge and $\\eta$ is the prediction error. We show these techniques\nimmediately extend to the all-pairs shortest-path setting as well. Our\nalgorithms are consistent (performing nearly as fast as the offline algorithm)\nwhen predictions are nearly perfect, have a smooth degradation in performance\nwith respect to the prediction error and, in the worst case, match the best\noffline algorithm up to logarithmic factors.\n  As a building block, we study the offline incremental approximate\nsingle-source shortest-paths problem. In this problem, the edge sequence\n$\\sigma$ is known a priori and the goal is to efficiently return the length of\nthe shortest paths in the intermediate graph $G_t$ consisting of the first $t$\nedges, for all $t$. Note that the offline incremental problem is defined in the\nworst-case setting (without predictions) and is of independent interest.",
        "We present a new approach to symmetry breaking for pairs of real forms of\n$(GL(n, \\mathbb{C}), GL(n-1, \\mathbb{C}))$. While translation functors are a\nuseful tool for studying a family of representations of a single reductive\ngroup $G$, when applied to a pair of groups $G \\supset G'$,translation functors\ncan significantly alter the nature of symmetry breaking between the\nrepresentations of $G$ and $G'$, even within the same Weyl chamber of the\ndirect product group $G \\times G'$. We introduce the concept of \\lq\\lq{fences\nfor the interlacing pattern}\\rq\\rq,which provides a refinement of the usual\nnotion of \\lq\\lq{walls for Weyl chambers}\\rq\\rq. We then present a theorem that\nstates that multiplicity is constant unless these \\lq\\lq{fences}\\rq\\rq\\ are\ncrossed. This general theorem is illustrated with examples of both tempered and\nnon-tempered representations. Additionally,we provide a new non-vanishing\ntheorem of period integrals for pairs of reductive symmetric spaces,which is\nfurther strengthened through this approach.",
        "Exponential moving average (EMA) has recently gained significant popularity\nin training modern deep learning models, especially diffusion-based generative\nmodels. However, there have been few theoretical results explaining the\neffectiveness of EMA. In this paper, to better understand EMA, we establish the\nrisk bound of online SGD with EMA for high-dimensional linear regression, one\nof the simplest overparameterized learning tasks that shares similarities with\nneural networks. Our results indicate that (i) the variance error of SGD with\nEMA is always smaller than that of SGD without averaging, and (ii) unlike SGD\nwith iterate averaging from the beginning, the bias error of SGD with EMA\ndecays exponentially in every eigen-subspace of the data covariance matrix.\nAdditionally, we develop proof techniques applicable to the analysis of a broad\nclass of averaging schemes.",
        "In this paper, models that approximate stochastic processes from the space\n$Sub_\\varphi(\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\nconsidered for some specific functions $\\varphi(t)$. For processes that are\ndecomposited in series using orthonormal bases, such models are constructed in\nthe case where elements of such decomposition cannot be found explicitly."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Quantifying the performance of machine learning models in materials discovery"
      ],
      "abstract":[
        "The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "High emissivity surfaces stable at high temperatures",
        "In-operando test of tunable Heusler alloys for thermomagnetic harvesting\n  of low-grade waste heat",
        "Pressure Tuning of Layer-hybridized Excitons in Trilayer WSe2",
        "On the extension of the concept of rheological connections to a finite\n  deformation framework using multiple natural configurations",
        "Antiferromagnetic two-dimensional transition-metal nitride Co$_2$N$_2$\n  layer with high N$\\rm \\acute{\\textbf e}$el temperature and Dirac fermions",
        "Optimizing Lead-Free Chalcogenide Perovskites for High-Efficiency\n  Photovoltaics via Alloying Strategies",
        "Vacancy-induced Modification of Electronic Band Structure of LiBO$_{2}$\n  Material as Cathode Surface Coating of Lithium-ion Batteries",
        "Improvement of Morphology and Electrical Properties of Boron-doped\n  Diamond Films via Seeding with HPHT Nanodiamonds Synthesized from\n  9-Borabicyclononane",
        "A generalized calculation of the rate independent single crystal yield\n  surface",
        "Predicting Mode-I\/II fracture toughness and crack growth in diboride\n  ceramics via machine-learning potentials",
        "Visualizing Nanodomain Superlattices in Halide Perovskites Giving\n  Picosecond Quantum Transients",
        "Interactive Multiscale Modeling to Bridge Atomic Properties and\n  Electrochemical Performance in Li-CO$_2$ Battery Design",
        "Evidence of strong electron correlation effects and magnetic topological\n  excitation in low carbon steel",
        "On the speed of coming down from infinity for (sub)critical branching\n  processes with pairwise interactions",
        "Comment on \"Optimal conversion of Kochen-Specker sets into bipartite\n  perfect quantum strategies\"",
        "A simple extrapolation criterion with an application to wavelet\n  characterization of various function spaces",
        "Predicted Neutrino Signal Features of Core-Collapse Supernovae",
        "A Space Mapping approach for the calibration of financial models with\n  the application to the Heston model",
        "Euclid Quick Data Release (Q1), A first look at the fraction of bars in\n  massive galaxies at $z<1$",
        "H$\\alpha$ Variability of AB Aur b with the Hubble Space Telescope:\n  Probing the Nature of a Protoplanet Candidate with Accretion Light Echoes",
        "Surfaces in 4-manifolds and extendible mapping classes",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Monge-Kantorovich quantiles and ranks for image data",
        "Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Stochastic Equilibrium Raman Spectroscopy (STERS)",
        "The Light Neutralino Dark Matter in the Generalized Minimal Supergravity\n  (GmSUGRA)",
        "Fusion Dynamics of Majorana Zero Modes"
      ],
      "abstract":[
        "Thermal radiative energy transport is essential for high-temperature energy\nharvesting technologies, including thermophotovoltaics (TPVs) and grid-scale\nthermal energy storage. However, the inherently low emissivity of conventional\nhigh-temperature materials constrains radiative energy transfer, thereby\nlimiting both system performance and technoeconomic viability. Here, we\ndemonstrate ultrafast femtosecond laser-material interactions to transform\ndiverse materials into near-blackbody surfaces with broadband spectral\nemissivity above 0.96. This enhancement arises from hierarchically engineered\nlight-trapping microstructures enriched with nanoscale features, effectively\ndecoupling surface optical properties from bulk thermomechanical properties.\nThese laser blackened surfaces (LaBS) exhibit exceptional thermal stability,\nretaining high emissivity for over 100 hours at temperatures exceeding\n1000{\\deg}C, even in oxidizing environments. When applied as TPV thermal\nemitters, Ta LaBS double electrical power output from 2.19 to 4.10 W cm-2 at\n2200{\\deg}C while sustaining TPV conversion efficiencies above 30%. This\nversatile, largely material-independent technique offers a scalable and\neconomically viable pathway to enhance emissivity for advanced thermal energy\napplications.",
        "Thermomagnetic generation stands out as a promising technology for harvesting\nand converting low-grade waste heat below 100 {\\deg}C. Despite the exponential\ngrowth in research on thermomagnetic materials and prototypes over the last\ndecade, there remains, to unlock the full potential of this technology, a\ncritical gap between fundamental research on materials and the design of\nadvanced devices. In this study, we present the in-operando assessment of\nthermomagnetic performance of three representative Ni,Mn-based Heusler alloys\noptimized for harvesting low-grade waste heat below 373 K. These materials were\ntested under operational conditions using a specially designed laboratory-scale\nprototype of a thermomagnetic motor. The mechanical power output of the motor,\noperated with NiMnIn, NiMnSn and NiMnCuGa alloys, was correlated with the\nmagnetic properties of the materials, highlighting the critical role of the\nmagnetic transition temperature and saturation magnetization in determining the\nefficiency of thermomagnetic energy conversion. Austenitic Heusler alloys were\nconfirmed to be promising thermomagnetic materials due to their highly tunable\nCurie temperature and significant magnetization changes in the 300-360 K\ntemperature range. Among the tested materials, the Ni48Mn36In16 alloy\ndemonstrated the highest thermomagnetic performance, surpassing the benchmark\nmaterial Gd in the 320-340 K range. From an experimental perspective, the\ndeveloped prototype of thermomagnetic motor serves as a flexible test-bench for\nevaluating and comparing the thermomagnetic performance of small amounts (less\nthan 0.3 g) of new materials under variable conditions. Additionally, its\nmodular design facilitates testing and optimization of its various components,\nthereby contributing to the advancement of thermomagnetic motor technology.",
        "We demonstrate dynamic pressure tuning (0-6.6 GPa) of layer-hybridized\nexcitons in AB-stacked trilayer WSe$_2$ via diamond-anvil-cell-integrated\nreflectance spectroscopy. Pressure-controlled interlayer coupling manifests in\nenhanced energy-level anti-crossings and oscillator strength redistribution,\nwith Stark shift analysis revealing a characteristic dipole moment reduction of\n11%. Notably, the hybridization strength between the intra- and interlayer\nexcitons triples from $\\sim$10 meV to above $\\sim$30 meV, exhibiting a\nnear-linear scaling of 3.5$\\pm$0.2 meV\/GPa. Spectral density simulations\nresolve four distinct components, i.e., intralayer ground\/excited and\ninterlayer ground\/excited excitons, with their relative weights transitioning\nfrom one component dominant to strongly hybridized at higher pressures. Our\nfindings highlight the potential for controlling excitonic properties and\nengineering novel optoelectronic devices through interlayer compression.",
        "The constitutive behaviors of materials are often modeled using a network of\ndifferent rheological elements. These rheological models are mostly developed\nwithin a one-dimensional small strain framework. One of the key impediments of\nextending these models to a three-dimensional finite deformation setting is to\ndetermine how the different types of connections, i.e., a series and a parallel\nconnection, are incorporated into the material models. The primary objective of\nthis article is to develop an appropriate strategy to address this issue. We\nshow that both the series and the parallel connection between two rheological\nelements can be modeled within a multiple natural configurations framework\nwithout changing or introducing new configurations. The difference in a series\nand a parallel connection is manifested in the ratio of the stress powers\nexpended during the deformations of the associated rheological elements. Finite\ndeformation version of some well-known rheological models have been used to\ndemonstrate the utility of the proposed theory.",
        "Two-dimensional (2D) transition metal nitrides have a wide prospect of\napplications in the fields of physics, chemistry, materials, etc. However, 2D\ntransition metal nitrides with strong magnetism, especially high N$\\rm\n\\acute{e}$el temperature, are very scarce. Based on the first-principles\ncalculations within the framework of density functional theory, we design two\n2D transition-metal nitrides \\textit{M}$_2$N$_2$ (\\textit{M} = Ti, Co), in\nwhich the transition metal atoms and the N atoms form a 2D layer with a\nwrinkled structure. The structural stability is demonstrated by the cohesive\nenergy, formation energy, elastic constants, phonon spectra and molecular\ndynamics simulations. Elastic moduli calculations reveal that the mechanical\nproperties of the two structures are anisotropic. Spin-polarized calculations\nshow that Ti$_2$N$_2$ is a 2D ferromagnetic material while Co$_2$N$_2$ is a 2D\nantiferromagnetic semimetal with a Dirac point at Fermi level. Furthermore, by\nsolveing the Heisenberg model by Monte Carlo method, we discover that the 2D\nCo$_2$N$_2$ layer is a high-temperature antiferromagnetic material and the\nN$\\rm \\acute{e}$el temperature is up to 474 K. Therefore, our findings provide\na rare antiferromagnetic 2D material with both high critical temperature and\nDirac Fermions.",
        "Lead-free chalcogenide perovskites are emerging as game-changers in the race\nfor sustainable, high-performance photovoltaics. These materials offer a\nperfect trifecta: non-toxic elemental composition, exceptional phase stability,\nand outstanding optoelectronic properties. However, unlocking their full\npotential for solar cell applications requires advanced strategies to fine-tune\ntheir electronic and optical behavior. In this study, we take CaHfS$_{3}$-a\npromising but underexplored candidate-and revolutionize its performance by\nintroducing targeted substitutions: Ti at the cation site and Se at the anion\nsite. Using cutting-edge computational techniques, including density functional\ntheory, GW calculations, and the Bethe-Salpeter equation (BSE), we reveal how\nthese substitutions transform the material's properties. Our findings highlight\nthat alloyed compounds such as CaHfS$_{3-x}$Se$_{x}$ and\nCaHf$_{1-y}$Ti$_{y}$X$_{3}$ (X = S, Se) are not only phase-stable but also\nfeature adjustable direct G$_{0}$W$_{0}$@PBE bandgaps (1.29-2.67 eV), reduced\nexciton binding energies, and significantly improved polaron mobility. These\nmodifications enable better light absorption, reduced electron-hole\nrecombination, longer exciton lifetimes, and enhanced quantum yield.\nImpressively, the alloyed perovskites, specifically, for the Ti-rich Se-based\nperovskites, achieve a spectroscopic-limited maximum efficiency of up to\n28.06%, outperforming traditional lead-based halide perovskites. Our results\ndemonstrate that strategic alloying is a powerful tool to supercharge the\noptoelectronic properties of lead-free chalcogenide perovskites, positioning\nthem as strong contenders for next-generation photovoltaic technologies.",
        "LiBO$_{2}$ is an electronic insulator and a promising surface coating for\nstabilizing high-voltage cathodes in lithium-ion batteries. Despite its\npotential, the functional mechanisms of this coating remain unclear,\nparticularly the transport of lithium ions and electrons through LiBO$_{2}$ in\nthe presence of lattice vacancies. This understanding is critical for the\ndesign and development of LiBO$_{2}$-based materials. In our previous work\n[Ziemke $\\textit{et al.}$, J. Mater. Chem. A, 2025, $\\textbf{13}$, 3146-3162],\nwe used density functional theory (DFT) calculations to investigate the impact\nof lattice vacancies on Li-ion transport in both tetragonal (t-LBO) and\nmonoclinic (m-LBO) polymorphs of LiBO$_{2}$, revealing that B vacancies in\neither polymorph enhanced lithium-ion transport. In this study, we expand on\nthese findings by using DFT calculations to examine the effects of lattice\nvacancies on the electronic properties of both t-LBO and m-LBO\npolymorphs,focusing on the electronic band structure. Our analysis shows that B\nvacancies can enhance the electronic insulation of t-LBO while improving the\nionic conduction of m-LBO. The combined results of our previous and current\nworks indicate that B vacancy generation in LiBO$_{2}$ may enable t-LBO to\nfunction as a promising solid electrolyte and enhance the performance of m-LBO\nas a conformal cathode coating in lithium-ion batteries. Overall, generating B\nvacancies, such as through neutron irradiation, would offer a viable strategy\nto improve the functionality of LiBO$_{2}$ as a promising material for energy\nstorage applications.",
        "Boron-doped diamond (BDD) films are becoming increasingly popular as\nelectrode materials due to their broad potential window and stability in harsh\nconditions and environments. Therefore, optimizing the crystal quality and\nminimizing defect density to maximize electronic properties (e.g. conductivity)\nof BDD is of great importance. This study investigates the influence of\ndifferent hydrogenated nanodiamond (H-ND) seeding layers on the growth and\nproperties of BDD films. Three types of seeding H-NDs were examined: detonation\n(H-DND) and top-down high-pressure high-temperature NDs (TD_HPHT H-ND), and\nboron-doped NDs (H-BND) newly synthesized at high-pressure high-temperature\nfrom an organic precursor. Purified and oxidized BND (O-BND) samples yielded\nclear, blue, and stable colloidal dispersions. Subsequent thermal hydrogenation\nreversed their zeta potential from - 32 mV to + 44 mV and promoted the seeding\nof negatively charged surfaces. All three H-ND types formed dense seeding\nlayers on SiO2 and Si\/SiOx substrates, which enabled the growth of BDD films by\nchemical vapor deposition (CVD). Despite variations in initial surface coverage\namong the seeding layers (13-25%), all NDs facilitated the growth of fully\nclosed BDD films approximately 1 {\\mu}m thick. Significant differences in film\nmorphology and electrical properties were observed. H-BND nucleation yielded\nthe BDD films with the largest crystals (up to 1 000 nm) and lowest sheet\nresistance (400 ohm\/sq). This superior performance is attributed to the uniform\nparticle shape and monocrystalline character of H-BND, as corroborated by FTIR,\nTEM, and SAXS measurements. These findings highlight the critical role of\nseeding layer properties in determining consequent diamond film evolution and\nestablish H-BNDs as promising seeding material for the growth of high-quality\nBDD films suitable for electronic and electrochemical applications.",
        "In this paper, we discuss a method to calculate the topology of the rate\nindependent single crystal yield surface for materials with arbitrary slip\nsystems and arbitrary slip strengths. We describe the general problem, as\nmotivated by Schmid's law, and detail the calculation of hyperplanes in\ndeviatoric stress space, $\\mathbb{D}^5$, which describe the criteria for slip\non individual slip systems. We focus on finding the intersection of five\nlinearly independent hyperplanes which represent stresses necessary to satisfy\nthe criteria for general plastic deformation. Finally, we describe a method for\ncalculating the inner convex hull of these intersection points, which describe\nthe vertices of the five dimensional polytope that represents the single\ncrystal yield surface. Our method applies to arbitrary crystal structure,\nallowing for an arbitrary number and type of slip systems and families,\nconsiders plastic anisotropy via inter- and intra-family strength anisotropy,\nand further considers strength anisotropy between slip in the positive and\nnegative direction. We discuss the calculation and possible applications, and\nshare a computational implementation of the calculation of the single crystal\nyield surface.",
        "Fracture toughness and strength are critical for structural ceramics, which\nare prone to brittle failure. However, accurately characterizing these\nproperties is challenging, especially for thin films on substrates. In-situ\nmicroscopy often fails to resolve crack initiation, while fractured samples\nprovide limited insight into fracture modes and crack paths. Here, we employ\nstress intensity factor ($K$) controlled atomistic simulations of fracture to\ncharacterize the crack-initiation properties of hard but brittle diboride\nceramics. Our molecular statics calculations are based on moment-tensor\nmachine-learning interatomic potentials (MLIPs) trained on {\\it{ab initio}}\ninformation collected for a variety of atomistic environments. TMB$_{2}$\n(TM$=$Ti, Zr, Hf) lattice models with six distinct atomically-sharp crack\ngeometries subjected to Mode-I (opening) and\/or Mode-II (sliding) deformation\nserve as platforms to illustrate the capability of the approach. The Mode-I\ninitiation toughness $K_{Ic}$ and fracture strength $\\sigma_{f}$ -- within\nranges 1.8-2.9~MPa$\\cdot\\sqrt{m}$ and 1.6-2.4~GPa -- are extrapolated at the\nmacroscale limit by fitting the results of finite (up to 10$^{6}$ atoms)\ncracked plate models with constitutive scaling relations. Our simulations show\nthat most diboride lattice models fail by extension of the native crack.\nHowever, crack-deflection on the $(1\\overline{1}01)$ plane is observed for the\n$(10\\overline{1}0)[\\overline{1}2\\overline{1}0]$ crystal geometry. As\nexemplified by TiB$_{2}$, varying Mode-I\/II loading ratios have little\ninfluence on crack propagation paths, which overall occurs by decohesion of\nlow-energy fracture planes or combined sliding. Our predictions are supported\nby cube-corner nanoindentation on TiB$_{2}$ samples along the [0001] direction,\nrevealing the same fracture plane as observed in simulations.",
        "The high optoelectronic quality of halide perovskites lends them to be\nutilized in optoelectronic devices and recently in emerging quantum emission\napplications. Advancements in perovskite nanomaterials have led to the\ndiscovery of processes in which luminescence decay times are sub-100\npicoseconds, stimulating the exploration of even faster radiative rates for\nadvanced quantum applications, which have only been prominently realised in\nIII-V materials grown through costly epitaxial growth methods. Here, we\ndiscovered ultrafast quantum transients of time scales ~2 picoseconds at low\ntemperature in bulk formamidinium lead iodide films grown through scalable\nsolution or vapour approaches. Using a multimodal strategy, combining ultrafast\nspectroscopy, optical and electron microscopy, we show that these transients\noriginate from quantum tunnelling in nanodomain superlattices. The outcome of\nthe transient decays, photoluminescence, mirrors the photoabsorption of the\nstates, with an ultra-narrow linewidth at low temperature as low as <2 nm (~4\nmeV). Localized correlation of the emission and structure reveals that the\nnanodomain superlattices are formed by alternating ordered layers of corner\nsharing and face sharing octahedra. This discovery opens new applications\nleveraging intrinsic quantum properties and demonstrates powerful multimodal\napproaches for quantum investigations.",
        "Li-CO$_2$ batteries show promise as energy storage solutions, offering high\ntheoretical energy density and CO$_2$ fixation. Their operation is based on the\nformation and decomposition of Li$_2$CO$_3$\/C during discharge and charge\ncycles, respectively. We used a multiscale modeling framework that integrates\nDensity Functional Theory (DFT), Ab-Initio Molecular Dynamics (AIMD), classical\nMolecular Dynamics (MD), and Finite Element Analysis (FEA) to investigate\natomic and cell-level properties. The considered Li-CO$_2$ battery consists of\na lithium metal anode, an ionic liquid electrolyte, and a carbon cloth porous\ncathode with Sb$_{0.67}$Bi$_{1.33}$Te$_3$ as a catalyst. DFT and AIMD\ndetermined the electrical conductivities of Sb$_{0.67}$Bi$_{1.33}$Te$_3$ and\nLi$_2$CO$_3$ using the Kubo-Greenwood formalism and studied the CO$_2$\nreduction mechanism on the cathode catalyst. MD simulations calculated the\nCO$_2$ diffusion coefficient, Li$^+$ transference number, ionic conductivity,\nand Li$^+$ solvation structure. The FEA model, incorporating results from\natomistic simulations, reproduced experimental voltage-capacity profiles at 1\nmA\/cm$^2$ and revealed spatio-temporal variations in Li$_2$CO$_3$\/C deposition,\nporosity, and CO$_2$ concentration dependence on discharge rates in the\ncathode. Accordingly, Li$_2$CO$_3$ can form large and thin film deposits,\nleading to dispersed and local porosity changes at 0.1 mA\/cm$^2$ and 1\nmA\/cm$^2$, respectively. The capacity decreases exponentially from 81,570 mAh\/g\nat 0.1 mA\/cm$^2$ to 6,200 mAh\/g at 1 mA\/cm$^2$, due to pore clogging from\nexcessive discharge product deposition that limits CO$_2$ transport to the\ncathode interior. Therefore, the performance of Li-CO$_2$ batteries can be\nimproved by enhancing CO$_2$ transport, regulating Li$_2$CO$_3$ deposition, and\noptimizing cathode architecture.",
        "Present study explores how thermal treatments and strain affect the magnetic\nresponse of two plain-carbon steels, with 0.05 wt% and 0.7 wt% carbon.\nElectron-backscattered diffraction and high-frequency magnetic susceptibility\n($\\chi$) measurements in 0.05%C steel reveal that annealing increases $\\chi$ by\nenlarging grains, while quenching reduces it by decreasing grain size. We also\nstudy the 0.7%C steel, to delineate effects of quench-induced strain and\npossible carbon-rich (Fe$_3$C) phase admixture in 0.05%C steel which could\naffect its magnetic response. In 0.7%C steel, uniaxial tensile strain enhances\n$\\chi$ via altered magnetic anisotropy, avoiding the reduction seen in quenched\n0.05%C steel. Micro-magnetic modelling and magnetic force microscopy identify\nmagnetic topological structures (MTS) in 0.05%C steel, especially with\nquenching. Low-temperature transport measurement suggests strong electron\ncorrelations drive Kondo effect in 0.05%C steel and MTS is an emergent feature\nof interplay with local strain. Thus, steel exhibits strong electron\ncorrelations and novel magnetic excitations, making it a promising quantum\nmaterial platform for developing new device applications.",
        "In this paper, we investigate the phenomenon of coming down from infinity for\n(sub)critical cooperative branching processes with pairwise interactions (BPI\nprocesses for short) under appropriate conditions. BPI processes are\ncontinuous-time Markov chains that extend pure branching dynamics by\nincorporating additional mechanisms that allow both competition and cooperation\nevents between pairs of individuals.\n  Specifically, we focus on characterising the speed at which BPI processes\nevolve when starting from a very large initial population in the subcritical\nand critical cooperative regimes. Further, in the subcritical cooperative\nregime, we analyse their second-order fluctuations.",
        "A recent paper of Trandafir and Cabello [Phys. Rev. A, 111, 022408 (2025)]\ncontains a number of errors, inconsistencies, and inefficiencies. They are too\nnumerous to be listed here, so we identify and discuss them in the main body of\nthe comment.",
        "The aim of this paper is to obtain an extrapolation result without using\nconvexification. What is new about this criterion is that the convexification\nof Banach spaces does not come into play. As an application, a characterization\nof ball Banach function spaces in terms of wavelets can be obtained. The result\ncan be formulated so that we can take into account the smoothness property of\nthe function spaces under consideration. The same technique can be used for the\nproof of the vector-valued inequalities for example. Also, the result in the\npresent paper refines a recent result on the extension operator.",
        "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode.",
        "We present a novel approach for parameter calibration of the Heston model for\npricing an Asian put option, namely space mapping. Since few parameters of the\nHeston model can be directly extracted from real market data, calibration to\nreal market data is implicit and therefore a challenging task. In addition,\nsome of the parameters in the model are non-linear, which makes it difficult to\nfind the global minimum of the optimization problem within the calibration. Our\napproach is based on the idea of space mapping, exploiting the residuum of a\ncoarse surrogate model that allows optimization and a fine model that needs to\nbe calibrated. In our case, the pricing of an Asian option using the Heston\nmodel SDE is the fine model, and the surrogate is chosen to be the Heston model\nPDE pricing a European option. We formally derive a gradient descent algorithm\nfor the PDE constrained calibration model using well-known techniques from\noptimization with PDEs. Our main goal is to provide evidence that the space\nmapping approach can be useful in financial calibration tasks. Numerical\nresults underline the feasibility of our approach.",
        "Stellar bars are key structures in disc galaxies, driving angular momentum\nredistribution and influencing processes such as bulge growth and star\nformation. Quantifying the bar fraction as a function of redshift and stellar\nmass is therefore important for constraining the physical processes that drive\ndisc formation and evolution across the history of the Universe. Leveraging the\nunprecedented resolution and survey area of the Euclid Q1 data release combined\nwith the Zoobot deep-learning model trained on citizen-science labels, we\nidentify 7711 barred galaxies with $M_* \\gtrsim 10^{10}M_\\odot$ in a\nmagnitude-selected sample $I_E < 20.5$ spanning $63.1 deg^2$. We measure a mean\nbar fraction of $0.2-0.4$, consistent with prior studies. At fixed redshift,\nmassive galaxies exhibit higher bar fractions, while lower-mass systems show a\nsteeper decline with redshift, suggesting earlier disc assembly in massive\ngalaxies. Comparisons with cosmological simulations (e.g., TNG50, Auriga)\nreveal a broadly consistent bar fraction, but highlight overpredictions for\nhigh-mass systems, pointing to potential over-efficiency in central stellar\nmass build-up in simulations. These findings demonstrate Euclid's\ntransformative potential for galaxy morphology studies and underscore the\nimportance of refining theoretical models to better reproduce observed trends.\nFuture work will explore finer mass bins, environmental correlations, and\nadditional morphological indicators.",
        "Giant planets generate accretion luminosity as they form. Much of this energy\nis radiated in strong H$\\alpha$ line emission, which has motivated direct\nimaging surveys at optical wavelengths to search for accreting protoplanets.\nHowever, compact disk structures can mimic accreting planets by scattering\nemission from the host star. This can complicate the interpretation of\nH$\\alpha$ point sources, especially if the host star itself is accreting. We\ndescribe an approach to distinguish accreting protoplanets from scattered-light\ndisk features using \"accretion light echoes.\" This method relies on variable\nH$\\alpha$ emission from a stochastically accreting host star to search for a\ndelayed brightness correlation with a candidate protoplanet. We apply this\nmethod to the candidate protoplanet AB Aur b with a dedicated Hubble Space\nTelescope Wide Field Camera 3 program designed to sequentially sample the host\nstar and the candidate planet in H$\\alpha$ while accounting for the light\ntravel time delay and orbital geometry of the source within the protoplanetary\ndisk. Across five epochs spanning 14 months, AB Aur b is over 20 times more\nvariable than its host star; AB Aur's H$\\alpha$ emission changes by 15% while\nAB Aur b varies by 330%. These brightness changes are not correlated, which\nrules out unobstructed scattered starlight from the host star as the only\nsource of AB Aur b's H$\\alpha$ emission and is consistent with tracing emission\nfrom an independently accreting protoplanet, inner disk shadowing effects, or a\nphysically evolving compact disk structure. More broadly, accretion light\nechoes offer a novel tool to explore the nature of protoplanet candidates with\nwell-timed observations of the host star prior to deep imaging in H$\\alpha$.",
        "We study smooth proper embeddings of compact orientable surfaces in compact\norientable $4$-manifolds and elements in the mapping class group of that\nsurface which are induced by diffeomorphisms of the ambient $4$-manifolds. We\ncall such mapping classes extendible. An embedding for which all mapping\nclasses are extendible is called flexible. We show that for most of the\nsurfaces there exists no flexible embedding in a $4$-manifold with homology\ntype of a $4$-ball or of a $4$-sphere. As an application of our method, we\naddress a question of Etnyre and Lekili and show that there exists no simple\nopen book decomposition of $S^5$ with a spin page where all $3$-dimensional\nopen books admit open book embeddings. We also provide many constructions and\ncriteria for extendible and non-extendible mapping classes, and discuss a\nconnection between extendibility and sliceness of links in a homology $4$-ball\nwith $S^3$ boundary. Finally, we give a new generating set of the group of\nextendible mapping classes for the trivial embedding of a closed genus $g$\nsurface in $S^4$, consisting of $3g$ generators. This improves a previous\nresult of Hirose giving a generating set of size $6g-1$.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "This paper defines quantiles, ranks and statistical depths for image data by\nleveraging ideas from measure transportation. The first step is to embed a\ndistribution of images in a tangent space, with the framework of linear optimal\ntransport. Therein, Monge-Kantorovich quantiles are shown to provide a\nmeaningful ordering of image data, with outward images having unusual shapes.\nNumerical experiments showcase the relevance of the proposed procedure, for\ndescriptive analysis, outlier detection or statistical testing.",
        "The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "We theoretically propose a new method in cavity- and surface-enhanced Raman\nspectroscopy (SERS) with improved temporal resolution in the measurement of\nstochastic Raman spectral fluctuations. Our approach combines Fourier\nspectroscopy and photon correlation to decouple the integration time from the\ntemporal resolution. Using statistical optics simulations, we establish the\nrelationship between time resolution and Raman signal strength, revealing that\ntypical Raman spectral fluctuations, commensurate with molecular conformational\ndynamics, can theoretically be resolved on micro- to millisecond timescales.\nThe method can further extract average single-molecule dynamics from small\nsub-ensembles, thereby potentially mitigating challenges in achieving strictly\nsingle-molecule isolation on SERS substrates.",
        "We investigate both the $Z$ and $H$ poles solutions for the Higgsino mass\nparameter $\\mu>0$ and $\\mu<0$ for the neutralino dark matter in light of the\nLHC supersymmetry searches and the direct detection dark matter experiments,\nLUX-ZEPLIN (LZ), in the Generalized Minimal Supergravity (GmSUGRA). Our study\nindicates that the latest experimental constraints from the LHC and LZ\nCollaborations exclude the light Higgsinos in the $Z$ and $H$ pole regions for\nthe $\\mu>0$ case. Interestingly, for the $\\mu < 0$ case, a very light Higgsinos\ncan still be consistent with the current constraints from the electroweakino\nsearches and LZ experiment in the $Z$ and $H$ poles. Consequently, the $\\mu <\n0$ case appears more promising and thus requires the dedicated efforts to make\ndefinitive conclusions about their current status from the experimental\nCollaborations. In this framework, our findings indicate a deviation of up to\n$2\\sigma$ from the central value of \\( a_\\mu \\equiv (g-2)_\\mu\/2 \\), resonating\nwith the experimental results reported by CMD and BDM.",
        "Braiding and fusion of Majorana zero modes are key elements of any future\ntopological Majorana-based quantum computer. Here, we investigate the fusion\ndynamics of Majorana zero modes in the spinless Kitaev model, as well as in a\nspinfull model describing magnet-superconductor hybrid structures. We consider\nvarious scenarios allowing us to reproduce the fusion rules of the Ising anyon\nmodel. Particular emphasis is given to the charge of the fermion obtained after\nfusing two Majorana zero modes: as long as it remains on the superconductor,\ncharge quantization is absent. When moving the fermion to a non-superconducting\nregion, such as a quantum dot, nearly-quantized charge can be measured. Our\nfindings confirm for both platforms that fusion dynamics of Majorana zero modes\ncan indeed be used for the readout of Majorana qubits."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Quantifying the performance of machine learning models in materials discovery",
    "start_abstract":"The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
        "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol",
        "Agency Is Frame-Dependent",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Reinforced Large Language Model is a formal theorem prover",
        "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "Detection of LLM-Paraphrased Code and Identification of the Responsible\n  LLM Using Coding Style Features",
        "A Survey on Mathematical Reasoning and Optimization with Large Language\n  Models",
        "SHACL-SKOS Based Knowledge Representation of Material Safety Data Sheet\n  (SDS) for the Pharmaceutical Industry",
        "Automatic Curriculum Design for Zero-Shot Human-AI Coordination",
        "Artificial Intelligence-Driven Clinical Decision Support Systems",
        "A Law Reasoning Benchmark for LLM with Tree-Organized Structures\n  including Factum Probandum, Evidence and Experiences",
        "RTBAgent: A LLM-based Agent System for Real-Time Bidding",
        "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
        "One citation, one vote! A new approach for analysing\n  check-all-that-apply (CATA) data in sensometrics, using L1 norm methods",
        "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
        "Analysis of kinematics of mechanisms containing revolute joints",
        "Controllable Emotion Generation with Emotion Vectors",
        "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
        "DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu)\n  Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions",
        "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff\n  Sets in Multi-Objective MDPs and its Impact on Randomised Strategies",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "On Nash Equilibria in Play-Once and Terminal Deterministic Graphical\n  Games",
        "Optimal generalisation and learning transition in extensive-width\n  shallow neural networks near interpolation",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\n  Control",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "A conjecture on monomial realizations and polyhedral realizations for\n  crystal bases"
      ],
      "abstract":[
        "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
        "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings.",
        "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "To take advantage of Large Language Model in theorem formalization and proof,\nwe propose a reinforcement learning framework to iteratively optimize the\npretrained LLM by rolling out next tactics and comparing them with the expected\nones. The experiment results show that it helps to achieve a higher accuracy\ncompared with directly fine-tuned LLM.",
        "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1\/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https:\/\/github.com\/zzli2022\/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
        "Recent progress in large language models (LLMs) for code generation has\nraised serious concerns about intellectual property protection. Malicious users\ncan exploit LLMs to produce paraphrased versions of proprietary code that\nclosely resemble the original. While the potential for LLM-assisted code\nparaphrasing continues to grow, research on detecting it remains limited,\nunderscoring an urgent need for detection system. We respond to this need by\nproposing two tasks. The first task is to detect whether code generated by an\nLLM is a paraphrased version of original human-written code. The second task is\nto identify which LLM is used to paraphrase the original code. For these tasks,\nwe construct a dataset LPcode consisting of pairs of human-written code and\nLLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of\nhuman-written and LLM-paraphrased code, particularly in terms of naming\nconsistency, code structure, and readability. Based on these findings, we\ndevelop LPcodedec, a detection method that identifies paraphrase relationships\nbetween human-written and LLM-generated code, and discover which LLM is used\nfor the paraphrasing. LPcodedec outperforms the best baselines in two tasks,\nimproving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and\n213x, respectively. Our code and data are available at\nhttps:\/\/github.com\/Shinwoo-Park\/detecting_llm_paraphrased_code_via_coding_style_features.",
        "Mathematical reasoning and optimization are fundamental to artificial\nintelligence and computational problem-solving. Recent advancements in Large\nLanguage Models (LLMs) have significantly improved AI-driven mathematical\nreasoning, theorem proving, and optimization techniques. This survey explores\nthe evolution of mathematical problem-solving in AI, from early statistical\nlearning approaches to modern deep learning and transformer-based\nmethodologies. We review the capabilities of pretrained language models and\nLLMs in performing arithmetic operations, complex reasoning, theorem proving,\nand structured symbolic computation. A key focus is on how LLMs integrate with\noptimization and control frameworks, including mixed-integer programming,\nlinear quadratic control, and multi-agent optimization strategies. We examine\nhow LLMs assist in problem formulation, constraint generation, and heuristic\nsearch, bridging theoretical reasoning with practical applications. We also\ndiscuss enhancement techniques such as Chain-of-Thought reasoning, instruction\ntuning, and tool-augmented methods that improve LLM's problem-solving\nperformance. Despite their progress, LLMs face challenges in numerical\nprecision, logical consistency, and proof verification. Emerging trends such as\nhybrid neural-symbolic reasoning, structured prompt engineering, and multi-step\nself-correction aim to overcome these limitations. Future research should focus\non interpretability, integration with domain-specific solvers, and improving\nthe robustness of AI-driven decision-making. This survey offers a comprehensive\nreview of the current landscape and future directions of mathematical reasoning\nand optimization with LLMs, with applications across engineering, finance, and\nscientific research.",
        "We report the development of a knowledge representation and reasoning (KRR)\nsystem built on hybrid SHACL-SKOS ontologies for globally harmonized system\n(GHS) material Safety Data Sheets (SDS) to enhance chemical safety\ncommunication and regulatory compliance. SDS are comprehensive documents\ncontaining safety and handling information for chemical substances. Thus, they\nare an essential part of workplace safety and risk management. However, the\nvast number of Safety Data Sheets from multiple organizations, manufacturers,\nand suppliers that produce and distribute chemicals makes it challenging to\ncentralize and access SDS documents through a single repository. To accomplish\nthe underlying issues of data exchange related to chemical shipping and\nhandling, we construct SDS related controlled vocabulary and conditions\nvalidated by SHACL, and knowledge systems of similar domains linked via SKOS.\nThe resulting hybrid ontologies aim to provide standardized yet adaptable\nrepresentations of SDS information, facilitating better data sharing,\nretrieval, and integration across various platforms. This paper outlines our\nSHACL-SKOS system architectural design and showcases our implementation for an\nindustrial application streamlining the generation of a composite shipping\ncover sheet.",
        "Zero-shot human-AI coordination is the training of an ego-agent to coordinate\nwith humans without using human data. Most studies on zero-shot human-AI\ncoordination have focused on enhancing the ego-agent's coordination ability in\na given environment without considering the issue of generalization to unseen\nenvironments. Real-world applications of zero-shot human-AI coordination should\nconsider unpredictable environmental changes and the varying coordination\nability of co-players depending on the environment. Previously, the multi-agent\nUED (Unsupervised Environment Design) approach has investigated these\nchallenges by jointly considering environmental changes and co-player policy in\ncompetitive two-player AI-AI scenarios. In this paper, our study extends the\nmulti-agent UED approach to a zero-shot human-AI coordination. We propose a\nutility function and co-player sampling for a zero-shot human-AI coordination\nsetting that helps train the ego-agent to coordinate with humans more\neffectively than the previous multi-agent UED approach. The zero-shot human-AI\ncoordination performance was evaluated in the Overcooked-AI environment, using\nhuman proxy agents and real humans. Our method outperforms other baseline\nmodels and achieves a high human-AI coordination performance in unseen\nenvironments.",
        "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
        "While progress has been made in legal applications, law reasoning, crucial\nfor fair adjudication, remains unexplored. We propose a transparent law\nreasoning schema enriched with hierarchical factum probandum, evidence, and\nimplicit experience, enabling public scrutiny and preventing bias. Inspired by\nthis schema, we introduce the challenging task, which takes a textual case\ndescription and outputs a hierarchical structure justifying the final decision.\nWe also create the first crowd-sourced dataset for this task, enabling\ncomprehensive evaluation. Simultaneously, we propose an agent framework that\nemploys a comprehensive suite of legal analysis tools to address the challenge\ntask. This benchmark paves the way for transparent and accountable AI-assisted\nlaw reasoning in the ``Intelligent Court''.",
        "Real-Time Bidding (RTB) enables advertisers to place competitive bids on\nimpression opportunities instantaneously, striving for cost-effectiveness in a\nhighly competitive landscape. Although RTB has widely benefited from the\nutilization of technologies such as deep learning and reinforcement learning,\nthe reliability of related methods often encounters challenges due to the\ndiscrepancies between online and offline environments and the rapid\nfluctuations of online bidding. To handle these challenges, RTBAgent is\nproposed as the first RTB agent system based on large language models (LLMs),\nwhich synchronizes real competitive advertising bidding environments and\nobtains bidding prices through an integrated decision-making process.\nSpecifically, obtaining reasoning ability through LLMs, RTBAgent is further\ntailored to be more professional for RTB via involved auxiliary modules, i.e.,\nclick-through rate estimation model, expert strategy knowledge, and daily\nreflection. In addition, we propose a two-step decision-making process and\nmulti-memory retrieval mechanism, which enables RTBAgent to review historical\ndecisions and transaction records and subsequently make decisions more adaptive\nto market changes in real-time bidding. Empirical testing with real advertising\ndatasets demonstrates that RTBAgent significantly enhances profitability. The\nRTBAgent code will be publicly accessible at:\nhttps:\/\/github.com\/CaiLeng\/RTBAgent.",
        "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
        "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
        "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and\/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity\/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
        "Kinematics of rigid bodies can be analyzed in many different ways. The\nadvantage of using Euler parameters is that the resulting equations are\npolynomials and hence computational algebra, in particular Gr\\\"obner bases, can\nbe used to study them. The disadvantage of the Gr\\\"obner basis methods is that\nthe computational complexity grows quite fast in the worst case in the number\nof variables and the degree of polynomials. In the present article we show how\nto simplify computations when the mechanism contains revolute joints. The idea\nis based on the fact that the ideal representing the constraints of the\nrevolute joint is not prime. Choosing the appropriate prime component reduces\nsignificantly the computational cost. We illustrate the method by applying it\nto the well known Bennett's and Bricard's mechanisms, but it can be applied to\nany mechanism which has revolute joints.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
        "By overlaying time-synced user comments on videos, Danmu creates a\nco-watching experience for online viewers. However, its visual-centric design\nposes significant challenges for blind and low vision (BLV) viewers. Our\nformative study identified three primary challenges that hinder BLV viewers'\nengagement with Danmu: the lack of visual context, the speech interference\nbetween comments and videos, and the disorganization of comments. To address\nthese challenges, we present DanmuA11y, a system that makes Danmu accessible by\ntransforming it into multi-viewer audio discussions. DanmuA11y incorporates\nthree core features: (1) Augmenting Danmu with visual context, (2) Seamlessly\nintegrating Danmu into videos, and (3) Presenting Danmu via multi-viewer\ndiscussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y\nsignificantly improved Danmu comprehension, provided smooth viewing\nexperiences, and fostered social connections among viewers. We further\nhighlight implications for enhancing commentary accessibility in video-based\nsocial media and live-streaming platforms.",
        "We consider multi-dimensional payoff functions in Markov decision processes,\nand ask whether a given expected payoff vector can be achieved or not. In\ngeneral, pure strategies (i.e., not resorting to randomisation) do not suffice\nfor this problem.\n  We study the structure of the set of expected payoff vectors of all\nstrategies given a multi-dimensional payoff function and its consequences\nregarding randomisation requirements for strategies. In particular, we prove\nthat for any payoff for which the expectation is well-defined under all\nstrategies, it is sufficient to mix (i.e., randomly select a pure strategy at\nthe start of a play and committing to it for the rest of the play) finitely\nmany pure strategies to approximate any expected payoff vector up to any\nprecision. Furthermore, for any payoff for which the expected payoff is finite\nunder all strategies, any expected payoff can be obtained exactly by mixing\nfinitely many strategies.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We consider finite $n$-person deterministic graphical games and study the\nexistence of pure stationary Nash-equilibrium in such games. We assume that all\ninfinite plays are equivalent and form a unique outcome, while each terminal\nposition is a separate outcome. It is known that for $n=2$ such a game always\nhas a Nash equilibrium, while that may not be true for $n > 2$.\n  A game is called {\\em play-once} if each player controls a unique position\nand {\\em terminal} if any terminal outcome is better than the infinite one for\neach player. We prove in this paper that play-once games have Nash equilibria.\n  We also show that terminal games have Nash equilibria if they have at most\nthree terminals.",
        "We consider a teacher-student model of supervised learning with a\nfully-trained 2-layer neural network whose width $k$ and input dimension $d$\nare large and proportional. We compute the Bayes-optimal generalisation error\nof the network for any activation function in the regime where the number of\ntraining data $n$ scales quadratically with the input dimension, i.e., around\nthe interpolation threshold where the number of trainable parameters $kd+k$ and\nof data points $n$ are comparable. Our analysis tackles generic weight\ndistributions. Focusing on binary weights, we uncover a discontinuous phase\ntransition separating a \"universal\" phase from a \"specialisation\" phase. In the\nfirst, the generalisation error is independent of the weight distribution and\ndecays slowly with the sampling rate $n\/d^2$, with the student learning only\nsome non-linear combinations of the teacher weights. In the latter, the error\nis weight distribution-dependent and decays faster due to the alignment of the\nstudent towards the teacher network. We thus unveil the existence of a highly\npredictive solution near interpolation, which is however potentially hard to\nfind.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Despite significant advancements in video generation, inserting a given\nobject into videos remains a challenging task. The difficulty lies in\npreserving the appearance details of the reference object and accurately\nmodeling coherent motions at the same time. In this paper, we propose\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\ndetail preservation and precise motion control. Starting from a text-to-video\nmodel, we utilize an ID extractor to inject the global identity and leverage a\nbox sequence to control the overall motion. To preserve the detailed appearance\nand meanwhile support fine-grained motion control, we design a pixel warper. It\ntakes the reference image with arbitrary key-points and the corresponding\nkey-point trajectories as inputs. It warps the pixel details according to the\ntrajectories and fuses the warped features with the diffusion U-Net, thus\nimproving detail preservation and supporting users in manipulating the motion\ntrajectories. In addition, we propose a training strategy involving both videos\nand static images with a weighted loss to enhance insertion quality.\nVideoAnydoor demonstrates significant superiority over existing methods and\nnaturally supports various downstream applications (e.g., talking head\ngeneration, video virtual try-on, multi-region editing) without task-specific\nfine-tuning.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Crystal bases are powerful combinatorial tools in the representation theory\nof quantum groups $U_q(\\mathfrak{g})$ for a symmetrizable Kac-Moody algebras\n$\\mathfrak{g}$. The polyhedral realizations are combinatorial descriptions of\nthe crystal base $B(\\infty)$ for Verma modules in terms of the set of integer\npoints of a polyhedral cone, which equals the string cone when $\\mathfrak{g}$\nis finite dimensional simple. It is a fundamental and natural problem to find\nexplicit forms of the polyhedral cone. The monomial realization expresses\ncrystal bases $B(\\lambda)$ of integrable highest weight representations as\nLaurent monomials with double indexed variables. In this paper, we give a\nconjecture between explicit forms of the polyhedral cones and monomial\nrealizations. We prove the conjecture is true when $\\mathfrak{g}$ is a\nclassical Lie algebra, a rank $2$ Kac-Moody algebra or a classical affine Lie\nalgebra."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation",
    "start_abstract":"Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Deep learning-Based 3D inpainting of brain MR images"
      ],
      "abstract":[
        "Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI"
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Unlocking tropical forest complexity: How tree assemblages in secondary\n  forests boost biodiversity conservation",
        "From random walks to epidemic spreading: Compartment model with\n  mortality for vector transmitted diseases",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Multicellular self-organization in Escherichia coli",
        "Cytogenetic, Hematobiochemical, and Histopathological Assessment of\n  Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
        "On the Ising Phase Transition in the Infrared-Divergent Spin Boson Model",
        "Rotational decoherence dynamics in ultracold molecules induced by a\n  tunable spin environment: The Central Rotor Model",
        "Pseudorapidity density distributions of charged particles and transverse\n  momentum spectra of identified particles in pp collisions in PACIAE 4.0 model",
        "Laser-based aberration corrector",
        "Dissociated Neuronal Cultures as Model Systems for Self-Organized\n  Prediction",
        "Geometric origin of supercurrents in Berry phase: Formula for computing\n  currents from wavefunctions with correlation and particle number variation",
        "Center vortices and the $\\mathrm{SU}(3)$ conformal window",
        "High-aspect-ratio silica meta-optics for high-intensity structured light",
        "Reporting on pTP sublimation during evaporation deposition",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "An Approach to Use Depletion Charges for Modifying Band Profiles for\n  Field-Effect Transistors",
        "On the Gauge Invariance of Secondary Gravitational Waves",
        "A physical model approach to order lot sizing"
      ],
      "abstract":[
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Secondary forests now dominate tropical landscapes and play a crucial role in\nachieving COP15 conservation objectives. This study develops a replicable\nnational approach to identifying and characterising forest ecosystems, with a\nfocus on the role of secondary forests. We hypothesised that dominant tree\nspecies in the forest canopy serve as reliable indicators for delineating\nforest ecosystems and untangling biodiversity complexity. Using national\ninventories, we identified in situ clusters through hierarchical clustering\nbased on dominant species abundance dissimilarity, determined using the\nImportance Variable Index. These clusters were characterised by analysing\nspecies assemblages and their interactions. We then applied object-oriented\nRandom Forest modelling, segmenting the national forest cover using NDVI to\nidentify the forest ecosystems derived from in situ clusters. Freely available\nspectral (Sentinel-2) and environmental data were used in the model to\ndelineate and characterise key forest ecosystems. We finished with an\nassessment of distribution of secondary and old-growth forests within\necosystems. In Costa Rica, 495 dominant tree species defined 10 in situ\nclusters, with 7 main clusters successfully modelled. The modelling (F1-score:\n0.73, macro F1-score: 0.58) and species-based characterisation highlighted the\nmain ecological trends of these ecosystems, which are distinguished by specific\nspecies dominance, topography, climate, and vegetation dynamics, aligning with\nlocal forest classifications. The analysis of secondary forest distribution\nprovided an initial assessment of ecosystem vulnerability by evaluating their\nrole in forest maintenance and dynamics. This approach also underscored the\nmajor challenge of in situ data acquisition",
        "We focus on the propagation of vector-transmitted diseases in complex\nnetworks such as Barab\\'asi-Albert (BA) and Watts-Strogatz (WS) types. The\nclass of such diseases includes Malaria, Dengue (vectors are mosquitos),\nPestilence (vectors are fleas), and many others. There is no direct\ntransmission of the disease among individuals. Individuals are mimicked by\nindependent random walkers and the vectors by the nodes of the network. The\nwalkers and nodes can be either susceptible (S) or infected and infectious (I)\nrepresenting their states of health. Walkers in compartment I may die from the\ninfection (entering the dead compartment D) whereas infected nodes never die.\nThis assumption is based on the observation that vectors do not fall ill from\ntheir infection. A susceptible walker can be infected with a certain\nprobability by visiting an infected node, and a susceptible node by visits of\ninfected walkers. The time spans of infection of walkers and nodes as well as\nthe survival time span of infected walkers are assumed to be independent random\nvariables following specific probability density functions (PDFs). We implement\nthis approach into a multiple random walkers model. We establish macroscopic\nstochastic evolution equations for the mean-field compartmental population\nfractions and compare this dynamics with the outcome of the random walk\nsimulations. We obtain explicit expressions for the basic reproduction numbers\n$ R_M , R_0$ with and without mortality, respectively, and prove that $R_M <\nR_0$ . For $R_M , R_0 > 1$ the healthy state is unstable, and the disease is\nstarting to spread in presence of at least one infected walker or node. For\nzero mortality, we obtain in explicit form the stable endemic equilibrium which\nexists for $R_0 > 1$ and which is independent of the initial conditions.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Background: Literature shows that most of the information on the toxicity of\ngluten is generated from survey and observational studies, resulting in\ninconsistent outcomes and a decrease in the acceptability of gluten-rich foods.\nTo determine gluten's safety, an in-depth in vitro and in vivo toxicological\nexamination is required. This enables scientists to come up with ameliorative\nstrategies if it turns out to have side effects, and consumers' trust can be\nrestored. Objectives: The objective of this study was to assess the toxicity of\ngluten extracts on albino rats (Rattus norvegicus). Materials and Methods:\nTwenty-four rats were randomly selected and divided into four groups, each\ncomprising six rats. Group 1 (control) rats were fed on pellet feeds and groups\n2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts,\nrespectively. The rats' body weights and reactions were observed for 90 days\nbefore blood samples were collected for hematobiochemical and micronucleus\ntests. Histopathological examinations of the liver and kidneys were also\nperformed. Results: There was no difference (P > 0.05) in body weight, blood\nglucose level, or micronuclei between the control and treated rats. The\nlymphocytes, alkaline phosphatase, alanine transaminase, total protein, and\ncalcium ions of the test rats were all significantly (P < 0.05) altered but\nremained within the normal ranges. Other hematobiochemical parameters,\nincluding packed cell volume, hemoglobin, white and red blood cells, aspartate\ntransaminase, albumin, sodium ions, potassium ions, chloride ions, and urea,\nrevealed no marked changes. The treated rats' livers and kidneys showed no\nhistopathological changes. Conclusion: Gluten had no adverse effects. However,\nit altered hematobiochemical parameters, particularly the lymphocytes, alkaline\nphosphatase, alanine transaminase, total protein, and calcium ions.",
        "We prove absence of ground states in the infrared-divergent spin boson model\nat large coupling. Our key argument reduces the proof to verifying long range\norder in the dual one-dimensional continuum Ising model, i.e., to showing that\nthe respective two point function is lower bounded by a strictly positive\nconstant. We can then use known results from percolation theory to establish\nlong range order at large coupling. Combined with the known existence of ground\nstates at small coupling, our result proves that the spin boson model undergoes\na phase transition with respect to the coupling strength. We also present an\nexpansion for the vacuum overlap of the spin boson ground state in terms of the\nIsing $n$-point functions, which implies that the phase transition is unique,\ni.e., that there is a critical coupling constant below which a ground state\nexists and above which none can exist.",
        "We show that quantum rotational wavepacket dynamics in molecules can be\ndescribed by a new system-environment model, which consists of a rotational\nsubsystem coupled to a magnetically tunable spin bath formed by the nuclear\nspins within the molecule. The central rotor model shares similarities with the\nparadigmatic central spin model, but features much richer rotational dynamics\nthat is sensitive to the molecule's environment, which can be initiated and\nprobed with short laser pulses used to control molecular orientation and\nalignment. We present numerical simulations of the nuclear-spin-bath-induced\nrotational decoherence dynamics of KRb molecules, which exhibit remarkable\nsensitivity to an external magnetic field. Our results show that ultracold\nmolecular gases provide a natural platform for the experimental realization of\nthe CRM.",
        "The pseudorapidity density distributions of charged particles and the\ntransverse momentum spectra of identified particles in proton-proton (pp)\ncollisions at the center-of-mass energies ranging from $\\sqrt{s}=200$ GeV to 13\nTeV have been systematically studied using the newly released parton and\ncascade model PACIAE 4.0 based on PYTHIA 8.3. The available experimental data\nare well reproduced across all analyzed aspects. This theoretical method can be\neasily extended to anywhere the experimental data for pp collisions are\ncurrently unavailable. Furthermore, since pp collisions serve as the baseline\nfor heavy-ion collisions, our results can provide a valuable resource for both\nexperimentalists and theorists.",
        "Aberration correctors are essential elements for achieving atomic resolution\nin state-of-the-art electron microscopes. Conventional correctors are based on\na series of multipolar electron lenses, but more versatile alternatives are\nintensively sought. Here we suggest spatially tailored intense laser pulses as\none such alternative. Our simulations demonstrate that the free-space\nelectron-photon interaction can be used to compensate for spherical and\nchromatic aberrations of subsequent electron lenses. We show a significant\nimprovement in the simulated electron probe sizes and discuss the prospects of\nutilizing the tailored laser fields as a platform for novel electron optics in\nultrafast electron microscope setups.",
        "Dissociated neuronal cultures provide a simplified yet effective model system\nfor investigating self-organized prediction and information processing in\nneural networks. This review consolidates current research demonstrating that\nthese in vitro networks display fundamental computational capabilities,\nincluding predictive coding, adaptive learning, goal-directed behavior, and\ndeviance detection. We examine how these cultures develop critical dynamics\noptimized for information processing, detail the mechanisms underlying learning\nand memory formation, and explore the relevance of the free energy principle\nwithin these systems. Building on these insights, we discuss how findings from\ndissociated neuronal cultures inform the design of neuromorphic and reservoir\ncomputing architectures, with the potential to enhance energy efficiency and\nadaptive functionality in artificial intelligence. The reduced complexity of\nneuronal cultures allows for precise manipulation and systematic investigation,\nbridging theoretical frameworks with practical implementations in bio-inspired\ncomputing. Finally, we highlight promising future directions, emphasizing\nadvancements in three-dimensional culture techniques, multi-compartment models,\nand brain organoids that deepen our understanding of hierarchical and\npredictive processes in both biological and artificial systems. This review\naims to provide a comprehensive overview of how dissociated neuronal cultures\ncontribute to neuroscience and artificial intelligence, ultimately paving the\nway for biologically inspired computing solutions.",
        "The complexity of itinerant and many-body nature in Bardeen-Cooper-Schrieffer\n(BCS) wavefunctions has traditionally led to the use of coarse-grained order\nparameters for describing currents in superconductors (SC), rather than\ndirectly utilizing wavefunctions. In this work, we introduce a phase-based\nformula that enables the direct computation of currents from microscopic\nwavefunctions, accounting for correlation and particle number variations.\nInterestingly, the formulation draws parallels with insulators, suggesting a\nunified framework for understanding (intra-band) charge transport across two\nextremes of conductivity. A group velocity current\n$J_{band}{\\propto}\\frac{1}{\\hbar}{\\partial}_kE(k)$ is derived from Berry phase,\nindependent of wave package dynamics, robust against correlation. Additionally,\nwe identify a correlation-driven contribution, $J_{corr}$, which reveals that\nthe pairing correlations ${\\langle}c_kc_{-k}{\\rangle}$ among dancing partners\nprovide a current component beyond the velocity operator.",
        "A novel approach for estimating the lower end of the $\\mathrm{SU}(3)$\nconformal window is presented through the study of center vortex geometry and\nits dependence on the number of fermion flavors $N_f$. Values ranging from $N_f\n= 2$--$8$ are utilized to infer an upper limit for vortex behavior in the low\n$N_f$ phase, which may inform the transition to the conformal window. The\nsimulations are performed at a single lattice spacing and pion mass, both fixed\nfor all $N_f$. Visualizations of the center vortex structure in\nthree-dimensional slices of the lattice reveal a growing roughness in the\nvortex matter as a function of $N_f$, embodied by an increase in the density of\nvortex matter in the percolating cluster and a simultaneous reduction in\nsecondary clusters disconnected from the percolating cluster in 3D slices. This\nis quantified by various bulk properties, including the vortex and branching\npoint densities. A correlation of the vortex structure reveals a turning point\nnear $N_f \\simeq 5$ past which a randomness in the vortex field becomes the\ndominant aspect of its evolution with $N_f$. As a byproduct, extrapolations to\nthe vortex content of a uniform-random gauge field provide a critical point at\nwhich there must be a drastic shift in vacuum field structure. A precise\nestimate for the critical value is extracted as $N_f^* = 11.43(16)(17)$, close\nto various other estimates.",
        "Structured light and high-intensity ultrafast lasers are two rapidly\nadvancing frontiers in photonics, yet their intersection remains largely\nunexplored. While ultrafast lasers continue to push the boundaries of peak\nintensities, structured light has enabled unprecedented control over light's\nspatial, temporal, and polarization properties. However, the lack of robust\noptical devices capable of bridging structured light with the high-intensity\ndomain has constrained progress in combining these directions. Here, we\ndemonstrate high-aspect-ratio silica meta-optics, which close this gap by\ncombining silica's extraordinary damage resistance with the advanced phase and\npolarization control offered by metasurfaces. By leveraging anisotropic etching\ntechniques, we fabricate nanopillars exceeding 3 $\\mu$m in height with aspect\nratios up to 14, enabling precise manipulation of complex light fields at\nintensities far beyond the thresholds of conventional metasurfaces. We showcase\ntheir functionality in generating vortex beams and achieving polarization\nmanipulation with large phase retardance at challenging long-visible\nwavelengths. High-aspect-ratio silica meta-optics unlock structured\nlaser-matter interactions in extreme regimes, that will surpass plasma\nionization thresholds and enable applications such as relativistic particle\nacceleration and high-harmonic generation with structured beams, for both\ntabletop ultrafast systems and large-scale laser facilities.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "We present the study of using depletion charges for tailoring lateral band\nprofiles and applying it to the promising gate-all-around field-effect\ntransistors (GAAFET). Specifically, we introduce heavily p-type doped Si next\nto the channel, but outside the channel, of a transistor. They are connected to\nthe heavily n-type doped source and drain for generating the depletion charges.\nThe finite difference method was used for simulations and the results show\nsignificant modifications of the conduction band along the channel. The\ndepletion charges act as built-in electrodes capable of significantly modifying\nthe band profiles of field-effect transistors. Quantum confinement within the\nchannel has been attempted with different approaches, such as additional\nelectrodes and point contacts. The results presented show two aspects of this\napproach, namely, realizing quantum confinement in an all-Si structure and\ntailoring band profiles within channels to modify their transport properties.",
        "Second-order tensor perturbations induced by primordial fluctuations play a\ncrucial role in probing small-scale physics, but gauge dependence of their\nenergy density has remained a fundamental challenge in cosmological\nperturbation theory. We address this issue by introducing a boundary\ncondition-based filtering method that extracts physical radiation through the\nSommerfeld criterion. We demonstrate that after filtering non-physical modes,\nthe energy density of secondary gravitational waves becomes gauge-invariant and\nexhibits physically consistent behavior in the sub-horizon limit. This approach\nprovides a unified framework for both adiabatic and isocurvature perturbations,\nenhancing theoretical predictions and observational signatures of early\nuniverse physics.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Deep learning-Based 3D inpainting of brain MR images",
    "start_abstract":"Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation"
      ],
      "abstract":[
        "Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
        "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM",
        "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
        "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language\n  Models for Navigation Applications",
        "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic\n  Long-Context Reasoning Capabilities",
        "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
        "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical\n  Classification",
        "The Einstein Test: Towards a Practical Test of a Machine's Ability to\n  Exhibit Superintelligence",
        "Generating Causally Compliant Counterfactual Explanations using ASP",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Guided Code Generation with LLMs: A Multi-Agent Framework for Complex\n  Code Tasks",
        "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
        "Non-linear Partition of Unity method",
        "Linear statistics at the microscopic scale for the 2D Coulomb gas",
        "The role of effective mass and long-range interactions in the band-gap\n  renormalization of photo-excited semiconductors",
        "Machine-learning potentials for structurally and chemically complex MAB\n  phases: strain hardening and ripplocation-mediated plasticity",
        "On Elephant Random Walk with Random Memory",
        "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization",
        "Security and Quality in LLM-Generated Code: A Multi-Language,\n  Multi-Model Analysis",
        "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
        "Analysis of Information Loss on Composition Measurement in Stiff\n  Chemically Reacting Systems",
        "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning",
        "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education",
        "Non-ergodic Phase Transition in the Global Hysteresis of the Frustrated\n  Magnet DyRu2Si2",
        "Evaluation for Regression Analyses on Evolving Data Streams",
        "Many-body perturbation theory for moir\\'{e} systems"
      ],
      "abstract":[
        "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
        "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.",
        "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
        "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications.",
        "We present DocPuzzle, a rigorously constructed benchmark for evaluating\nlong-context reasoning capabilities in large language models (LLMs). This\nbenchmark comprises 100 expert-level QA problems requiring multi-step reasoning\nover long real-world documents. To ensure the task quality and complexity, we\nimplement a human-AI collaborative annotation-validation pipeline. DocPuzzle\nintroduces an innovative evaluation framework that mitigates guessing bias\nthrough checklist-guided process analysis, establishing new standards for\nassessing reasoning capacities in LLMs. Our evaluation results show that:\n1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and\nDeepSeek-R1(66.3%) significantly outperform best general instruct models like\nClaude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like\nDeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model,\nsuggesting challenges to maintain the generalization of reasoning capabilities\nrelying solely on distillation.",
        "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
        "Multi-level Hierarchical Classification (MLHC) tackles the challenge of\ncategorizing items within a complex, multi-layered class structure. However,\ntraditional MLHC classifiers often rely on a backbone model with independent\noutput layers, which tend to ignore the hierarchical relationships between\nclasses. This oversight can lead to inconsistent predictions that violate the\nunderlying taxonomy. Leveraging Large Language Models (LLMs), we propose a\nnovel taxonomy-embedded transitional LLM-agnostic framework for multimodality\nclassification. The cornerstone of this advancement is the ability of models to\nenforce consistency across hierarchical levels. Our evaluations on the MEP-3M\ndataset - a multi-modal e-commerce product dataset with various hierarchical\nlevels - demonstrated a significant performance improvement compared to\nconventional LLM structures.",
        "Creative and disruptive insights (CDIs), such as the development of the\ntheory of relativity, have punctuated human history, marking pivotal shifts in\nour intellectual trajectory. Recent advancements in artificial intelligence\n(AI) have sparked debates over whether state of the art models possess the\ncapacity to generate CDIs. We argue that the ability to create CDIs should be\nregarded as a significant feature of machine superintelligence (SI).To this\nend, we propose a practical test to evaluate whether an approach to AI\ntargeting SI can yield novel insights of this kind. We propose the Einstein\ntest: given the data available prior to the emergence of a known CDI, can an AI\nindependently reproduce that insight (or one that is formally equivalent)? By\nachieving such a milestone, a machine can be considered to at least match\nhumanity's past top intellectual achievements, and therefore to have the\npotential to surpass them.",
        "This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "Large Language Models (LLMs) have shown remarkable capabilities in code\ngeneration tasks, yet they face significant limitations in handling complex,\nlong-context programming challenges and demonstrating complex compositional\nreasoning abilities. This paper introduces a novel agentic framework for\n``guided code generation'' that tries to address these limitations through a\ndeliberately structured, fine-grained approach to code generation tasks. Our\nframework leverages LLMs' strengths as fuzzy searchers and approximate\ninformation retrievers while mitigating their weaknesses in long sequential\nreasoning and long-context understanding. Empirical evaluation using OpenAI's\nHumanEval benchmark with Meta's Llama 3.1 8B model (int4 precision)\ndemonstrates a 23.79\\% improvement in solution accuracy compared to direct\none-shot generation. Our results indicate that structured, guided approaches to\ncode generation can significantly enhance the practical utility of LLMs in\nsoftware development while overcoming their inherent limitations in\ncompositional reasoning and context handling.",
        "Based on analyzing the character of cascaded decoder architecture commonly\nadopted in existing DETR-like models, this paper proposes a new decoder\narchitecture. The cascaded decoder architecture constrains object queries to\nupdate in the cascaded direction, only enabling object queries to learn\nrelatively-limited information from image features. However, the challenges for\nobject detection in natural scenes (e.g., extremely-small, heavily-occluded,\nand confusingly mixed with the background) require an object detection model to\nfully utilize image features, which motivates us to propose a new decoder\narchitecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables\nobject queries to learn more comprehensive information, and our MI based model,\nMI-DETR, outperforms all existing DETR-like models on COCO benchmark under\ndifferent backbones and training epochs, achieving +2.3 AP and +0.6 AP\nimprovements compared to the most representative model DINO and SOTA model\nRelation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and\nvisualization experiments demonstrate the effectiveness, rationality, and\ninterpretability of MI.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We consider the classical Coulomb gas in two dimensions at the inverse\ntemperature $\\beta=2$, confined within a droplet of radius $R$ by a\nrotationally invariant potential $U(r)$. For $U(r)\\sim r^2$ this describes the\neigenvalues of the complex Ginibre ensemble of random matrices. We study linear\nstatistics of the form ${\\cal L}_N = \\sum_{i=1}^N f(|{\\bf x}_i|)$, where ${\\bf\nx}_i$'s are the positions of the $N$ particles, in the large $N$ limit with\n$R=O(1)$. It is known that for smooth functions $f(r)$ the variance ${\\rm Var}\n\\,{\\cal L}_N= O(1)$, while for an indicator function relevant for the disk\ncounting statistics, all cumulants of ${\\cal L}_N$ of order $q \\geq 2$ behave\nas $\\sim \\sqrt{N}$. In addition, for smooth functions, it was shown that the\ncumulants of ${\\cal L}_N$ of order $q \\geq 3$ scale as $\\sim N^{2-q}$.\nSurprisingly it was found that they depend only on $f'(|\\bf x|)$ and its\nderivatives evaluated exactly at the boundary of the droplet. To understand\nthis property, and interpolate between the two behaviors (smooth versus\nstep-like), we study the microscopic linear statistics given by $f(r) \\to\nf_N(r) = \\phi((r-\\hat r) \\sqrt{N}\/\\xi)$, which probes the fluctuations at the\nscale of the inter-particle distance. We compute the cumulants of ${\\cal L}_N$\nat large $N$ for a fixed $\\phi(u)$ at arbitrary $\\xi$. For large $\\xi$ they\nmatch the predictions for smooth functions which shows that the leading\ncontribution in that case comes from a boundary layer of size $1\/\\sqrt{N}$ near\nthe boundary of the droplet. Finally we show that the full probability\ndistribution of ${\\cal L}_N$ take two distinct large deviation forms, in the\nregime ${\\cal L}_N \\sim \\sqrt{N}$ and ${\\cal L}_N \\sim N$ respectively. We also\ndiscuss applications of our results to fermions in a rotating harmonic trap and\nto the Ginibre symplectic ensemble.",
        "Understanding how to control changes in electronic structure and related\ndynamical renormalizations by external driving fields is the key for\nunderstanding ultrafast spectroscopy and applications in electronics. Here we\nfocus on the band-gap's modulation by external electric fields and uncover the\neffect of band dispersion on the gap renormalization. We employ the Green's\nfunction formalism using the real-time Dyson expansion to account for dynamical\ncorrelations induced by photodoping. The many-body formalism captures the\ndynamics of systems with long-range interactions, carrier mobility, and\nvariable electron and hole effective mass. We also demonstrate that mean-field\nsimulations based on the Hartree-Fock Hamiltonian, which lacks dynamical\ncorrelations, yields a qualitatively incorrect picture of band-gap\nrenormalization. We find the trend that increasing effective mass, thus\ndecreasing mobility, leads to as much as a 6\\% enhancement in band-gap\nrenormalization. Further, the renormalization is strongly dependent on the\ndegree of photodoping. As the screening induced by free electrons and holes\neffectively reduces any long-range and interband interactions for highly\nexcited systems, we show that there is a specific turnover point with minimal\nband-gap. We further demonstrate that the optical gap renormalization follows\nthe same trend though its magnitude is altered by the Moss-Burstein effect.",
        "Though offering unprecedented pathways to molecular dynamics (MD) simulations\nof technologically-relevant materials and conditions, machine-learning\ninteratomic potentials (MLIPs) are typically trained for ``simple'' materials\nand properties with minor size effects. Our study of MAB phases (MABs) -\nalternating transition metal boride (MB) and group A element layers -\nexemplifies that MLIPs for complex materials can be fitted and used in a\nhigh-throughput fashion: for predicting structural and mechanical properties\nacross a large chemical\/phase\/temperature space. Considering group 4-6\ntransition metal based MABs, with A=Al and the 222, 212, and 314 type phases,\nthree MLIPs are trained and tested, including lattice and elastic constants\ncalculations at temperatures $T\\in\\{0,300,1200\\}$ K, extrapolation grade and\nenergy (force, stress) error analysis for $\\approx{3\\cdot10^6}$ ab initio MD\nsnapshots. Subsequently, nanoscale tensile tests serve to quantify upper limits\nof strength and toughness attainable in single-crystal MABs at 300~K as well as\ntheir temperature evolution. In-plane tensile deformation is characterised by\nrelatively high strength, {110}$\\langle001\\rangle$ type slipping, and failure\nby shear banding. The response to [001] loading is softer, triggers work\nhardening, and failure by kinking and layer delamination. Furthermore,\nW$_2$AlB$_2$ able to retard fracture via ripplocations and twinning from 300 up\nto 1200~K.",
        "In this paper, we introduce the elephant random walk (ERW) with memory\nconsisting of randomly selected steps from its history. It is a time-changed\nvariant of the standard elephant random walk with memory consisting of its full\nhistory. At each time point, the time changing component is the composition of\ntwo uniformly distributed independent random variables with support over all\nthe past steps. Several conditional distributional properties including the\nconditional mean increments and conditional displacement of ERW with random\nmemory are obtained. Using these conditional results, we derive the recursive\nand explicit expressions for the mean increments and mean displacement of the\nwalk.",
        "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings.",
        "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages.",
        "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
        "Gas sampling methods have been crucial for the advancement of combustion\nscience, enabling analysis of reaction kinetics and pollutant formation.\nHowever, the measured composition can deviate from the true one because of the\npotential residual reactions in the sampling probes. This study formulates the\ninitial composition estimation in stiff chemically reacting systems as a\nBayesian inference problem, solved using the No-U-Turn Sampler (NUTS).\nInformation loss arises from the restriction of system dynamics by low\ndimensional attracting manifold, where constrained evolution causes initial\nperturbations to decay or vanish in fast eigen-directions in composition space.\nThis study systematically investigates the initial value inference in\ncombustion systems and successfully validates the methodological framework in\nthe Robertson toy system and hydrogen autoignition. Furthermore, a gas sample\ncollected from a one-dimensional hydrogen diffusion flame is analyzed to\ninvestigate the effect of frozen temperature on information loss. The research\nhighlights the importance of species covariance information from observations\nin improving estimation accuracy and identifies how the rank reduction in the\nsensitivity matrix leads to inference failures. Critical failure times for\nspecies inference in the Robertson and hydrogen autoignition systems are\nanalyzed, providing insights into the limits of inference reliability and its\nphysical significance.",
        "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.",
        "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.",
        "Some frustrated magnets exhibit a huge hysteresis called \"global hysteresis\n(GH)\", where the magnetic plateaus appearing in the increasing field process\nare skipped in the decreasing field process from the high magnetic field state.\nIn this paper, we focused on the frustrated magnet DyRu2Si2 and measured\nmagnetization relaxations from two plateau states inside the GH loop, the\nphases III and IV, and investigated the phase transitions into them. As a\nresult of the relaxation measurements, no relaxation is observed in the phase\nIII, whereas long-time relaxations of more than 105 sec are observed at the\nphase IV plateau. Moreover, a Mpemba-effect-like relaxation phenomenon where\nthe relaxation from an initial state prepared in the zero-field-cooled\ncondition overtakes that from an initial state prepared in the field-cooled\ncondition is observed. These results indicate that the phase IV is the\nnon-ergodic state with a complex free-energy landscape with multiple local\nminima, while the phase III has a simple free energy structure. Therefore, the\nIII-IV phase transition is considered to be the ergodic to non-ergodic phase\ntransition. Although this type of phase transition typically occurs in random\nglassy systems, the phase IV in DyRu2Si2 has a regular long-range ordered\nmagnetic structure and yet exhibits non-ergodic properties, which is highly\nnontrivial. Our findings open the possibility of observing non-ergodic states\nin frustrated magnets with regular long-range orders.",
        "The paper explores the challenges of regression analysis in evolving data\nstreams, an area that remains relatively underexplored compared to\nclassification. We propose a standardized evaluation process for regression and\nprediction interval tasks in streaming contexts. Additionally, we introduce an\ninnovative drift simulation strategy capable of synthesizing various drift\ntypes, including the less-studied incremental drift. Comprehensive experiments\nwith state-of-the-art methods, conducted under the proposed process, validate\nthe effectiveness and robustness of our approach.",
        "Moir\\'{e} systems such as magic-angle twisted bilayer graphene have attracted\nsignificant attention due to their ability to host correlated phenomena\nincluding superconductivity and strongly correlated insulating states. By\ndefining the single-particle Green's function in the band basis, we\nsystematically develop a many-body perturbation theory framework to address\ncorrelations beyond the usual mean-field Hartree-Fock approaches. As a specific\nexample, we first analyze twisted bilayer graphene within the Hartree-Fock\napproximation. We derive analytical solutions for symmetry-breaking states at\ninteger fillings and the finite-temperature metal-insulator transition that\nclosely match previously known numerical results in the literature. Moving\nbeyond Hartree-Fock, we incorporate self-consistent GW corrections\ndemonstrating that first-order diagrams significantly overestimate the\nfilling-dependent fluctuations in the electronic compressibility. This\nframework provides a comprehensive pathway for exploring strong electronic\ncorrelations in moir\\'{e} systems beyond mean-field, giving new insights into\nthe interplay of symmetry breaking and electron correlations."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Advances and Open Problems in Federated Learning",
    "start_abstract":"The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "The future of digital health with federated learning"
      ],
      "abstract":[
        "Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Inverse problems with experiment-guided AlphaFold",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Reducing Size Bias in Sampling for Infectious Disease Spread on Networks",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Iterative phase retrieval algorithm for space-variant PSF in optical\n  systems with aberrations",
        "Network-Driven Global Stability Analysis: SVIRS Epidemic Model",
        "VAEs and GANs: Implicitly Approximating Complex Distributions with\n  Simple Base Distributions and Deep Neural Networks -- Principles, Necessity,\n  and Limitations",
        "The Equation of State of QCD up to very high temperatures",
        "Toward Large-Scale Distributed Quantum Long Short-Term Memory with\n  Modular Quantum Computers",
        "Domination Parameters of Graph Covers",
        "$L^2$-estimates on flat vector bundles and Pr\\'ekopa's theorem",
        "Unveiling potential candidates for rare-earth-free permanent magnet and\n  magnetocaloric effect applications: a high throughput screening in Fe-N\n  alloys",
        "QPEs as Lense-Thirring precession of super-Eddington flows",
        "On monotonicity of heat kernels: a new example and counterexamples",
        "3D Surface Reconstruction and Volume Approximation via the meshless\n  methods",
        "SOE's ESG Performance on Financial Flexibility: The Evidence from the\n  Hong Kong Stock Market",
        "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Epidemiological models can aid policymakers in reducing disease spread by\npredicting outcomes based on disease dynamics and contact network\ncharacteristics. Calibrating these models requires representative network\nsamples. In this connection, we investigate two sampling algorithms, Random\nWalk (RW), and Metropolis-Hastings Random Walk (MHRW), across three network\ntypes: Erd\\H{o}s-R\\'enyi (ER), Small-world (SW), and Scale-free (SF). Disease\ntransmission is simulated using a susceptible-infected-recovered (SIR)\nframework. Our findings show that RW overestimates infected individuals and\nsecondary infections by $25\\%$ for ER and SW networks due to size bias,\nfavouring highly connected nodes. MHRW, which corrects for size bias, provides\nestimates that are more consistent with the underlying network. Also, both\nmethods yield estimates significantly closer to the underlying network for\ntime-to-infection. However, sampling SF networks exhibits significant\nvariability, for both algorithms. Removing duplicate sampled nodes reduces\nMHRW's accuracy across all network types. We apply both algorithms to a cattle\nmovement network of $46,512$ farms, exhibiting ER, SW, and SF network features.\nRW overestimates infected farms by approximately $100\\%$ and secondary\ninfections by $>900\\%$, reflecting size bias whereas MHRW estimates align\nclosely with the cattle network dynamics. Time-to-infection estimates reveal\nthat RW underestimates by approximately $40\\%$, while MHRW slightly\noverestimates by $10\\%$. Estimates differ greatly when duplicate nodes are\nremoved. These findings underscore choosing algorithms based on network\nstructure and disease severity. RW's conservative estimates suit\nhigh-mortality, fast-spreading diseases, while MHRW provides precise\ninterventions suitable for less severe outbreaks. These insights can guide\npolicymakers in optimizing resource allocation and disease control strategies.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Iterative phase retrieval algorithms are widely used in digital optics for\ntheir efficiency and simplicity. Conventionally, these algorithms do not\nconsider aberrations as they assume an ideal, aberration-free optical system.\nHere, we propose modified iterative phase retrieval algorithms that take into\naccount the space-invariant and space-variant point spread function of the\noptical system.",
        "An epidemic Susceptible-Vaccinated-Infected-Removed-Susceptible (SVIRS) model\nis presented on a weighted-undirected network with graph Laplacian diffusion.\nDisease-free equilibrium always exists while the existence and uniqueness of\nendemic equilibrium have been shown. When the basic reproduction number is\nbelow unity, the disease-free equilibrium is asymptotically globally stable.\nThe endemic equilibrium is asymptotically globally stable if the basic\nreproduction number is above unity. Numerical analysis is illustrated with a\nroad graph of the state of Minnesota. The effect of all important model\nparameters has been discussed.",
        "This tutorial focuses on the fundamental architectures of Variational\nAutoencoders (VAE) and Generative Adversarial Networks (GAN), disregarding\ntheir numerous variations, to highlight their core principles. Both VAE and GAN\nutilize simple distributions, such as Gaussians, as a basis and leverage the\npowerful nonlinear transformation capabilities of neural networks to\napproximate arbitrarily complex distributions. The theoretical basis lies in\nthat a linear combination of multiple Gaussians can almost approximate any\nprobability distribution, while neural networks enable further refinement\nthrough nonlinear transformations. Both methods approximate complex data\ndistributions implicitly. This implicit approximation is crucial because\ndirectly modeling high-dimensional distributions explicitly is often\nintractable. However, the choice of a simple latent prior, while\ncomputationally convenient, introduces limitations. In VAEs, the fixed Gaussian\nprior forces the posterior distribution to align with it, potentially leading\nto loss of information and reduced expressiveness. This restriction affects\nboth the interpretability of the model and the quality of generated samples.",
        "We present the non-perturbative computation of the entropy density in QCD for\ntemperatures ranging from 3 GeV up to the electro-weak scale, using $N_f=3$\nflavours of massless O$(a)$-improved Wilson fermions. We adopt a new strategy\ndesigned to be computationally efficient and based on formulating thermal QCD\nin a moving reference frame, where the fields satisfy shifted boundary\nconditions in the temporal direction and periodic boundary conditions along the\nspatial ones. In this setup the entropy density can be computed as the\nderivative of the free-energy density with respect to the shift parameter. For\neach physical temperature, we perform Monte Carlo simulations at four values of\nthe lattice spacing in order to extrapolate the numerical data of the entropy\ndensity to the continuum limit. We achieve a final accuracy of approximatively\n$0.5$-$1.0\\%$ and our results are compared with predictions from\nhigh-temperature perturbation theory.",
        "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
        "A graph $G$ is a \\emph{cover} of a graph $F$ if there exists an onto mapping\n$\\pi : V(G) \\to V(F)$, called a (\\emph{covering}) \\emph{projection}, such that\n$\\pi$ maps the neighbours of any vertex $v$ in $G$ bijectively onto the\nneighbours of $\\pi(v)$ in $F$. This paper is the first attempt to study the\nconnection between domination parameters and graph covers. We focus on the\ndomination number, the total domination number, and the connected domination\nnumber. We prove tight upper bounds for the domination parameters of $G$.\nMoreover, we prove lower bounds for the domination parameters of $G$. Finally,\nwe propose a conjecture on the lower bound for the domination number of $G$ and\nprovide evidence to support the conjecture.",
        "In this paper, we will construct H\\\"ormander's $L^2$-estimate of the operator\n$d$ on a flat vector bundle over a $p$-convex Riemannian manifold and discuss\nsome geometric applications of it. In particular, we will generalize the\nclassical Pr\\'ekopa's theorem in convex analysis.",
        "Based on high-throughput density functional theory calculations, we have\nfound 49 ferromag-netic cases in FexN1-x (0<x<1) compounds, focusing especially\non permanent magnet and giant magnetocaloric effect applications. It is found\nthat 15 compounds are potential permanent mag-nets with a magneto-crystalline\nanisotropy energy more than 1 MJ\/m3, filling in the gap of appli-cation\nspectrum between high-performance and widely used permanents. Among the\npotential permanent magnets, Fe2N can be classified as a hard magnet while the\nother 14 compounds can be classified as semi-hard magnets. According to the\ncalculations of magnetic deformation proxy, 40 compounds are identified as\npotential giant magnetocaloric effect candidates. We suspect that Fe-N\ncompounds provide fine opportunities for applications in both rare-earth free\npermanent magnets and magnetocaloric effect.",
        "Quasi-periodic eruptions (QPEs) are a recently identified class of X-ray\ntransient associated with tidal disruption events by supermassive black holes,\nand for which there are multiple possible explanations. In this paper we\npresent a simple model which requires the black hole be spinning, be misaligned\nwith the accretion flow (both conditions of which are almost certainly met) and\nthat the accretion rate is a few times the Eddington limit. We speculate that\nthe resulting Lense-Thirring torques force the disc and entrained outflows to\nprecess, leading to increased X-ray flux when the wind-cone is oriented at\nlower inclinations to the observer. We test the range of parameters for which\nthis model could explain the period and brightness of the QPE events discovered\nthus far, and make qualitative comparisons between the observed X-ray spectra\nand lightcurves to those extracted from GR-RMHD simulations. Overall, we find\nsome areas of promising concordance, and identify challenges related to the\ndetails of current simulations.",
        "We discover a new, non-radial example of a manifold whose heat kernel\ndecreases monotonically along all minimal geodesics. We also classify the flat\ntori with this monotonicity property. Furthermore, we show that for a generic\nmetric on any smooth manifold the monotonicity property fails at large times.\nThis answers a recent question of Alonso-Or\\'an, Chamizo, Mart\\'inez, and Mas.",
        "In this paper, we propose several mathematical models for 3D surface\nreconstruction and volume estimation from a set of scattered cloud data. Three\nmeshless methods including the interpolation-based method by RBF, PDE-based\napproach by Kansa's method and the Method of Fundamental Solutions are employed\nand compared. For the optimal recovery of the surfaces, the selection of free\nparameters in related PDE models are further studied and analyzed. Besides,\nseveral criteria like distance are employed in above methods instead of the\nclassical parameter lambda determination strategy, which leads to a more\nreliable reconstruction performance. Finally, the volume estimation of 3D\nirregular objects is proposed based on the optimal reconstructed geometric\nmodels via proposed meshless methods. Numerous numerical examples are presented\nto demonstrate the effectiveness of the proposed surface reconstruction methods\nand the volume estimation strategy.",
        "As the global economic environment becomes increasingly unstable, enhancing\nfinancial flexibility to cope with risks has become the consensus of many\ncompanies. At the same time, environmental, social, and governance (ESG)\nperformance may be one of the effective ways. We studied the impact of a firm's\nESG performance on its financial flexibility with a sample of companies listed\non the Hong Kong stock market from 2018 to 2022. The empirical results show\nthat good environmental, social and governance performance can significantly\nimprove a firm's financial flexibility. In addition, this paper also finds that\nthe influence of ESG performance on financial flexibility is weak for\nstate-owned enterprises due to the influence of governance structure and market\ncharacteristics. Finally, the further analysis shows that there is a mediating\nrole played by financing constraints in this process. This study can provide\nbackground information for state-owned enterprises' governance, information\ndisclosure, and corporate operations. It also has guiding significance for\nrelevant investors, management and officials.",
        "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps:\/\/github.com\/LancelotXWX\/SWIFT."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"The future of digital health with federated learning",
    "start_abstract":"Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Advances and Open Problems in Federated Learning"
      ],
      "abstract":[
        "The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Sparse Binary Representation Learning for Knowledge Tracing",
        "Safety Representations for Safer Policy Learning",
        "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits",
        "The Role, Trends, and Applications of Machine Learning in Undersea\n  Communication: A Bangladesh Perspective",
        "Unifying Prediction and Explanation in Time-Series Transformers via\n  Shapley-based Pretraining",
        "Task Vector Quantization for Memory-Efficient Model Merging",
        "UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node\n  Classification",
        "PiKE: Adaptive Data Mixing for Multi-Task Learning Under Low Gradient\n  Conflicts",
        "COSMOS: Continuous Simplicial Neural Networks",
        "Structure based SAT dataset for analysing GNN generalisation",
        "Structural Entropy Guided Unsupervised Graph Out-Of-Distribution\n  Detection",
        "Metering Error Estimation of Fast-Charging Stations Using Charging Data\n  Analytics",
        "Discovering Directly-Follows Graph Model for Acyclic Processes",
        "The Impact of Artificial Intelligence on Emergency Medicine: A Review of\n  Recent Advances",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Physical Layer Design for Ambient IoT",
        "Nice and precise $K^*(892) \\to K\\pi$ branching fractions",
        "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
        "A plethora of long-range neutrino interactions probed by DUNE and T2HK",
        "SPRI: Aligning Large Language Models with Context-Situated Principles",
        "Discrete Markov Probabilistic Models",
        "Estimation of the generalized Laplace distribution and its projection\n  onto the circle",
        "Efficient Parallel Scheduling for Sparse Triangular Solvers",
        "Electron spin dynamics guide cell motility",
        "Towards More Trustworthy Deep Code Models by Enabling\n  Out-of-Distribution Detection",
        "Real-Time LiDAR Point Cloud Compression and Transmission for\n  Resource-constrained Robots"
      ],
      "abstract":[
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Knowledge tracing (KT) models aim to predict students' future performance\nbased on their historical interactions. Most existing KT models rely\nexclusively on human-defined knowledge concepts (KCs) associated with\nexercises. As a result, the effectiveness of these models is highly dependent\non the quality and completeness of the predefined KCs. Human errors in labeling\nand the cost of covering all potential underlying KCs can limit model\nperformance.\n  In this paper, we propose a KT model, Sparse Binary Representation KT\n(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can\naugment the predefined KCs to address the limitations of relying solely on\nhuman-defined KCs. These are learned through a binary vector representation,\nwhere each bit indicates the presence (one) or absence (zero) of an auxiliary\nKC. The resulting discrete representation allows these auxiliary KCs to be\nutilized in training any KT model that incorporates KCs. Unlike pre-trained\ndense embeddings, which are limited to models designed to accept such vectors,\nour discrete representations are compatible with both classical models, such as\nBayesian Knowledge Tracing (BKT), and modern deep learning approaches.\n  To generate this discrete representation, SBRKT employs a binarization method\nthat learns a sparse representation, fully trainable via stochastic gradient\ndescent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to\ncapture temporal dynamics and predict future student responses by effectively\ncombining the auxiliary and predefined KCs. Experimental results demonstrate\nthat SBRKT outperforms the tested baselines on several datasets and achieves\ncompetitive performance on others. Furthermore, incorporating the learned\nauxiliary KCs consistently enhances the performance of BKT across all tested\ndatasets.",
        "Reinforcement learning algorithms typically necessitate extensive exploration\nof the state space to find optimal policies. However, in safety-critical\napplications, the risks associated with such exploration can lead to\ncatastrophic consequences. Existing safe exploration methods attempt to\nmitigate this by imposing constraints, which often result in overly\nconservative behaviours and inefficient learning. Heavy penalties for early\nconstraint violations can trap agents in local optima, deterring exploration of\nrisky yet high-reward regions of the state space. To address this, we introduce\na method that explicitly learns state-conditioned safety representations. By\naugmenting the state features with these safety representations, our approach\nnaturally encourages safer exploration without being excessively cautious,\nresulting in more efficient and safer policy learning in safety-critical\nscenarios. Empirical evaluations across diverse environments show that our\nmethod significantly improves task performance while reducing constraint\nviolations during training, underscoring its effectiveness in balancing\nexploration with safety.",
        "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity.",
        "Harnessing large offline datasets is vital for training foundation models\nthat can generalize across diverse tasks. Offline Reinforcement Learning (RL)\noffers a powerful framework for these scenarios, enabling the derivation of\noptimal policies even from suboptimal data. The Prompting Decision Transformer\n(PDT) is an offline RL multi-task model that distinguishes tasks through\nstochastic trajectory prompts, which are task-specific tokens maintained in\ncontext during rollouts. However, PDT samples these tokens uniformly at random\nfrom per-task demonstration datasets, failing to account for differences in\ntoken informativeness and potentially leading to performance degradation. To\naddress this limitation, we introduce a scalable bandit-based prompt-tuning\nmethod that dynamically learns to construct high-performance trajectory\nprompts. Our approach significantly enhances downstream task performance\nwithout modifying the pre-trained Transformer backbone. Empirical results on\nbenchmark tasks and a newly designed multi-task environment demonstrate the\neffectiveness of our method, creating a seamless bridge between general\nmulti-task offline pre-training and task-specific online adaptation.",
        "The rapid evolution of machine learning (ML) has brought about groundbreaking\ndevelopments in numerous industries, not the least of which is in the area of\nundersea communication. This domain is critical for applications like ocean\nexploration, environmental monitoring, resource management, and national\nsecurity. Bangladesh, a maritime nation with abundant resources in the Bay of\nBengal, can harness the immense potential of ML to tackle the unprecedented\nchallenges associated with underwater communication. Beyond that, environmental\nconditions are unique to the region: in addition to signal attenuation,\nmultipath propagation, noise interference, and limited bandwidth. In this\nstudy, we address the necessity to bring ML into communication via undersea; it\ninvestigates the latest technologies under the domain of ML in that respect,\nsuch as deep learning and reinforcement learning, especially concentrating on\nBangladesh scenarios in the sense of implementation. This paper offers a\ncontextualized regional perspective by incorporating region-specific needs,\ncase studies, and recent research to propose a roadmap for deploying ML-driven\nsolutions to improve safety at sea, promote sustainable resource use, and\nenhance disaster response systems. This research ultimately highlights the\npromise of ML-powered solutions for transforming undersea communication,\nleading to more efficient and cost-effective technologies that subsequently\ncontribute to both economic growth and environmental sustainability.",
        "In this paper, we propose ShapTST, a framework that enables time-series\ntransformers to efficiently generate Shapley-value-based explanations alongside\npredictions in a single forward pass. Shapley values are widely used to\nevaluate the contribution of different time-steps and features in a test\nsample, and are commonly generated through repeatedly inferring on each sample\nwith different parts of information removed. Therefore, it requires expensive\ninference-time computations that occur at every request for model explanations.\nIn contrast, our framework unifies the explanation and prediction in training\nthrough a novel Shapley-based pre-training design, which eliminates the\nundesirable test-time computation and replaces it with a single-time\npre-training. Moreover, this specialized pre-training benefits the prediction\nperformance by making the transformer model more effectively weigh different\nfeatures and time-steps in the time-series, particularly improving the\nrobustness against data noise that is common to raw time-series data. We\nexperimentally validated our approach on eight public datasets, where our\ntime-series model achieved competitive results in both classification and\nregression tasks, while providing Shapley-based explanations similar to those\nobtained with post-hoc computation. Our work offers an efficient and\nexplainable solution for time-series analysis tasks in the safety-critical\napplications.",
        "Model merging enables efficient multi-task models by combining task-specific\nfine-tuned checkpoints. However, storing multiple task-specific checkpoints\nrequires significant memory, limiting scalability and restricting model merging\nto larger models and diverse tasks. In this paper, we propose quantizing task\nvectors (i.e., the difference between pre-trained and fine-tuned checkpoints)\ninstead of quantizing fine-tuned checkpoints. We observe that task vectors\nexhibit a narrow weight range, enabling low precision quantization (up to 4\nbit) within existing task vector merging frameworks. To further mitigate\nquantization errors within ultra-low bit precision (e.g., 2 bit), we introduce\nResidual Task Vector Quantization, which decomposes the task vector into a base\nvector and offset component. We allocate bits based on quantization\nsensitivity, ensuring precision while minimizing error within a memory budget.\nExperiments on image classification and dense prediction show our method\nmaintains or improves model merging performance while using only 8% of the\nmemory required for full-precision checkpoints.",
        "Graph-structured datasets often suffer from class imbalance, which\ncomplicates node classification tasks. In this work, we address this issue by\nfirst providing an upper bound on population risk for imbalanced transductive\nnode classification. We then propose a simple and novel algorithm,\nUncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels\nassigned to unlabeled nodes to mitigate the adverse effects of imbalance on\nclassification accuracy. Furthermore, the UPL algorithm enhances the accuracy\nof pseudo-labeling by reducing training noise of pseudo-labels through a novel\nuncertainty-aware approach. We comprehensively evaluate the UPL algorithm\nacross various benchmark datasets, demonstrating its superior performance\ncompared to existing state-of-the-art methods.",
        "Modern machine learning models are trained on diverse datasets and tasks to\nimprove generalization. A key challenge in multitask learning is determining\nthe optimal data mixing and sampling strategy across different data sources.\nPrior research in this multi-task learning setting has primarily focused on\nmitigating gradient conflicts between tasks. However, we observe that many\nreal-world multitask learning scenarios-such as multilingual training and\nmulti-domain learning in large foundation models-exhibit predominantly positive\ntask interactions with minimal or no gradient conflict. Building on this\ninsight, we introduce PiKE (Positive gradient interaction-based K-task weights\nEstimator), an adaptive data mixing algorithm that dynamically adjusts task\ncontributions throughout training. PiKE optimizes task sampling to minimize\noverall loss, effectively leveraging positive gradient interactions with almost\nno additional computational overhead. We establish theoretical convergence\nguarantees for PiKE and demonstrate its superiority over static and\nnon-adaptive mixing strategies. Additionally, we extend PiKE to promote fair\nlearning across tasks, ensuring balanced progress and preventing task\nunderrepresentation. Empirical evaluations on large-scale language model\npretraining show that PiKE consistently outperforms existing heuristic and\nstatic mixing strategies, leading to faster convergence and improved downstream\ntask performance.",
        "Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.",
        "Satisfiability (SAT) solvers based on techniques such as conflict driven\nclause learning (CDCL) have produced excellent performance on both synthetic\nand real world industrial problems. While these CDCL solvers only operate on a\nper-problem basis, graph neural network (GNN) based solvers bring new benefits\nto the field by allowing practitioners to exploit knowledge gained from solved\nproblems to expedite solving of new SAT problems. However, one specific area\nthat is often studied in the context of CDCL solvers, but largely overlooked in\nGNN solvers, is the relationship between graph theoretic measure of structure\nin SAT problems and the generalisation ability of GNN solvers. To bridge the\ngap between structural graph properties (e.g., modularity, self-similarity) and\nthe generalisability (or lack thereof) of GNN based SAT solvers, we present\nStructureSAT: a curated dataset, along with code to further generate novel\nexamples, containing a diverse set of SAT problems from well known problem\ndomains. Furthermore, we utilise a novel splitting method that focuses on\ndeconstructing the families into more detailed hierarchies based on their\nstructural properties. With the new dataset, we aim to help explain problematic\ngeneralisation in existing GNN SAT solvers by exploiting knowledge of\nstructural graph properties. We conclude with multiple future directions that\ncan help researchers in GNN based SAT solving develop more effective and\ngeneralisable SAT solvers.",
        "With the emerging of huge amount of unlabeled data, unsupervised\nout-of-distribution (OOD) detection is vital for ensuring the reliability of\ngraph neural networks (GNNs) by identifying OOD samples from in-distribution\n(ID) ones during testing, where encountering novel or unknown data is\ninevitable. Existing methods often suffer from compromised performance due to\nredundant information in graph structures, which impairs their ability to\neffectively differentiate between ID and OOD data. To address this challenge,\nwe propose SEGO, an unsupervised framework that integrates structural entropy\ninto OOD detection regarding graph classification. Specifically, within the\narchitecture of contrastive learning, SEGO introduces an anchor view in the\nform of coding tree by minimizing structural entropy. The obtained coding tree\neffectively removes redundant information from graphs while preserving\nessential structural information, enabling the capture of distinct graph\npatterns between ID and OOD samples. Furthermore, we present a multi-grained\ncontrastive learning scheme at local, global, and tree levels using triplet\nviews, where coding trees with essential information serve as the anchor view.\nExtensive experiments on real-world datasets validate the effectiveness of\nSEGO, demonstrating superior performance over state-of-the-art baselines in OOD\ndetection. Specifically, our method achieves the best performance on 9 out of\n10 dataset pairs, with an average improvement of 3.7\\% on OOD detection\ndatasets, significantly surpassing the best competitor by 10.8\\% on the\nFreeSolv\/ToxCast dataset pair.",
        "Accurate electric energy metering (EEM) of fast charging stations (FCSs),\nserving as critical infrastructure in the electric vehicle (EV) industry and as\nsignificant carriers of vehicle-to-grid (V2G) technology, is the cornerstone\nfor ensuring fair electric energy transactions. Traditional on-site\nverification methods, constrained by their high costs and low efficiency,\nstruggle to keep pace with the rapid global expansion of FCSs. In response,\nthis paper adopts a data-driven approach and proposes the measuring performance\ncomparison (MPC) method. By utilizing the estimation value of state-of-charge\n(SOC) as a medium, MPC establishes comparison chains of EEM performance of\nmultiple FCSs. Therefore, the estimation of EEM errors for FCSs with high\nefficiency is enabled. Moreover, this paper summarizes the interfering factors\nof estimation results and establishes corresponding error models and\nuncertainty models. Also, a method for discriminating whether there are EEM\nperformance defects in FCSs is proposed. Finally, the feasibility of MPC method\nis validated, with results indicating that for FCSs with an accuracy grade of\n2\\%, the discriminative accuracy exceeds 95\\%. The MPC provides a viable\napproach for the online monitoring of EEM performance for FCSs, laying a\nfoundation for a fair and just electricity trading market.",
        "Process mining is the common name for a range of methods and approaches aimed\nat analysing and improving processes. Specifically, methods that aim to derive\nprocess models from event logs fall under the category of process discovery.\nWithin the range of processes, acyclic processes form a distinct category. In\nsuch processes, previously performed actions are not repeated, forming chains\nof unique actions. However, due to differences in the order of actions,\nexisting process discovery methods can provide models containing cycles even if\na process is acyclic. This paper presents a new process discovery algorithm\nthat allows to discover acyclic DFG models for acyclic processes. A model is\ndiscovered by partitioning an event log into parts that provide acyclic DFG\nmodels and merging them while avoiding the formation of cycles. The resulting\nalgorithm was tested both on real-life and artificial event logs. Absence of\ncycles improves model visual clarity and precision, also allowing to apply\ncycle-sensitive methods or visualisations to the model.",
        "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "There is a growing demand for ultra low power and ultra low complexity\ndevices for applications which require maintenance-free and battery-less\noperation. One way to serve such applications is through backscatter devices,\nwhich communicate using energy harvested from ambient sources such as radio\nwaves transmitted by a reader. Traditional backscatter devices, such as RFID,\nare limited by range, interference, low connection density, and security\nissues. To address these problems, the Third Generation Partnership Project\n(3GPP) has started working on Ambient IoT (A-IoT). For the realization of A-IoT\ndevices, various aspects ranging from physical layer design, to the protocol\nstack, to the device architecture should be standardized. In this paper, we\nprovide an overview of the standardization efforts on the physical layer design\nfor A-IoT devices. The various physical channels and signals are discussed,\nfollowed by link level simulations to compare the performance of various\nconfigurations of reader to device and device to reader channels.",
        "Although discovered more than sixty years ago, direct measurement of the\n$K^*(892) \\to K\\pi$ branching fractions is a formidable challenge that has not\nbeen attempted. Typically they are assumed to obey the isospin limit in\nhundreds of particle data measurements. We show that an abundance of recent\namplitude analyses and other data, however, enables recovery of the ratios\n$\\mathcal{B}(K^{*+} \\to K^+ \\pi^0)\/\\mathcal{B}(K^{*+} \\to K_S^0 \\pi^+)$ and\n$4\\mathcal{B}(K^{*0} \\to K_S^0 \\pi^0)\/\\mathcal{B}(K^{*0} \\to K^+ \\pi^-)$ at\n$\\sim 5\\%$ precision.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
        "The next-generation neutrino oscillation experiments would be sensitive to\nthe new neutrino interactions that would strengthen the search for physics\nbeyond the Standard Model. In this context, we explore the capabilities of the\ntwo leading future long-baseline neutrino oscillation experiments, DUNE and\nT2HK, to search for new flavor-dependent neutrino interactions with electrons,\nprotons, and neutrons that could potentially modify neutrino flavor\ntransitions. We forecast their sensitivities in the context of long-range\nneutrino interactions mediated by a neutral vector boson lighter than\n$10^{-10}$ eV and sourced by the vast amount of nearby and distant matter in\nthe Earth, Moon, Sun, Milky Way, and local Universe. For the first time, we\nexplore a plethora of $U(1)^\\prime$ symmetries inducing the new interactions\nbuilt from the combination of lepton and baryon numbers. We find that in all\ncases, DUNE and T2HK may constrain or discover the existence of new long-range\nneutrino interaction, and in some favorable cases, may identify the new\n$U(1)^\\prime$ symmetry responsible for it. In this short proceeding, we only\nsummarize the prospects of constraining the new interaction in case of all our\ncandidate $U(1)^\\prime$ symmetries, which have been discussed in JHEP 09 (2024)\n055.",
        "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https:\/\/github.com\/honglizhan\/SPRI-public.",
        "This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel\nalgorithm for discrete data generation. The algorithm operates in the space of\nbits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain\nthat can be sampled exactly via a Poissonian clock that flips labels uniformly\nat random. The time-reversal process, like the forward noise process, is a jump\nprocess, with its intensity governed by a discrete analogue of the classical\nscore function. Crucially, this intensity is proven to be the conditional\nexpectation of a function of the forward process, strengthening its theoretical\nalignment with score-based generative models while ensuring robustness and\nefficiency. We further establish convergence bounds for the algorithm under\nminimal assumptions and demonstrate its effectiveness through experiments on\nlow-dimensional Bernoulli-distributed datasets and high-dimensional binary\nMNIST data. The results highlight its strong performance in generating discrete\nstructures. This work bridges theoretical foundations and practical\napplications, advancing the development of effective and theoretically grounded\ndiscrete generative modeling.",
        "The generalized Laplace (GL) distribution, which falls in the larger family\nof generalized hyperbolic distributions, provides a versatile model to deal\nwith a variety of applications thanks to its shape parameters. The elliptically\nsymmetric GL admits a polar representation that can be used to yield a circular\ndistribution, which we call \\emph{projected} GL distribution. The latter does\nnot appear to have been considered yet in practical applications. In this\narticle, we explore an easy-to-implement maximum likelihood estimation strategy\nbased on Gaussian quadrature for the scale-mixture representation of the GL and\nits projection onto the circle. A simulation study is carried out to benchmark\nthe fitting routine against alternative estimation methods to assess its\nfeasibility, while the projected GL model is contrasted with other popular\ncircular distributions.",
        "We develop and analyze new scheduling algorithms for solving sparse\ntriangular linear systems (SpTRSV) in parallel. Our approach, which we call\nbarrier list scheduling, produces highly efficient synchronous schedules for\nthe forward- and backward-substitution algorithm. Compared to state-of-the-art\nbaselines HDagg and SpMP, we achieve a $3.24\\times$ and $1.45\\times$\ngeometric-mean speed-up, respectively. We achieve this by obtaining an up to\n$11\\times$ geometric-mean reduction in the number of synchronization barriers\nover HDagg, whilst maintaining a balanced workload, and by applying a matrix\nreordering step for locality. We show that our improvements are consistent\nacross a variety of input matrices and hardware architectures.",
        "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin\/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
        "Numerous machine learning (ML) models have been developed, including those\nfor software engineering (SE) tasks, under the assumption that training and\ntesting data come from the same distribution. However, training and testing\ndistributions often differ, as training datasets rarely encompass the entire\ndistribution, while testing distribution tends to shift over time. Hence, when\nconfronted with out-of-distribution (OOD) instances that differ from the\ntraining data, a reliable and trustworthy SE ML model must be capable of\ndetecting them to either abstain from making predictions, or potentially\nforward these OODs to appropriate models handling other categories or tasks.\n  In this paper, we develop two types of SE-specific OOD detection models,\nunsupervised and weakly-supervised OOD detection for code. The unsupervised OOD\ndetection approach is trained solely on in-distribution samples while the\nweakly-supervised approach utilizes a tiny number of OOD samples to further\nenhance the detection performance in various OOD scenarios. Extensive\nexperimental results demonstrate that our proposed methods significantly\noutperform the baselines in detecting OOD samples from four different scenarios\nsimultaneously and also positively impact a main code understanding task.",
        "LiDARs are widely used in autonomous robots due to their ability to provide\naccurate environment structural information. However, the large size of point\nclouds poses challenges in terms of data storage and transmission. In this\npaper, we propose a novel point cloud compression and transmission framework\nfor resource-constrained robotic applications, called RCPCC. We iteratively fit\nthe surface of point clouds with a similar range value and eliminate redundancy\nthrough their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT)\nto transform the unfit points and reduce the data volume by quantizing the\ntransformed coefficients. We design an adaptive bitrate control strategy based\non QoE as the optimization goal to control the quality of the transmitted point\ncloud. Experiments show that our framework achieves compression rates of\n40$\\times$ to 80$\\times$ while maintaining high accuracy for downstream\napplications. our method significantly outperforms other baselines in terms of\naccuracy when the compression rate exceeds 70$\\times$. Furthermore, in\nsituations of reduced communication bandwidth, our adaptive bitrate control\nstrategy demonstrates significant QoE improvements. The code will be available\nat https:\/\/github.com\/HITSZ-NRSL\/RCPCC.git."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Meeting Strangers and Friends of Friends: How Random Are Social Networks?",
    "start_abstract":"We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)",
    "start_categories":[
      "q-fin.EC"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "An agent-based spatial urban social network generator: A case study of beijing, china"
      ],
      "abstract":[
        "This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective."
      ],
      "categories":[
        "cs.CE"
      ]
    },
    "list":{
      "title":[
        "Modular Photobioreactor Fa\\c{c}ade Systems for Sustainable Architecture:\n  Design, Fabrication, and Real-Time Monitoring",
        "DCAMamba: Mamba-based Rapid Response DC Arc Fault Detection",
        "Capturing Lifecycle System Degradation in Digital Twin Model Updating",
        "Data-Driven Discovery of Population Balance Equations for the\n  Particulate Sciences",
        "Applying Computational Engineering Modelling to Analyse the Social\n  Impact of Conflict and Violent Events",
        "Competitive algorithms for calculating the ground state properties of\n  Bose-Fermi mixtures",
        "The computation of average kernel with Gauss-Laguerre quadrature for\n  double integrals",
        "Assessment of ChatGPT for Engineering Statics Analysis",
        "How to introduce an initial crack in phase field simulations to\n  accurately predict the linear elastic fracture propagation threshold?",
        "Towards personalised assessment of abdominal aortic aneurysm structural\n  integrity",
        "Isogeometric Analysis for 2D Magnetostatic Computations with Multi-level\n  B\\'{e}zier Extraction for Local Refinement",
        "Multiphysics Continuous Shape Optimization of the TAP Reactor Components",
        "A Coupled PFEM-DEM Model for Fluid-Granular Flows with Free-Surface\n  Dynamics Applied to Landslides",
        "Ferroelectric Properties of van der Waals Chalcogenides: DFT perspective",
        "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
        "Contact process for the spread of knowledge",
        "Site-Decorated Model for Unconventional Frustrated Magnets: Ultranarrow\n  Phase Crossover and Spin Reversal Transition",
        "Path-dependency and emergent computing under vectorial driving",
        "Enhancing the De-identification of Personally Identifiable Information\n  in Educational Data",
        "Multi-compartment diffusion-relaxation MR signal representation in the\n  spherical 3D-SHORE basis",
        "Advancing ATLAS DCS Data Analysis with a Modern Data Platform",
        "Non-linear Partition of Unity method",
        "Network fault costs based on minimum leaf spanning trees",
        "Study of long-term spectral evolution and X-ray and Gamma-ray\n  correlation of blazars seen by HAWC",
        "Optimized Relay Lens Design For High-Resolution Image Transmission In\n  Military Target Detection Systems",
        "Reinforcement Learning in Strategy-Based and Atari Games: A Review of\n  Google DeepMinds Innovations",
        "Detection of Somali-written Fake News and Toxic Messages on the Social\n  Media Using Transformer-based Language Models",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)"
      ],
      "abstract":[
        "This paper proposes an innovative solution to the growing issue of greenhouse\ngas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate\ngreenhouse gas (GHG) concentrations. With digital fabrication technology, this\nstudy explores the transition from traditional, single function building\nfacades to multifunctional, integrated building systems. It introduces a\nphotobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG)\nconcentrations while addressing the challenge of large-scale prefabricated\ncomponents transportation. This research introduces a novel approach by\ndesigning the fa\\c{c}ade system as modular, user-friendly and\ntransportation-friendly bricks, enabling the creation of a user-customized and\nself-assembled photobioreactor (PBR) system. The single module in the system is\nproposed to be \"neutralization bricks\", which embedded with algae and equipped\nwith an air circulation system, facilitating the photobioreactor (PBR)'s\nfunctionality. A connection system between modules allows for easy assembly by\nusers, while a limited variety of brick styles ensures modularity in\nmanufacturing without sacrificing customization and diversity. The system is\nalso equipped with an advanced microalgae status detection algorithm, which\nallows users to monitor the condition of the microalgae using monocular camera.\nThis functionality ensures timely alerts and notifications for users to replace\nthe algae, thereby optimizing the operational efficiency and sustainability of\nthe algae cultivation process.",
        "In electrical equipment, even minor contact issues can lead to arc faults.\nTraditional methods often struggle to balance the accuracy and rapid response\nrequired for effective arc fault detection. To address this challenge, we\nintroduce DCAMamba, a novel framework for arc fault detection. Specifically,\nDCAMamba is built upon a state-space model (SSM) and utilizes a hardware-aware\nparallel algorithm, designed in a cyclic mode using the Mamba architecture. To\nmeet the dual demands of high accuracy and fast response in arc fault\ndetection, we have refined the original Mamba model and incorporated a Feature\nAmplification Strategy (FAS), a simple yet effective method that enhances the\nmodel's ability to interpret arc fault data. Experimental results show that\nDCAMamba, with FAS, achieves a 12$\\%$ improvement in accuracy over the original\nMamba, while maintaining an inference time of only 1.87 milliseconds. These\nresults highlight the significant potential of DCAMamba as a future backbone\nfor signal processing. Our code will be made open-source after peer review.",
        "Digital twin (DT) has emerged as a powerful tool to facilitate monitoring,\ncontrol, and other decision-making tasks in real-world engineering systems.\nOnline update methods have been proposed to update DT models. Considering the\ndegradation behavior in the system lifecycle, these methods fail to enable DT\nmodels to predict the system responses affected by the system degradation over\ntime. To alleviate this problem, degradation models of measurable parameters\nhave been integrated into DT construction. However, identifying the degradation\nparameters relies on prior knowledge of the system and expensive experiments.\nTo mitigate those limitations, this paper proposes a lifelong update method for\nDT models to capture the effects of system degradation on system responses\nwithout any prior knowledge and expensive offline experiments on the system.\nThe core idea in the work is to represent the system degradation during the\nlifecycle as the dynamic changes of DT configurations (i.e., model parameters\nwith a fixed model structure) at all degradation stages. During the lifelong\nupdate process, an Autoencoder is adopted to reconstruct the model parameters\nof all hidden layers simultaneously, so that the latent features taking into\naccount the dependencies among hidden layers are obtained for each degradation\nstage. The dynamic behavior of latent features among successive degradation\nstages is then captured by a long short-term memory model, which enables\nprediction of the latent feature at any unseen stage. Based on the predicted\nlatent features, the model configuration at future degradation stage is\nreconstructed to determine the new DT model, which predicts the system\nresponses affected by the degradation at the same stage. The test results on\ntwo engineering datasets demonstrate that the proposed update method could\ncapture effects of system degradation on system responses during the lifecycle.",
        "Understanding the behavior of particles in a dispersed phase system via\npopulation balances holds fundamental importance in studies of particulate\nsciences across various fields. Particle behavior, however, is sophisticated as\na single particle can undergo internal property changes (e.g., size, cell age,\nand energy content) through various mechanisms. When confronted with an unknown\ndistributed particulate system, discovering the underlying population balance\nequation (PBE) entails firstly learning the underlying particulate phenomena\nfollowed by the associated phenomenological laws that govern the kinetics and\nmechanisms of particle transformations in their local conditions. Conventional\ninverse problem approaches reveal the shape of phenomenological functions for\npredetermined forms of PBE (e.g., pure breakage\/aggregation PBE, etc.).\nHowever, these methods can be limited in their ability to uncover the\nmechanisms which govern uncharacterized particulate systems from data.\nLeveraging the increasing abundance of data, we devise a data-driven framework\nbased on sparse regression to learn PBEs as linear combinations of an extensive\npool of candidate terms. Thus, this approach enables effective and accurate\nfunctional identification of PBEs without assuming the structure a priori,\nhence mitigating any potential loss of details, while minimizing model\noverfitting and providing a more interpretable representation of particulate\nsystems. We showcase the proficiency of our approach across a wide spectrum of\nparticulate systems, ranging from simple canonical pure breakage and pure\naggregation systems to complex systems with multiple particulate processes. Our\napproach holds the potential to generalize the discovery of PBEs along with\ntheir phenomenological laws from data, thus facilitating wider adoption of\npopulation balances.",
        "This thesis presents a novel framework for analysing the societal impacts of\narmed conflict by applying principles from engineering and material science.\nBuilding on the idea of a \"social fabric\", it recasts communities as plates\nwith properties, such as resilience and vulnerability, analogous to material\nparameters like thickness or elasticity. Conflict events are treated as\nexternal forces that deform this fabric, revealing how repeated shocks and\nlocal weaknesses can compound over time. Using a custom Python-based Finite\nElement Analysis implementation, the thesis demonstrates how data on\nsocioeconomic indicators (e.g., infrastructure, health, and demographics) and\nconflict incidents can be translated into a single computational model.\nPreliminary tests validate that results align with expected physical\nbehaviours, and a proof-of-concept highlights how this approach can capture\nindirect or spillover effects and illuminate the areas most at risk of\nlong-term harm. By bridging social science insights with computational\nmodelling, this work offers an adaptable frame to inform both academic research\nand on-the-ground policy decisions for communities affected by violence.",
        "In this work we define, analyze, and compare different numerical schemes that\ncan be used to study the ground state properties of Bose-Fermi systems, such as\nmixtures of different atomic species under external forces or self-bound\nquantum droplets. The bosonic atoms are assumed to be condensed and are\ndescribed by the generalized Gross-Pitaevskii equation. The fermionic atoms, on\nthe other hand, are treated individually, and each atom is associated with a\nwave function whose evolution follows the Hartree-Fock equation. We solve such\na formulated set of equations using a variety of methods, including those based\non adiabatic switching of interactions and the imaginary time propagation\ntechnique combined with the Gram-Schmidt orthonormalization or the\ndiagonalization of the Hamiltonian matrix. We show how different algorithms\ncompete at the numerical level by studying the mixture in the range of\nparameters covering the formation of self-bound quantum Bose-Fermi droplets.",
        "The use of average kernel method based on the Laplace transformation can\nsignificantly simplify the procedure for obtaining approximate analytical\nsolution of Smoluchowski equation. However, this method also has its own\nshortcomings, one of which is the higher computational complexity of the binary\nLaplace transformation for a nonlinear collision kernel. In this study, a\nuniversal algorithm based on the Gauss-Laguerre quadrature for treating the\ndouble integral is developed to obtain easily and quickly pre-exponential\nfactor of the average kernel. Furthermore, the corresponding truncation error\nestimate also provided.",
        "Large language models (LLMs) such as OpenAI's ChatGPT hold potential for\nautomating engineering analysis, yet their reliability in solving multi-step\nstatics problems remains uncertain. This study evaluates the performance of\nChatGPT-4o and ChatGPT-o1-preview on foundational statics tasks, from simple\ncalculations of Newton's second law of motion to beam and truss analyses and\ncompares their results to first-year engineering students on a typical statics\nexam. To enhance accuracy, we developed a Custom GPT, embedding refined prompts\ndirectly into its instructions. This optimized model achieved an 82% score,\nsurpassing the 75% student average, demonstrating the impact of tailored\nguidance. Despite these improvements, LLMs continued to exhibit errors in\nnuanced or open-ended problems, such as misidentifying tension and compression\nin truss members. These findings highlight both the promise and current\nlimitations of AI in structural analysis, emphasizing the need for improved\nreasoning, multimodal capabilities, and targeted training data for future\nAI-driven automation in civil and mechanical engineering.",
        "Variational phase field fracture models are now widely used to simulate crack\npropagation in structures. A critical aspect of these simulations is the\ncorrect determination of the propagation threshold of pre-existing cracks, as\nit highly relies on how the initial cracks are implemented. While prior studies\nbriefly discuss initial crack implementation techniques, we present here a\nsystematic investigation. Various techniques to introduce initial cracks in\nphase field fracture simulations are tested, from the crack explicit meshing to\nthe replacement by a fully damaged phase field, including different variants\nfor the boundary conditions. Our focus here is on phase field models aiming to\napproximate, in the $\\Gamma$-convergence limit, Griffith quasi-static\npropagation in the framework of Linear Elastic Fracture Mechanics. Therefore, a\nsharp crack model from classic linear elastic fracture mechanics based on\nGriffith criterion is the reference in this work. To assess the different\ntechniques to introduce initial cracks, we rely on path-following methods to\ncompute the sharp crack and the phase field smeared crack solutions. The\nunderlying idea is that path-following ensures staying at equilibrium at each\ninstant so that any difference between phase field and sharp crack models can\nbe attributed to numerical artifacts. Thus, by comparing the results from both\nmodels, we can provide practical recommendations for reliably incorporating\ninitial cracks in phase field fracture simulations. The comparison shows that\nan improper initial crack implementation often requires the smeared crack to\ntransition to a one-element-wide phase band to adequately represent a\ndisplacement jump along a crack. This transition increases the energy required\nto propagate the crack, leading to a significant overshoot in the\nforce-displacement response. The take-home message is that to predict the\npropagation threshold accurately and avoid artificial toughening; the crack\nmust be initialized either setting the phase field to its damage state over a\none-element-wide band or meshing the crack explicitly as a one-element-wide\nslit and imposing the fully cracked state on the crack surface.",
        "Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the\npermanent dilation of the aorta, often detected incidentally through imaging\nfor some other condition. The standard clinical approach to managing AAA\nfollows a one-size-fits-all model based on aneurysm size and growth rate,\nleading to underestimation or overestimation of rupture risk in individual\npatients. The widely studied stress-based rupture risk estimation using\ncomputational biomechanics requires wall strength information. However,\nnon-invasive methods for local patient-specific wall strength measurement have\nnot yet been developed. Recently, we introduced an image-based approach for\npatient-specific, in vivo, non-invasive AAA kinematic analysis using\ntime-resolved 3D computed tomography angiography (4D-CTA) images to measure\nwall strain throughout the cardiac cycle. In the present study, we integrated\nwall tension computation and strain measurement to develop a novel measure of\nlocal structural integrity of AAA wall - Relative Structural Integrity Index\n(RSII), independent of material properties and thickness of the wall and\nconditions of blood pressure measurement. Our methods provide a visual map of\nAAA wall structural integrity for individual patients using only their medical\nimages and blood pressure data. We applied our methods to twelve patients.\nAdditionally, we compared our measure of structural integrity of aneurysmal and\nnon-aneurysmal aortas. Our results show similar values of the wall structural\nintegrity measure across the patients, indicating the reliability of our\nmethods. In line with experimental observations reported in the literature, our\nanalysis revealed that localized low stiffness areas are primarily found in the\nmost dilated AAA regions. Our results clearly demonstrate that the AAA wall is\nstiffer than the non-aneurysmal aorta.",
        "Local refinement is vital for efficient numerical simulations. In the context\nof Isogeometric Analysis (IGA), hierarchical B-splines have gained prominence.\nThe work applies the methodology of truncated hierarchical B-splines\n(THB-splines) as they keep additional properties. The framework is further\nenriched with B\\'{e}zier extraction, resulting in the multi-level B\\'{e}zier\nextraction method. We apply this discretization method to 2D magnetostatic\nproblems. The implementation is based on an open-source Octave\/MATLAB IGA code\ncalled GeoPDEs, which allows us to compare our routines with globally refined\nspline models as well as locally refined ones where the solver does not rely on\nB\\'{e}zier extraction.",
        "The Transatomic Power (TAP) reactor has an unusual design for a molten salt\nreactor technology, building upon the foundation laid by the Molten Salt\nReactor Experiment (MSRE). This design introduces three key modifications to\nenhance efficiency and compactness: a revised fuel salt composition, an\nalternative moderator material, and moderator pins surrounded by the molten\nsalt fuel. Unlike traditional solid-fueled reactors that rely on excess\npositive reactivity at the beginning of life, the TAP concept employs a dynamic\napproach. The core's design, featuring a cylindrical geometry with square\nassemblies of moderator rods surrounded by flowing fuel salt, provides\nflexibility in adjusting the moderator-to-fuel ratio during operation - using\nmovable moderator rods - further adding criticality control capability in\naddition to the control rods system. Shape optimization of the core can play a\ncrucial role in enhancing performance and efficiency. By applying multiphysics\ncontinuous shape optimization techniques to key components, such as the unit\ncells of the TAP reactor or its moderator assemblies, we can fine-tune the\nreactor's geometry to achieve optimal performance in key physics like\nneutronics and thermal hydraulics. We explore this aspect using the\noptimization module in the Multiphysics Object Oriented Simulation Environment\n(MOOSE) framework which allows for multiphysics continuous shape optimization.\nThe results reported here illustrate the benefits of applying continuous shape\noptimization in the design of nuclear reactor components and can help in\nextending the TAP reactor's performance.",
        "Free surface and granular fluid mechanics problems combine the challenges of\nfluid dynamics with aspects of granular behaviour. This type of problem is\nparticularly relevant in contexts such as the flow of sediments in rivers, the\nmovement of granular soils in reservoirs, or the interactions between a fluid\nand granular materials in industrial processes such as silos. The numerical\nsimulation of these phenomena is challenging because the solution depends not\nonly on the multiple phases that strongly interact with each other, but also on\nthe need to describe the geometric evolution of the different interfaces. This\npaper presents an approach to the simulation of fluid-granular phenomena\ninvolving strongly deforming free surfaces. The Discrete Element Method (DEM)\nis combined with the Particle Finite Element Method (PFEM) and the fluid-grain\ninterface is treated by a two-way coupling between the two phases. The\nfluid-air interface is solved by a free surface model. The geometric and\ntopological variations are therefore naturally provided by the full Lagrangian\ndescription of all phases. The approach is validated on benchmark test cases\nsuch as two-phase dam failures and then applied to a real landslide problem.",
        "Layered materials with non-centrosymmetric stacking order are attracting\nincreasing interest due to the presence of ferroelectric polarization, which is\ndictated by weak interlayer hybridization of atomic orbitals. Here, we use\ndensity functional theory modelling to systematically build a library of van\nder Waals chalcogenides that exhibit substantial ferroelectric polarization.\nFor the most promising materials, we also analyse the pressure dependence of\nthe ferroelectric effect and charge accumulation of photo-induced electrons and\nholes at surfaces and internal twin boundaries in thin films of such materials.",
        "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), tensor processing units (TPUs), and\nfield-programmable gate arrays (FPGAs). The goal is to inspire further research\nwith a contemporary guide on optimizing ViTs for efficient deployment on edge\ndevices.",
        "This paper is concerned with a natural variant of the contact process\nmodeling the spread of knowledge on the integer lattice. Each site is\ncharacterized by its knowledge, measured by a real number ranging from 0 =\nignorant to 1 = omniscient. Neighbors interact at rate $\\lambda$, which results\nin both neighbors attempting to teach each other a fraction $\\mu$ of their\nknowledge, and individuals die at rate one, which results in a new individual\nwith no knowledge. Starting with a single omniscient site, our objective is to\nstudy whether the total amount of knowledge on the lattice converges to zero\n(extinction) or remains bounded away from zero (survival). The process dies out\nwhen $\\lambda \\leq \\lambda_c$ and\/or $\\mu = 0$, where $\\lambda_c$ denotes the\ncritical value of the contact process. In contrast, we prove that, for all\n$\\lambda > \\lambda_c$, there is a unique phase transition in the direction of\n$\\mu$, and for all $\\mu > 0$, there is a unique phase transition in the\ndirection of $\\lambda$. Our proof of survival relies on block constructions\nshowing more generally convergence of the knowledge to infinity, while our\nproof of extinction relies on martingale techniques showing more generally an\nexponential decay of the knowledge.",
        "The site-decorated Ising model is introduced to advance the understanding and\nexperimental realization of the recently discovered one-dimensional\nfinite-temperature ultranarrow phase crossover in an external magnetic field,\nwhile mitigating the geometric complexities of traditional bond-decorated\nmodels. Furthermore, although higher-dimensional Ising models in an external\nfield remain unsolved, an exact solution for a novel spin-reversal transition\n-- driven by an exotic, hidden ``half-ice, half-fire'' state induced by site\ndecoration -- is derived. This transition, triggered by a slight variation in\ntemperature or magnetic field even in the weak-field limit, offers a promising\nroute toward energy-efficient applications such as data storage and processing.\nThe results establish site decoration as a compelling new avenue for materials\nand device design, particularly in systems such as mixed $d$-$f$ compounds,\noptical lattices, and neural networks.",
        "The sequential response of frustrated materials - ranging from crumpled\nsheets and amorphous media to metamaterials - reveals their memory effects and\nemergent computational potential. Despite their spatial extension, most studies\nrely on a single global stimulus, such as compression, effectively reducing the\nproblem to scalar driving. Here, we introduce vectorial driving by applying\nmultiple spatially localized stimuli to explore path-dependent, sequential\nresponses. We uncover a wealth of phenomena absent in scalar driving, including\nnon-Abelian responses, mixed-mode behavior, and chiral loop transients. We find\nthat such path dependencies arise from elementary motifs linked to fold\nsingularities, which connect triplets of states - ancestor, descendant, and\nsibling; and develop a general framework using pt-graphs to describe responses\nunder any vectorial driving protocol. Leveraging binarized vectorial driving,\nwe establish a natural connection to computation, showing that a single sample\ncan encode multiple sequential Boolean circuits, which are selectable by\ndriving strength and reprogrammable via additional inputs. Finally, we\nintroduce graph-based motifs to manage the complexity of high-dimensional\ndriving. Our work paves the way for strategies to explore, harness, and\nunderstand complex materials and memory, while advancing embodied intelligence\nand in-materia computing.",
        "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https:\/\/github.com\/AnonJD\/PrivacyAI",
        "Modelling the diffusion-relaxation magnetic resonance (MR) signal obtained\nfrom multi-parametric sequences has recently gained immense interest in the\ncommunity due to new techniques significantly reducing data acquisition time. A\npreferred approach for examining the diffusion-relaxation MR data is to follow\nthe continuum modelling principle that employs kernels to represent the tissue\nfeatures, such as the relaxations or diffusion properties. However,\nconstructing reasonable dictionaries with predefined signal components depends\non the sampling density of model parameter space, thus leading to a geometrical\nincrease in the number of atoms per extra tissue parameter considered in the\nmodel. That makes estimating the contributions from each atom in the signal\nchallenging, especially considering diffusion features beyond the\nmono-exponential decay.\n  This paper presents a new Multi-Compartment diffusion-relaxation MR signal\nrepresentation based on the Simple Harmonic Oscillator-based Reconstruction and\nEstimation (MC-SHORE) representation, compatible with scattered acquisitions.\nThe proposed technique imposes sparsity constraint on the solution via the\n$\\ell_1$ norm and enables the estimation of the microstructural measures, such\nas the return-to-the-origin probability, and the orientation distribution\nfunction, depending on the compartments considered in a single voxel. The\nprocedure has been verified with in silico and in vivo data and enabled the\napproximation of the diffusion-relaxation MR signal more accurately than\nsingle-compartment non-Gaussian representations and multi-compartment\nmono-exponential decay techniques, maintaining a low number of atoms in the\ndictionary. Ultimately, the MC-SHORE procedure allows for separating\nintra-\/extra-axonal and free water contributions from the signal, thus reducing\nthe partial volume effect observable in the boundaries of the tissues.",
        "This paper presents a modern and scalable framework for analyzing Detector\nControl System (DCS) data from the ATLAS experiment at CERN. The DCS data,\nstored in an Oracle database via the WinCC OA system, is optimized for\ntransactional operations, posing challenges for large-scale analysis across\nextensive time periods and devices. To address these limitations, we developed\na data pipeline using Apache Spark, CERN's Hadoop service, and the CERN SWAN\nplatform. This framework integrates seamlessly with Python notebooks, providing\nan accessible and efficient environment for data analysis using\nindustry-standard tools. The approach has proven effective in troubleshooting\nData Acquisition (DAQ) links for the ATLAS New Small Wheel (NSW) detector,\ndemonstrating the value of modern data platforms in enabling detector experts\nto quickly identify and resolve critical issues.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We study the fault-tolerance of networks from both the structural and\ncomputational point of view using the minimum leaf number of the corresponding\ngraph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and\nits vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed,\ni.e. which satisfy a certain stability condition with respect to minimum leaf\nnumbers and vertex-deletion. Next to this, our main notion is the so-called\nfault cost, which is based on the number of vertices that have different\ndegrees in minimum leaf spanning trees of the network and its vertex-deleted\nsubgraphs. We characterise networks with vanishing fault cost via\nleaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed\nnetworks containing $N$. We determine for all non-negative integers $k \\le 8$\nexcept $1$ the smallest network with fault cost $k$. We also give a detailed\ntreatment of the fault cost $1$ case, prove that there are infinitely many\n$3$-regular networks with fault cost $3$, and show that for any non-negative\ninteger $k$ there exists a network with fault cost exactly $k$.",
        "The HAWC Observatory collected 6 years of extensive data, providing an ideal\nplatform for long-term monitoring of blazars in the Very High Energy (VHE)\nband, without bias towards specific flux states. HAWC continuously monitors\nblazar activity at TeV energies, focusing on sources with a redshift of {z \\lt\n0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We\nspecifically focused our analysis on Mrk 421 and Mrk 501, as they are the\nbrightest blazars observed by the HAWC Observatory. With a dataset of 2143\ndays, this work significantly extends the monitoring previously published,\nwhich was based on 511 days of observation. By utilizing HAWC data for the VHE\n{\\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction\nwith Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore\npotential correlations between these two bands. For Mrk 501, we found evidence\nof a long-term correlation. Additionally, we identified a period in the light\ncurve where the flux was very low for more than two years. On the other hand,\nour analysis of Mrk 421 measured a strong linear correlation for\nquasi-simultaneous observations collected by HAWC and Swift-XRT. This result is\nconsistent with a linear dependence and a multiple-zone synchrotron\nself-Compton model to explain the X-ray and the {\\gamma}-ray emission. Finally,\nas suggested by previous findings, we confirm a harder-when-brighter behavior\nin the spectral evolution of the flux properties for Mrk 421. These findings\ncontribute to the understanding of blazar emissions and their underlying\nmechanisms.",
        "The design and performance analysis of relay lenses that provide\nhigh-performance image transmission for target acquisition and tracking in\nmilitary optical systems. Relay lenses are critical components for clear and\nlossless image transmission over long distances. In this study, the optical\nperformance of a relay lens system designed and optimized using ZEMAX software\nis investigated in detail. The analysis focuses on important optical properties\nsuch as modulation transfer function (MTF), spot diagrams, Seidel diagram,\nfield curvature and distortion. The results show that the lens has significant\npotential in military applications for target detection and tracking with high\nresolution and low aberration.",
        "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
        "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products)."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"An agent-based spatial urban social network generator: A case study of beijing, china",
    "start_abstract":"This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective.",
    "start_categories":[
      "cs.CE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Meeting Strangers and Friends of Friends: How Random Are Social Networks?"
      ],
      "abstract":[
        "We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)"
      ],
      "categories":[
        "q-fin.EC"
      ]
    },
    "list":{
      "title":[
        "On the numerical approximation of minimax regret rules via fictitious\n  play",
        "Optimal investment and consumption under $g$- expected utility and\n  general constraints in incomplete market",
        "Nonlinear Forecast Error Variance Decompositions with Hermite\n  Polynomials",
        "Robust distortion risk measures with linear penalty under distribution\n  uncertainty",
        "Revealed Social Networks",
        "Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
        "Locally Robust Policy Learning: Inequality, Inequality of Opportunity\n  and Intergenerational Mobility",
        "Women's Status and Fertility: A Novel Perspective on Low Fertility Issue",
        "A mixture transition distribution approach to portfolio optimization",
        "A Class of Practical and Acceptable Social Welfare Orderings That\n  Satisfy the Principles of Aggregation and Non-Aggregation: Reexamination of\n  the Tyrannies of Aggregation and Non-Aggregation",
        "Techno-Economic Analysis of Hydrogen Production: Costs, Policies, and\n  Scalability in the Transition to Net-Zero",
        "Clearing Sections of Lattice Liability Networks",
        "Robust Quantile Factor Analysis",
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Insights from leptohadronic modelling of the brightest blazar flare",
        "Ultra-cold neutrons in qBounce experiments as laboratory for test of\n  chameleon field theories and cosmic acceleration",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes",
        "The algebraic and geometric classification of Jordan superalgebras",
        "Flipped Rotating Axion Non-minimally Coupled to Gravity: Baryogenesis\n  and Dark Matter",
        "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Can supermassive stars form in protogalaxies due to internal\n  Lyman-Werner feedback?",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Elasticity of a Freely Jointed Chain with Quenched Disorder",
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "Quantile-Based Randomized Kaczmarz for Corrupted Tensor Linear Systems",
        "Free Perpetuities I: Existence, Subordination and Tail Asymptotics"
      ],
      "abstract":[
        "Finding numerical approximations to minimax regret treatment rules is of key\ninterest. To do so when potential outcomes are in {0,1} we discretize the\naction space of nature and apply a variant of Robinson's (1951) algorithm for\niterative solutions for finite two-person zero sum games. Our approach avoids\nthe need to evaluate regret of each treatment rule in each iteration. When\npotential outcomes are in [0,1] we apply the so-called coarsening approach. We\nconsider a policymaker choosing between two treatments after observing data\nwith unequal sample sizes per treatment and the case of testing several\ninnovations against the status quo.",
        "This article studies the problem of utility maximization in an incomplete\nmarket under a class of nonlinear expectations and general constraints on\ntrading strategies. Using a $g$-martingale method, we provide an explicit\nsolution to our optimization problem for different utility functions and\ncharacterize an optimal investment-consumption strategy through the solutions\nto quadratic BSDEs.",
        "A novel approach to Forecast Error Variance Decompositions (FEVD) in\nnonlinear Structural Vector Autoregressive models with Gaussian innovations is\nproposed, called the Hermite FEVD (HFEVD). This method employs a Hermite\npolynomial expansion to approximate the future trajectory of a nonlinear\nprocess. The orthogonality of Hermite polynomials under the Gaussian density\nfacilitates the construction of the decomposition, providing a separation of\nshock effects by time horizon, by components of the structural innovation and\nby degree of nonlinearity. A link between the HFEVD and nonlinear Impulse\nResponse Functions is established and distinguishes between marginal and\ninteraction contributions of shocks. Simulation results from standard nonlinear\nmodels are provided as illustrations and an application to fiscal policy shocks\nis examined.",
        "The paper investigates the robust distortion risk measure with linear penalty\nfunction under distribution uncertainty. The distribution uncertainties are\ncharacterized by predetermined moment conditions or constraints on the\nWasserstein distance. The optimal quantile distribution and the optimal value\nfunction are explicitly characterized. Our results partially extend the results\nof Bernard, Pesenti and Vanduffel (2024) and Li (2018) to robust distortion\nrisk measures with linear penalty. In addition, we also discuss the influence\nof the penalty parameter on the optimal solution.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
        "On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
        "Policy makers need to decide whether to treat or not to treat heterogeneous\nindividuals. The optimal treatment choice depends on the welfare function that\nthe policy maker has in mind and it is referred to as the policy learning\nproblem. I study a general setting for policy learning with semiparametric\nSocial Welfare Functions (SWFs) that can be estimated by locally\nrobust\/orthogonal moments based on U-statistics. This rich class of SWFs\nsubstantially expands the setting in Athey and Wager (2021) and accommodates a\nwider range of distributional preferences. Three main applications of the\ngeneral theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality\nof Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the\nPanel Study of Income Dynamics (PSID) to assess the effect of attending\npreschool on adult earnings and estimate optimal policy rules based on parental\nyears of education and parental income.",
        "This model offers a compelling explanation for the observed decline in\nfertility rates in developed countries, correlating it with the rising economic\nstatus of women, and thereby providing valuable insights for policy-making. In\nsummary, through a game theory model, we identify the rising economic status of\nwomen as the underlying cause of low fertility rates in modern developed\ncountries.",
        "Understanding the dependencies among financial assets is critical for\nportfolio optimization. Traditional approaches based on correlation networks\noften fail to capture the nonlinear and directional relationships that exist in\nfinancial markets. In this study, we construct directed and weighted financial\nnetworks using the Mixture Transition Distribution (MTD) model, offering a\nricher representation of asset interdependencies. We apply local assortativity\nmeasures--metrics that evaluate how assets connect based on similarities or\ndifferences--to guide portfolio selection and allocation. Using data from the\nDow Jones 30, Euro Stoxx 50, and FTSE 100 indices constituents, we show that\nportfolios optimized with network-based assortativity measures consistently\noutperform the classical mean-variance framework. Notably, modalities in which\nassets with differing characteristics connect enhance diversification and\nimprove Sharpe ratios. The directed nature of MTD-based networks effectively\ncaptures complex relationships, yielding portfolios with superior risk-adjusted\nreturns. Our findings highlight the utility of network-based methodologies in\nfinancial decision-making, demonstrating their ability to refine portfolio\noptimization strategies. This work thus underscores the potential of leveraging\nadvanced financial networks to achieve enhanced performance, offering valuable\ninsights for practitioners and setting a foundation for future research.",
        "This paper revisits impossibility results on the tyrannies of aggregation and\nnon-aggregation. I propose two aggregation principles (quantitative aggregation\nand ratio aggregation) and investigate theoretical implications. As a result, I\nshow that quantitative aggregation and minimal non-aggregation are incompatible\nwhile ratio aggregation and minimal non-aggregation are compatible under the\nassumption of standard axioms in social choice theory. Furthermore, this study\nprovides a new characterization of the leximin rule by using replication\ninvariance and the strong version of non-aggregation. Finally, I propose a\nclass of practical and acceptable social welfare orderings that satisfy the\nprinciples of aggregation and non-aggregation, which has various advantages\nover the standard rank-discounted generalized utilitarianism.",
        "This study presents a comprehensive techno-economic analysis of gray, blue,\nand green hydrogen production pathways, evaluating their cost structures,\ninvestment feasibility, infrastructure challenges, and policy-driven cost\nreductions. The findings confirm that gray hydrogen (1.50-2.50\/kg) remains the\nmost cost-effective today but is increasingly constrained by carbon pricing.\nBlue hydrogen (2.00-3.50\/kg) offers a transitional pathway but depends on CCS\ncosts, natural gas price volatility, and regulatory support. Green hydrogen\n(3.50-6.00\/kg) is currently the most expensive but benefits from declining\nrenewable electricity costs, electrolyzer efficiency improvements, and\ngovernment incentives such as the Inflation Reduction Act (IRA), which provides\ntax credits of up to 3.00\/kg. The analysis shows that renewable electricity\ncosts below 20-30\/MWh are essential for green hydrogen to achieve cost parity\nwith fossil-based hydrogen. The DOE's Hydrogen Shot Initiative aims to lower\ngreen hydrogen costs to 1.00\/kg by 2031, emphasizing the need for CAPEX\nreductions, economies of scale, and improved electrolyzer efficiency.\nInfrastructure remains a critical challenge, with pipeline retrofitting\nreducing transport costs by 50-70%, though liquefied hydrogen and chemical\ncarriers remain costly due to energy losses and reconversion expenses.\nInvestment trends indicate a shift toward green hydrogen, with over 250 billion\nprojected by 2035, surpassing blue hydrogen's expected 100 billion. Carbon\npricing above $100\/ton CO2 will likely make gray hydrogen uncompetitive by\n2030, accelerating the shift to low-carbon hydrogen. Hydrogen's long-term\nviability depends on continued cost reductions, policy incentives, and\ninfrastructure expansion, with green hydrogen positioned as a cornerstone of\nthe net-zero energy transition by 2035.",
        "Modern financial networks involve complex obligations that transcend simple\nmonetary debts: multiple currencies, prioritized claims, supply chain\ndependencies, and more. We present a mathematical framework that unifies and\nextends these scenarios by recasting the classical Eisenberg-Noe model of\nfinancial clearing in terms of lattice liability networks. Each node in the\nnetwork carries a complete lattice of possible states, while edges encode\nnominal liabilities. Our framework generalizes the scalar-valued clearing\nvectors of the classical model to lattice-valued clearing sections, preserving\nthe elegant fixed-point structure while dramatically expanding its descriptive\npower. Our main theorem establishes that such networks possess clearing\nsections that themselves form a complete lattice under the product order. This\nstructure theorem enables tractable analysis of equilibria in diverse domains,\nincluding multi-currency financial systems, decentralized finance with\nautomated market makers, supply chains with resource transformation, and\npermission networks with complex authorization structures. We further extend\nour framework to chain-complete lattices for term structure models and\nmultivalued mappings for complex negotiation systems. Our results demonstrate\nhow lattice theory provides a natural language for understanding complex\nnetwork dynamics across multiple domains, creating a unified mathematical\nfoundation for analyzing systemic risk, resource allocation, and network\nstability.",
        "We propose a factor model and an estimator of the factors and loadings that\nare robust to weak factors. The factors can have an arbitrarily weak influence\non the mean or quantile of the outcome variable at most quantile levels; each\nfactor only needs to have a strong impact on the outcome's quantile near one\nunknown quantile level. The estimator for every factor, loading, and common\ncomponent is asymptotically normal at the $\\sqrt{N}$ or $\\sqrt{T}$ rate. It\ndoes not require the knowledge of whether the factors are weak and how weak\nthey are. We also develop a weak-factor-robust estimator of the number of\nfactors and a consistent selectors of factors of any desired strength of\ninfluence on the quantile or mean of the outcome variable. Monte Carlo\nsimulations demonstrate the effectiveness of our methods.",
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "The blazar 3C 454.3 experienced a major flare in November 2010 making it the\nbrightest $\\gamma$-ray source in the sky of the Fermi-LAT. We obtain seven\ndaily consecutive spectral-energy distributions (SEDs) of the flare in the\ninfra-red, optical, ultra-violet, X-ray and $\\gamma$-ray bands with publicly\navailable data. We simulate the physical conditions in the blazar and show that\nthe observed SEDs are well reproduced in the framework of a \"standing feature\"\nwhere the position of the emitting region is almost stationary, located beyond\nthe outer radius of the broad-line region and into which fresh blobs of\nrelativistically moving magnetized plasma are continuously injected. Meanwhile,\na model with a single \"moving blob\" does not describe the data well. We obtain\na robust upper limit to the amount of high-energy protons in the jet of 3C\n454.3 from the electromagnetic SED. We construct a neutrino light curve of 3C\n454.3 and estimate the expected neutrino yield at energies $\\geq 100$ TeV for\n3C 454.3 to be up to $6 \\times 10^{-3}$ $\\nu_{\\mu}$ per year. Finally, we\nextrapolate our model findings to the light curves of all Fermi-LAT\nflat-spectrum radio quasars. We find that next-generation neutrino telescopes\nare expected to detect approximately one multimessenger ($\\gamma + \\nu_{\\mu}$)\nflare per year from bright blazars with neutrino peak energy in the hundreds\nTeV -- hundreds PeV energy range and show that the electromagnetic flare peak\ncan precede the neutrino arrival by months to years.",
        "The accelerating expansion of the Universe, attributed to dark energy, has\nspurred interest in theories involving scalar fields such as chameleon field\ntheories. These fields, which couple to matter with density-dependent effective\nmass, offer a promising explanation for cosmic acceleration. Experiments\nleveraging ultra-cold neutrons (UCNs) provide an innovative approach to testing\nthese theories. The existence of a chameleon field, being responsible for the\ncurrent phase of cosmic acceleration, is investigated by analysing a free fall\nof ultra-cold neutrons from the gap between two mirrors after their bouncing\nbetween these two mirrors. We analyse a deformation of the wave functions of\nthe quantum gravitational states of ultra-cold neutrons, induced by a chameleon\nfield, and find a new upper bound $\\beta\\leq6.5\\times10^8$ on the\nchameleon-matter coupling constant $\\beta$ from the unitarity condition. This\nresult refines previous estimates and highlights the potential of ultra-cold\nneutron experiments as laboratories for exploring scalar field theories and\nfundamental physics.",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures.",
        "We give the algebraic and geometric classification of complex\nfour-dimensional Jordan superalgebras. In particular, we describe all\nirreducible components in the corresponding varieties.",
        "We demonstrate that the co-genesis of baryon asymmetry and dark matter can be\nachieved through the rotation of an axion-like particle, driven by a flip in\nthe vacuum manifold's direction at the end of inflation. This can occur if the\naxion has a periodic non-minimal coupling to gravity, while preserving the\ndiscrete shift symmetry. In non-oscillating inflation models, after inflation\nthere is typically a period of kination (with $w = 1$). In this case, it is\nshown that the vacuum manifold of the axion is flipped and the axion begins\nrotating in field space, because it can slide across the decreasing potential\nbarrier as in Ricci reheating. Such a rotating axion can generate the baryon\nasymmetry of the Universe through spontaneous baryogenesis, while at later\nepochs it can oscillate as dark matter. The period of kination makes the\nprimordial gravitational waves (GW) generated during inflation sharply\nblue-tilted which constrains the parameter space due to GW overproduction,\nwhile being testable by next generation CMB experiments. As a concrete example,\nwe show that such a cogenesis of baryon asymmetry and dark matter can be\nrealized for the axion as the Majoron in the Type-I seesaw setup, predicting\nmass ranges for the Majoron below sub eVs, with right-handed neutrino mass\nabove $\\mathcal{O}(10^{8})$ GeV. We also show that in order to avoid\nfragmentation of the axion condensate during the rotation, we require the\nnon-minimal coupling \\mbox{$\\xi \\sim (f\/m_P)^2 $} or somewhat larger, where $f$\nis the axion decay constant.",
        "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "Population III stars are possible precursors to early massive and\nsupermassive black holes (BHs). The presence of soft UV Lyman Werner (LW)\nbackground radiation can suppress Population III star formation in minihalos\nand allow them to form in pristine atomic cooling halos. In the absence of\nmolecular hydrogen ($\\rm H_2$) cooling, atomic-cooling halos enable rapid\ncollapse with suppressed fragmentation. High background LW fluxes from\npreceding star-formation have been proposed to dissociate $\\rm H_2$. This flux\ncan be supplemented by LW radiation from one or more Population III star(s) in\nthe same halo, reducing the necessary background level. Here we consider\natomic-cooling halos in which multiple protostellar cores form close to one\nanother nearly simultaneously. We assess whether the first star's LW radiation\ncan dissociate nearby $\\rm H_2$, enabling the prompt formation of a second,\nsupermassive star (SMS) from warm, atomically-cooled gas. We use a set of\nhydrodynamical simulations with the code ENZO, with identical LW backgrounds\ncentered on a halo with two adjacent collapsing gas clumps. When an additional\nlarge local LW flux is introduced, we observe immediate reductions in both the\naccretion rates and the stellar masses that form within these clumps. While the\nLW flux reduces the $\\text{H}_2$ fraction and increases the gas temperature,\nthe halo core's potential well is too shallow to promptly heat the gas to\n$\\gtrsim$ 1000 K and increase the accretion rate onto the second protostar. We\nconclude that internal LW feedback inside atomic-cooling halos is unlikely to\nfacilitate the formation of SMSs or massive BH seeds.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a simple theoretical model, the Freely Jointed Chain with\nquenched hinges (qFJC), which captures the quenched disorder in the local\nbending stiffness of the polymer. In this article, we analyze the tensile\nelasticity of the qFJC in the Gibbs (fixed-force) ensemble. For finite-size\nsystems, we obtain a recurrence relation of the exact free energy, which allows\nus to calculate the exact force-extension relation numerically for an arbitrary\nsize of the system. In the thermodynamic limit, when $L({\\rm contour\n\\;length})\\gg L_p({\\rm persistence \\;length})$, we obtain a framework to deal\nwith quenched disorder in the polymer configuration. This allows us to obtain\nthe response function for the discrete and continuous qFJC in the thermodynamic\nlimit. It turns out that the extension of the continuous qFJC can be cast in a\nsimple form. Furthermore, we have applied our analysis to rod-coil multiblock\ncopolymers.",
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "The reconstruction of tensor-valued signals from corrupted measurements,\nknown as tensor regression, has become essential in many multi-modal\napplications such as hyperspectral image reconstruction and medical imaging. In\nthis work, we address the tensor linear system problem $\\mathcal{A}\n\\mathcal{X}=\\mathcal{B}$, where $\\mathcal{A}$ is a measurement operator,\n$\\mathcal{X}$ is the unknown tensor-valued signal, and $\\mathcal{B}$ contains\nthe measurements, possibly corrupted by arbitrary errors. Such corruption is\ncommon in large-scale tensor data, where transmission, sensory, or storage\nerrors are rare per instance but likely over the entire dataset and may be\narbitrarily large in magnitude. We extend the Kaczmarz method, a popular\niterative algorithm for solving large linear systems, to develop a Quantile\nTensor Randomized Kaczmarz (QTRK) method robust to large, sparse corruptions in\nthe observations $\\mathcal{B}$. This approach combines the tensor Kaczmarz\nframework with quantile-based statistics, allowing it to mitigate adversarial\ncorruptions and improve convergence reliability. We also propose and discuss\nthe Masked Quantile Randomized Kaczmarz (mQTRK) variant, which selectively\napplies partial updates to handle corruptions further. We present convergence\nguarantees, discuss the advantages and disadvantages of our approaches, and\ndemonstrate the effectiveness of our methods through experiments, including an\napplication for video deblurring.",
        "We study the free analogue of the classical affine fixed-point (or\nperpetuity) equation\n  \\[\n  \\mathbb{X} \\stackrel{d}{=} \\mathbb{A}^{1\/2}\\mathbb{X}\\,\\mathbb{A}^{1\/2} +\n\\mathbb{B},\n  \\] where $\\mathbb{X}$ is assumed to be $*$-free from the pair\n$(\\mathbb{A},\\mathbb{B})$, with $\\mathbb{A}\\ge 0$ and\n$\\mathbb{B}=\\mathbb{B}^*$. Our analysis covers both the subcritical regime,\nwhere $\\tau(\\mathbb{A})<1$, and the critical case $\\tau(\\mathbb{A})=1$, in\nwhich the solution $\\mathbb{X}$ is necessarily unbounded. When\n$\\tau(\\mathbb{A})=1$, we prove that the series defining $\\mathbb{X}$ converges\nbilaterally almost uniformly (and almost uniformly under additional tail\nassumptions), while the perpetuity fails to have higher moments even if all\nmoments of $\\mathbb{A}$ and $\\mathbb{B}$ exist.\n  Our approach relies on a detailed study of the asymptotic behavior of moments\nunder free multiplicative convolution, which reveals a markedly different\nbehavior from the classical setting. By employing subordination techniques for\nnon-commutative random variables, we derive precise asymptotic estimates for\nthe tail of the distributions of $\\mathbb{X}$ in both one-sided and symmetric\ncases. Interestingly, in the critical case, the free perpetuity exhibits a\npower-law tail behavior that mirrors the phenomenon observed in the celebrated\nKesten's theorem."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network",
    "start_abstract":"Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice"
      ],
      "abstract":[
        "Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073"
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "An Adaptive Proton FLASH Therapy Using Modularized Pin Ridge Filter",
        "Positronium Lifetime Imaging with the Biograph Vision Quadra using 124I",
        "ISIT-GEN: An in silico imaging trial to assess the inter-scanner\n  generalizability of CTLESS for myocardial perfusion SPECT on defect-detection\n  task",
        "Comprehensive investigation of HYPERSCINT RP-FLASH scintillator for\n  electron FLASH research",
        "Ultrasound imaging of cortical bone: cortex geometry and measurement of\n  porosity based on wave speed for bone remodeling estimation",
        "Electrical and Mechanical Modeling of Uterine Contractions Analysis\n  Using Connectivity Methods and Graph Theory",
        "Pulsed laser diode excitation for transcranial photoacoustic imaging",
        "A Probabilistic Model of Bilateral Lymphatic Spread in Head and Neck\n  Cancer",
        "A coupled planar transmit RF array for ultrahigh field spine MR imaging",
        "3D printed human skull phantoms for transcranial photoacoustic imaging",
        "Evaluation of patient activation and dosimetry after Boron Neutron\n  Capture Therapy",
        "The New CMS Measure of Excessive Radiation Dose or Inadequate CT Image\n  Quality: Methods for Size-Adjusted Dose and Their Variabilities",
        "A variant of \\v{S}emrl's preserver theorem for singular matrices",
        "Optimal domain of Volterra operators in Korenblum spaces",
        "On general versions of the Petty projection inequality",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Homotopical Entropy",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Multiwavelength Variability Analysis of the Blazar PKS 0727-11: A\n  $\\sim$168 Days Quasi-periodic Oscillation in Gamma-ray",
        "Chirality, Nonreciprocity and Symmetries for a Giant Atom",
        "The period-index problem for hyper-K\\\"ahler varieties via\n  hyperholomorphic bundles",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Benchmarking ANN extrapolations of the ground-state energies and radii\n  of Li isotopes",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Global Convergence and Rate Analysis of the Steepest Descent Method for\n  Uncertain Multiobjective Optimization via a Robust Optimization Approach",
        "Topological Invariants in Invasion Percolation"
      ],
      "abstract":[
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "In this paper, we proposed a method to optimize adaptive proton FLASH therapy\n(ADP FLASH) using modularized pin ridge filters (pRFs) by recycling module pins\nfrom the initial plan while reducing pRF adjustments in adaptive FLASH\nplanning. Initially, single energy (250 MeV) FLASH pRF plans were created using\npencil beam directions (PBDs) from initial IMPT plans on the planning CT (pCT).\nPBDs are classified as new\/changed ($\\Delta$E > > 5 MeV) or unchanged by\ncomparing spot maps for targets between pCT and re-CT. We used an iterative\nleast square regression model to identify recyclable PBDs with minimal relative\nchanges to spot MU weighting. Two PBDs with the least square error were\nretrieved per iteration and added to the background plan, and the remaining\nPBDs were reoptimized for the adaptive plan in subsequent iterations. The\nmethod was validated on three liver SBRT cases (50 Gy in 5 fractions) by\ncomparing various dosimetric parameters across initial pRF plans on pCT, reCT\nand the ADP FLASH pRF plans on reCT. V100 for initial pRF plans on pCT, reCT,\nand ADP FLASH pRF plans for the three cases were as follows: (93.7%, 89.2%,\n91.4%), (93.5%, 60.2%, 91.7%), (97.3%, 69.9%, 98.8%). We observe a decline in\nplan quality when applying the initial pRF to the reCT, whereas the ADP FLASH\npRF approach restores quality comparable to the initial pRF on the pCT. FLASH\neffect of the initial pRF and ADP pRF plans were evaluated with a dose and dose\nrate threshold of 1Gy and 40Gy\/s, respectively, using the FLASH effectiveness\nmodel. The proposed method recycled 91.2%, 71%, and 64.7% of PBDs from initial\npRF plans for the three cases while maintaining all clinical goals and\npreserving FLASH effects across all cases.",
        "Purpose: Measuring the ortho-positronium (oPs) lifetime in human tissue bears\nthe potential of adding clinically relevant information about the tissue\nmicroenvironment to conventional positron emission tomography (PET). Through\nphantom measurements, we investigate the voxel-wise measurement of oPs lifetime\nusing a commercial long-axial field-of-view (LAFOV) PET scanner. Methods: We\nprepared four samples with mixtures of Amberlite XAD4, a porous polymeric\nadsorbent, and water and added between 1.12 MBq and 1.44 MBq of $^{124}$I. The\nsamples were scanned in two different setups: once with a couple of centimeters\nbetween each sample (15 minutes scan time) and once with all samples taped\ntogether (40 minutes scan time). For each scan, we determine the oPs lifetime\nfor the full samples and at the voxel level. The voxel sizes under\nconsideration are $10.0^3$ mm$^3$, $7.1^3$ mm$^3$ and $4.0^3$ mm$^3$. Results:\nAmberlite XAD4 allows the preparation of samples with distinct oPs lifetime.\nUsing a Bayesian fitting procedure, the oPs lifetimes in the whole samples are\n$2.52 \\pm 0.03$ ns, $2.37\\pm 0.03$ ns, $2.27\\pm0.04$ ns and $1.82\\pm 0.02$ ns,\nrespectively. The voxel-wise oPs lifetime fits showed that even with $4.0^3$\nmm$^3$ voxels the samples are clearly distinguishable and a central voxels have\ngood count statistics. However, the situation with the samples close together\nremains challenging with respect to the spatial distinction of regions with\ndifferent oPs lifetimes. Conclusion: Our study shows that positronium lifetime\nimaging on a commercial LAFOV PET\/CT should be feasible under clinical\nconditions using $^{124}$I.",
        "A recently proposed scatter-window and deep learning-based attenuation\ncompensation (AC) method for myocardial perfusion imaging (MPI) by\nsingle-photon emission computed tomography (SPECT), namely CTLESS, demonstrated\npromising performance on the clinical task of myocardial perfusion defect\ndetection with retrospective data acquired on SPECT scanners from a single\nvendor. For clinical translation of CTLESS, it is important to assess the\ngeneralizability of CTLESS across different SPECT scanners. For this purpose,\nwe conducted a virtual imaging trial, titled in silico imaging trial to assess\ngeneralizability (ISIT-GEN). ISIT-GEN assessed the generalizability of CTLESS\non the cardiac perfusion defect detection task across SPECT scanners from three\ndifferent vendors. The performance of CTLESS was compared with a\nstandard-of-care CT-based AC (CTAC) method and a no-attenuation compensation\n(NAC) method using an anthropomorphic model observer. We observed that CTLESS\nhad receiver operating characteristic (ROC) curves and area under the ROC\ncurves similar to those of CTAC. Further, CTLESS was observed to significantly\noutperform the NAC method across three scanners. These results are suggestive\nof the inter-scanner generalizability of CTLESS and motivate further clinical\nevaluations. The study also highlights the value of using in silico imaging\ntrials to assess the generalizability of deep learning-based AC methods\nfeasibly and rigorously.",
        "Accurate, dose-rate-independent, fast-response dosimeters that capture the\nspatiotemporal characteristics of ultra-high dose rate irradiation (>40Gy\/s,\nUHDR) beams are urgently needed to facilitate FLASH research and support the\nclinical translation of FLASH radiotherapy. This study evaluated the\nperformance of HYPERSCINT RP-FLASH scintillator in UHDR electron beam\ndosimetry, with sampling frequency(fs) up to 1 kHz. Four-component calibration\nand 18MeV conventional dose rate irradiation(CONV, ~0.1Gy\/s) were used to\ncalibrate the spectral characteristics of the scintillator and its\nsignal-to-dose conversion. Dosimetric accuracy for CONV and UHDR electron beams\nwas quantified against ion chamber or EBT-XD film measurements. The effects of\nbeam energy, field size, dose per pulse(DPP), pulse repetition frequency(PRF),\nand accumulated dose on the scintillator's response were investigated.\nTime-resolved dose measurements were verified using a PMT-fiber optic detector,\nwhich measures the relative output of electron pulses by scattered radiation.\nThe scintillator system achieved <0.5% accuracy compared to ion chamber\nmeasurements under 0.1-35Gy CONV irradiation at 1Hz and <3% accuracy against\nEBT-XD film in UHDR conditions up to 40Gy at 1 kHz. There was minimal energy\ndependence between 6 and 18MeV, field size dependence for 2x1-25x25cm2, DPP\ndependence for 1-2.3Gy, and PRF dependence for 30-180Hz. The scintillator\nexhibited -2.6%\/kGy signal degradation for 0-2kGy, indicating radiation damage.\nThe time-resolved dose results at 1kHz were verified within 3% accuracy\ncompared to the PMT-fiber optic detector. With proper calibration, the\nscintillator system can serve as an accurate, dose-rate-independent,\nfast-response, millisecond-resolved UHDR electron beam dosimeter, with minimal\ndependence on dose, dose rate, energy, field size, DPP, and PRF within ranges\ncommon to UHDR electron beams.",
        "Intracortical US imaging extends B-mode imaging into bone using a dedicated\nimage reconstruction algorithm that corrects for refraction at the bone-soft\ntissue interfaces. It has shown promising results in a few healthy,\npredominantly young adults, providing anatomical images of the cortex\n(periosteal and endosteal surfaces) along with estimations of US wave speed.\nHowever, its reliability in older or osteoporotic bones remains uncertain. In\nthis study, we critically assessed the performance of intracortical US imaging\nex vivo in bones with various microstructural patterns, including bones\nexhibiting signs of unbalanced intracortical remodeling. We analyzed factors\ninfluencing US image quality, particularly endosteal surface reconstruction, as\nwell as the accuracy of wave speed estimation and its relationship with\nporosity. We imaged 20 regions of interest from the femoral diaphysis of five\nelderly donors using a 2.5 MHz US transducer. The reconstructed US images were\ncompared to site-matched high-resolution micro-CT (HR-muCT) images. In samples\nwith moderate porosity, the endosteal surface was accurately identified, and\nthickness estimates from US and HR-muCT differed by less than 10%. In highly\nremodeled bones with increased porosity, the reconstructed endosteal surface\nappeared less bright and was located above the cortex region containing\nresorption cavities. We observed a decrease in US wave speed with increasing\ncortical porosity suggesting that the method could discriminate between bones\nwith low porosity (less than 5%) and those with moderate to high porosity\n(greater than ~10%). This study paves the way for the application of US imaging\nin diagnosing cortical bone health, particularly for detecting increased\ncortical porosity and reduced cortical thickness.",
        "Premature delivery is a leading cause of fetal death and morbidity, making\nthe prediction and treatment of preterm contractions critical. The\nelectrohysterographic (EHG) signal measures the electrical activity controlling\nuterine contraction. Analyzing EHG features can provide valuable insights for\nlabor detection. In this paper, we propose a framework using simulated EHG\nsignals to identify features sensitive to uterine connectivity. We focus on EHG\nsignal propagation during delivery, recorded by multiple electrodes. Simulated\nEHG signals were generated using electrical diffusion (ED) and\nmechanotransduction (EDM) to identify which connectivity methods and graph\nparameters best represent uterine synchronization. The signals were simulated\nin two scenarios: using only ED by modifying tissue resistance, and using both\nED and EDM by varying mechanotransduction model parameters. A matrix of 16\nsurface electrodes was used for the simulations. Our results show that a\nsimplified electromechanical model can monitor uterine synchronization. Feature\nselection using Fscore on real and simulated EHG signals highlighted that the\nbest features for detecting mechanotransduction shifts were H2 alone or\ncombined with Str, R2(PR), and ICOH(Str). The best features for detecting\nelectrical diffusion shifts were H2, Eff, PR, and BC.",
        "Photoacoustic (PA) imaging of deep tissue tends to employ Q-switched lasers\nwith high pulse energy to generate high optical fluence and therefore high PA\nsignal. Compared to Q-switched lasers, pulsed laser diodes (PLDs) typically\ngenerate low pulse energy. In PA imaging applications with strong acoustic\nattenuation, such as through human skull bone, the broadband PA waves generated\nby nanoseconds laser pulses are significantly reduced in bandwidth during their\npropagation to a detector. As high-frequency PA signal components are not\ntransmitted through skull, we propose to not generate them by increasing\nexcitation pulse duration. Because PLDs are mainly limited in their peak power\noutput, an increase in pulse duration linearly increases pulse energy and\ntherefore PA signal amplitude. Here we show that the optimal pulse duration for\ndeep PA sensing through thick skull bone is far higher than in typical PA\napplications. Counterintuitively, this makes PLD excitation well-suited for\ntranscranial photoacoustics. We show this in PA sensing experiments on ex vivo\nhuman skull bone.",
        "Current guidelines for elective nodal irradiation in oropharyngeal squamous\ncell carcinoma (OPSCC) recommend including large portions of the contralateral\nlymph system in the clinical target volume (CTV-N), even for lateralized tumors\nwith no clinical lymph node involvement in the contralateral neck. This study\nintroduces a probabilistic model of bilateral lymphatic tumor progression in\nOPSCC to estimate personalized risks of occult disease in specific lymph node\nlevels (LNLs) based on clinical involvement, T-stage, and tumor lateralization.\nBuilding on a previously developed hidden Markov model for ipsilateral spread,\nwe extend the approach to the contralateral neck. The model represents LNLs I,\nII, III, IV, V, and VII on both sides of the neck as binary hidden variables\n(healthy\/involved), connected via arcs representing spread probabilities. These\nprobabilities are learned using Markov chain Monte Carlo (MCMC) sampling from a\ndataset of 833 OPSCC patients, enabling the model to reflect the underlying\nlymphatic progression dynamics. The model accurately and precisely describes\nobserved patterns of involvement with a compact set of interpretable\nparameters. Midline extension of the primary tumor is identified as the primary\nrisk factor for contralateral involvement, with advanced T-stage and extensive\nipsilateral involvement further increasing risk. Occult disease in\ncontralateral LNL III is highly unlikely if upstream LNL II is clinically\nnegative, and in contralateral LNL IV, occult disease is exceedingly rare\nwithout LNL III involvement. For lateralized tumors not crossing the midline,\nthe model suggests the contralateral neck may safely be excluded from the\nCTV-N. For tumors extending across the midline but with a clinically negative\ncontralateral neck, the CTV-N could be limited to LNL II, reducing unnecessary\nexposure of normal tissue while maintaining regional tumor control.",
        "Ultrahigh-field MRI, such as those operating at 7 Tesla, enhances diagnostic\ncapabilities but also presents unique challenges, including the need for\nadvanced RF coil designs to achieve an optimal signal-to-noise ratio and\ntransmit efficiency, particularly when imaging large samples. In this work, we\nintroduce the coupled planar array, a novel technique for high-frequency,\nlarge-size RF coil design with enhanced the RF magnetic field (B1) efficiency\nand transmit performance for ultrahigh-field spine imaging applications. This\narray comprises multiple resonators that are electromagnetically coupled to\nfunction as a single multimodal resonator. The field distribution of its\nhighest frequency mode is suitable for spine imaging applications. Based on the\nnumerical modeling and calculation, a prototype of the coupled planar array was\nconstructed and its performance was evaluated through comprehensive numerical\nsimulations, rigorous RF measurements, empirical tests, and a comparison\nagainst a conventional surface coil with the same size and geometry. The\nresults of this study demonstrate that the proposed coupled planar array\nexhibits superior performance compared to conventional surface coils in terms\nof B1 efficiency for both transmit (B1+) and receive (B1-) fields, specific\nabsorption rate (SAR), and the ability to operate at high frequencies. This\nstudy suggests a promising and efficient approach to the design of\nhigh-frequency, large-size RF coils for spine MR imaging at ultrahigh magnetic\nfields.",
        "Photoacoustic (PA) waves are strongly distorted and attenuated in skull bone.\nTo study these effects on PA imaging, we designed and 3D-printed\ntissue-mimicking phantoms of human skull. We present a comparison of results in\nphantom and ex vivo skull.",
        "Boron Neutron Capture Therapy (BNCT) is a form of radiotherapy based on the\nirradiation of the tumour with a low energy neutron beam, after the\nadministration of a selective drug enriched in boron-10. The therapy exploits\nthe high cross section of thermal neutron capture in boron, generating two\nlow-range charged particles. The availability of accelerators able to generate\nhigh-intensity neutron beams via proton nuclear interaction is boosting the\nconstruction of new clinical centres. One of these is under development in\nItaly, using a 5 MeV, 30 mA proton radiofrequency accelerator coupled to a\nberyllium target, funded by the Complementary Plan to the Recovery and\nResilience National Plan, under the project ANTHEM. The present study focuses\non radiation protection aspects of patients undergoing BNCT, specifically on\nthe activation of their organs and tissues. A criterion to establish the\nrelevance of such activation after BNCT has been proposed. Based on the current\nItalian regulatory framework, the level of patient activation following BNCT\ntreatment does not pose a significant radiological concern, even shortly after\nirradiation. Another aspect is the activation of patient's excretions, which\ncan impact on the design of the building and requires a process for the\ndischarge. The described study contributes to the radiation protection study\nfor the ANTHEM BNCT centre in Italy.",
        "The Centers for Medicare & Medicaid Services (CMS) has introduced CMS1074v2,\na quality measure for computed tomography (CT) that assesses radiation dose and\nimage quality across 18 CT exam categories. This measure mandates the\ncalculation of size-adjusted dose (SAD) using patient effective diameter and\npredefined size-adjustment coefficients. However, variability in SAD\ncalculation methods raises concerns about standardization, compliance, and\nclinical applicability. This study evaluates five commonly used methods for\nestimating effective diameter and their impact on SAD determination in thoracic\nand abdominal CT protocols. A retrospective analysis of 719 CT exams was\nperformed, comparing SAD values across different calculation approaches.\nResults indicate significant variability in SAD, with attenuation-based methods\noverestimating SAD in chest exams and projection-based methods exhibiting\ngreater variability in abdominal exams. The findings highlight potential\ninconsistencies in CMS-defined dose thresholds and challenges in applying the\nmeasure across diverse patient populations and institutional imaging practices.\nAddressing these inconsistencies is critical for ensuring accurate dose\nreporting and maintaining diagnostic integrity in CT imaging.",
        "For positive integers $1 \\leq k \\leq n$ let $M_n$ be the algebra of all $n\n\\times n$ complex matrices and $M_n^{\\le k}$ its subset consisting of all\nmatrices of rank at most $k$. We first show that whenever $k>\\frac{n}{2}$, any\ncontinuous spectrum-shrinking map $\\phi : M_n^{\\le k} \\to M_n$ (i.e.\n$\\mathrm{sp}(\\phi(X)) \\subseteq \\mathrm{sp}(X)$ for all $X \\in M_n^{\\le k}$)\neither preserves characteristic polynomials or takes only nilpotent values.\nMoreover, for any $k$ there exists a real analytic embedding of $M_n^{\\le k}$\ninto the space of $n\\times n$ nilpotent matrices for all sufficiently large\n$n$. This phenomenon cannot occur when $\\phi$ is injective and either $k > n -\n\\sqrt{n}$ or the image of $\\phi$ is contained in $M_n^{\\le k}$. We then\nestablish a main result of the paper -- a variant of \\v{S}emrl's preserver\ntheorem for $M_n^{\\le k}$: if $n \\geq 3$, any injective continuous map $\\phi\n:M_n^{\\le k} \\to M_n^{\\le k}$ that preserves commutativity and shrinks spectrum\nis of the form $\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$,\nfor some invertible matrix $T\\in M_n$. Moreover, when $k=n-1$, which\ncorresponds to the set of singular $n\\times n$ matrices, this result extends to\nmaps $\\phi$ which take values in $M_n$. Finally, we discuss the\nindispensability of assumptions in our main result.",
        "The aim of this article is to study the largest domain space $[T,X]$,\nwhenever it exists, of a given continuous linear operator $T\\colon X\\to X$,\nwhere $X\\subseteq H(\\mathbb{D})$ is a Banach space of analytic functions on the\nopen unit disc $\\mathbb{D}\\subseteq \\mathbb{C}$. That is, $[T,X]\\subseteq\nH(\\mathbb{D})$ is the \\textit{largest} Banach space of analytic functions\ncontaining $X$ to which $T$ has a continuous, linear, $X$-valued extension\n$T\\colon [T,X]\\to X$. The class of operators considered consists of generalized\nVolterra operators $T$ acting in the Korenblum growth Banach spaces\n$X:=A^{-\\gamma}$, for $\\gamma>0$. Previous studies dealt with the classical\nCes\\`aro operator $T:=C$ acting in the Hardy spaces $H^p$, $1\\leq p<\\infty$,\n\\cite{CR}, \\cite{CR1}, in $A^{-\\gamma}$, \\cite{ABR-R}, and more recently,\ngeneralized Volterra operators $T$ acting in $X:=H^p$, \\cite{BDNS}.",
        "The classical Petty projection inequality is an affine isoperimetric\ninequality which constitutes a cornerstone in the affine geometry of convex\nbodies. By extending the polar projection body to an inter-dimensional\noperator, Petty's inequality was generalized to the so-called $(L_p,Q)$\nsetting, where $Q$ is an $m$-dimensional compact convex set. In this work, we\nfurther extend the $(L_p,Q)$ Petty projection inequality to the broader realm\nof rotationally invariant measures with concavity properties, namely, those\nwith $\\gamma$-concave density (for $\\gamma\\geq-1\/nm$). Moreover, when $p=1$,\nand motivated by a contemporary empirical reinterpretation of Petty's result,\nwe explore empirical analogues of this inequality.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We present a \"homotopification\" of fundamental concepts from information\ntheory. Using homotopy type theory, we define homotopy types that behave\nanalogously to probability spaces, random variables, and the exponentials of\nShannon entropy and relative entropy. The original analytic theories emerge\nthrough homotopy cardinality, which maps homotopy types to real numbers and\ngeneralizes the cardinality of sets.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "We performed variability analysis of the multiwavelength light curves for the\nflat-spectrum radio quasar PKS 0727-11. Using the generalized Lomb-Scargle\nperiodogram, we identified a possible quasi-periodic oscillation (QPO) of\n$\\sim$ 168.6 days (persisted for 6 cycles, with a significance of $3.8\\sigma$)\nin the gamma-ray light curve during the flare period (MJD 54687-55738). It is\nthe first time that periodic variations have been detected in this source, and\nfurther supported by other methods: weighted wavelet $z$-transform, phase\ndispersion minimization, REDFIT, autoregressive integrated moving average\nmodel, and structure function analysis. Cross-correlation analysis shows that\nthere is a strong correlation between multi-band light variations, indicating\nthat gamma-ray and radio flares may originate from the same disturbance, and\nthe distance between the emission regions of gamma-ray and radio flares is\ncalculated based on the time lag. We demonstrate that QPO arising from the\nnon-ballistic helical jet motion driven by the orbital motion in a supermassive\nbinary black hole is a plausible physical explanation. In this scenario, the\nestimated mass of the primary black hole is\n$M\\sim3.66\\times10^8-5.79\\times10^{9}M_\\odot$.",
        "Chiral and nonreciprocal quantum devices are crucial for signal routing and\nprocessing in a quantum network. In this work, we study the chirality and\nnonreciprocity of a giant atom coupled to a one-dimensional waveguide. We\nclarify that the chiral emission of the giant atom is not directly related to\nthe time-reversal symmetry breaking but to the mirror-symmetry breaking. We\npropose a passive scheme to realize the chiral emission of a giant atom without\nbreaking time-reversal symmetry by extending the legs of the giant atom. We\nfind the time-reversal symmetry breaking via nonuniform coupling phases is\nartificial and thus cannot result in nonreciprocal single-photon scattering for\nthe giant atom. The nonreciprocity of the giant atom can be obtained by the\nexternal dissipation of the giant atom that truly breaks the time-reversal\nsymmetry. Our work clarifies the roles of symmetries in the chirality and\nnonreciprocity of giant-atom systems and paves the way for the design of\non-chip functional devices with superconducting giant atoms.",
        "We prove new bounds for the period-index problem for hyper-K\\\"ahler varieties\nof $K3^{[n]}$-type using projectively hyperholomorphic bundles constructed by\nMarkman. We show that $\\mathrm{dim}(X)$ is a bound for any $X$ of\n$K3^{[n]}$-type. We also show that the bound can be reduced to\n$\\frac{1}{2}\\mathrm{dim}(X)$, as conjectured by Huybrechts, when the Picard\nrank of $X$ is at least 3.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "We present a comparison of model-space extrapolation methods for No-Core\nShell Model calculations of ground-state energies and root-mean-square radii in\nLi isotopes. In particular, we benchmark the latest machine learning tools\nagainst widely used exponential and infrared extrapolations for energies and\ncrossing point estimates for radii. Our findings demonstrate that machine\nlearning-based approaches provide reliable predictions with robust statistical\nuncertainties for both observables even in small model spaces. These\npredictions are compatible with established exponential and IR extrapolations\nof energies and mark a notable improvement over conventional radius estimates.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "In this article, we extend our previous work (Applicable Analysis, 2024, pp.\n1-25) on the steepest descent method for uncertain multiobjective optimization\nproblems. While that study established local convergence, it did not address\nglobal convergence and the rate of convergence of the steepest descent\nalgorithm. To bridge this gap, we provide rigorous proofs for both global\nconvergence and the linear convergence rate of the steepest descent algorithm.\nGlobal convergence analysis strengthens the theoretical foundation of the\nsteepest descent method for uncertain multiobjective optimization problems,\noffering deeper insights into its efficiency and robustness across a broader\nclass of optimization problems. These findings enhance the method's practical\napplicability and contribute to the advancement of robust optimization\ntechniques.",
        "Based on bond percolation theory, a method is presented here to calculate the\nrelationship between capillary pressure and saturation in porous media from\nfirst principles. The governing equations are formulated on the undirected\ngraph of the pore network. The graph is a simplified mathematical object that\naccounts for the topology of the pore structure. Thus, the calculation is\nextremely computationally efficient since it is mesh-free and voxel-free. Two\ntopological invariants are identified: The bond percolation threshold and the\nresidual saturation. Bond percolation theory is used to obtain a closed-form\npressure-saturation relation in terms of the geometry of the pores (pore throat\ndistribution) and material parameters (contact angle and interfacial tension),\nuniversal exponents, and topological invariants, based on scaling relations."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice",
    "start_abstract":"Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network"
      ],
      "abstract":[
        "Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "EdgeRegNet: Edge Feature-based Multimodal Registration Network between\n  Images and LiDAR Point Clouds",
        "Solution for 8th Competition on Affective & Behavior Analysis\n  in-the-wild",
        "EigenActor: Variant Body-Object Interaction Generation Evolved from\n  Invariant Action Basis Reasoning",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
        "MMHMER:Multi-viewer and Multi-task for Handwritten Mathematical\n  Expression Recognition",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared\n  Imaging",
        "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
        "Text-Driven Diffusion Model for Sign Language Production",
        "Tuning-Free Long Video Generation via Global-Local Collaborative\n  Diffusion",
        "CeTAD: Towards Certified Toxicity-Aware Distance in Vision Language\n  Models",
        "SegAnyPET: Universal Promptable Segmentation from Positron Emission\n  Tomography Images",
        "Measuring Anxiety Levels with Head Motion Patterns in Severe Depression\n  Population",
        "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
        "Theory-to-Practice Gap for Neural Networks and Neural Operators",
        "Quantitative Derivation of the Two-Component Gross-Pitaevskii Equation\n  with Uniform-in-Time Convergence Rate",
        "Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation\n  Problems",
        "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
        "Characterizing Data Visualization Literacy: a Systematic Literature\n  Review",
        "Geodesic Diffusion Models for Medical Image-to-Image Generation",
        "Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size\n  and Data",
        "Optimisation of space-time periodic eigenvalues",
        "The least balanced graphs and trees",
        "One Stack, Diverse Vehicles: Checking Safe Portability of Automated\n  Driving Software",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Diffusion on Graph: Augmentation of Graph Structure for Node\n  Classification",
        "Prophet Inequalities for Bandits, Cabinets, and DAGs",
        "Fast Debiasing of the LASSO Estimator"
      ],
      "abstract":[
        "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.",
        "In this report, we present our solution for the Action Unit (AU) Detection\nChallenge, in 8th Competition on Affective Behavior Analysis in-the-wild. In\norder to achieve robust and accurate classification of facial action unit in\nthe wild environment, we introduce an innovative method that leverages\naudio-visual multimodal data. Our method employs ConvNeXt as the image encoder\nand uses Whisper to extract Mel spectrogram features. For these features, we\nutilize a Transformer encoder-based feature fusion module to integrate the\naffective information embedded in audio and image features. This ensures the\nprovision of rich high-dimensional feature representations for the subsequent\nmultilayer perceptron (MLP) trained on the Aff-Wild2 dataset, enhancing the\naccuracy of AU detection.",
        "This paper explores a cross-modality synthesis task that infers 3D\nhuman-object interactions (HOIs) from a given text-based instruction. Existing\ntext-to-HOI synthesis methods mainly deploy a direct mapping from texts to\nobject-specific 3D body motions, which may encounter a performance bottleneck\nsince the huge cross-modality gap. In this paper, we observe that those HOI\nsamples with the same interaction intention toward different targets, e.g.,\n\"lift a chair\" and \"lift a cup\", always encapsulate similar action-specific\nbody motion patterns while characterizing different object-specific interaction\nstyles. Thus, learning effective action-specific motion priors and\nobject-specific interaction priors is crucial for a text-to-HOI model and\ndominates its performances on text-HOI semantic consistency and body-object\ninteraction realism. In light of this, we propose a novel body pose generation\nstrategy for the text-to-HOI task: infer object-agnostic canonical body action\nfirst and then enrich object-specific interaction styles. Specifically, the\nfirst canonical body action inference stage focuses on learning intra-class\nshareable body motion priors and mapping given text-based semantics to\naction-specific canonical 3D body motions. Then, in the object-specific\ninteraction inference stage, we focus on object affordance learning and enrich\nobject-specific interaction styles on an inferred action-specific body motion\nbasis. Extensive experiments verify that our proposed text-to-HOI synthesis\nsystem significantly outperforms other SOTA methods on three large-scale\ndatasets with better semantic consistency and interaction realism performances.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE",
        "Handwritten Mathematical Expression Recognition (HMER) methods have made\nremarkable progress, with most existing HMER approaches based on either a\nhybrid CNN\/RNN-based with GRU architecture or Transformer architectures. Each\nof these has its strengths and weaknesses. Leveraging different model\nstructures as viewers and effectively integrating their diverse capabilities\npresents an intriguing avenue for exploration. This involves addressing two key\nchallenges: 1) How to fuse these two methods effectively, and 2) How to achieve\nhigher performance under an appropriate level of complexity. This paper\nproposes an efficient CNN-Transformer multi-viewer, multi-task approach to\nenhance the model's recognition performance. Our MMHMER model achieves 63.96%,\n62.51%, and 65.46% ExpRate on CROHME14, CROHME16, and CROHME19, outperforming\nPosformer with an absolute gain of 1.28%, 1.48%, and 0.58%. The main\ncontribution of our approach is that we propose a new multi-view, multi-task\nframework that can effectively integrate the strengths of CNN and Transformer.\nBy leveraging the feature extraction capabilities of CNN and the sequence\nmodeling capabilities of Transformer, our model can better handle the\ncomplexity of handwritten mathematical expressions.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "Thermal imaging is often compromised by dynamic, complex degradations caused\nby hardware limitations and unpredictable environmental factors. The scarcity\nof high-quality infrared data, coupled with the challenges of dynamic,\nintricate degradations, makes it difficult to recover details using existing\nmethods. In this paper, we introduce thermal degradation simulation integrated\ninto the training process via a mini-max optimization, by modeling these\ndegraded factors as adversarial attacks on thermal images. The simulation is\ndynamic to maximize objective functions, thus capturing a broad spectrum of\ndegraded data distributions. This approach enables training with limited data,\nthereby improving model performance.Additionally, we introduce a\ndual-interaction network that combines the benefits of spiking neural networks\nwith scale transformation to capture degraded features with sharp spike signal\nintensities. This architecture ensures compact model parameters while\npreserving efficient feature representation. Extensive experiments demonstrate\nthat our method not only achieves superior visual quality under diverse single\nand composited degradation, but also delivers a significant reduction in\nprocessing when trained on only fifty clear images, outperforming existing\ntechniques in efficiency and accuracy. The source code will be available at\nhttps:\/\/github.com\/LiuZhu-CV\/DEAL.",
        "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps:\/\/github.com\/Intern-Nexus\/Infinite-Mobility",
        "We introduce the hfut-lmc team's solution to the SLRTP Sign Production\nChallenge. The challenge aims to generate semantically aligned sign language\npose sequences from text inputs. To this end, we propose a Text-driven\nDiffusion Model (TDM) framework. During the training phase, TDM utilizes an\nencoder to encode text sequences and incorporates them into the diffusion model\nas conditional input to generate sign pose sequences. To guarantee the high\nquality and accuracy of the generated pose sequences, we utilize two key loss\nfunctions. The joint loss function L_{joint} is used to precisely measure and\nminimize the differences between the joint positions of the generated pose\nsequences and those of the ground truth. Similarly, the bone orientation loss\nfunction L_{bone} is instrumental in ensuring that the orientation of the bones\nin the generated poses aligns with the actual, correct orientations. In the\ninference stage, the TDM framework takes on a different yet equally important\ntask. It starts with noisy sequences and, under the strict constraints of the\ntext conditions, gradually refines and generates semantically consistent sign\nlanguage pose sequences. Our carefully designed framework performs well on the\nsign language production task, and our solution achieves a BLEU-1 score of\n20.17, placing second in the challenge.",
        "Creating high-fidelity, coherent long videos is a sought-after aspiration.\nWhile recent video diffusion models have shown promising potential, they still\ngrapple with spatiotemporal inconsistencies and high computational resource\ndemands. We propose GLC-Diffusion, a tuning-free method for long video\ngeneration. It models the long video denoising process by establishing\ndenoising trajectories through Global-Local Collaborative Denoising to ensure\noverall content consistency and temporal coherence between frames.\nAdditionally, we introduce a Noise Reinitialization strategy which combines\nlocal noise shuffling with frequency fusion to improve global content\nconsistency and visual diversity. Further, we propose a Video Motion\nConsistency Refinement (VMCR) module that computes the gradient of pixel-wise\nand frequency-wise losses to enhance visual consistency and temporal\nsmoothness. Extensive experiments, including quantitative and qualitative\nevaluations on videos of varying lengths (\\textit{e.g.}, 3\\times and 6\\times\nlonger), demonstrate that our method effectively integrates with existing video\ndiffusion models, producing coherent, high-fidelity long videos superior to\nprevious approaches.",
        "Recent advances in large vision-language models (VLMs) have demonstrated\nremarkable success across a wide range of visual understanding tasks. However,\nthe robustness of these models against jailbreak attacks remains an open\nchallenge. In this work, we propose a universal certified defence framework to\nsafeguard VLMs rigorously against potential visual jailbreak attacks. First, we\nproposed a novel distance metric to quantify semantic discrepancies between\nmalicious and intended responses, capturing subtle differences often overlooked\nby conventional cosine similarity-based measures. Then, we devise a regressed\ncertification approach that employs randomized smoothing to provide formal\nrobustness guarantees against both adversarial and structural perturbations,\neven under black-box settings. Complementing this, our feature-space defence\nintroduces noise distributions (e.g., Gaussian, Laplacian) into the latent\nembeddings to safeguard against both pixel-level and structure-level\nperturbations. Our results highlight the potential of a formally grounded,\nintegrated strategy toward building more resilient and trustworthy VLMs.",
        "Positron Emission Tomography (PET) imaging plays a crucial role in modern\nmedical diagnostics by revealing the metabolic processes within a patient's\nbody, which is essential for quantification of therapy response and monitoring\ntreatment progress. However, the segmentation of PET images presents unique\nchallenges due to their lower contrast and less distinct boundaries compared to\nother structural medical modalities. Recent developments in segmentation\nfoundation models have shown superior versatility across diverse natural image\nsegmentation tasks. Despite the efforts of medical adaptations, these works\nprimarily focus on structural medical images with detailed physiological\nstructural information and exhibit poor generalization ability when adapted to\nmolecular PET imaging. In this paper, we collect and construct PETS-5k, the\nlargest PET segmentation dataset to date, comprising 5,731 three-dimensional\nwhole-body PET images and encompassing over 1.3M 2D images. Based on the\nestablished dataset, we develop SegAnyPET, a modality-specific 3D foundation\nmodel for universal promptable segmentation from PET images. To issue the\nchallenge of discrepant annotation quality of PET images, we adopt a cross\nprompting confident learning (CPCL) strategy with an uncertainty-guided\nself-rectification process to robustly learn segmentation from high-quality\nlabeled data and low-quality noisy labeled data. Experimental results\ndemonstrate that SegAnyPET can correctly segment seen and unseen targets using\nonly one or a few prompt points, outperforming state-of-the-art foundation\nmodels and task-specific fully supervised models with higher accuracy and\nstrong generalization ability for universal segmentation. As the first\nfoundation model for PET images, we believe that SegAnyPET will advance the\napplications to various downstream tasks for molecular imaging.",
        "Depression and anxiety are prevalent mental health disorders that frequently\ncooccur, with anxiety significantly influencing both the manifestation and\ntreatment of depression. An accurate assessment of anxiety levels in\nindividuals with depression is crucial to develop effective and personalized\ntreatment plans. This study proposes a new noninvasive method for quantifying\nanxiety severity by analyzing head movements -- specifically speed,\nacceleration, and angular displacement -- during video-recorded interviews with\npatients suffering from severe depression. Using data from a new CALYPSO\nDepression Dataset, we extracted head motion characteristics and applied\nregression analysis to predict clinically evaluated anxiety levels. Our results\ndemonstrate a high level of precision, achieving a mean absolute error (MAE) of\n0.35 in predicting the severity of psychological anxiety based on head movement\npatterns. This indicates that our approach can enhance the understanding of\nanxiety's role in depression and assist psychiatrists in refining treatment\nstrategies for individuals.",
        "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting\/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "This work studies the sampling complexity of learning with ReLU neural\nnetworks and neural operators. For mappings belonging to relevant approximation\nspaces, we derive upper bounds on the best-possible convergence rate of any\nlearning algorithm, with respect to the number of samples. In the\nfinite-dimensional case, these bounds imply a gap between the parametric and\nsampling complexities of learning, known as the \\emph{theory-to-practice gap}.\nIn this work, a unified treatment of the theory-to-practice gap is achieved in\na general $L^p$-setting, while at the same time improving available bounds in\nthe literature. Furthermore, based on these results the theory-to-practice gap\nis extended to the infinite-dimensional setting of operator learning. Our\nresults apply to Deep Operator Networks and integral kernel-based neural\noperators, including the Fourier neural operator. We show that the\nbest-possible convergence rate in a Bochner $L^p$-norm is bounded by\nMonte-Carlo rates of order $1\/p$.",
        "We derive the time-dependent two-component Gross-Pitaevskii equation as an\neffective description of the dynamics of a dilute two-component Bose gas near\nits ground state, which exhibits a two-component mixture Bose-Einstein\ncondensate, in the Gross-Pitaevskii limit regime. Our main result establishes a\nuniform-in-time bound on the convergence rate between the many-body dynamics\nand the effective description, explicitly quantified in terms of the particle\nnumber $N$. This improves upon the works of Michelangeli and Olgliati [73, 85]\nby providing a sharper, $N$-dependent, time-independent convergence rate. Our\napproach also extends the framework of Benedikter, de Oliveira, and Schlein\n[10] to the multi-component Bose gas setting. More specifically, we develop the\nnecessary Bogoliubov theory to analyze the dynamics of multi-component Bose\ngases in the Gross-Pitaevskii regime.",
        "We consider a learning-augmented framework for NP-hard permutation problems.\nThe algorithm has access to predictions telling, given a pair $u,v$ of\nelements, whether $u$ is before $v$ or not in an optimal solution. Building on\nthe work of Braverman and Mossel (SODA 2008), we show that for a class of\noptimization problems including scheduling, network design and other graph\npermutation problems, these predictions allow to solve them in polynomial time\nwith high probability, provided that predictions are true with probability at\nleast $1\/2+\\epsilon$. Moreover, this can be achieved with a parsimonious access\nto the predictions.",
        "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps:\/\/github.com\/LuckyGirl-XU\/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
        "With the advent of the data era, and of new, more intelligent interfaces for\nsupporting decision making, there is a growing need to define, model and assess\nhuman ability and data visualizations usability for a better encoding and\ndecoding of data patterns. Data Visualization Literacy (DVL) is the ability of\nencoding and decoding data into and from a visual language. Although this\nability and its measurement are crucial for advancing human knowledge and\ndecision capacity, they have seldom been investigated, let alone\nsystematically. To address this gap, this paper presents a systematic\nliterature review comprising 43 reports on DVL, analyzed using the PRISMA\nmethodology. Our results include the identification of the purposes of DVL, its\nsatellite aspects, the models proposed, and the assessments designed to\nevaluate the degree of DVL of people. Eventually, we devise many research\ndirections including, among the most challenging, the definition of a\n(standard) unifying construct of DVL.",
        "Diffusion models transform an unknown data distribution into a Gaussian prior\nby progressively adding noise until the data become indistinguishable from pure\nnoise. This stochastic process traces a path in probability space, evolving\nfrom the original data distribution (considered as a Gaussian with near-zero\nvariance) to an isotropic Gaussian. The denoiser then learns to reverse this\nprocess, generating high-quality samples from random Gaussian noise. However,\nstandard diffusion models, such as the Denoising Diffusion Probabilistic Model\n(DDPM), do not ensure a geodesic (i.e., shortest) path in probability space.\nThis inefficiency necessitates the use of many intermediate time steps, leading\nto high computational costs in training and sampling. To address this\nlimitation, we propose the Geodesic Diffusion Model (GDM), which defines a\ngeodesic path under the Fisher-Rao metric with a variance-exploding noise\nscheduler. This formulation transforms the data distribution into a Gaussian\nprior with minimal energy, significantly improving the efficiency of diffusion\nmodels. We trained GDM by continuously sampling time steps from 0 to 1 and\nusing as few as 15 evenly spaced time steps for model sampling. We evaluated\nGDM on two medical image-to-image generation tasks: CT image denoising and MRI\nimage super-resolution. Experimental results show that GDM achieved\nstate-of-the-art performance while reducing training time by a 50-fold compared\nto DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than\nDDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable\nrapid model exploration and real-time clinical applications. Our code is\npublicly available at: https:\/\/github.com\/mirthAI\/GDM-VE.",
        "In this work, we will establish that the Langevin Monte-Carlo algorithm can\nlearn depth-2 neural nets of any size and for any data and we give\nnon-asymptotic convergence rates for it. We achieve this via showing that under\nTotal Variation distance and q-Renyi divergence, the iterates of Langevin Monte\nCarlo converge to the Gibbs distribution of Frobenius norm regularized losses\nfor any of these nets, when using smooth activations and in both classification\nand regression settings. Most critically, the amount of regularization needed\nfor our results is independent of the size of the net. This result combines\nseveral recent observations, like our previous papers showing that two-layer\nneural loss functions can always be regularized by a certain constant amount\nsuch that they satisfy the Villani conditions, and thus their Gibbs measures\nsatisfy a Poincare inequality.",
        "The goal of this paper is to provide a qualitative analysis of the\noptimisation of space-time periodic principal eigenvalues. Namely, considering\na fixed time horizon $T$ and the $d$-dimensional torus $\\mathbb{T}^d$, let, for\nany $m\\in L^\\infty((0,T)\\times\\mathbb{T}^d)$, $\\lambda(m)$ be the principal\neigenvalue of the operator $\\partial_t-\\Delta-m$ endowed with (time-space)\nperiodic boundary conditions. The main question we set out to answer is the\nfollowing: how to choose $m$ so as to minimise $\\lambda(m)$? This question\nstems from population dynamics. We prove that in several cases it is always\nbeneficial to rearrange $m$ with respect to time in a symmetric way, which is\nthe first comparison result for the rearrangement in time of parabolic\nequations. Furthermore, we investigate the validity (or lack thereof) of\nTalenti inequalities for the rearrangement in time of parabolic equations. The\nnumerical simulations which illustrate our results were obtained by developing\na framework within which it is possible to optimise criteria with respect to\nfunctions having a prescribed rearrangement (or distribution function).",
        "Given a connected graph, the principal eigenvector of the adjacency matrix\n(often called the Perron vector) can be used to assign positive weights to the\nvertices. A natural way to measure the homogeneousness of this vector is by\nconsidering the ratio of its $\\ell^1$ and $\\ell^2$ norms.\n  It is easy to see that the most balanced graphs in this sense (i.e., the ones\nwith the largest ratio) are the regular graphs. What about the least balanced\ngraphs with the smallest ratio? It was conjectured by R\\\"ucker, R\\\"ucker and\nGutman that, for any given $n \\geq 6$, among $n$-vertex connected graphs the\nsmallest ratio is achieved by the complete graph $K_4$ with a single path\n$P_{n-4}$ attached to one of its vertices. In this paper we confirm this\nconjecture.\n  We also verify the analogous conjecture for trees: for any given $n \\geq 8$,\namong $n$-vertex trees the smallest ratio is achieved by the star graph $S_5$\nwith a path $P_{n-5}$ attached to its central vertex.",
        "Integrating an automated driving software stack into vehicles with variable\nconfiguration is challenging, especially due to different hardware\ncharacteristics. Further, to provide software updates to a vehicle fleet in the\nfield, the functional safety of every affected configuration has to be ensured.\nThese additional demands for dependability and the increasing hardware\ndiversity in automated driving make rigorous automatic analysis essential. This\npaper addresses this challenge by using formal portability checking of adaptive\ncruise controller code for different vehicle configurations. Given a formal\nspecification of the safe behavior, models of target configurations are\nderived, which capture relevant effects of sensors, actuators and computing\nplatforms. A corresponding safe set is obtained and used to check if the\ndesired behavior is achievable on all targets. In a case study, portability\nchecking of a traditional and a neural network controller are performed\nautomatically within minutes for each vehicle hardware configuration. The check\nprovides feedback for necessary adaptations of the controllers, thus, allowing\nrapid integration and testing of software or parameter changes.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "Graph diffusion models have recently been proposed to synthesize entire\ngraphs, such as molecule graphs. Although existing methods have shown great\nperformance in generating entire graphs for graph-level learning tasks, no\ngraph diffusion models have been developed to generate synthetic graph\nstructures, that is, synthetic nodes and associated edges within a given graph,\nfor node-level learning tasks. Inspired by the research in the computer vision\nliterature using synthetic data for enhanced performance, we propose Diffusion\non Graph (DoG), which generates synthetic graph structures to boost the\nperformance of GNNs. The synthetic graph structures generated by DoG are\ncombined with the original graph to form an augmented graph for the training of\nnode-level learning tasks, such as node classification and graph contrastive\nlearning (GCL). To improve the efficiency of the generation process, a Bi-Level\nNeighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse\neffect of the noise introduced by the synthetic graph structures, a low-rank\nregularization method is proposed for the training of graph neural networks\n(GNNs) on the augmented graphs. Extensive experiments on various graph datasets\nfor semi-supervised node classification and graph contrastive learning have\nbeen conducted to demonstrate the effectiveness of DoG with low-rank\nregularization. The code of DoG is available at\nhttps:\/\/github.com\/Statistical-Deep-Learning\/DoG.",
        "A decisionmaker faces $n$ alternatives, each of which represents a potential\nreward. After investing costly resources into investigating the alternatives,\nthe decisionmaker may select one, or more generally a feasible subset, and\nobtain the associated reward(s). The objective is to maximize the sum of\nrewards minus total costs invested. We consider this problem under a general\nmodel of an alternative as a \"Markov Search Process,\" a type of undiscounted\nMarkov Decision Process on a finite acyclic graph. Even simple cases generalize\nNP-hard problems such as Pandora's Box with nonobligatory inspection.\n  Despite the apparently adaptive and interactive nature of the problem, we\nprove optimal prophet inequalities for this problem under a variety of\ncombinatorial constraints. That is, we give approximation algorithms that\ninteract with the alternatives sequentially, where each must be fully explored\nand either selected or else discarded before the next arrives. In particular,\nwe obtain a computationally efficient $\\frac{1}{2}-\\epsilon$ prophet inequality\nfor Combinatorial Markov Search subject to any matroid constraint. This result\nimplies incentive-compatible mechanisms with constant Price of Anarchy for\nserving single-parameter agents when the agents strategically conduct\nindependent, costly search processes to discover their values.",
        "In high-dimensional sparse regression, the \\textsc{Lasso} estimator offers\nexcellent theoretical guarantees but is well-known to produce biased estimates.\nTo address this, \\cite{Javanmard2014} introduced a method to ``debias\" the\n\\textsc{Lasso} estimates for a random sub-Gaussian sensing matrix\n$\\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse\"\n$\\boldsymbol{M}$ of the matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}\/n$ by\nsolving a convex optimization problem. This matrix $\\boldsymbol{M}$ plays a\ncritical role in mitigating bias and allowing for construction of confidence\nintervals using the debiased \\textsc{Lasso} estimates. However the computation\nof $\\boldsymbol{M}$ is expensive in practice as it requires iterative\noptimization. In the presented work, we re-parameterize the optimization\nproblem to compute a ``debiasing matrix\" $\\boldsymbol{W} :=\n\\boldsymbol{AM}^{\\top}$ directly, rather than the approximate inverse\n$\\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the\ndebiased \\textsc{Lasso} estimates, as they depend on the \\emph{product}\n$\\boldsymbol{AM}^{\\top}$ rather than on $\\boldsymbol{M}$ alone. Notably, we\nprovide a simple, computationally efficient, closed-form solution for\n$\\boldsymbol{W}$ under similar conditions for the sensing matrix\n$\\boldsymbol{A}$ used in the original debiasing formulation, with an additional\ncondition that the elements of every row of $\\boldsymbol{A}$ have uncorrelated\nentries. Also, the optimization problem based on $\\boldsymbol{W}$ guarantees a\nunique optimal solution, unlike the original formulation based on\n$\\boldsymbol{M}$. We verify our main result with numerical simulations."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)",
    "start_abstract":"Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence.",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"
      ],
      "abstract":[
        "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
        "Building Knowledge Graphs Towards a Global Food Systems Datahub",
        "Intention Recognition in Real-Time Interactive Navigation Maps",
        "A Study on Educational Data Analysis and Personalized Feedback Report\n  Generation Based on Tags and ChatGPT",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "FedGAI: Federated Style Learning with Cloud-Edge Collaboration for\n  Generative AI in Fashion Design",
        "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
        "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework",
        "Representation and Interpretation in Artificial and Natural Computing",
        "Assessing instructor-AI cooperation for grading essay-type questions in\n  an introductory sociology course",
        "A Guide to Bayesian Networks Software Packages for Structure and\n  Parameter Learning -- 2025 Edition",
        "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal\n  Plot Graphs",
        "Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of\n  Vision-Language Models",
        "Velocity-free task-space regulator for robot manipulators with external\n  disturbances",
        "Learning from Active Human Involvement through Proxy Value Propagation",
        "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance",
        "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets",
        "HoneypotNet: Backdoor Attacks Against Model Extraction",
        "Integral gains for non-autonomous Wazewski systems",
        "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
        "The Large-Scale Structure of Entanglement in Quantum Many-body Systems",
        "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
        "Equivariant localization in Batalin-Vilkovisky formalism",
        "Insights into dendritic growth mechanisms in batteries: A combined\n  machine learning and computational study",
        "General Stability Estimates in NonLocal Traffic Models for Several\n  Populations",
        "A non-D-continuum with weakly infinite-dimensional closed\n  set-aposyndetic Whitney levels",
        "We Can't Understand AI Using our Existing Vocabulary"
      ],
      "abstract":[
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
        "Sustainable agricultural production aligns with several sustainability goals\nestablished by the United Nations (UN). However, there is a lack of studies\nthat comprehensively examine sustainable agricultural practices across various\nproducts and production methods. Such research could provide valuable insights\ninto the diverse factors influencing the sustainability of specific crops and\nproduce while also identifying practices and conditions that are universally\napplicable to all forms of agricultural production. While this research might\nhelp us better understand sustainability, the community would still need a\nconsistent set of vocabularies. These consistent vocabularies, which represent\nthe underlying datasets, can then be stored in a global food systems datahub.\nThe standardized vocabularies might help encode important information for\nfurther statistical analyses and AI\/ML approaches in the datasets, resulting in\nthe research targeting sustainable agricultural production. A structured method\nof representing information in sustainability, especially for wheat production,\nis currently unavailable. In an attempt to address this gap, we are building a\nset of ontologies and Knowledge Graphs (KGs) that encode knowledge associated\nwith sustainable wheat production using formal logic. The data for this set of\nknowledge graphs are collected from public data sources, experimental results\ncollected at our experiments at Kansas State University, and a Sustainability\nWorkshop that we organized earlier in the year, which helped us collect input\nfrom different stakeholders throughout the value chain of wheat. The modeling\nof the ontology (i.e., the schema) for the Knowledge Graph has been in progress\nwith the help of our domain experts, following a modular structure using KNARM\nmethodology. In this paper, we will present our preliminary results and schemas\nof our Knowledge Graph and ontologies.",
        "In this demonstration, we develop IntentRec4Maps, a system to recognise\nusers' intentions in interactive maps for real-world navigation. IntentRec4Maps\nuses the Google Maps Platform as the real-world interactive map, and a very\neffective approach for recognising users' intentions in real-time. We showcase\nthe recognition process of IntentRec4Maps using two different Path-Planners and\na Large Language Model (LLM).\n  GitHub: https:\/\/github.com\/PeijieZ\/IntentRec4Maps",
        "This study introduces a novel method that employs tag annotation coupled with\nthe ChatGPT language model to analyze student learning behaviors and generate\npersonalized feedback. Central to this approach is the conversion of complex\nstudent data into an extensive set of tags, which are then decoded through\ntailored prompts to deliver constructive feedback that encourages rather than\ndiscourages students. This methodology focuses on accurately feeding student\ndata into large language models and crafting prompts that enhance the\nconstructive nature of feedback. The effectiveness of this approach was\nvalidated through surveys conducted with over 20 mathematics teachers, who\nconfirmed the reliability of the generated reports. This method can be\nseamlessly integrated into intelligent adaptive learning systems or provided as\na tool to significantly reduce the workload of teachers, providing accurate and\ntimely feedback to students. By transforming raw educational data into\ninterpretable tags, this method supports the provision of efficient and timely\npersonalized learning feedback that offers constructive suggestions tailored to\nindividual learner needs.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Collaboration can amalgamate diverse ideas, styles, and visual elements,\nfostering creativity and innovation among different designers. In collaborative\ndesign, sketches play a pivotal role as a means of expressing design\ncreativity. However, designers often tend to not openly share these\nmeticulously crafted sketches. This phenomenon of data island in the design\narea hinders its digital transformation under the third wave of AI. In this\npaper, we introduce a Federated Generative Artificial Intelligence Clothing\nsystem, namely FedGAI, employing federated learning to aid in sketch design.\nFedGAI is committed to establishing an ecosystem wherein designers can exchange\nsketch styles among themselves. Through FedGAI, designers can generate sketches\nthat incorporate various designers' styles from their peers, drawing\ninspiration from collaboration without the need for data disclosure or upload.\nExtensive performance evaluations indicate that our FedGAI system can produce\nmulti-styled sketches of comparable quality to human-designed ones while\nsignificantly enhancing efficiency compared to hand-drawn sketches.",
        "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
        "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.",
        "Artificial computing machinery transforms representations through an\nobjective process, to be interpreted subjectively by humans, so the machine and\nthe interpreter are different entities, but in the putative natural computing\nboth processes are performed by the same agent. The method or process that\ntransforms a representation is called here \\emph{the mode of computing}. The\nmode used by digital computers is the algorithmic one, but there are others,\nsuch as quantum computers and diverse forms of non-conventional computing, and\nthere is an open-ended set of representational formats and modes that could be\nused in artificial and natural computing. A mode based on a notion of computing\ndifferent from Turing's may perform feats beyond what the Turing Machine does\nbut the modes would not be of the same kind and could not be compared. For a\nmode of computing to be more powerful than the algorithmic one, it ought to\ncompute functions lacking an effective algorithm, and Church Thesis would not\nhold. Here, a thought experiment including a computational demon using a\nhypothetical mode for such an effect is presented. If there is natural\ncomputing, there is a mode of natural computing whose properties may be causal\nto the phenomenological experience. Discovering it would come with solving the\nhard problem of consciousness; but if it turns out that such a mode does not\nexist, there is no such thing as natural computing, and the mind is not a\ncomputational process.",
        "This study explores the use of artificial intelligence (AI) as a\ncomplementary tool for grading essay-type questions in higher education,\nfocusing on its consistency with human grading and potential to reduce biases.\nUsing 70 handwritten exams from an introductory sociology course, we evaluated\ngenerative pre-trained transformers (GPT) models' performance in transcribing\nand scoring students' responses. GPT models were tested under various settings\nfor both transcription and grading tasks. Results show high similarity between\nhuman and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in\naccuracy. For grading, GPT demonstrated strong correlations with the human\ngrader scores, especially when template answers were provided. However,\ndiscrepancies remained, highlighting GPT's role as a \"second grader\" to flag\ninconsistencies for assessment reviewing rather than fully replace human\nevaluation. This study contributes to the growing literature on AI in\neducation, demonstrating its potential to enhance fairness and efficiency in\ngrading essay-type questions.",
        "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
        "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
        "Test-time adaptation (TTA) is crucial in maintaining Vision-Language Models\n(VLMs) performance when facing real-world distribution shifts, particularly\nwhen the source data or target labels are inaccessible. Existing TTA methods\nrely on CLIP's output probability distribution for feature evaluation, which\ncan introduce biases under domain shifts. This misalignment may cause features\nto be misclassified due to text priors or incorrect textual associations. To\naddress these limitations, we propose Bidirectional Prototype-Reward\nco-Evolution (BPRE), a novel TTA framework for VLMs that integrates feature\nquality assessment with prototype evolution through a synergistic feedback\nloop. BPRE first employs a Multi-Dimensional Quality-Aware Reward Module to\nevaluate feature quality and guide prototype refinement precisely. The\ncontinuous refinement of prototype quality through Prototype-Reward Interactive\nEvolution will subsequently enhance the computation of more robust\nMulti-Dimensional Quality-Aware Reward Scores. Through the bidirectional\ninteraction, the precision of rewards and the evolution of prototypes mutually\nreinforce each other, forming a self-evolving cycle. Extensive experiments are\nconducted across 15 diverse recognition datasets encompassing natural\ndistribution shifts and cross-dataset generalization scenarios. Results\ndemonstrate that BPRE consistently achieves superior average performance\ncompared to state-of-the-art methods across different model architectures, such\nas ResNet-50 and ViT-B\/16. By emphasizing comprehensive feature evaluation and\nbidirectional knowledge refinement, BPRE advances VLM generalization\ncapabilities, offering a new perspective on TTA.",
        "This paper addresses the problem of task-space robust regulation of robot\nmanipulators subject to external disturbances. A velocity-free control law is\nproposed by combining the internal model principle and the passivity-based\noutput-feedback control approach. The developed output-feedback controller\nensures not only asymptotic convergence of the regulation error but also\nsuppression of unwanted external step\/sinusoidal disturbances. The potential of\nthe proposed method lies in its simplicity, intuitively appealing, and simple\ngain selection criteria for synthesis of multi-joint robot manipulator control\nsystems.",
        "Learning from active human involvement enables the human subject to actively\nintervene and demonstrate to the AI agent during training. The interaction and\ncorrective feedback from human brings safety and AI alignment to the learning\nprocess. In this work, we propose a new reward-free active human involvement\nmethod called Proxy Value Propagation for policy optimization. Our key insight\nis that a proxy value function can be designed to express human intents,\nwherein state-action pairs in the human demonstration are labeled with high\nvalues, while those agents' actions that are intervened receive low values.\nThrough the TD-learning framework, labeled values of demonstrated state-action\npairs are further propagated to other unlabeled data generated from agents'\nexploration. The proxy value function thus induces a policy that faithfully\nemulates human behaviors. Human-in-the-loop experiments show the generality and\nefficiency of our method. With minimal modification to existing reinforcement\nlearning algorithms, our method can learn to solve continuous and discrete\ncontrol tasks with various human control devices, including the challenging\ntask of driving in Grand Theft Auto V. Demo video and code are available at:\nhttps:\/\/metadriverse.github.io\/pvp",
        "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https:\/\/arc2avatar.github.io for more\nresources.",
        "In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.",
        "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
        "We show that the thermodynamic limit of a many-body system can reveal\nentanglement properties that are hard to detect in finite-size systems --\nsimilar to how phase transitions only sharply emerge in the thermodynamic\nlimit. The resulting operational entanglement properties are in one-to-one\ncorrespondence with abstract properties of the local observable algebras that\nemerge in the thermodynamic limit. These properties are insensitive to finite\nperturbations and hence describe the \\emph{large-scale structure of\nentanglement} of many-body systems. We formulate and discuss the emerging\nstructures and open questions, both for gapped and gapless many-body systems.\nIn particular, we show that every gapped phase of matter, even the trivial one,\nin $D\\geq 2$ dimensions contains models with the strongest possible bipartite\nlarge-scale entanglement. Conversely, we conjecture the existence of\ntopological phases of matter, where all representatives have the strongest form\nof entanglement.",
        "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
        "We derive equivariant localization formulas of Atiyah--Bott and cohomological\nfield theory types in the Batalin-Vilkovisky formalism and discuss their\napplications in Poisson geometry and quantum field theory.",
        "In recent years, researchers have increasingly sought batteries as an\nefficient and cost-effective solution for energy storage and supply, owing to\ntheir high energy density, low cost, and environmental resilience. However, the\nissue of dendrite growth has emerged as a significant obstacle in battery\ndevelopment. Excessive dendrite growth during charging and discharging\nprocesses can lead to battery short-circuiting, degradation of electrochemical\nperformance, reduced cycle life, and abnormal exothermic events. Consequently,\nunderstanding the dendrite growth process has become a key challenge for\nresearchers. In this study, we investigated dendrite growth mechanisms in\nbatteries using a combined machine learning approach, specifically a\ntwo-dimensional artificial convolutional neural network (CNN) model, along with\ncomputational methods. We developed two distinct computer models to predict\ndendrite growth in batteries. The CNN-1 model employs standard convolutional\nneural network techniques for dendritic growth prediction, while CNN-2\nintegrates additional physical parameters to enhance model robustness. Our\nresults demonstrate that CNN-2 significantly enhances prediction accuracy,\noffering deeper insights into the impact of physical factors on dendritic\ngrowth. This improved model effectively captures the dynamic nature of dendrite\nformation, exhibiting high accuracy and sensitivity. These findings contribute\nto the advancement of safer and more reliable energy storage systems.",
        "We prove global existence, uniqueness and $\\L1$ stability of solutions to\ngeneral systems of nonlocal conservation laws modeling multiclass vehicular\ntraffic. Each class follows its own speed law and has specific effects on the\nother classes' speeds. Moreover, general explicit dependencies of the speed\nlaws on space and time are allowed. Solutions are proved to depend continuously\n-- in suitable norms -- on all terms appearing in the equations, as well as on\nthe initial data. Numerical simulations show the relevance and the effects of\nthe nonlocal terms.",
        "In this paper, we introduce the new class of continua; weakly\ninfinite-dimensional closed set-aposyndetic continua. With this notion, we show\nthat there exists a non-D-continuum such that each positive Whitney level of\nthe hyperspace of the continuum is a weakly infinite-dimensional closed\nset-aposyndetic continuum. This result strengthens those of van Douwen and\nGoodykoontz [2], Illanes [7], and the main result of Illanes et al. [9].",
        "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation",
    "start_abstract":"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)"
      ],
      "abstract":[
        "Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence."
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Assessing ultrasonic and optical flow velocimetry in a millifluidic\n  device using oil-in-water emulsions as blood mimicking fluid",
        "Myocardial T1 mapping at 5T using multi-inversion recovery real-time\n  spoiled GRE",
        "Non-Invasive Temporal Interference Electrical Stimulation for Spinal\n  Cord Injury Rehabilitation: A Simulation Study",
        "Fabrication of Fibers with Complex Features Using Thermal Drawing of\n  3D-Printed Preforms",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Numerical Analysis of Antenna Parameter Influence on Brightness\n  Temperature in Medical Microwave Radiometers",
        "Analysis of the sensitivity of tumor control probability in molecular\n  radiotherapy to uncertainties in the dose rate curves",
        "Compound Mask for Divergent Wave Imaging in Medical Ultrasound",
        "Neutron relative effectiveness factors in Boron Neutron Capture Therapy:\n  estimation of their values from the secondary charged particles and\n  evaluation of weighted kerma factors for a standard tissue",
        "Developing an Agent-Based Mathematical Model for Simulating\n  Post-Irradiation Cellular Response: A Crucial Component of a Digital Twin\n  Framework for Personalized Radiation Treatment",
        "A self-contact electromechanical framework for intestinal motility",
        "Advancing Tumor Budding Detection with Fourier Ptychography Microscopy",
        "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection",
        "Efficient sampling approaches based on generalized Golub-Kahan methods\n  for large-scale hierarchical Bayesian inverse problems",
        "Optimal Insurance under Endogenous Default and Background Risk",
        "Optimizing confidence in negative-partial-transpose-based entanglement\n  criteria",
        "On almost Gallai colourings in complete graphs",
        "Stabilization of an unstable reaction-diffusion PDE with input delay\n  despite state and input quantization",
        "Rigidity in a Fixed Number Field and a Directional $p$-Adic Littlewood\n  Conjecture for Algebraic Vectors",
        "Enhancing finite-difference based derivative-free optimization methods\n  with machine learning",
        "New Representations of Catalan's Constant, Apery's Constant and the\n  Euler Numbers Obtained from the Half Hyperbolic Secant Distribution",
        "The COSMOS-Web deep galaxy group catalog up to $z=3.7$",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Explaining the Unexplainable: A Systematic Review of Explainable AI in\n  Finance",
        "A new transcendence measure for the values of the exponential function\n  at algebraic arguments",
        "Sensitivity analysis of path-dependent options in an incomplete market\n  with pathwise functional Ito calculus",
        "Scattering resonances and pairing in a Rabi-coupled Fermi gas",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry"
      ],
      "abstract":[
        "Blood-mimicking fluids (BMFs) play a critical role in ultrasonic imaging and\nDoppler flow studies by replicating the physical and acoustic properties of\nblood. This study introduces a novel soybean oil-in-water emulsion as a BMF\nwith particle size and deformability akin to red blood cells. Using a\nmillifluidic device, we cross-validated flow profiles through both Doppler\nvelocimetry and optical particle tracking, demonstrating compatibility with\ntheoretical Poiseuille flow models. The millifluidic chip, fabricated via\nstereolithography, provided an optimized platform for dual optical and\nultrasonic assessments. Results showed strong agreement between the two methods\nacross a range of flow rates, affirming the suitability of the emulsion for\nvelocimetry applications. Furthermore, the acoustic properties of soybean oil\ndroplets support their potential as an echogenic and stable alternative to\nconventional BMFs.",
        "Purpose: To develop an accurate myocardial T1 mapping technique at 5T using\nLook-Locker-based multiple inversion-recovery with the real-time spoiled\ngradient echo (GRE) acquisition. Methods: The proposed T1 mapping technique\n(mIR-rt) samples the recovery of inverted magnetization using the real-time GRE\nand the images captured during diastole are selected for T1 fitting.\nMultiple-inversion recoveries are employed to increase the sample size for\naccurate fitting. Furthermore, the inversion pulse (IR) was tailored for\ncardiac imaging at 5T, optimized to maximize the inversion efficiency over\nspecified ranges of B1 and off-resonance. The T1 mapping method was validated\nusing Bloch simulation, phantom studies, and in 16 healthy volunteers at 5T.\nResults: The optimized IR pulse based on the tangent\/hyperbolic tangent pulse\nwas found to outperform the conventional hyperbolic secant IR pulse within a\nlimited peak amplitude of 10.6 {\\mu}T at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014 within a B0 range of\n+\/-250Hz and a B1 range of -50% to 20%. In both simulation and phantom studies,\nthe T1 values measured by mIR-rt closely approximate the reference T1 values,\nwith errors less than 3%, while the conventional MOLLI sequence underestimates\nT1 values. The myocardial T1 values at 5T are 1553 +\/- 52 ms, 1531 +\/- 53 ms,\nand 1526 +\/- 60 ms (mean +\/- standard deviation) at the apex, middle, and base,\nrespectively. Conclusion: The proposed method is feasible for myocardial T1\nmapping at 5T and provides better accuracy than the conventional MOLLI\nsequence.\n  Keywords: Myocardial T1 mapping, 5T, Look-Locker",
        "Background: Spinal cord injury (SCI) rehabilitation remains a major clinical\nchallenge, with limited treatment options for functional recovery. Temporal\ninterference (TI) electrical stimulation has emerged as a promising\nnon-invasive neuromodulation technique capable of delivering deep and targeted\nstimulation. However, the application of TI stimulation in SCI rehabilitation\nremains largely unexplored. Methods: This study aims to investigate the\nfeasibility of applying non-invasive TI electrical stimulation for SCI\nrehabilitation. Through computational modeling, we analyzed the electric field\ndistribution characteristics in the spinal cord under different TI stimulation\nconfigurations. Based on these findings, we propose a clinically applicable TI\nstimulation protocol for SCI rehabilitation. Results: The results demonstrate\nthat TI stimulation can effectively deliver focused electric fields to targeted\nspinal cord segments while maintaining non-invasiveness. The electric field\nintensity varied depending on individual anatomical differences, highlighting\nthe need for personalized stimulation parameters. The proposed protocol\nprovides a practical framework for applying TI stimulation in SCI\nrehabilitation and offers a non-invasive alternative to traditional spinal cord\nstimulation techniques. Conclusions: This study establishes the feasibility of\nusing non-invasive TI stimulation for SCI rehabilitation. The proposed\nstimulation protocol enables precise and targeted spinal cord modulation.\nHowever, further research is needed to refine personalized stimulation\nparameters and validate the clinical efficacy of this approach.",
        "High-aspect-ratio polymer materials are widely utilized in applications\nranging from everyday materials such as clothing to specialized equipment in\nindustrial and medical fields. Traditional fabrication methods, such as\nextrusion and molding, face challenges in integrating diverse materials and\nachieving complex geometries. Additionally, these methods are limited in their\nability to provide low-cost and rapid prototyping, which are critical for\nresearch and development processes. In this work, we investigated the use of\ncommercially available 3D printers to fabricate fiber preforms, which were\nsubsequently thermally drawn into fibers. By optimizing 3D printing parameters,\nwe achieved the fabrication of fibers with diameters as small as 200 um having\ncomplex shapes, with features down to a few microns. We demonstrated the\nversatility of this method by fabricating fibers from diverse set of materials,\nsuch as fibers with different stiffnesses and fibers with magnetic\ncharacteristics, which are beneficial for developing tendon-driven and\nmagnetically actuated robotic fibers. In addition, by designing novel preform\ngeometries, we produced tapered fibers and fibers with interlocking mechanisms,\nalso tailored for use in medical steerable catheter applications. These\nadvancements highlight the scalability and versatility of this approach,\noffering a robust platform for producing high-precision polymer fibers for\ndiverse applications.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "This article presents a study on the influence of antenna parameters in\nmedical microwave radiometers on brightness temperature. A series of\ncomputational experiments was conducted to analyse the dependence of brightness\ntemperature on antenna characteristics. Various antenna parameters and their\neffect on the distribution of electromagnetic fields in biological tissues were\nexamined. It was demonstrated that considering the antenna mismatch parameter\nis crucial when modelling the brightness temperature of biological tissues,\ncontributing about 2 percent to its formation. The depth range of brightness\ntemperature measurement was determined. The dependence of brightness\ntemperature on the antenna diameter and frequency was established. The findings\nof this study can be applied to improve medical microwave radiometers and\nenhance their efficiency in the early diagnosis of various diseases.",
        "In this work, we have investigated the sensitivity of the effectiveness (TCP)\nof molecular radiotherapy (MRT) treatment to uncertainties of the dose rate\ncurves that may appear when reconstructing those curves.\n  We generated different dose rate curves from experimental data, imposing the\nconstraint of equal dose for each of them. Then, we computed TCPs and looked\nfor correlations between metrics measuring the differences between the dose\nrate curves and differences in TCP. Finally, according to these results, we\nestimated the range of tolerable uncertainties in the dose rate curves. The\nstudy was performed for different radiopharmaceuticals and different\nradiosensitive parameters that can affect the dose rate response\n($\\alpha\/\\beta$, sub-lethal repair rate).\n  The best correlation between differences in the dose rate curves and TCP was\nfound for a metric that computes averaged linear differences between the\ncurves. With this metric, we quantified differences in dose rate curves that\nwould lead to differences in TCP of 0.02, a parameter denoted $m_{1,\\: 0.02}$\nthat is a surrogate of the dependence of the TCP on the dose rate profile. The\nresults showed that the sensitivity of the TCP to dose rate variations\ndecreases (i.e. larger values of $m_{1,\\: 0.02}$) with increasing\n$\\alpha\/\\beta$ and sub-lethal damage repair rate of the tumor cells, and\nincreasing biological half-life of the dose rate curves.\n  The radiobiological effect of a MRT treatment on a tumor depends on the\nabsorbed dose and the dose rate profile. Ideally, both magnitudes should be\nmeasured with accuracy in order to progress towards the optimization of\nindividualized MRT treatments. Our study suggests that this would be more\nimportant for tumors with low $\\alpha\/\\beta$ and moderately slow sub-lethal\ndamage repair treated with fast-decaying radiopharmaceuticals.",
        "Divergent wave imaging with coherent compounding allows obtaining broad field\nof view and higher frame rate with respect to line by line insonification.\nHowever, the spatial and contrast resolution crucially depends on the weights\napplied in the compound phase, whose optimization is often cumbersome and based\non trial and error. This study addresses these limitations by introducing a\nclosed-form approach that maps the transmit apodization weights used in\nsynthetic aperture imaging into the compound mask applied to divergent wave\nimaging. The approach draws inspiration from a successful technique developed\nfor plane wave imaging, leveraging synthetic aperture imaging as a reference\ndue to its superior image quality. It works for both linear and convex\ngeometries and arbitrary spatial arrangements of virtual sources generating\ndivergent waves. The approach has been validated through simulated data using\nboth linear and convex probes, demonstrating that the Full Width at Half\nMaximum (FWHM) in Divergent Wave Linear Array (DWLA) increased by 7.5% at 20 mm\nand 9% at 30 mm compared to Synthetic Aperture Linear Array (SALA). For\nDivergent Wave Convex Array (DWCA), the increase was 1.64% at 20 mm and 26.56%\nat 30 mm compared to Synthetic Aperture Convex Array (SACA), witnessing the\nmethod's effectiveness.",
        "The average Relative Biological effectiveness (RBE) factors for neutron\nirradiation in the context of a BNCT treatment are studied. This research\nconsiders the various interactions and secondary particles of each process and\nestimates the RBE based on the damage induced in tissues by all of these\nparticles. A novel concept of estimating the biological dose by means of\nweighted kerma factors is introduced. These weighted kerma factors include the\nRBE of each energy deposition based on an RBE-LET relationship for secondary\ncharged particles and can be directly incorporated in weighted dose\ncalculations from Monte Carlo simulations. Furthermore, the dependence of the\nneutron weighting factor on neutron energy for standard soft tissue is\ndiscussed.",
        "In this study, we present the Physical-Bio Translator, an agent-based\nsimulation model designed to simulate cellular responses following irradiation.\nThis simulation framework is based on a novel cell-state transition model that\naccurately reflects the characteristics of irradiated cells. To validate the\nPhysical-Bio Translator, we performed simulations of cell phase evolution, cell\nphenotype evolution, and cell survival. The results indicate that the\nPhysical-Bio Translator effectively replicates experimental cell irradiation\noutcomes, suggesting that digital cell irradiation experiments can be conducted\nvia computer simulation, offering a more sophisticated model for radiation\nbiology. This work lays the foundation for developing a robust and versatile\ndigital twin at multicellular or tissue scales, aiming to comprehensively study\nand predict patient responses to radiation therapy.",
        "This study introduces an advanced multiphysics and multiscale modeling\napproach to investigate intestinal motility. We propose a generalized\nelectromechanical framework that incorporates contact mechanics, enabling the\ndevelopment of a unique and innovative model for intestinal motility. The\ntheoretical framework includes an electromechanical model coupling a\nmicrostructural material model, which describes the intestinal structure, with\nan electrophysiological model that captures the propagation of slow waves.\nAdditionally, it integrates a self-contact detection algorithm based on a\nnearest-neighbour search and the penalty method, along with boundary conditions\nthat account for the influence of surrounding organs. A staggered finite\nelement scheme implemented in FEniCS is employed to solve the governing\nequations using the finite element method. The model is applied to study cases\nof moderate and severe strangulation hernia, as well as intestinal adhesion\nsyndrome. The results demonstrate that low peristalsis takes place in the\npre-strangulation zone. At the same time, very high pressure is recorded in the\nstrangulation zone, and peristaltic contractions persisted in the healthy\nregion. For adhesions, the results indicate a complete absence of peristalsis\nin the adherent region. The model successfully reproduces both qualitatively\nand quantitatively propagative contractions in complex scenarios, such as pre-\nand post-surgical conditions, thereby highlighting its potential to provide\nvaluable insights for clinical applications.",
        "Background: Tumour budding is an independent predictor of metastasis and\nprognosis in colorectal cancer and is a vital part of the pathology\nspecification report. In a conventional pathological section observation\nprocess, pathologists have to repeatedly switch from 10x objective to 20x\nobjective several times to localize and image the target region. Besides the\nswitching operations, repeated manual or electro-mechanical focusing is also\nvery time-consuming, affecting the total time for pathological diagnosis. In\naddition, It is usually necessary to remove the manually marked symbols on the\nstained pathology slides used for classification and management before\nobservation. Methods: In this paper, we utilize Fourier ptychographic\nmicroscopy (FPM) in the pathological diagnosis process to realize large\nspace-bandwidth product imaging, quantitative phase imaging, and digital\nrefocusing in the observation process without any mechanical operations, which\ncan therefore simplify the above-mentioned cumbersome diagnostic processes. We\nfirst verify the effectiveness and efficiency of the proposed method with\nseveral typical pathological sections. Then, instead of manually erasing, we\nalso prove that FP framework can digitally remove the artificial markers with\nits digital refocusing ability. Results: At last, we demonstrated pathologists\ncan achieve 100% diagnostic accuracy with FPM imaging results. Conclusions: The\nproposed method can greatly simplify the process of pathological diagnosis, and\nthe related addon hardware system does not require expensive components, which\nmakes it have great potential for promotion in the field of pathological\ndiagnosis.",
        "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.86(1.51) mm,\n10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,\nand 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.",
        "Uncertainty quantification for large-scale inverse problems remains a\nchallenging task. For linear inverse problems with additive Gaussian noise and\nGaussian priors, the posterior is Gaussian but sampling can be challenging,\nespecially for problems with a very large number of unknown parameters (e.g.,\ndynamic inverse problems) and for problems where computation of the square root\nand inverse of the prior covariance matrix are not feasible. Moreover, for\nhierarchical problems where several hyperparameters that define the prior and\nthe noise model must be estimated from the data, the posterior distribution may\nno longer be Gaussian, even if the forward operator is linear. Performing\nlarge-scale uncertainty quantification for these hierarchical settings requires\nnew computational techniques. In this work, we consider a hierarchical Bayesian\nframework where both the noise and prior variance are modeled as\nhyperparameters. Our approach uses Metropolis-Hastings independence sampling\nwithin Gibbs where the proposal distribution is based on generalized\nGolub-Kahan based methods. We consider two proposal samplers, one that uses a\nlow rank approximation to the conditional covariance matrix and another that\nuses a preconditioned Lanczos method. Numerical examples from seismic imaging,\ndynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate\nthe effectiveness of the described approaches.",
        "This paper studies an optimal insurance problem for a utility-maximizing\nbuyer of insurance, subject to the seller's endogenous default and background\nrisk. An endogenous default occurs when the buyer's contractual indemnity\nexceeds the seller's available reserve, which is random due to the background\nrisk. We obtain an analytical solution to the optimal contract for two types of\ncontracts, differentiated by whether their indemnity functions depend on the\nseller's background risk. The results shed light on the joint effect of the\nseller's default and background risk on the buyer's insurance demand.",
        "A key requirement of any separable quantum state is that its density matrix\nhas a positive partial transpose. For continuous bipartite quantum states,\nviolation of this condition may be tested via the hierarchy of\nnegative-partial-transpose (NPT) based entanglement criteria introduced by\nShchukin and Vogel [Phys. Rev. Lett. 95, 230502 (2005)]. However, a procedure\nfor selecting the optimal NPT-based criterion is currently lacking. Here, we\ndevelop a framework to select the optimal criterion by determining the level of\nconfidence of criteria within the Shchukin and Vogel hierarchy for finite\nmeasurement number, environmental noise, and the optimal allocation of\nmeasurement resources. To demonstrate the utility of our approach, we apply our\nstatistical framework to prominent example Gaussian and non-Gaussian states,\nincluding the two-mode squeezed vacuum state, the quanta-subtracted two-mode\nsqueezed vacuum state, and the two-mode Schr\\\"odinger-cat state. Beyond\nbipartite inseparability tests, our framework can be applied to any Hermitian\nmatrix constructed of observable moments and thus can be utilized for a wide\nvariety of other nonclassicality criteria and multi-mode entanglement tests.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "We solve the global asymptotic stability problem of an unstable\nreaction-diffusion Partial Differential Equation (PDE) subject to input delay\nand state quantization developing a switched predictor-feedback law. To deal\nwith the input delay, we reformulate the problem as an actuated transport PDE\ncoupled with the original reaction-diffusion PDE. Then, we design a quantized\npredictor-based feedback mechanism that employs a dynamic switching strategy to\nadjust the quantization range and error over time. The stability of the\nclosed-loop system is proven properly combining backstepping with a small-gain\napproach and input-to-state stability techniques, for deriving estimates on\nsolutions, despite the quantization effect and the system's instability. We\nalso extend this result to the input quantization case.",
        "Let $X_n$ be the space of unimodular lattices in $\\RR^n$ and let $A$ be the\nfull diagonal group in $\\on{SL}_n(\\RR)$. It is known that compact $A$-orbits\noriginate from moduls in totally real degree $n$ number fields. Our first\nresult shows that for a natural family of compact orbits $(Ax_k)_k$ all\noriginating from a fixed number field $K$, every weak limit of the Haar\nmeasures on those orbits $m_{Ax_k}$ must contain the Haar measure $m_{X_n}$ as\nan ergodic component. This result generalizes certain aspects of the work by\nAka and Shapira in \\cite{Shapira-Aka} to arbitrary dimensions, as well as\nelements from Shapira-Zheng in \\cite{shapira2021translates}.\n  For every vector $\\overline \\alpha\\in \\RR^n$ and for every rational\napproximation $(\\overline p,q)\\in \\RR^n\\times\\RR$ we can associate the\ndisplacement vector $q\\alpha-\\overline p$. We focus on algebraic vectors,\nnamely $\\overline \\alpha=(\\alpha_1,\\dots,\\alpha_n)$ such that $1, \\alpha_1,\n\\dots, \\alpha_n$ span a rank $n$ number field. For these vectors, we\ninvestigate the size of their displacements as well as the distribution of\ntheir directions. We establish that algebraic vector $\\overline \\alpha$ satisfy\nthe $p$-adic Littlewood Conjecture. Namely, we prove that \\begin{equation}\n  \\liminf_{k \\to \\infty} \\left( k \\|k\\|_p \\right)^{1\/n} \\| k (\\alpha_1, \\dots,\n\\alpha_n) \\|_\\infty = 0. \\end{equation} Additionally, we classify all limiting\ndistributions, with a special weighting, of the sequence of directions of the\ndefects in the $\\varepsilon$-approximations of $(\\alpha_1, \\dots, \\alpha_n)$.\nEach such limiting measure is expressed as the pushforward of an algebraic\nmeasure on $X_n$ to the sphere.\n  Our proof relies on estimates of the asymptotic orders of units in fixed\nnumber fields modulo families of natural numbers and on rigidity results from\n\\cite{ELMV1}.",
        "Derivative-Free Optimization (DFO) involves methods that rely solely on\nevaluations of the objective function. One of the earliest strategies for\ndesigning DFO methods is to adapt first-order methods by replacing gradients\nwith finite-difference approximations. The execution of such methods generates\na rich dataset about the objective function, including iterate points, function\nvalues, approximate gradients, and successful step sizes. In this work, we\npropose a simple auxiliary procedure to leverage this dataset and enhance the\nperformance of finite-difference-based DFO methods. Specifically, our procedure\ntrains a surrogate model using the available data and applies the gradient\nmethod with Armijo line search to the surrogate until it fails to ensure\nsufficient decrease in the true objective function, in which case we revert to\nthe original algorithm and improve our surrogate based on the new available\ninformation. As a proof of concept, we integrate this procedure with the\nderivative-free method proposed in (Optim. Lett. 18: 195--213, 2024). Numerical\nresults demonstrate significant performance improvements, particularly when the\napproximate gradients are also used to train the surrogates.",
        "New expressions and bounds for Catalan's and Apery's constants, derived from\nthe half hyperbolic secant distribution, are presented. These constants are\nobtained by using expressions for the Lorenz curve, the Gini and Theil indices,\nconvolutions and a mixture of distributions, among other approaches. The new\nexpressions are presented both in terms of integral (simple and double)\nrepresentation and also as an interesting series representation. Some of these\nfeatures are well known, while others are new. In addition, some integral\nrepresentations of Euler's numbers are obtained.",
        "Galaxy groups with $M_{tot} \\lesssim 10^{14}$ $M_\\odot$ and up to a few tens\nof members are the most common galaxy environment, marking the transition\nbetween field and massive clusters. Identifying groups plays a crucial role in\nunderstanding structure formation and galaxy evolution. Modern deep surveys\nallow us to build well-characterized samples of groups up to the regime where\nstructures were taking shape. We aimed to build the largest deep catalog of\ngalaxy groups to date over the COSMOS-Web field effective area of 0.45 deg$^2$,\nleveraging the deep high quality data of the new COSMOS-Web photometric catalog\nresulted from the James Webb Space Telescope observations of the COSMOS-Web\nfield. We performed the group search with the AMICO algorithm, a linear matched\nfilter based on an analytical model for the group signal. AMICO has already\nbeen tested in wide and deep field surveys, including COSMOS data up to $z=2$.\nIn this work, we tested the algorithm performances at even higher redshift and\nsearched for protocluster cores at $z>2$. We compiled a list of known\nprotoclusters in COSMOS at $2 \\leq z \\leq 3.7$, matched them with our\ndetections and studied the clustering of the detected cores. We estimated\npurity and completeness of our sample by creating data-driven mocks with the\nSinFoniA code and linked signal-to-noise to purity. We detected 1678 groups in\nthe COSMOS-Web field up to $z=3.7$, including lists of members extending nearly\ntwo magnitudes deeper than the previous AMICO-COSMOS catalog. 756 groups were\ndetected with purity of 80\\%. More than 500 groups have their redshift\nconfirmed by assigning spectroscopic counterparts. This group catalog offers a\nunique opportunity to explore galaxy evolution in different environments\nspanning $\\sim$12 Gyr and to study groups, from the least rich population to\nthe formation of the most massive clusters.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
        "Let $P\\in \\mathbb Z[X]\\setminus\\{0\\}$ be of degree $\\delta\\ge 1$ and usual\nheight $H\\ge 1$, and let $\\alpha\\in \\overline{\\mathbb Q}^*$ be of degree $d\\ge\n2$. Mahler proved in 1931 the following transcendence measure for $e^\\alpha$:\nfor any $\\varepsilon\\&gt;0$, there exists $c\\&gt;0$ such that $\\vert\nP(e^\\alpha)\\vert\\&gt;c\/H^{\\mu(d,\\delta)+\\varepsilon}$ where the exponent\n$\\mu(d,\\delta)=(4d^2-2d)\\delta+2d-1$. Zheng obtained a better result in 1991\nwith $\\mu(d,\\delta)=(4d^2-2d)\\delta-1$. In this paper, we provide a new\nexplicit exponent $\\mu(d,\\delta)$ which improves on Zheng's transcendence\nmeasure for all $\\delta\\ge 2$ and all $d\\ge 2$. When $\\delta=1$, we recover his\nbound for all $d\\ge 2$, which had in fact already been obtained by Kappe in\n1966. Our improvement rests upon the optimization of an accessory parameter in\nSiegel's classical determinant method applied to Hermite-Pad{\\'e} approximants\nto powers of the exponential function.",
        "Functional It^o calculus is based on an extension of the classical It^o\ncalculus to functionals depending on the entire past evolution of the\nunderlying paths and not only on its current value. The calculus builds on\nFollmer's deterministic proof of the It^o formula, see [3], and a notion of\npathwise functional derivatives introduced by [5]. There are no smoothness\nassumptions required on the functionals, however, they are required to possess\ncertain directional derivatives which may be computed pathwise, see [6, 9, 8].\nUsing functional It^o calculus and the notion of quadratic variation, we derive\nthe functional It^o formula along with the Feynman-Kac formula for functional\nprocesses. Furthermore, we express the Greeks for path-dependent options as\nexpectations, which can be efficiently computed numerically using Monte Carlo\nsimulations. We illustrate these results by applying the formulae to digital\noptions within the Black-Scholes model framework.",
        "We investigate the possibility of using a Rabi drive to tune the interactions\nin an atomic Fermi gas. Specifically, we consider the scenario where two\nfermion species (spins) are Rabi coupled and interacting with a third uncoupled\nspecies. Using an exact calculation within a minimal low-energy model, we\nderive analytical expressions for the effective scattering length and effective\nrange that characterize the collisions between a Rabi-dressed atom and an atom\nfrom the third species. In particular, we find that new scattering resonances\nemerge in the Rabi-coupled system, which we demonstrate are linked to the\nexistence of hybrid two-body bound states. Furthermore, we show via a\ngeneralized Thouless criterion that the scattering properties have a direct\nimpact on the superfluid transitions in the Rabi-coupled Fermi gas. The\npresence of Rabi-induced resonances thus has implications for the investigation\nof many-body physics with driven atomic gases.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks."
      ]
    }
  }
]